Key,URL,Summary,Description,Created,All_Comments,Labels,Assignee,Status,Creator
SEEN-126,https://miggbo.atlassian.net/browse/SEEN-126,[Auton]Test_TC24_DNAC_Device_Inventory_verifications_configure_roles_on_devices - test1_verify_device_role_correctly_showing_up,"+Testcase+

+[Test_TC24_DNAC_Device_Inventory_verifications_configure_roles_on_devices - test1_verify_device_role_correctly_showing_up|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11117984&size=290526&archive=env_auto_job.2022Apr23_00:30:20.292146.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS]+

 

+Error+
{code}
43923:  DEVINFO:: ['TB7-SJ-eCA-BORDER-CP', 'TB7-Transit', 'TB7-NY-FIAB', 'TB7-SJ-EDGE', 'TB7-eWLC']
43924: 
43925:  INFO Obtained:: [{'name': 'TB7-SJ-eCA-BORDER-CP', 'role': 'DISTRIBUTION'}, {'name': 'TB7-Transit', 'role': 'DISTRIBUTION'}, {'name': 'TB7-NY-FIAB', 'role': 'ACCESS'}, {'name': 'TB7-SJ-EDGE', 'role': 'ACCESS'}, {'name': 'TB7-eWLC', 'role': 'ACCESS'}]
43926: 
43927:  !!!Device{'name': 'TB7-SJ-eCA-BORDER-CP', 'role': 'BORDER ROUTER'}:  attribute  and value is not as expected!!!
43928: 
43929:  !!!Device{'name': 'TB7-Transit', 'role': 'BORDER ROUTER'}:  attribute  and value is not as expected!!!

{code}
 ",2022-04-27T02:25:25.552+0000,,"['Auton', 'Groot', 'Guardian', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-127,https://miggbo.atlassian.net/browse/SEEN-127,[Auton]Test_TC29_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision - test2_update_vlan_membership,"Testcase:

+[Test_TC29_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision - test2_update_vlan_membership|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12590956&size=757002&archive=env_auto_job.2022Apr23_00:30:20.292146.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS]+

 

+Error:+

+52885:  Errored reason: 500 Server Error:  for url: [https://10.30.0.100/api/v1/interface/071f257c-c56b-4334-8068-8573fb65b4ac?deploymentMode=Deploy]+",2022-04-27T02:34:08.408+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2756/diff#services/dnaserv/lib/api_groups/inventory/group.py we are tracking it as part of SEEN-129 TC is  passed, hence moving  to close state
Guradian Pass Log:
https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep18_16:11:21.732010.zip&atstype=ATS","['Auton', 'Guardian', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-128,https://miggbo.atlassian.net/browse/SEEN-128,[Auton]Test_TC81_verify_nw_device_global_assurance_metrics  /   test1_verify_nw_device_global_assurance_metrics,"Testcase:

+[Test_TC81_verify_nw_device_global_assurance_metrics  /   test1_verify_nw_device_global_assurance_metrics|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=41822662&size=115247&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-04%2Fenv_auto_job.2022Apr23_12:04:35.059145.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]+

 

Issue:

148339:  \{'isMetricApplicable': True, 'request': {'entity': {'_id': '__global__', 'managementIpAddr': None, 'macAddress': None, 'uuid': None, 'uuidFieldName': 'uuid'}, 'name': 'globalKpiAggregation', 'dimensions': [{'name': 'category', *'value': 'routers'*}, \{'name': 'metricName', 'value': 'cpuScore'}], 'window': '15 min', 'timeRange': \{'start': '2022-04-24T01:58:50.503Z', 'end': '2022-04-24T02:13:50.631Z', 'current': None}, 'field': None, 'type': None, 'fields': ['metricGoodCount', 'metricFairCount', 'metricBadCount']}, 'pagination': \{'page': 1, 'pageSize': 100, 'order': 'DESC', 'totalResults': None}}
 148340:  No records in response, no assurance data received",2022-04-27T02:35:34.608+0000,"Closing this issue for now, we will re-open if seen agian.","['Auton', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-129,https://miggbo.atlassian.net/browse/SEEN-129,[Auton]Test_TC97_wireless_policy_PSK  /   test4_deploy_wireless_policy_PSK,"Testcase:

+[Test_TC97_wireless_policy_PSK  /   test4_deploy_wireless_policy_PSK|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=46075343&size=962264&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-04%2Fenv_auto_job.2022Apr23_12:04:35.059145.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]+ 

 

Issue:
|161853:  Resource path full url: [https://10.30.0.100/api/v1/task/787670d9-9524-4bef-bf73-d64b885c353c]
 161855:  \{'version': 1650769877829, 'progress': 'TASK_INTENT', 'data': 'workflow_id=04a28a7f-d798-449f-8d3e-7e345747570a;scratchpad_id=9c972d13-e250-417e-bc70-80e8dea191a5;rollback_status=not_supported;rollback_taskid=0;failure_task=Determination of network intent deployment status:97786560-7d31-416d-be94-d808e39695d9;processcfs_complete=true', 'endTime': 1650769878329, 'startTime': 1650769874321, 'errorCode': 'NCSP11000', 'serviceType': 'NCSP', 'username': '', 'rootId': '787670d9-9524-4bef-bf73-d64b885c353c', 'failureReason': 'Invalid Message Code NCSP11000', 'lastUpdate': 1650769877829, 'isError': True, 'instanceTenantId': '626395ffb08833170be9e4c1', 'id': '787670d9-9524-4bef-bf73-d64b885c353c'}
 161856:  Library group ""task"" method ""wait_for_task_complete"" returned in 0:00:04.156853
 161857:  Issue in response of the policy:Invalid Message Code NCSP11000|",2022-04-27T02:36:25.792+0000,Rakesh PR Resolved [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2734/overview],"['Auton', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-130,https://miggbo.atlassian.net/browse/SEEN-130,[Auton]Test_TC97_wireless_policy_PSK  /   test7_delete_existing_wireless_policy,"Testcase:

+[Test_TC97_wireless_policy_PSK  /   test7_delete_existing_wireless_policy|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=46075343&size=962264&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-04%2Fenv_auto_job.2022Apr23_12:04:35.059145.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]+

 

Issue:

162025:  Policy undeployment failed:NCSP01001: Error occurred while processing the 'provision' request. Additional info for support: taskId: 'a762a51b-d795-433b-b60f-6175c4d02f58'. Empty 'cfs create, update and delete lists' provided in the request.",2022-04-27T02:36:58.587+0000,"Rakesh PR Resolved [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2734/overview] Issue is still seen with Guardian latest code (Pulled on 05/19)

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17620796&size=13268&archive=env_auto_job.2022May20_03:24:56.792746.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  Hi Nethra, 

Thanks for reporting the issue. Here is the PR for below error 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2880/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2880/overview*+]

+*Error:*+

 

{color:#ffab00}5057: File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/DMZ-Guardian-sanity-common-Multi-job/services/dnaserv/lib/api_groups/app_policy/group.py"", line 423, in fileID_dev{color}
 {color:#ffab00}75058: fileid = x[""fileId""]{color}
 {color:#ffab00}75059: KeyError: 'fileId'{color}

Why we are getting the below error is, from the script we doing the wireless policy deployment for ECA and EWLC devices. For ECA device wireless policy deployment is successfully but for EWLC we are unable to fetch the fileid and deployment is failing. Due to this we are seeing the below error while deleting wireless policy. I have skipped the EWLC device and ran the script its working without any failures you can find the below passed log file.

Even in my setup also deployment is failing for ewlc as unable to fetch the fileid I havd filed bug for this.

+*CSCwb80128:Guardian: Wireless policy deployment is failing with Invalid Message Code NCSP11000*+

I had debug session with DE and noticed that while creating a wireless policy and when we select site scope EWLC device is not listing out please find the screenshot for your ref.  Please check in your setup if this device is listing out or not. Based on that we can re-open the same bug and work with DE.

 

In my setup looks like Andrew was doing some testing with vewlc so I have checked this in Groot it was listing out and it was passing. You can more details and log in https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-131

  !policy.png!

 

+*Error:*+

162025:  Policy undeployment failed:NCSP01001: Error occurred while processing the 'provision' request. Additional info for support: taskId: 'a762a51b-d795-433b-b60f-6175c4d02f58'. Empty 'cfs create, update and delete lists' provided in the request.

+*Failed log:*+

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=592847&size=353713&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F05%2F24%2F02%2F45%2Fsanity_TB1.2022May24_02:45:43.445539.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=rakdomma&from=trade&view=all&atstype=pyATS]

+*Passed log [file:*+|file://%2A+/]

[https://earms-trade.cisco.com/tradeui/logs/details?ats=%2Fusers%2Fpawansi%2Fpyatsnew&client=web&host=st-ucs-sanity.cisco.com&archive=%2Fusers%2Fpawansi%2Fpyatsnew%2Fusers%2Frakdomma%2Farchive%2F22-05%2Fsanity_TB1.2022May24_02:37:49.687239.zip]

  Please commit the fix in Ghost branch as well 

Currently running Ghost ISO 2.1.610.70338 - I am seeing the similar error for the same TC 

Please find the failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=48991205&size=1039534&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug16_13:03:43.653275.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

  Production test is failing, so it's sev2

  in latest ghost its passing got log from [~5e1415780242870e996f0b2f]

 

[TRADe v2 | Logs: env_optimized_auto_job (cisco.com)|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep25_01:37:05.802026.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Ghost', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-131,https://miggbo.atlassian.net/browse/SEEN-131,[Auton]Test_TC98_tagging_application,"Testcase:

 +[Test_TC98_tagging_application|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=47037607&size=429945&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-04%2Fenv_auto_job.2022Apr23_12:04:35.059145.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]+ 

 

Issue:

Same error as TC97.1 and TC97.4",2022-04-27T02:37:32.700+0000,"I have tested this in sanity tb1, below are my observations:



deployment is passing for border where in its failing for vewlc controller. When i go and check in UI i dont see vewlc device is showing up eventho we have selected site scope as global. But we are doing policy deployment for this device. suspecting its failing due to this in my setup. I see in multisite setup there I could see vewlc also showing up. Currently we are deploying the sanity tb1 with groot let me check in that.

!policy.png! deployed sanity tb1 with Groot and ran all the initial test cases. I could see that vewlc also showing in policy preview configurations page when we select site scope as Global and Test_TC98_tagging_application (ran using guardian branch code) also passing without any issue.

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=514430&size=780932&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F05%2F13%2F10%2F36%2Fsanity_TB1.2022May13_10:36:19.404026.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=rakdomma&from=trade&view=all&atstype=pyATS]

 

 

 

  Please commit the fix in Ghost branch as well 

Currently running Ghost ISO 2.1.610.70338 - I am seeing the similar error for the same TC 

Please find the failed log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=50199714&size=208409&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug16_13:03:43.653275.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  this is test case also passed in latest ghost. Confirmed with [~5e1415780242870e996f0b2f]

 

[TRADe v2 | Logs: env_optimized_auto_job (cisco.com)|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep25_01:37:05.802026.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Ghost', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-132,https://miggbo.atlassian.net/browse/SEEN-132,[Auton]Test_TC123_aaa_per_ssid  /   test3_onboard_wireless_segment_for_new_ssid,"Testcase:

+[Test_TC123_aaa_per_ssid  /   test3_onboard_wireless_segment_for_new_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=71888702&size=207845&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-04%2Fenv_auto_job.2022Apr23_12:04:35.059145.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]+

 

Issue:

214586:  Requested SSID test_AAAtb7 ID is not found in :\{'response': [], 'version': '1.0'} with name space 339919d4-2dd3-42f7-80ce-71ad3ab5a84b and in ['814620bb-00ea-4c78-8320-823fb8adc11c', '55c509e3-77c0-45e3-ba3c-b0febcb40ccc', 'e12d3236-951f-43df-a995-361f70288d4c', '4469d66e-1a74-4af3-8a9e-372e5fcee8a8', '15b1a2d9-eb85-4471-ba86-821e67df06fa', '068a0916-8832-4a51-bc1f-7cf142bce004', 'f28fc0ec-c2f3-4f7b-90f3-dc20259f9410', '350a0179-72d6-459b-9f4e-07a04eecb63b', '16974812-8398-4868-b1fb-528e1581dad4', '46fcc114-0d0d-4753-b62a-abf1acb6e18b']",2022-04-27T02:37:59.641+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2756/diff#services/dnaserv/lib/api_groups/inventory/group.py **ISO*:*2.1.390.72158
**Script*:*solution_test_sanityecamb.py

**Impacted Testcases:**Test_TC123_aaa_per_ssid/test3_onboard_wireless_segment_for_new_ssid

 

**NOTE:** Double commit the same for shockwave
**Failure Log:**
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=74318751&size=182781&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F26%2F23%2F54%2Fenv_auto_job.2022Jul26_23:54:09.596172.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS

]**Snip from failure log:**
**

230062: Requested SSID test_AAAtb5 is not present in 

230065: Failed to onboard wireless segment at dev TB5-DM-eCA-BORDER

 
*[|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=74318751&size=182781&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F26%2F23%2F54%2Fenv_auto_job.2022Jul26_23:54:09.596172.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]* **ISO*:* Groot RC4 2.1.560.70508
**Script*:*job/sanity_tb3/sanity_TB3_cert.py ( Optimized job: TB6 )

**Impacted Testcases:**[Task-cmx_config_push_and_device_validations.py-156-cmxConfigsAndValidations|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-cmx_config_push_and_device_validations.py-156-cmxConfigsAndValidations&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug09_22:32:57.908150.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&atstype=PYATS]  /   [Test_TC3_aaa_per_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-cmx_config_push_and_device_validations.py-156-cmxConfigsAndValidations&begin=1605565&size=1285455&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug09_22:32:57.908150.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&atstype=PYATS]  /   test3_onboard_wireless_segment_for_new_ssid 

**NOTE:** Double commit the same for groot


**Failure Log:**
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-cmx_config_push_and_device_validations.py-156-cmxConfigsAndValidations&begin=2703905&size=102588&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug09_22:32:57.908150.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS] 

**Snip from failure log:**
**

Requested SSID test_AAAtb6 ID is not found in :\{'response': [], 'version': '1.0'} with name space 6d03ed7e-9a9f-4aa0-bb96-56f2f8078426 and in ['59a8a8db-4283-4459-8012-b4b716dea71a', 'a9b12426-37b6-42f6-8a92-0040309988f4', '68d2fc85-845e-48de-a521-ba403df7c669', '6e2c0a7f-c78f-4df8-9ec1-5495c9a65e13', '63467282-d1f5-409e-9cc2-0fb47375a54b', '840e0fcd-4d2a-4517-b523-f31fd354875e', 'ff643a5d-f1f7-450a-a009-74d9a60b8454', '1a0bab7a-a387-442c-8c7a-dc5e3944e42d', '35c5d7f0-d142-465e-abb6-840f488d5856', 'ec930d3b-d24b-4bb2-938b-0d91328c2935', '1936f8c9-932f-4560-a46e-a448ac6aca02', '84f1851c-fb68-4b1a-b43f-0e2f642b9d75', '42730e93-30e7-4933-8929-300e093e7f26', '7c9b7d4e-1140-4cfe-a40d-89c67ad851b7', 'dbc52292-9c8a-40da-9197-c942ef57b0e4']

Failed to onboard wireless segment at dev TB6-DM-eCA-BORDER

  [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c74c41ba2d60134261988a260a428249ee16c7c5#services/dnaserv/lib/api_groups/network_profile/group.py]

 

Groot issue is not related to the other issues, simple typo fixed here.

  Hi Andrew,

    Same issue we are still seeing on Shockwave P3 RC3.

Branch Used : private/Shockwave-ms/sanity_api_auto

I have checked the typo which you mentioned in this branch, it is not there.

Checked the shockwave main branch also(private/Shockwave-ms/api-auto), it is not there.

 

*Fail Log* : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=70557513&size=182730&archive=env_auto_job.2022Sep14_07:51:47.661486.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]  Latest comment is for error SEEN-624, fixed here https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/8aa228f28eeaebc7b541af4b3787ed80b567db11#services/dnaserv/lib/api_groups/network_profile/group.py Cannot reproduce, please share cluster when issue comes again, or raise a product bug. Ssid should be present, since its added successfully in subtest1. Can manually check when issue comes if ssid is still there or not.","['Auton', 'Groot', 'Guardian', 'Issue', 'shockwave']",Andrew Chen,Resolved,Avril Bower
SEEN-133,https://miggbo.atlassian.net/browse/SEEN-133,[Auton]Test_TC135_security_advsiory  /   test2_verifies_security_advsiories,"Testcase:

+[Test_TC135_security_advsiory  /   test2_verifies_security_advsiories|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=76355670&size=902150&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-04%2Fenv_auto_job.2022Apr23_12:04:35.059145.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]+

 

Issue:

 227124:  cisco-sa-ssh-dos-Un22sd2A Advisory is as not Expected.
227125:  Verification Failed!!! [{'cisco-sa-ssh-dos-Un22sd2A': {'expected': ['SN-FOC2311Y129.cisco.com'], 'GOT': []}}]",2022-04-27T02:38:28.703+0000,"PR with added retries merged into main branch: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2772/overview Testcase passing in latest run: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/Users/enewcome/pyats/users/enewcome/archive/22-04/sanity_TB2.2022Apr29_21:53:25.368943.zip&atstype=ATS]

PR Merged with added retries, to address timing problem that could be causing intermittent issue: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2772/overview Issue is still seen with Groot 2.1.560.70297

Tc is still checking for cisco-sa-ssh-dos-Un22sd2A issue on the Extended node.

Our current Extended node is with 15.2(7.2.13)E image, which is not supported for Security advisory. In UI, we can see below warning message,

""The device is running an image version or software that is not supported for identifying security vulnerabilities""

 

Attached screenshot.

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=96165251&size=10579708&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

Error snip from log :
352247:  Resource path full url: [https://10.195.227.48/api/v1/file/b6aeeadf-643f-4f78-ae37-d98028e87272]
352250:  [\{'deviceUuid': 'da567869-c15e-4f12-b9e2-9869ebbaeb38', 'commandResponses': {'SUCCESS': {'show ip ssh': 'show ip ssh\nSSH Enabled - version 2.0\nAuthentication methods:publickey,keyboard-interactive,password\nAuthentication Publickey Algorithms:x509v3-ssh-rsa,ssh-rsa\nHostkey Algorithms:x509v3-ssh-rsa,ssh-rsa\nEncryption Algorithms:aes128-ctr,aes192-ctr,aes256-ctr\nMAC Algorithms:hmac-sha2-256,hmac-sha2-512,hmac-sha1,hmac-sha1-96\nKEX Algorithms:diffie-hellman-group-exchange-sha1,diffie-hellman-group14-sha1\nAuthentication timeout: 120 secs; Authentication retries: 3\nMinimum expected Diffie Hellman key size : 2048 bits\nIOS Keys in SECSH format(ssh-rsa, base64 encoded): dnac-sda\nModulus Size : 2048 bits\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7UoukHeRBtGboOwFexuWORG5u80pUuj7UkaExstkl\nMsxDrfM+Ayi9Qv2k3laG5BWzjVKXvNlKQXHX2AMiIIHP7BrsZPI/ZTfw5CQJ2hhjW4MsZI+oMC5q+CL6\n9aYUFh+DOod97vLlvWOXJKh+MDwPjH6TESuVaU4KNxItMRSSvTSn3n9QgiNAnqw80E+kXx9Y+AU4AdoT\n9ClQ7cs5Llge8vsPLAbhWvRlF9xA1VAhbqpq2mbqetwIXnxW7fqyV57if7RJTGQiX8fx+w1hKfNzG4NB\niDbQ3NrnQWVswA6huWP3jdquen3hh/WOI9NTwJTxQ8WpkS3WqLeUdJZ34CBb \nSN-FOC2327V1GH#'}, 'FAILURE': {}, 'BLACKLISTED': {}}}]
352251:  Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:02.283092
352252:  cisco-sa-ssh-dos-Un22sd2A Issue Found on the Device SN-FOC2327V1GH.cisco.com
352253:  ************************************************************
352254:  Following Devices \{'cisco-sa-telnetd-EFJrEzPx': [], 'cisco-sa-ssh-dos-Un22sd2A': ['SN-FOC2327V1GH.cisco.com']} having Security Advisories
352255:  ************************************************************
352256:  cisco-sa-telnetd-EFJrEzPx Advisory is as Expected. Alerts on following devices []
352257:  cisco-sa-ssh-dos-Un22sd2A Advisory is as not Expected.
352258:  Verification Failed!!! [\{'cisco-sa-ssh-dos-Un22sd2A': {'expected': ['SN-FOC2327V1GH.cisco.com'], 'GOT': []}}]
352259:  Test returned in 0:14:07.717127
352260:  Failed reason: Verification Failed for Security
352261:  The result of section test2_verifies_security_advsiories is => FAILED
  Hi [~603e0198678612006b9f8e30] ,

The extended node does not support scanning for security advisories, so it should be excluded during the security advisory check in TC 135.2. Is this correct? Hi [~557058:fbb9c502-e858-4224-932a-86d595cefda2],

Yes that's right, this particular Extended node image - 15.2(7.2.13)E does not support security advisory, so it has to be excluded during validation. Opened a PR with a device version check here: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2909/overview]

  PR merged to Groot.

Eric, can you PR to Guardian too? [~62d2fe9f8afb5805e5d5af49] sure, will do. [~62d2fe9f8afb5805e5d5af49] Added a PR to Guardian here: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3006/overview","['Auton', 'Groot', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-134,https://miggbo.atlassian.net/browse/SEEN-134,[Auton]Test_TC142_Add_interace_description  /   test1_add_interface_description,"Testcase:

[Test_TC142_Add_interace_description|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=83435917&size=985202&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-03%2Fenv_auto_job.2022Mar29_03:29:36.559586.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]{color:#000000}  /   test1_add_interface_description{color}

solution_test_sanityecamb_lan.py

 

Issue:
 Test failed in TB7 where the device role is access
  

Branch: private/Guardian-ms/sanity_api_auto

Logs:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=83436505&size=984444&archive=env_auto_job.2022Mar29_03:29:36.559586.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS]

Source: Sanity

Date: 01 Apr 2022

PoC: Nethral

Notes:

TC142 enhancement need to be added, Shared the details with Rakesh for the enhacments 

Detailed info sent in the email:

Check the issue in device-360 page and Global issue page:  if the device role are 'distribution, core, Map-Server, Border' 

Check the issue only in device-360 page: if the device role is access.

 ",2022-04-27T02:39:11.060+0000,"Filed a bug CSCwb79577 for this feature. Blocked with this. Requested regression team for setup to test the enhancement. DE shared this [https://cdetsng.cisco.com/summary/#/defect/CSCwa35510] for this issue.

We have to use ndp-base-analytics 1.8.256 or newer. Otherwise 1.9.50 or newer

I am checking with DE on exact version where fix is available in Groot functionality has been changed Issue will create only for network devices like ap, switch , routers and these devices must be there in inventory. Tested manually need to be add code in Groot branch.

 

In Guardina bug has been raised waiting for fix that to we are not having guardin in any cluster HI Raju, Lets go with option#1 given this is a new test while the team changes the severity to S6 and picks it up after the ghost IW3 window.

 

Thanks,

Lakshmi

 

 

*From:* Raju Saran -X (rajsaran - TERRALOGIC SOLUTIONS INC at Cisco) <[rajsaran@cisco.com|mailto:rajsaran@cisco.com]>
*Date:* Thursday, June 16, 2022 at 9:46 PM
*To:* Raghuveer Vemuri (ragvemur) <[ragvemur@cisco.com|mailto:ragvemur@cisco.com]>, Honey Maharana (hmaharan) <[hmaharan@cisco.com|mailto:hmaharan@cisco.com]>
*Cc:* Nethra Thorali Suthagar -X (nethoral - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[nethoral@cisco.com|mailto:nethoral@cisco.com]>, Rakesh Dommaraju -X (rakdomma - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[rakdomma@cisco.com|mailto:rakdomma@cisco.com]>, Tran Lam (tranlam) <[tranlam@cisco.com|mailto:tranlam@cisco.com]>, Solomon Raja Padavettan John Ganesan (sojohnga) <[sojohnga@cisco.com|mailto:sojohnga@cisco.com]>, Lakshmi Chivukula (lbhetana) <[lbhetana@cisco.com|mailto:lbhetana@cisco.com]>
*Subject:* Re: CSCwb79577: Impacting test integration in Sanity test

Hi Raghu,

 

We have this test after couple of operations in Sanity and upgrade scenario’s so the link flap would be already generated. We are testing this feature after updating the interface description so it’s failing.

 

There is two approaches to handle:
 # Update the script to not update the description during the link-flap test and move the defect to sev6
 # Keep the defect to Sev3 and have ETA for the fix.

 

Please advise.

 

Thanks,

Raju Made changes and provided PR details to Raju.

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3040/overview

Raju will test and confirm us then we can close this Jira

 

  Needs the fix in Shockwave too:

 

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=81831935&size=580623&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F03%2F20%2F56%2Fenv_auto_job.2022Jul03_20:56:37.829092.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS Need the fix in shockwave too Severe since this is a shockwave feature and need to fix asap will work on it today Just had a sync up with Raju and DE manually its working fine. DE sugested some changes will do them and run in sanity regression testbed.

  Made some more changes in lib file and raised the PR for Guardian and shockwave.

 

Provided PRs to Raju and requested him to test in regression either on Guardian or shockwave and share the results.

 

If still fails requested regression team to provide their setup to automation team to test and do the necessary changes as automations team does not have the N-1 clusters. Current flow cannot work on Groot.

 

on Guardian :

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3145/overview]

 

Shockwave:

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3146/overview Will re-open once we test latest code in regression  *ISO*:*2.1.390.72158
**Script*:*solution_test_sanityecamb.py

**Impacted Testcases:**Test_TC142_Add_interace_description/test1_add_interface_description

**NOTE:** Double commit the same for shockwave
**Failure Log:**
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=79218149&size=718722&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F26%2F23%2F54%2Fenv_auto_job.2022Jul26_23:54:09.596172.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

**Snip from Failure Log:**

**

Enter configuration commands, one per line. End with CNTL/Z.TB5-DM-eCA-BORDER(config)#interface TwoGigabitEthernet1/0/13TB5-DM-eCA-BORDER(config-if)# no shutdownTB5-DM-eCA-BORDER(config-if)#endTB5-DM-eCA-BORDER#

247659: Failed reason: interface description validation failed

  [~61efa8c457b25b006877eda3] already code is merged to Shockwave. Please find the below PR. I see from your log you dont have latest code. Could you please check the below PR and get the latest code from main branch.

 

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3146/diff Please commit the fix in Ghost branch as well 

Currently running Ghost ISO 2.1.610.70338 - I am seeing the similar error for the same TC 

Please find the failed log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=86198732&size=398588&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug16_13:03:43.653275.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  Recent changes fixed in guardian. Already committed same changes into groot and Ghost but not working as expected. Need to check with DE for the changes and Implement it.

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=106675439&size=133734&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep18_16:11:21.732010.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Please file new Jira for Groot and Ghost as design changed. Till Guardian it will work as per the latest changes in code. 

 

here is the regression log ran in latest guardian

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=106675439&size=133734&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep18_16:11:21.732010.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 ","['Auton', 'Ghost', 'Guardian', 'Issue', 'Shockwave']",Avril Bower,Closed,Avril Bower
SEEN-135,https://miggbo.atlassian.net/browse/SEEN-135,[Auton]Test_TC157_Validate_sda_multicast_external_apis  /   test1_clearing_existing_multicast_and_validate,"Testcase:
+[Test_TC157_Validate_sda_multicast_external_apis  /   test1_clearing_existing_multicast_and_validate|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=84023496&size=289667&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-04%2Fenv_auto_job.2022Apr23_12:04:35.059145.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]+

 

Issue:
|251405:  Flagging result as FAIL!
 251406:  Reason: 400 Client Error: Bad Request for url: [https://10.30.0.100/dna/intent/api/v1/business/sda/multicast?siteNameHierarchy=Global%2FUSA%2FBayAreaGuest]
 251407:  Kwargs:
 251408:  \{'params': {'siteNameHierarchy': 'Global/USA/BayAreaGuest'}}
 251409:  Error Caught While Querying With External API|",2022-04-27T02:40:26.861+0000,"Similar to [https://cdetsng.cisco.com/webui/#view=CSCwb62188]

 

ISO : Guardian RC3 #2.1.510.70395 (FIPS Disabled)

Description:
----------------- 
When checking if the multicast is enabled on a site, Below External api is returning 400 error code instead of 404 error code. 
The multicast does not exist in the site where we are checking, and hence it should return 404 error code

Below API is returning 400 instead of 404 error code when the resource doesn’t exist
This worked in Guardian RC1 2.1.510.70366, attached pass log in regression note

720796: External API handler called:
720797: \{'params': {'siteNameHierarchy': 'Global/USA/SAN-FRANCISCO'}}
720798: Resource path full url: https://10.30.0.100/dna/intent/api/v1/business/sda/multicast
720799: Error Code: 400 URL:https://10.30.0.100/dna/intent/api/v1/business/sda/multicast Data:\{'timeout': 30, 'params': {'siteNameHierarchy': 'Global/USA/SAN-FRANCISCO'}} Headers:\{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2MjU4ZWU3MWVlYzMzZjM5ZjMwNTAxZGIiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLRgfNw9-nUTAZz0DpAW-D_mqw;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:{
720800: ""status"" : ""failed"",
720801: ""description"" : ""multicast is not configured on given siteNameHierarchy. siteNameHierarchy =not-available"",
720802: ""taskId"" : null,
720803: ""taskStatusUrl"" : ""/dna/intent/api/v1/task/null"",
720804: ""executionStatusUrl"" : ""/dna/intent/api/v1/dnacaap/management/execution-status/14470852-abeb-4600-bcbc-6504f0bc2699"",
720805: ""executionId"" : ""14470852-abeb-4600-bcbc-6504f0bc2699""
720806: }


RCA : http://172.21.236.183/sanity_rca/maglev-10.30.0.100-rca-2022-04-19_13-30-53_UTC.tar.gz

Cluster Details : 10.30.0.100 (admin/Maglev123) Closing the issue, We will create a new issue if the issue is seen again ","['Auton', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-150,https://miggbo.atlassian.net/browse/SEEN-150,[Auton][IBSTE]Test_TC18_DNAC_Wireless_SSID_creation_open_enterprise  test1_ssid_creation_custome_rf_profile,"Testcase:

[Test_TC18_DNAC_Wireless_SSID_creation_open_enterprise|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5173547&size=129112&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F22-04%2Fsr_ibste.2022Apr18_23:56:47.810137.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&atstype=PYATS] 
[test1_ssid_creation_custome_rf_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4213249&size=87378&archive=sr_ibste.2022Apr19_04:41:15.559932.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Branch: private/Guardian-ms/api-auto

 

[Failed Log|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4213249&size=87378&archive=sr_ibste.2022Apr19_04:41:15.559932.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]
| |Script got failed because of 500 Server Error: Server Error while creating rf profile. I took a copy of solution input json from original IBSTE directory and try to check differences with our existing solution input json file. we didnt see any changes too.| |

!https://wiki.cisco.com/download/attachments/1127266705/image2022-4-26_9-34-39.png?version=1&modificationDate=1650945881000&api=v2!

 

 ",2022-04-28T00:24:27.744+0000,"HTTP 500 Server error doesn't give any details.

I tried to manually create a custom profile, only when selecting the Parent Profile of Low, it gives an error

!image-2022-06-07-20-06-35-528.png|width=682,height=471!

So I updated the two custom profiles using the ""Custom"" Parent Profile.

Also I notice that there's AI RF Profile section so I created one.

!image-2022-06-07-20-08-54-779.png|width=940,height=422!

 

Trade log:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-phannguy-seen-53/users/phannguy/archive/22-06/sr_intg.2022Jun07_19:47:38.310241.zip&atstype=ATS]

 ","['Auton', 'Guardian', 'Issue']",Phan Nguyen,Closed,Avril Bower
SEEN-151,https://miggbo.atlassian.net/browse/SEEN-151,[Auton][IBSTE] Test_TC35_DNAC_verifying_configuration_lisp_on_devices,"Testcase

[Test_TC35_DNAC_verifying_configuration_lisp_on_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13592762&size=12698153&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F22-04%2Fsr_ibste.2022Apr19_23:54:49.982154.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&atstype=PYATS] 
 # [test1_verify_configuration_fabric1|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13593538&size=2486328&archive=sr_ibste.2022Apr19_23:54:49.982154.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]
 # [test1_verify_configuration_l2_lisp_instance_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16079866&size=3834390&archive=sr_ibste.2022Apr19_23:54:49.982154.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Branch: private/Guardian-ms/api-auto

 

[Failled Log|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13593538&size=2486328&archive=sr_ibste.2022Apr19_23:54:49.982154.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]

[Failed Log|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16079866&size=3834390&archive=sr_ibste.2022Apr19_23:54:49.982154.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]

 

!https://wiki.cisco.com/download/attachments/1127266705/image2022-4-26_9-39-9.png?version=1&modificationDate=1650946150000&api=v2!

!https://wiki.cisco.com/download/attachments/1127266705/image2022-4-26_12-49-22.png?version=1&modificationDate=1650957563000&api=v2!

 

It is getting failed for Spain site devices after modifying topology and roles of the devices. some expected configurations are not pushed on few devices on ESP site after changing device roles.

Earlier, we have two 3850 devices(ESP-FIAB-3850-1, ESP-FIAB-3850-2) as role: MAPSERVER,BORDERNODE,INTERNAL,EXTERNAL,SDATRANSIT,NOIPTRANSIT . Now we changed ESP-FIAB-3850-2(ESP-FE-9300) device role to EDGENODE",2022-04-28T00:26:01.164+0000,"For [test1_verify_configuration_fabric1|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13593538&size=2486328&archive=sr_ibste.2022Apr19_23:54:49.982154.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]
{code:python}
15821:  Cannot track test: tracking auth info must be set in order to transfer test tracking data
15822:  Executing testcase Test_TC35_DNAC_verifying_configuration_lisp_on_devices test 35.1 ""test1_verify_configuration_fabric1"".
15823:  Action: configuration on devices after provision on fabric1.
15824:  Initializing function group ""fabric_wired""
15825:  Group ""fabric_wired"" initialized successfully
15826:  Traceback (most recent call last):
15827:    File ""/auto/dna-sol/ws/phannguy8/dnac-auto/services/commonlibs/test_wrapper.py"", line 301, in wrapper
15828:      result = testfunc(func_self, **kwargs)
15829:    File ""/auto/dna-sol/ws/phannguy8/dnac-auto/testcases/ibste/ibste_script.py"", line 856, in test1_verify_configuration_fabric1
15830:      if(ibste_handle.update_script_device_config_for_verification(device_state=""DEPLOYED"")):
15831:    File ""/auto/dna-sol/ws/phannguy8/dnac-auto/services/dnaserv/ibstelibs/decorators.py"", line 32, in wrapper
15832:      result = method(*args, **kwargs)
15833:    File ""/auto/dna-sol/ws/phannguy8/dnac-auto/services/dnaserv/ibstelibs/feature_groups/fabric_wired/group.py"", line 88, in update_script_device_config_for_verification
15834:      ap_pool= get_segment_vn_map(self.services.all_sites_dna_handles[site],site, self.services.all_sites_dnac_input[site], onboard_type=""ap"")
15835:  NameError: name 'get_segment_vn_map' is not defined
15836:  Test returned in 0:00:00.005121
15837:  Errored reason: name 'get_segment_vn_map' is not defined
{code}
Fixed NameError: name 'get_segment_vn_map' is not defined in commit [d291f88df70|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d291f88df70d92716fe2999d631cc9530b16714f]

[|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-phannguy-seen-53/users/phannguy/archive/22-06/sr_ibste.2022Jun07_22:41:54.894448.zip&atstype=ATS] For [test1_verify_configuration_l2_lisp_instance_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16079866&size=3834390&archive=sr_ibste.2022Apr19_23:54:49.982154.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]

Issue was due to connectivity issue during regression run.

No issue when executing now: [test1_verify_configuration_l2_lisp_instance_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3632558&size=377420&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F22-06%2Fsr_ibste.2022Jun07_22:41:54.894448.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS]

  For [test1_verify_configuration_l3_lisp_instance_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4009978&size=627071&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F22-06%2Fsr_ibste.2022Jun07_22:41:54.894448.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS]

Two Vlans not having the ""global"" in ""ip helper-address""

{code:java}
15992:  ERROR Following line ip helper-address global 204.192.3.40 of output not present on device unexpectedly: 
15993:  ESP-FE-3850
{code}
 
ESP-FE-3850
{noformat}
interface Vlan1021
 description Configured from Cisco DNA-Center
 mac-address 0000.0c9f.fb7f
 ip address 204.8.128.1 255.255.240.0
 ip helper-address 204.192.3.40
 no ip redirects
 ip route-cache same-interface
 ip tcp adjust-mss 1250
 no lisp mobility liveness test
 lisp mobility APPool_esp-INFRA_VN-IPV4
!
interface Vlan1022
 description Configured from Cisco DNA-Center
 mac-address 0000.0c9f.ff2a
 ip address 204.32.128.1 255.255.240.0
 ip helper-address 204.192.3.40
 no ip redirects
 ip route-cache same-interface
 ip tcp adjust-mss 1250
 no lisp mobility liveness test
 lisp mobility EXT_POOL_esp-INFRA_VN-IPV4
!
{noformat}

Checking another Edge such as SJC-FE-9300-1

{noformat}
interface Vlan1021
 description Configured from Cisco DNA-Center
 mac-address 0000.0c9f.f777
 ip address 204.32.16.1 255.255.240.0
 ip helper-address global 204.192.3.40
 no ip redirects
 ip route-cache same-interface
 ip tcp adjust-mss 1250
 no lisp mobility liveness test
 lisp mobility EXT_POOL_sjc-INFRA_VN-IPV4
!
interface Vlan1022
 description Configured from Cisco DNA-Center
 mac-address 0000.0c9f.f11a
 ip address 204.8.16.1 255.255.240.0
 ip helper-address global 204.192.3.40
 no ip redirects
 ip route-cache same-interface
 ip tcp adjust-mss 1250
 no lisp mobility liveness test
 lisp mobility APPool_sjc-INFRA_VN-IPV4
!
{noformat}

 For some TCs calling the update_script_device_config_for_verification(), hitting the NameError: name 'get_segment_vn_map' is not defined
{noformat}
36851:  Traceback (most recent call last):
36852:    File ""/auto/dna-sol/ws/sr-ibste/guardian/services/commonlibs/test_wrapper.py"", line 301, in wrapper
36853:      result = testfunc(func_self, **kwargs)
36854:    File ""/auto/dna-sol/ws/sr-ibste/guardian/testcases/ibste/ibste_script.py"", line 650, in test2_verify_configuration_on_devices_fabric1
36855:      if(ibste_handle.update_script_device_config_for_verification(device_state=""PROVISIONED"")):
36856:    File ""/auto/dna-sol/ws/sr-ibste/guardian/services/dnaserv/ibstelibs/decorators.py"", line 32, in wrapper
36857:      result = method(*args, **kwargs)
36858:    File ""/auto/dna-sol/ws/sr-ibste/guardian/services/dnaserv/ibstelibs/feature_groups/fabric_wired/group.py"", line 88, in update_script_device_config_for_verification
36859:      ap_pool= get_segment_vn_map(self.services.all_sites_dna_handles[site],site, self.services.all_sites_dnac_input[site], onboard_type=""ap"")
36860:  NameError: name 'get_segment_vn_map' is not defined
36861:  Test returned in 0:00:00.024757
36862:  Errored reason: name 'get_segment_vn_map' is not defined
{noformat}
[Test_TC28_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4703348&size=3216864&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F22-06%2Fsr_ibste.2022Jun07_08:13:26.415715.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&atstype=PYATS]  /  [test2_verify_configuration_on_devices_fabric1|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7904084&size=6350&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F22-06%2Fsr_ibste.2022Jun07_08:13:26.415715.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]  (Errored)

Issue fixed with commit [6630af2b9b9|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6630af2b9b9d9c75289cffc790b4dabefd6de328] [~62ab7a399cd13c0068b18fe0], the changes have been merged to Groot by Tran.

FYI, [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2917/overview] ","['Auton', 'Groot', 'Guardian', 'Issue']",Phan Nguyen,Closed,Avril Bower
SEEN-175,https://miggbo.atlassian.net/browse/SEEN-175,[Auton] Test_TC64_verify_wlc_interface_stats,"Testcase:

[Test_TC64_verify_wlc_interface_stats|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10353940&size=17897&archive=sr_mb2_three_sites.2022Feb03_10:19:07.623138.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]

Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13179926&size=19132&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F04%2F28%2F03%2F36%2Fsr_mb2_three_sites.2022Apr28_03:36:34.989098.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS] 

Branch: private/Guardian-ms/api-auto

Source: Multisite TB2(Assurance Suite)

Date: 04Feb 2022

PoC: vgovind2/sandshiv

Diagnostic Notes:

 

 ",2022-05-04T08:52:44.494+0000,don't have proper failed log to check this issue. Attached different log instead the correct failed log. This specific issue is no more observed on Guardian Patch2 testing and hence closing the ticket This specific issue is no more observed on Guardian Patch2 testing and hence closing the ticket,"['Auton', 'Guardian', 'Issue', 'MSTB1', 'MSTB2', 'Multisite']",Avril Bower,Closed,Avril Bower
SEEN-176,https://miggbo.atlassian.net/browse/SEEN-176,[Auton] Test_TC31_SWIM_UPGRADE_ECA_DEVICE - test5_aduit_log_verify,"[Test_TC31_SWIM_UPGRADE_ECA_DEVICE|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11053540&size=17470821&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F02%2F26%2F15%2F14%2Fenv_auto_job.2022Feb26_15:14:39.204166.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS] - [test5_aduit_log_verify|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27852136&size=672058&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F02%2F26%2F15%2F14%2Fenv_auto_job.2022Feb26_15:14:39.204166.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

solution_test_sanityecamb_lan.py

Branch:  private/Guardian-ms/sanity_api_auto  (synced on 2/28)

Logs:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27852136&size=672058&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F02%2F26%2F15%2F14%2Fenv_auto_job.2022Feb26_15:14:39.204166.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

Source: Sanity

Date: 02 Mar 2022

PoC: Nethra

Diagnostic notes:

All the expected Audit log messages are seen in the dnac

08/04: Issue is seen with Guardian RC1 as well, Please check cluster - 10.195.227.31

Log : 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13395151&size=560394&archive=env_auto_job.2022Apr04_18:37:59.602731.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS]

 ",2022-05-04T08:57:35.128+0000,"[Raji 's PR: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2737/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2737/overview] ISO:2.1.390.72158
Polarissi:17.6.4prd7
Script Name: solution_test_sanityecamb_lan.py

*NOTE:* Please double commit same to Shockwave
*Failure Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=18662145&size=428878&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F06%2F06%2F23%2Fenv_auto_job.2022Aug06_06:23:45.283041.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS

*Snip from Failure Log:*
65194:  Could not find the expected parent response even after retry :: {'http://172.21.236.183/swim/sanity_image_regr/tb2/C9800-SW-iosxe-wlc.17.08.01a.SPA.bin': ['The OS image will be uploaded to the Cisco DNA Center swim repository using a url', ['Image import workflow started for url : [http://172.21.236.183/swim/sanity_image_regr/tb2/C9800-SW-iosxe-wlc.17.08.01a.SPA.bin']]}|http://172.21.236.183/swim/sanity_image_regr/tb2/C9800-SW-iosxe-wlc.17.08.01a.SPA.bin']]%7D]
65198:  Failed reason: Result : Audit response verification failed
  |[a)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/users/rmukkama]|[260a7de0b9a|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/260a7de0b9ab31e366665b72ae90ee6bccf8091d]|
 [b55de20ef30|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/b55de20ef30bee9c45ef12585e9273a63aaee1e7] Verified the fix in shockwave: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16724877&size=206308&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F11%2F02%2F12%2Fenv_auto_job.2022Oct11_02:12:58.177872.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 ","['Auton', 'Guardian', 'Issue', 'Shockwave']",Raji Mukkamala,Closed,Avril Bower
SEEN-177,https://miggbo.atlassian.net/browse/SEEN-177,[Auton] TC151_verify_fabric_360/test1_verify_fabric_SD_access,"[TC151_verify_fabric_360|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=57265401&size=132524&archive=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20%2Fusers%2Fwnbu%2Farchive%2F22-02%2Fsanitycombine.2022Feb22_11:52:16.872543.zip&ats=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20&submitter=wnbu&from=trade&view=all&atstype=pyATS]

[test1_verify_fabric_SD_access|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=57265986&size=131782&archive=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20%2Fusers%2Fwnbu%2Farchive%2F22-02%2Fsanitycombine.2022Feb22_11:52:16.872543.zip&ats=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20&submitter=wnbu&from=trade&view=all&atstype=pyATS]

Branch:  private/Guardian-ms/api-auto

Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=57265986&size=131782&archive=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20%2Fusers%2Fwnbu%2Farchive%2F22-02%2Fsanitycombine.2022Feb22_11:52:16.872543.zip&ats=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20&submitter=wnbu&from=trade&view=all&atstype=pyATS]

 

Source: Sanity

Date: 02 Mar 2022

PoC: apulamat

Notes:

194143: 2022-02-22T22:05:12: %API-GROUP-ASSURANCE_FABRIC-3-ERROR: Fabric site 'Global/USA/New York' has dont have proper devices count:0 compare with actual fabric site count 2. in the response

194160: 2022-02-22T22:05:13: %API-GROUP-ASSURANCE_FABRIC-3-ERROR: Fabric site 'Global/USA/SAN JOSE' has dont have proper devices count:1 compare with actual fabric site count 2. in the response

New/Defect for Access point were not counted.

From the script , The Unified APs are not taking as a part of fabric device count. Can you please check?",2022-05-04T09:04:56.597+0000,"This PR is not even merged, Resolved status is wrong  The log link in the Description has an error and shows :

{{Could not get content of log. Either the log file does not exist or is inaccessible.}}

This is a new log link when I run this TC on Sanity_TB1 and the branch private/Guardian-ms/api-auto

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-nhanhngu%2Fusers%2Fnhanhngu%2Farchive%2F23-03%2Fsanity_TB1.2023Mar27_23:38:51.752371.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-nhanhngu%2Fusers%2Fnhanhngu%2Farchive%2F23-03%2Fsanity_TB1.2023Mar27_23:38:51.752371.zip&atstype=ATS] PR-Guardian: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5155/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5155/overview]

This testcase was run passed on branch Ghost and Halleck. Please check the trade log link below:

Ghost: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB1.2023Mar29_01:02:23.083100.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB1.2023Mar29_01:02:23.083100.zip&atstype=ATS]

Or refer to this PR from Auton SEEN-814:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4991/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4991/overview]

Halleck: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB1.2023Mar29_01:29:24.761055.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB1.2023Mar29_01:29:24.761055.zip&atstype=ATS] [~accountid:63f50bcafb3ac4003fa2c6dd] not sure if Sanity TB1 has all required hardwares as it is getting PASS logs for this use-case.

However, if you check below execution logs shared by Test team:
1. TB7 Guardian - [TC151_verify_fabric_360|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13475325&size=250882&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr18_22:33:18.637015.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_fabric_SD_access

2. TB7 Halleck - [TC12_verify_fabric_360|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-156-SDAFabricAssurance&begin=1085566&size=157603&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr07_17:52:00.558453.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_fabric_SD_access

Currently this TC is failing due to [CSCwe24454|https://cdetsng.cisco.com/webui/#view=CSCwe24454]. Since the provided log in the description is no more available, I’m not sure what fix was required.

However, I see that you PR is handling some exceptions. Hence approving it.

Once the mentioned bug gets fixed, and if again this TC fails, team will raise a new Auton. [~accountid:63f50bcafb3ac4003fa2c6dd] , related PR has been approved and merged.

Please cherry-pick to other branches and then close this ticket. Okay, Thanks [~accountid:62ab7a399cd13c0068b18fe0].","['Auton', 'Ghost', 'Guardian', 'Halleck', 'Issue']",NhanHuu Nguyen,Closed,Avril Bower
SEEN-178,https://miggbo.atlassian.net/browse/SEEN-178,[Auton]  TC30_DNAC_Connectivity_domain_creation_in_dnac / test1_setup_SGT,"[TCC30_DNAC_Connectivity_domain_creation_in_dnac|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10625213&size=657406&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-03%2Fenv_auto_job.2022Mar06_00:01:57.553359.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&atstype=PYATS]  /   test1_setup_SGT  

solution_test_sanityecamb_lan.py

solution_test_sanityecamb.py

Branch: private/Guardian-ms/api-auto

Logs:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10625376&size=53075&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F03%2F06%2F00%2F01%2Fenv_auto_job.2022Mar06_00:01:57.553359.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

Source: Sanity

Date: 10 Mar 2022

PoC: Nethoral

Notes:

Need to delete sgt_port_test_sg as part of cleanup. Fails in rerun with below error

38398: Failed reason: Failed because could not create scalable group due to : NCCS16037: Security Group with the name sgt_port_test_sg already exists

Resolved (added aca migration to beginning)

Omkar:Ran the test with the latest code on 4/17 still same issue : [Log|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=115885358&size=61748&archive=env_auto_job.2022Apr14_22:30:45.061618.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS]

 ",2022-05-04T09:08:26.950+0000,"Todo: Add cleanup for the sgt before the usecase. https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2756/diff#services/dnaserv/lib/api_groups/inventory/group.py *ISO:*Ghost:2.1.610.70338
 *Script Name* :  solution_test_sanityecamb_lan.py 
 **Branch:**Ghost branch: private/Ghost-ms/sanity_api_auto

*Note:* Same issue seen Ghost ** 

*Fail Log* :

log from 0-14: 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug15_04:20:22.448173.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]


 **[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2914676&size=117135&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug15_20:13:31.146064.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Code is there in ghost, could be api change, sg is deleted in log file, but still showing up later. Need tb to investigate

 

 

Most likely though, aca migration was not completed in the beginning. Need the fix in Legacy script, This test worked fine in optimized code: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_optimized_auto_job.2022Aug24_20:54:16.390893.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]

  Same as 508 Hi Andrew,

     Seeing same issue Guardian execution in Sanity. Please double commit the changes in Guardian branch.

[TCC30_DNAC_Connectivity_domain_creation_in_dnac|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=690614&size=808255&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct12_02:14:49.936554.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] /  [test1_setup_SGT|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=691178&size=120844&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct12_02:14:49.936554.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

solution_test_sanityecamb.py

Branch: private/Guardian-ms/api-auto

Sanity Branch: private/Guardian-ms/sanity_api_auto

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=691178&size=120844&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct12_02:14:49.936554.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Fixed pulled to Guardian:

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6eda6711586c57662c37475d4f88d873c7ad98fc]

  FIx pulled on Guardian also. This issue is still seen for Guardian - Please find the log attached

script used for run: testcases/forty_eight_hour/solution_test_sanityecamb_lan.py

Failed log: 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=876272&size=117965&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_auto_job.2023Mar02_10:47:52.406685.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4893/overview]

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4895/overview]

 

Committed to ghost branch too

 https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/fbf3538e1330b439416969cf866a0f205017977a

Adding the fix to script files directly, as create_new_sg is not being called

  Marking this Auton as “Closed” as the below listed PRs are already merged to main release branches.","['Auton', 'Ghost', 'Guardian', 'Halleck', 'Hulk', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-179,https://miggbo.atlassian.net/browse/SEEN-179,[Auton] TC53_DNAC_provision_all_aps/test3_verfies_syslog_events_generated_wlc,"[TC53_DNAC_provision_all_aps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=30604877&size=703070&archive=sanitycombine.2022Mar09_13:03:24.501894.zip&ats=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20&submitter=wnbu&from=trade&view=all&atstype=pyATS]

[test3_verfies_syslog_events_generated_wlc|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=30941728&size=366052&archive=sanitycombine.2022Mar09_13:03:24.501894.zip&ats=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20&submitter=wnbu&from=trade&view=all&atstype=pyATS]

Branch: private/Guardian-ms/api-auto

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=30941728&size=366052&archive=sanitycombine.2022Mar09_13:03:24.501894.zip&ats=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20&submitter=wnbu&from=trade&view=all&atstype=pyATS]

Source: Sanity

Date: 10 Mar 2022

PoC: apulamat

Notes: 

Under Aireos controller, AP join disjoin events are not found and showing only Device reachability events found

103207: Capwap Events: AP_JOIN_DISJOIN not found on device 204.192.2.1
103208: Verification Failed on ['204.192.2.1']
103209: Test returned in 0:03:07.882670
103210: Failed reason: Verification of WLCs Events Failed
103211: The result of section test3_verfies_syslog_events_generated_wlc is => FAILED

New/Defect, Raju has the defect ID. 

Is this script depends on AP models? 

Raju Has Aire OS Access points ,but in my set up we have Click os aps . Can you please confirm once?

 

 

 ",2022-05-04T09:13:08.424+0000,"This feature is not supported on Airos WLC This feature is not supported on Airos WLC This test is skipped for airos WLC Double committed the fix in Guardian Sanity branch 

PR : https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/b52367acd439b32f880ecac6ff216b75772ca889","['Auton', 'Guardian', 'Issue']",Raju Saran,Closed,Avril Bower
SEEN-180,https://miggbo.atlassian.net/browse/SEEN-180,[Auton] Test_TC154_apply_custom_profile_issue_on_site_level,"  [Test_TC154_apply_|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=39708993&size=3260&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F03%2F28%2F21%2F09%2Fenv_auto_job.2022Mar28_21:09:49.877785.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS] [custom_profile_issue_on_site_level|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=39708993&size=3260&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F03%2F28%2F21%2F09%2Fenv_auto_job.2022Mar28_21:09:49.877785.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]
    solution_test_sanityecamb_lan.py

Branch: private/Guardian-ms/sanity_api_auto

Source: Sanity

Date: 04 Apr 2022

PoC: anusjohn

 

This is not an issue/error, just need a name change in input file. We are creating custom profile with name 'global_profile', This name would be confusing sometimes since we have global profile section separately.

so we wanted to change that 'global_profile' (custom profile) name to 'custom_global_profile'. 

 ",2022-05-04T09:19:47.644+0000,Merged PR https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2871/overview [~62d2fe9f8afb5805e5d5af49]  has fixed this issue.,"['Auton', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-181,https://miggbo.atlassian.net/browse/SEEN-181,[Auton] TC11_DNAC_adding_fabric_devices_to_site,"[TC11_DNAC_adding_fabric_devices_to_site|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=847970&size=39702&archive=%2Froot%2F.pyats%2Farchive%2F22-04%2Flanauto.2022Apr05_03:48:54.140034.zip&ats=%2Fws%2Fsmounasw-sjc%2FPyats&submitter=root&atstype=PYATS] 

Branch: private/DNAC_Guardian_cat6k_3/api-auto

Source: Cat6k S. R

Issue:

5823: 2022-04-05T04:01:59: %AETEST-3-ERROR: return result
5824: 2022-04-05T04:01:59: %AETEST-3-ERROR: UnboundLocalError: local variable 'result' referenced before assignment

Note:

Andrew: Looks like branch is pretty old compared to main, can changes be pulled from main?

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2753/overview]

 

pulled the latest Guardian code and executed, observing
UnboundLocalError: local variable 'result' referenced before assignment",2022-05-04T09:22:58.543+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2756/diff#services/dnaserv/lib/api_groups/inventory/group.py,"['Auton', 'Guardian', 'Issue']",Andrew Chen,Resolved,Avril Bower
SEEN-182,https://miggbo.atlassian.net/browse/SEEN-182,Test_TC3_generate_dhcp_server_config_on_fusion/test4_ise_cleanup_guest," [Test_TC3_generate_dhcp_server_config_on_fusion|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1143870&size=1910299&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-04%2Fenv_auto_job.2022Apr04_14:43:24.444661.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]

test4_ise_cleanup_guest  

Branch: private/Guardian-ms/api-auto

Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2288918&size=765071&archive=env_auto_job.2022Apr04_14:43:24.444661.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS]

Source: Sanity

Date: 07 April 2022

PoC: amadhav

 Issue:

16963: ""title"" : ""Deleting SelfRegistartion Portal By ID(d14f325b-5e7d-4792-aba5-b2f42a6b0ed5) failed due to com.cisco.epm.edf2.exceptions.EDF2SQLException: ORA-02292: integrity constraint (CEPM.REF_PORTAL_FLOW_PORTAL_TEMP) violated - child record found\n"",

16979: requests.exceptions.HTTPError: 500 Server Error: for url: [https://10.195.227.32:9060/ers/config/selfregportal/d14f325b-5e7d-4792-aba5-b2f42a6b0ed5]
16980: ###################################################
16981: #!!!FAILED TO DELETE SELF REG PORTAL in ISE. ERROR 500 Server Error: for url: [https://10.195.227.32:9060/ers/config/selfregportal/d14f325b-5e7d-4792-aba5-b2f42a6b0ed5----#|https://10.195.227.32:9060/ers/config/selfregportal/d14f325b-5e7d-4792-aba5-b2f42a6b0ed5----]
16982: ###################################################
16983: Failed to clear guest self reg portal from ISE: GUESTPORTALNEW",2022-05-04T09:25:34.588+0000,"Moe's PR: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2673/overview Didn't see the issue in latest code Not working in FQDN+Delay testing. Details are below,

 

Branch Used: private/Guardian-ms/sanity_delay_testing

Script file: testcases/forty_eight_hour/solution_test_sanityecamb_cert_lan.py

Failed Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1368627&size=510292&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fenv_auto_job.2022Dec08_04:27:39.900054.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  [~62ab7a399cd13c0068b18fe0], any updates on this [~63f50bfce8216251ae4d59d5], unlike Sanity TB2, the issue is not observed with Sanity TB4.
I have almost 3 executions back to back on the same testbed and didn't hit the problem. Hey [~63f53512263233e653a96a29],

Could you follow up with the DE about this issue? 
https://cdetsng.cisco.com/webui/#view=CSCwc44298 Close this ticket as this is the product issue driven by CSCwc44298.  Share testbed with DE when the issue is seen. Please follow up with the DE if the issue seen again Issue observed on Guardian P4 2.1.518.72273 Sanity TB2 Here is the Failed log on Guardian P4 2.1.518.72273 Sanity TB2

Log: https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2465097&size=509049&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_auto_job.2023Jan18_05:03:54.486169.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS [~63f50be24e86f362d39acde8],
Please do not open this ticket again. Try to close this issue with DE, and if there is an issue in the script then open it again.","['Auton', 'Guardian', 'Halleck', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-183,https://miggbo.atlassian.net/browse/SEEN-183,[Auton] TSIM AP config for 17.9,"TSIM AP config for 17.9

Branch: private/Guardian-ms/sanity_api_auto

Source: Sanity

PoC: Raju

Issue:
Creating an issue here to just keep track, more details sent in email to alias 

Hi Team,

We need to handle the following cli for 17.9 on the controller. Should we add in underlay or can we add it from script?

“(config)# ap dtls-version DTLS_1_0”

Thanks,

Raju

 

Raji: Adding this config after TC35 for eca and ewlc

Notes: eca/ewlc

 ",2022-05-04T09:30:49.306+0000,"Raji PR: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2737/overview The TC added to address this issue has failed with latest code pull

Please Note: Script needs to handle below scenario: 
      The config that is being pushed as part of this script should be done only to devices whose images are >=17.9 version 

Error snippet from the log: 

TB2-DM-eCA-BORDER(config)#                    ap dtls-version DTLS_1_0
Any change in this config will disconnect existing APs
TB2-DM-eCA-BORDER(config)#                    crypto pki certificate map map1 1
TB2-DM-eCA-BORDER(ca-certificate-map)#
5748:  Timeout of 120 seconds has been reached.
5749:  Prompt Recovery has commenced. Total timeout occurs in 100 seconds.
5750:  Sending prompt recovery command: b'\r'

TB2-DM-eCA-BORDER(ca-certificate-map)#
5752:  Sending prompt recovery command: b'\x15'
5753:  Sending prompt recovery command: b'\x1a'
^Z
TB2-DM-eCA-BORDER#
5755:  Exception occured Expected device to reach 'config' state, but landed on 'enable' state.


Please find the log:[test5_enable_dtls_version|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eca_enable_wireless_in_fabric.py-111-FEWembededWirelessECA&begin=2543127&size=12530&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr01_00:53:43.782267.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

Branch used:  private/Halleck-ms/sanity_api_auto
Job file: Optimized code 

Usecase:[FEWembededWirelessECA|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eca_enable_wireless_in_fabric.py-111-FEWembededWirelessECA&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr01_00:53:43.782267.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  Tracking this issue with [https://miggbo.atlassian.net/browse/SEEN-1414|https://miggbo.atlassian.net/browse/SEEN-1414|smart-link]  ","['Auton', 'Guardian', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-185,https://miggbo.atlassian.net/browse/SEEN-185,[Auton] TC153_generate_network_devices_report/  test1_generate_network_devices_CSV_report/  test2_generate_network_devices_JSON_report,"[TC153_generate_network_devices_report|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=55341910&size=288376&archive=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20%2Fusers%2Fwnbu%2Farchive%2F22-04%2Fsanitycombine.2022Apr09_13:30:45.293091.zip&ats=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20&submitter=wnbu&from=trade&view=all&atstype=pyATS]

[test1_generate_network_devices_CSV_report|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=55342495&size=171921&archive=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20%2Fusers%2Fwnbu%2Farchive%2F22-04%2Fsanitycombine.2022Apr09_13:30:45.293091.zip&ats=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20&submitter=wnbu&from=trade&view=all&atstype=pyATS]

[test2_generate_network_devices_JSON_report|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=55514416&size=115694&archive=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20%2Fusers%2Fwnbu%2Farchive%2F22-04%2Fsanitycombine.2022Apr09_13:30:45.293091.zip&ats=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20&submitter=wnbu&from=trade&view=all&atstype=pyATS]

Branch: private/Guardian-ms/api-auto

Log:

[csv|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=55342495&size=171921&archive=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20%2Fusers%2Fwnbu%2Farchive%2F22-04%2Fsanitycombine.2022Apr09_13:30:45.293091.zip&ats=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20&submitter=wnbu&from=trade&view=all&atstype=pyATS]

[json|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=55514416&size=115694&archive=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20%2Fusers%2Fwnbu%2Farchive%2F22-04%2Fsanitycombine.2022Apr09_13:30:45.293091.zip&ats=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20&submitter=wnbu&from=trade&view=all&atstype=pyATS]

Source: sanity

Date: 4/14/2022

PoC: apulamat

Issue: mismatching management ip address inventory  csv  and json reports",2022-05-04T09:36:25.545+0000,"It was Bug from DNAC. And it was fixed. No issue from Script side.
Got confirmation from issue reporter.","['Auton', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-186,https://miggbo.atlassian.net/browse/SEEN-186,[Auton] TC154_apply_custom_profile_issue_on_site_level / test4_verify_cpu_memory_issues,"[TC154_apply_custom_profile_issue_on_site_level|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=55630286&size=5841&archive=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20%2Fusers%2Fwnbu%2Farchive%2F22-04%2Fsanitycombine.2022Apr09_13:30:45.293091.zip&ats=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20&submitter=wnbu&from=trade&view=all&atstype=pyATS]

test4_verify_cpu_memory_issues

Branch: private/Guardian-ms/api-auto

Logs:

[https://ngdevx.cisco.com/services/taas/results/8e84e3c8-1d2c-45ab-ac10-2a2a8e1fafdc/run-results]

Source: sanity

Date: 4/14/2022

PoC: apulamat

Issue: 

217341: wnbust-ats-26.cisco.com: 2022-04-12T00:41:18: %API-GROUP-ASSURANCE-3-ERROR: %[pid=30502][pname=Task-1]: Failed in Generation Switch CPU Utilization Issue on 204.1.2.2217342: wnbust-ats-26.cisco.com: 2022-04-12T00:41:18: %API-GROUP-ASSURANCE-3-ERROR: %[pid=30502][pname=Task-1]: Verification Failed!!!!217343: 2022-04-12T00:41:18: Test returned in 0:05:41.082792217344: 2022-04-12T00:41:18: Failed reason: Unable Verify Configs!217345: 2022-04-12T00:41:18: The result of section test4_verify_cpu_memory_issues is => FAILED

 ",2022-05-04T09:40:22.596+0000,Dup https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-180 Dupe,"['Auton', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-187,https://miggbo.atlassian.net/browse/SEEN-187,[Auton] TC148_enable_maintenance_mode_on_devices_and_validate_maintenance_mode,"*Script:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py
*Testcase1:* TC148_enable_maintenance_mode_on_devices_and_validate_maintenance_mode
*Fix Required Test Sections:*
test2_verify_maintenance_mode_enablement_on_devices_in_inventory
test3_verify_maintenance_mode_count_in_assurance
*Testcase2:* TC149_disable_maintenance_mode_on_devices_and_validate_device_out_of_maintenance
*Fix Required Test Sections:*
test2_verify_maintenance_mode_disablement_on_devices_in_inventory
test3_verify_maintenance_mode_count_in_assurance

Branch: private/Guardian-ms/api-auto

Log: 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/yiyamper-bgl/dnac_soln_reg/pyats/users/yiyamper/archive/22-04/three_sites.2022Apr27_09:12:00.440174.zip&atstype=ATS]

 

Source: Solution Regression

Date: 28 Apr 2022

PoC: yiyamper

Issue: 

As per offline discussion with DE Guruprasad Bhat on expected behavior, Need to follow below steps:
1)Enable/Disable Maintenance Mode for Device on DNAC
2)Wait for 5 mins
3)Resync device
4)Assurance data will reflect by 10mins(before also it can show but max time to reflect is 10mins).  So handle wait accordingly
5)Assurance data verifications can be proceed after adding step2,step3,step4

 

 ",2022-05-04T09:45:41.984+0000,Harry PR https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2773/overview,"['Auton', 'Guardian', 'Issue']",Harry Yang,Closed,Avril Bower
SEEN-188,https://miggbo.atlassian.net/browse/SEEN-188,[Auton]  Test_155_verify_global_level_events,"*Testcase:* verify_global_level_events
*Solution Regression Script:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py
*Solution Regression Testcase ID:* Test_155_verify_global_level_events
*Solution Sanity Testcase ID:* Test_172_verify_global_level_events
*Solution Sanity Script*: dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

Branch: private/Guardian-ms/api-auto

Log:

[Solution_Regression_Fail_Log|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/yiyamper-bgl/dnac_soln_reg/pyats/users/yiyamper/archive/22-03/three_sites.2022Mar31_17:37:14.764579.zip&atstype=ATS]

[Solution_Sanity_Fail_Log|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=934572&size=155750&archive=sanitycombine.2022Apr05_02:26:59.508822.zip&ats=%2Fhome%2Fwnbu%2Fpythonenv%2Fjenkins%2Fsol_ats20&submitter=wnbu&from=trade&view=all&atstype=pyATS]

Source: Solution Regression, Solution Sanity

Date: 29 Apr 2022

PoC: yiyamper

Issue:

First, please append “TC” to 155/172 to maintain the same format as other testcases.
Second, trying to verify events by Assuming that in last 30 mins, there would be some events/traps on ALL DEVICEs. Are we sure, is this going to work without an issue in the long run?
In Solution Regression, encountering the issue with WLC and In Solution Sanity, its with AP. Tomorrow if we run the same testcase, we may face different issue. I request you to look at this case.
TC155 is failing for AireoS WLC. I see offset 30 mins.. might be no events in that time for AireOS WLC. So no data in response. I guess.

 

 

 ",2022-05-04T09:48:12.414+0000,"Test passed in Sanity with Guardian: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=117742807&size=311393&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep18_16:11:21.732010.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 ","['Auton', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-189,https://miggbo.atlassian.net/browse/SEEN-189,[Auton] est_TC58_Verify_DHCP_server_change_on_segments / test22_deploy_AP_specific_configs_to_controller,"[Test_TC58_Verify_DHCP_server_change_on_segments|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=656865&size=47189225&archive=sr_mb2_three_sites.2022Apr26_10:20:07.337201.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]

[test22_deploy_AP_specific_configs_to_controller|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=47183394&size=210420&archive=sr_mb2_three_sites.2022Apr26_10:20:07.337201.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]

Branch: private/Guardian-ms/api-auto

Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=47183394&size=210420&archive=sr_mb2_three_sites.2022Apr26_10:20:07.337201.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]

Source: Multisite Non-DR TB

Date: 29 Apr 2022

PoC: vgovind2/sandshiv

CSCwb71685

 ",2022-05-04T09:50:43.523+0000,Rakesh's PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2760/overview],"['Auton', 'Guardian', 'Issue', 'MSTB1', 'MSTB2', 'Multisite']",Avril Bower,Closed,Avril Bower
SEEN-195,https://miggbo.atlassian.net/browse/SEEN-195,[Groot-Auton] Test_TC29_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision - test2_update_vlan_membership," 
8412:  TB7-SJ-EDGE#
8413: 
8414: 
8415:  api_switch_call called:
8416:  {}
8417:  Resource path full url: [https://10.30.0.100/api/v1/task/12fe7fd5-5168-4797-ba81-6a5a475edc58]
8419:  Task response\{'response': {'version': 1651974831866, 'startTime': 1651974828094, 'progress': 'Provision Failed', 'errorCode': 'NCNP20100', 'endTime': 1651974831866, 'serviceType': 'Inventory service', 'username': 'admin', 'failureReason': 'Unable to push the invalid CLI to the device 204.1.1.72 using protocol ssh2. Invalid CLI - switchport access vlan 1004', 'isError': True, 'instanceTenantId': '62750723ddb961673a7fb0f8', 'id': '12fe7fd5-5168-4797-ba81-6a5a475edc58'}, 'version': '1.0'} for retry 5


8496:  Kwargs:
8497:  \{'data': {'vlanId': '1'}, 'params': \{'deploymentMode': 'Deploy'}}
8498:  Error Caught While Querying the Internal API
8499:  Encountered unhandled HTTPError in group ""inventory"" method ""Update_vlan_membership""!
8500:  Flagging result as FAIL!
8501:  Reason: 500 Server Error: for url: [https://10.30.0.100/api/v1/interface/f8f436ab-3732-4b3b-a38f-116e56809bfa?deploymentMode=Deploy]
 
Fail Log :
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1267113&size=543163&archive=env_auto_job.2022May07_18:33:21.331713.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 
 ",2022-05-10T14:29:36.816+0000,"Dup of https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-127. 

Fix was merged into Guardian, Groot. Tracking as part of https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-129

 ","['Auton', 'Groot', 'Issue', 'Shockwave']",Unassigned,Closed,Avril Bower
SEEN-196,https://miggbo.atlassian.net/browse/SEEN-196,[Auton] : Groot - Test_TC37_DNAC_verifying_configuration_lisp_on_devices  /   test1_verify_configuration_l3_instance_config,"Groot Version : 2.1.560.70272
 Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC37_DNAC_verifying_configuration_lisp_on_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6974340&size=2180042&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May07_19:16:48.963193.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_configuration_l3_instance_config 

[Test_TC63_DNAC_verify_pim_igmp_config_on_fabric_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=60536773&size=2317033&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May07_19:16:48.963193.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test2_verify_configuration_l3_instance_config


 Fail Log :
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7466128&size=66301&archive=env_auto_job.2022May07_19:16:48.963193.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
  

Facing below index error in TC37 - LAN-A script,

+*Snip from Log :*+
 22161: api_switch_call called:
 22162: \{'params': {'id': '8e02ef01-e9da-4113-ae73-e23907852227'}}
 22163: Resource path full url: [https://10.30.0.100/api/v2/data/customer-facing-service/L2Segment]
 22166: Traceback (most recent call last):
 22167: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 22168: result = testfunc(func_self, **kwargs)
 22169: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 2828, in test1_verify_configuration_l3_instance_config
 22170: if (dnac_handle.verify_lisp_l3instance_config_on_edges()):
 22171: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 22172: result = method(*args, **kwargs)
 22173: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/device_config_validation/group.py"", line 241, in verify_lisp_l3instance_config_on_edges
 22174: ip helper-address \{1}"""""".format(seg[""gateway""],seg[""dhcpSvrAddr""][0],segment['name'])
 22175: IndexError: list index out of range
  
  
 Script Name :  solution_test_sanityecamb_lan.py
 Fail Log :
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7466128&size=66301&archive=env_auto_job.2022May07_19:16:48.963193.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
  

 ",2022-05-10T14:37:44.804+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2931/diff https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2931/overview,"['Auton', 'Groot', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-197,https://miggbo.atlassian.net/browse/SEEN-197,[Auton] : Groot - Test_TC85_generate_exceutive_summary_report  /   test3_DNAC_verify_SSID_lan_on_device,"Groot Version : 2.1.560.70272
 Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC85_generate_exceutive_summary_report|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=84007419&size=872169&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May07_19:16:48.963193.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test3_DNAC_verify_SSID_lan_on_device

 [Test_TC146_Random_mac_enable|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=120061925&size=1100102&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May07_19:16:48.963193.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test3_DNAC_verify_SSID_lan_on_device  

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=84598012&size=281402&archive=env_auto_job.2022May07_19:16:48.963193.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

Below SSID's are deleted as part of cleanup in prior tc, hence this tc is failed. Need to update the validation in TC85.3

[From [Test_TC85_generate_exceutive_summary_report|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=84007419&size=872169&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May07_19:16:48.963193.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]%C2%A0%C2%A0/%C2%A0%C2%A0%C2%A0test2_cleanup_wireless_ssid]
 203670: SSID to delete ['ARUBA_SSID', 'Guest_webpassthrough', 'Guest_webauthinternal', 'Guest_passthrough_int']
  
  
 Snip from Fail Log :
 -----------------------
 205281: Resource path full url: [https://10.30.0.100/api/v1/file/a0c76bcc-6878-4da6-8410-3792d420f1c9]
 205284: [{'deviceUuid': 'f6e77285-da32-41bd-9c80-983d21292a57', 'commandResponses': {'SUCCESS':

{'show wlan summary ': 'show wlan summary\n\nNumber of WLANs: 9\n\nID Profile Name SSID Status Security \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n17 Radius_ssid_profile Radius_ssidtb7 UP [WPA2][802.1x][AES] \n18 SSIDDUAL BAND_profile SSIDDUAL BANDtb7 UP [WPA2][802.1x][FT + 802.1x][AES],[FT Enabled] \n20 posture_profile posturetb7 UP [WPA2][802.1x][AES] \n21 SSIDDot1XIndia_profile SSIDDot1XIndiatb7 UP [WPA2][802.1x][AES] \n22 GUEST2_profile GUEST2tb7 UP [open],MAC Filtering \n23 Random_mac_profile Random_mactb7 UP [WPA2][802.1x][AES] \n24 GUEST_profile GUESTtb7 UP [open],MAC Filtering \n25 Single5KBand_profile Single5KBandtb7 UP [WPA2][PSK][FT + PSK][AES],[FT Enabled],MAC Filtering \n29 OPEN_profile OPENtb7 UP [open] \n\nTB7-SJ-eCA-BORDER-CP#'}

, 'FAILURE': {}, 'BLACKLISTED': {}}}]
 205285: Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:02.228871
 205286: 
 205287: ERROR Following line of expected cli output not present on device:
 205288: \d+\s+.*Guest_webpassthroughtb7\s+UP\s+
 205289: 
 205290: ERROR Following line of expected cli output not present on device:
 205291: \d+\s+.*Guest_webauthinternaltb7\s+UP\s+
 205292: 
 205293: ERROR Following line of expected cli output not present on device:
 205294: \d+\s+.*Guest_passthrough_inttb7\s+UP\s+
 205295: 
 205296: ERROR Following line of expected cli output not present on device:
 205297: \d+\s+.*Guest_webpassthroughtb7\s+UP\s+
 205298:",2022-05-10T17:43:38.563+0000,Merged https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/520d56844bd1bb0f5a22325edab0a1c159abb61b The issue is fixed.,"['Auton', 'Groot', 'Issue']",Tran Lam,Closed,Avril Bower
SEEN-198,https://miggbo.atlassian.net/browse/SEEN-198,[Auton] : Groot - Test_TC123_aaa_per_ssid  /   test1_add_new_ssid,"Groot Version : 2.1.560.70297

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC123_aaa_per_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=88064576&size=1003842&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-05%2Fenv_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]  /   test1_add_new_ssid 

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=88065164&size=108861&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

Error Snip :
{""response"":\{""errorCode"":""NCND00002"",""message"":""NCND00002: The Network Profile request has validation errors"",""detail"":""[\""NCND05051: Empty WLAN/Policy profile name: wireless.ssid = test_AAAtb3\""]""},""version"":""1.0""}
283730:  Traceback (most recent call last):
283731:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 295, in call_api
283732:  response.raise_for_status()
283733:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal_cel7/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status
283734:  raise HTTPError(http_error_msg, response=self)
283735:  requests.exceptions.HTTPError: 406 Client Error: Not Acceptable for url: [https://10.195.227.48/api/v1/siteprofile/61ade223-b458-45b5-8a14-ae45e2125ae1]
283736:  Encountered unhandled HTTPError in Internal API Call
 ",2022-05-11T14:35:20.722+0000,"Merged https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/02f1130e2802b0a38f718dc8ac324f80fdaa08eb issue is fixed now: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=854773&size=1052296&archive=env_auto_job.2022May27_01:22:46.130285.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 ","['Auton', 'Groot', 'Issue']",Tran Lam,Closed,Avril Bower
SEEN-200,https://miggbo.atlassian.net/browse/SEEN-200,[Auton] : Groot - Test_TC63_DNAC_verify_pim_igmp_config_on_fabric_devices ,"*TC*: [Test_TC63_DNAC_verify_pim_igmp_config_on_fabric_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2173573&size=1368902&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May10_15:38:38.271885.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /test2_verify_configuration_l3_instance_config

Snip:

api_switch_call called:

9405: \{'params': {'id': 'a6f99c8e-e364-443a-baa0-7e338f99cb45'}}

9406: Resource path full url: [https://10.195.227.48/api/v2/data/customer-facing-service/L2Segment] 9409: Traceback (most recent call last):

9410: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper

9411: result = testfunc(func_self, **kwargs)

9412: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 4936, in test2_verify_configuration_l3_instance_config

9413: if (dnac_handle.verify_lisp_l3instance_config_on_edges()):

9414: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/[decorators.py|https://decorators.py/]"", line 32, in wrapper

9415: result = method(*args, **kwargs)

9416: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/device_config_validation/[group.py|https://group.py/]"", line 253, in verify_lisp_l3instance_config_on_edges

9417: """""".format(seg[""gateway""],seg['mask'],seg[""dhcpSvrAddr""][0],vlanId,segment['name'])

9418: IndexError: list index out of range

9419: Test returned in 0:00:00.592353

9420: Errored reason: list index out of range

 

Fail log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2312413&size=69925&archive=env_auto_job.2022May10_15:38:38.271885.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2022-05-12T05:06:53.705+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2931/diff https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2931/overview,"['Auton', 'Ghost', 'Groot', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-201,https://miggbo.atlassian.net/browse/SEEN-201,[Auton]: Groot/Guardian:   Test_TC96_wired_app_policy,"This test case validation needs to be improved, It doesn't print which policy is missing for which interface.

Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=97217558&size=148152&archive=env_auto_job.2022May07_17:10:08.102726.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Error: 
197412:  Validation of wired application policy config failed for reason :: [""Class-map and policy-map configuration verification failed ::TB3-DM-NF-Switch reason :: ['policy-map prm-dscp#APIC_QOS_Q_OUT']"", 'failed to push service policy config service-policy output DNA-dscp#APIC_QOS_Q_OUT/service-policy input DNA-MARKING_IN/service-policy input DNA-APIC_QOS_IN on TB3-DM-NF-Switch each interfaces']
197413:  Test returned in 0:00:21.708735
197414:  Failed reason: Result : Failed to push policy on device
 ",2022-05-12T06:04:12.226+0000,"PR: 

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2920/overview Merged  Can you PR or cherry pick the fix to Guardian? This TC passed in recent Groot RC5 2.1.560.70513 run. Please find pass log
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=388479&size=464297&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep01_05:26:42.110536.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

 ","['Auton', 'Groot', 'Guardian', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-202,https://miggbo.atlassian.net/browse/SEEN-202,TB3: IXIA port 1/12 connecting extended node doesn't need auto-negotiate to be enabled,"In TB3, 1/12 .For extended node port, we should not set to auto-negotiate,

 
 Setting Port Media and AutoNegSpeed
 8413: RESULT: IXIA Successfuly connected to IXIA ports
 8414: Creating Topology Group 1/10
 8415: Configuring IPv4
 8416: Configuring IPv6
 8417: Creating Topology Group 1/11
 8418: Configuring IPv4
 8419: Configuring IPv6
 8420: Creating Topology Group 1/12.  >>>>>should be non-nego, default should be 100
 8421: Configuring IPv4
 8422: Configuring IPv6
 8423: Creating Topology Group 2/21
 8424: Configuring IPv4
 8425: Configuring IPv6
 8426: Creating Topology Group 1/9
 8427: Configuring IPv4
 8428: Configuring IPv6

 

*Fail Log from Groot:* 

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2121620&size=44309&archive=env_auto_job.2022May10_15:38:38.271885.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS",2022-05-12T07:28:17.900+0000,"Fixed the input, ","['Auton', 'Ghost', 'Groot', 'Issue']",Raju Saran,Resolved,Avril Bower
SEEN-204,https://miggbo.atlassian.net/browse/SEEN-204,Need RCA collection in background after every 50 Testcases in Sanity," 

We need the RCA for defects, if we collect the RCA after full run the logs are getting rolled over. so can we add the RCA collection in background after every 50 testcases in Sanity.

 ",2022-05-12T19:42:28.381+0000,Not enough time to finish,"['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Nethra Thorali Suthagar,Resolved,Avril Bower
SEEN-205,https://miggbo.atlassian.net/browse/SEEN-205,"Test_TC116_validate_http_traffic - looks like timing, we need to increase time to look fot FTP traffic","Testcase: Test_TC116_validate_http_traffic

Error:
26326:  api_switch_call called:
26327:  \{'params': {'startTime': 1652347953260, 'endTime': 1652350353418}}
26328:  Resource path full url: [https://10.195.227.48/api/assurance/v1/network-device/37ecb1af-b73f-4e0b-900d-fcd7c0a410eb/app-experience]
26331:  Device API Response \{'version': '1.0'}
26332:  Calculating If the required result found in the Response!!
26333:  Result Generated is []
26334:  Calculating If the required result found in the Response!!
26335:  Result Generated is []
26336:  Calculating If the required result found in the Response!!
26337:  Result Generated is []
26338:  video-over-http Traffic Not Found!!! on Device TB3-DM-eCA-BORDER.cisco.com
26339:  FTP Traffic Not Found!!! on Device TB3-DM-eCA-BORDER.cisco.com
26340:  Cient URL /assurance/v1/host/00:50:56:BD:DB:19/app-experience Params \{'startTime': 1652347953260, 'endTime': 1652350353418}
26341: 
 

On the cluster manually we could see the ""FTP Traffic Found"". Hope its needs more sleep/time to wait.

 

 

 ",2022-05-12T23:20:57.099+0000,"Production test is failing, need fix ASAP This is being tracked by https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-219

 ","['Auton', 'Ghost', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-211,https://miggbo.atlassian.net/browse/SEEN-211,[Auton]:Guardian- Test_TC63_DNAC_verify_pim_igmp_config_on_fabric_devices  /   test2_verify_configuration_l3_instance_config,"*Groot : 2.1.560.70272*

*Guardian:P1 -2.1.511.72050*

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC37_DNAC_verifying_configuration_lisp_on_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9106179&size=2782607&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F05%2F16%2F01%2F45%2Fenv_auto_job.2022May16_01:45:23.810617.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]/ [test1_verify_configuration_l3_instance_config|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10116972&size=76964&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F05%2F16%2F01%2F45%2Fenv_auto_job.2022May16_01:45:23.810617.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]
 [Test_TC63_DNAC_verify_pim_igmp_config_on_fabric_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1600292&size=2006500&archive=env_auto_job.2022May17_22:13:46.127614.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] /[test2_verify_configuration_l3_instance_config

Fail Log :
 [[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May17_22:13:46.127614.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]

 

Fail Log :
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10116972&size=76964&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F05%2F16%2F01%2F45%2Fenv_auto_job.2022May16_01:45:23.810617.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]
  
 Snip from Fail Log :
 -----------------------
 9085: Exception:
 9086: Traceback (most recent call last):
 9087: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 9088: result = testfunc(func_self, **kwargs)
 9089: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 4944, in test2_verify_configuration_l3_instance_config
 9090: if (dnac_handle.verify_lisp_l3instance_config_on_edges()):
 9091: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 9092: result = method(*args, **kwargs)
 9093: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/device_config_validation/group.py"", line 241, in verify_lisp_l3instance_config_on_edges
 9094: ip helper-address \{1}"""""".format(seg[""gateway""],seg[""dhcpSvrAddr""][0],segment['name'])
 9095: IndexError: list index out of range
  ",2022-05-18T12:38:48.720+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2931/diff Can we please approve and merge it ? https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2960/overview,"['Auton', 'Groot', 'Guardian', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-217,https://miggbo.atlassian.net/browse/SEEN-217,[Auton] Test_TC51_verify_post_upgrade_migration_status ,"[Test_TC51_verify_post_upgrade_migration_status|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27026514&size=33396&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F04%2F08%2F01%2F21%2Fenv_auto_job.2022Apr08_01:21:02.577171.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS] 

PoC Santhosh

Log: [Fail Log|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27026514&size=33396&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F04%2F08%2F01%2F21%2Fenv_auto_job.2022Apr08_01:21:02.577171.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

Error: 77832: api_switch_call called:
77833: {}
77834: Encountered unhandled HTTPError in Internal API Call
77835: Flagging result as FAIL!
77836: Reason: 404 Client Error: Not Found for url: [https://10.195.227.67/api/v1/dna/telemetry/provision/subscription/getMigrationStatus]
77837: Kwargs:
77838: {}
77839: Error Caught While Querying the Internal API
77840: Encountered unhandled HTTPError in group ""upgrade"" method ""post_upgrade_poe_optimization""!
77841: Flagging result as FAIL!
77842: Reason: 404 Client Error: Not Found for url: [https://10.195.227.67/api/v1/dna/telemetry/provision/subscription/getMigrationStatus]
77843: Args: (<services.dnaserv.lib.api_groups.upgrade.group.Group object at 0x7ff2fe637cc0>,)
77844: Kwargs:
77845: {}
77846: Traceback:
77847: File ""/opt/jenkins/workspace/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-Upgrade-Verify-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
77848: result = method(*args, **kwargs)
77849: File ""/opt/jenkins/workspace/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-Upgrade-Verify-common-Multi-job/services/dnaserv/lib/api_groups/upgrade/group.py"", line 792, in post_upgrade_poe_optimization

 ",2022-05-20T06:09:51.935+0000,"Fixed pulled to Guardian from Groot
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/15d83068dbbe9f2cda34d9ef43aac1154fee6fa3#services/dnaserv/lib/api_groups/upgrade/group.py
","['Auton', 'Guardian', 'Issue']",Pawan Singh,Closed,Avril Bower
SEEN-218,https://miggbo.atlassian.net/browse/SEEN-218,[Auton] : Guardian - Test_TC107_Compliance_verification  /   test9_verify_POE_compliance,"Guardian P1 Version : 2.1.511.72050
 Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

 

[Test_TC107_Compliance_verification|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=68412461&size=12492772&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May16_01:45:23.810617.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&atstype=PYATS]  /   test9_verify_POE_compliance

Fail Log :
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=80085845&size=130656&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F05%2F16%2F01%2F45%2Fenv_auto_job.2022May16_01:45:23.810617.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS] 
  

+*Description*+ :

'_Show power inline_' cli is not supported in  IE-4000-16GT4G-E Extended node.

 +
 *Snip from Log :*+
230268:  Resource path full url: [https://10.195.227.92/api/v1/file/253c506c-5263-4f4e-a23f-7830aab8950a]
230271:  [\{'deviceUuid': 'c7447288-8796-4086-9afa-1c052e58a40b', 'commandResponses': {'SUCCESS': {}, 'FAILURE': \{'show power inline': ""Error occurred while executing command : show power inline\nshow power inline\n ^\n% Invalid input detected at '^' marker.\n\nSN-FDO2453J12H#""}, 'BLACKLISTED': {}}}]
230272:  Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:04.248668
230273:  Device cli output:: Error occurred while executing command : show power inline
230274:  show power inline
230275:  ^
230276:  % Invalid input detected at '^' marker.
  ",2022-05-20T07:23:40.012+0000,"here is the PR for errored 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2935/overview]

 

Also I see that script is unable to conncted exteneded nodes from dnac to device via ssh. I have tried manually from cluster its giving time out issue as shown below.

From script:

 
show power inline\nError connecting to device [Host: 204.1.34.150:22] : Connection timed out: /204.1.34.150:22'}}}]
5317:  Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:02:12.435558
5318:  Device cli output:: Error occurred while executing command : show power inline
5319:  Error connecting to device [Host: 204.1.34.150:22] : Connection timed out: /204.1.34.150:22
 
From cluster manually:
 
[Mon Jun 06 13:08:09 UTC] maglev@172.21.236.201 (maglev-master-172-21-236-201) ~
$ ssh wlcaccess@204.1.34.150
FIPS mode initialized
ssh: connect to host 204.1.34.150 port 22: Connection timed out
 
Where in ssh is working for other devices from dnac 
 
$ ssh wlcaccess@204.1.2.5
FIPS mode initialized
Password: 
This Device is part of Solution Regression
 Please log off if you are not intended user
 Contact pawansi for further details


TB4-DM1-9600B1# PR has been raised https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2968/diff Can you also add the fix to Guardian? Merged to Groot and Guardian. Test failed with the latest Groot code.

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-inventory_fabric_wireless_compliance_verification.py-141-complianceVerification&begin=14070685&size=176213&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fsanity_TB1_cert.2022Jun28_22:26:47.925928.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS]

 

  Raju looks like you are not having the latest code as per the log. Could you please check the below code in ur files and confirm me. WIll re-open based on Raju confirmation

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2968/diff#services/dnaserv/lib/api_groups/Compilance/group.py]","['Auton', 'Groot', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-219,https://miggbo.atlassian.net/browse/SEEN-219,[Auton]:Guardian-Test_TC116_validate_http_traffic /test1_validate_http_traffic,"*Guardian- P1_:2.1.511.72050*

 

[Bug id :CSCwb44057|https://cdetsweb-prd.cisco.com/apps/dumpcr?content=summary&format=html&identifier=CSCwb44057]

As per  bug comments, they are removing Business Irrelevant and Default tabs in the Application experience section in Client 360, so we need to remove that wired client validation for Appx tc.

 

Testcases Impacted : 

[Test_TC116_validate_http_traffic/|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=81550827&size=87421&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F05%2F16%2F01%2F45%2Fenv_auto_job.2022May16_01:45:23.810617.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

[test1_validate_http_traffic|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=81550976&size=87105&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F05%2F16%2F01%2F45%2Fenv_auto_job.2022May16_01:45:23.810617.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Fail Log:

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=81550976&size=87105&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F05%2F16%2F01%2F45%2Fenv_auto_job.2022May16_01:45:23.810617.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS

 

 ",2022-05-20T10:31:54.564+0000,"Sanity Production test is failing, Pls fix this  Production test needs the fix ASAP Started working on this new change Any update on this ticket ? Not yet Raju will try to check as soon as possible and update you busy with other autons fixses and usecases.  here is the PR for Ghost:   

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4097/overview]

Groot:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4099/overview]

guardian:

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4101/overview we are moving close state,
Tracking below auton for this issue
https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-277
https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-437","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-221,https://miggbo.atlassian.net/browse/SEEN-221,[Auton] : Guardian : Test_TC97_wireless_policy_PSK  /  test4_deploy_wireless_policy_PSK,"Guardian Version : 2.1.510.70299
 Script Name :  solution_test_sanityecamb_lan.py 
Latest code pulled on 05/21

Testcases Impacted : 

 [Test_TC97_wireless_policy_PSK|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10432718&size=1129712&archive=%2Fhome%2Frajsaran%2Fpyats_cl-sr-ubuntu%2Fusers%2Frajsaran%2Farchive%2F22-05%2Fsanity_dmzTB2_lan.2022May17_01:53:32.865955.zip&ats=%2Fhome%2Frajsaran%2Fpyats_cl-sr-ubuntu&submitter=rajsaran&atstype=PYATS]  /   test4_deploy_wireless_policy_PSK

[Test_TC98_tagging_application|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11562430&size=696759&archive=%2Fhome%2Frajsaran%2Fpyats_cl-sr-ubuntu%2Fusers%2Frajsaran%2Farchive%2F22-05%2Fsanity_dmzTB2_lan.2022May17_01:53:32.865955.zip&ats=%2Fhome%2Frajsaran%2Fpyats_cl-sr-ubuntu&submitter=rajsaran&atstype=PYATS]  /   test2_wireless_policy  

 

 

Fail Log :
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11017994&size=354378&archive=sanity_dmzTB2_lan.2022May17_01:53:32.865955.zip&ats=%2Fhome%2Frajsaran%2Fpyats_cl-sr-ubuntu&submitter=rajsaran&from=trade&view=all&atstype=pyATS] 
  

+*Description*+ :

Single5KBanddmz_tb2 ssid is present, but still the tc fails with below error,

'failureReason': 'NCAS10412: Wireless profile policy was not found for the device 10.12.5.2 and SSID Single5KBanddmz_tb2. Make sure the Wireless Controller and the connected APs are provisioned.' 
  

 

TB2-DMZ-eWLC#show wlan summary

Number of WLANs: 9
{quote}
h6. ID Profile Name SSID Status Security
{quote}
1 CiscoSensorProvisioning CiscoSensorProvisioning UP [WPA2][802.1x][AES]

17 SSIDDUALBA_Global_F_eec96472 SSIDDUAL BANDdmz_tb2 UP [WPA2][802.1x][FT + 802.1x][AES],[FT Enabled]

18 posturedmz_Global_F_a1af1886 posturedmz_tb2 UP [WPA2][802.1x][AES]

20 Radius_ssi_Global_F_8000ee36 Radius_ssiddmz_tb2 UP [WPA2][802.1x][AES]

21 GUEST2dmz__Global_F_1f5cbe1c GUEST2dmz_tb2 UP [open],MAC Filtering

22 SSIDDot1XI_Global_F_34661ec8 SSIDDot1XIndiadmz_tb2 UP [WPA2][802.1x][AES]

_*23 Single5KBa_New Yo_F_2126f2e7 Single5KBanddmz_tb2 UP [WPA2][PSK][FT + PSK][AES],[FT Enabled],MAC Filtering*_

24 OPENdmz_tb_Global_F_887dc429 OPENdmz_tb2 UP [open]

25 GUESTdmz_t_Global_F_adac9dbd GUESTdmz_tb2 UP [open],MAC Filtering

TB2-DMZ-eWLC#

 ",2022-05-20T19:45:41.901+0000,"51746: File ""/home/rajsaran/guardian_sanityApiAuto/dnac-auto/services/dnaserv/lib/api_groups/app_policy/group.py"", line 423, in fileID_dev
 51747: fileid = x[""fileId""]
 51748: KeyError: 'fileId'
 51749: Test returned in 0:03:42.305125
 51750: Errored reason: fileId
  
 This is already fixed in latest code 
  
 here is the snip
  
 if dev1[0] == dev:
 if ""fileId"" in x:
 fileid = x[""fileId""]
 flag=1
 break
  

 

This is same as https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-130

 

Please check and let us know 
  ","['Auton', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-222,https://miggbo.atlassian.net/browse/SEEN-222,[Auton] : Groot - Test_TC22_lan_automation_bringup_devices  /   test8_site_verify_device_to_provisioned_lan_automation_inventory,"Groot Version : 2.1.560.70297

Script Name :  solution_test_sanityecamb_lan.py 

Latest code pulled on 05/18

Testcases Impacted :  [Test_TC22_lan_automation_bringup_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9191577&size=434608&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-05%2Fenv_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]  /   test8_site_verify_device_to_provisioned_lan_automation_inventory 

  

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9467324&size=5198&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Description : 

Testcase need to wait and retry until the device comes to Managed state.

Here, the testcase is failing in 00:00:01 seconds itself. 

 
 34007: Test returned in 0:00:00.325123
 34008: Failed reason: Result: All devices are not in Managed State San Jose
 34009: The result of section test8_site_verify_device_to_provisioned_lan_automation_inventory is => FAILED",2022-05-23T10:30:05.215+0000,"PR

[Pull Request #2920: Moe groot fixes - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2920/overview] Merged  Issue not observed anymore:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8448243&size=257368&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep08_18:51:44.693470.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Marking this ticket as ""Closed"".","['Auton', 'Groot', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-223,https://miggbo.atlassian.net/browse/SEEN-223,[Auton] : Groot - Need to handle the ID returned in the Stop LAN-A response,"Groot Version : 2.1.560.70297

Script Name :  solution_test_sanityecamb_lan.py 

Latest code pulled on 05/18

Testcases Impacted :  

[Test_TC22_lan_automation_bringup_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9191577&size=434608&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-05%2Fenv_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]  /   test9_stop_lan_automation

[Test_TC23_lan_automation_bringup_devices_on_level2|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9626185&size=315628&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-05%2Fenv_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]  /   test6_stop_lan_automation

 

Fail Log :

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9472522&size=72194&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

Description : 

In Groot, there is a behavioural changes w.r.t LAN-Automation status, where the LAN-A id is returned in the response. The testcase should be updated to handle it. 

This change was done as part of bug - [CSCwb29870 |https://cdetsng.cisco.com/webui/#view=CSCwb29870]

 
35508:  Resource path full url: [https://10.195.227.48/api/v1/lan-automation/f7c670fd-73d7-40f4-9b68-bc3138a6af47]
35511:  \{'response': {'message': 'Stopping LAN Automation Session. For more info check LAN Automation Status.', 'id': 'f7c670fd-73d7-40f4-9b68-bc3138a6af47'}, 'version': '1.0'}
 
35523:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/underlay_automation/group.py"", line 340, in stop_active_lan_automation
35524:  if response['response'].find(""Stopping LAN Automation for Id:"") != -1:
35525:  TypeError: 'NoneType' object is not callable
 ",2022-05-23T11:23:29.710+0000,"PR:

[Pull Request #2920: Moe groot fixes - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2920/overview] Approved the LAN Changes, Can we pls merge it? Merged  Verified the fix in latest code Pass log :

[test9_stop_lan_automation|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7591298&size=24775&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul13_20:10:03.695160.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Groot', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-224,https://miggbo.atlassian.net/browse/SEEN-224,[Auton] : Groot - TCC30_DNAC_Connectivity_domain_creation_in_dnac  /   test1_setup_SGT,"Groot Version : 2.1.560.70297

Script Name :  solution_test_sanityecamb_lan.py 

Latest code pulled on 05/18

Testcases Impacted :  

[TCC30_DNAC_Connectivity_domain_creation_in_dnac|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11526313&size=756159&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-05%2Fenv_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]  /   test1_setup_SGT

 

Fail Log :

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11526895&size=110648&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Description : 

 

In the log, I can see the 'scalable group deletion successful' message but the tc fails with error 'deletion not success'

 
43717:  ########################################################################################
43718:  #----SUCESSFULLY DELETED SCALABLE GROUP sgt_port_test_sg----#
43719:  ########################################################################################
43720:  Library group ""aca"" method ""delete_sg"" returned in 0:00:02.319789
43721:  SG deletion not success
43722:  Test returned in 0:00:02.549479
43723:  Failed reason: Fail
43724:  The result of section test1_setup_SGT is => FAILED",2022-05-23T12:52:45.584+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2989/overview We are seeing the issue in Guardian P1 as well. Please double commit the changes to Guardian branch.

 

Fail log : 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10988974&size=110472&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun13_16:06:09.092858.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

  https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2989/overview Pushed changes directly in groot guardian frey","['Auton', 'Groot', 'Guardian', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-225,https://miggbo.atlassian.net/browse/SEEN-225,[Auton] : Groot - Test_TC34_check_sgt_port_config,"Groot Version : 2.1.560.70297

Script Name :  solution_test_sanityecamb_lan.py 

Latest code pulled on 05/18

Testcases Impacted :  

[Test_TC34_check_sgt_port_config|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14443949&size=665551&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS] - [test1_setup_variables|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14444531&size=174389&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS], [test3_push_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14629708&size=105154&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS], [test4_check_edge_config_after_deploy|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14734862&size=10803&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS], [test8_check_edge_config_after_provision|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14905173&size=10855&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Fail Log :

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14443949&size=665551&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Description : 

 Scalable group is deleted in Testcase [TCC30_DNAC_Connectivity_domain_creation_in_dnac|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11526313&size=756159&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-05%2Fenv_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]  /   test1_setup_SGT. 

Looks like this tc failure is due to that,

 
54302:  Failed reason: Could not retrieve SGT
54303:  The result of section test1_setup_variables is => FAILED
 
 
54506:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/aca/group.py"", line 326, in get_sg_tag_by_name
54507:  raise Exception(e)
54508:  Exception: Response is None or No sg was found
54509:  The result of section test3_push_configs is => ERRORED
 ",2022-05-23T12:59:50.331+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2989/overview Same issue is seen with Guardian P1. Please double commit the changes to Guardian branch as well

 

Fail log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14383442&size=757190&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun13_16:06:09.092858.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2989/overview pushed changes directly in groot guardian frey Hi Andrew,

We are still seeing this issue in Guardian,
Script:*solution_test_sanityecamb_lan.py*
*Failed log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17215784&size=508443&archive=env_auto_job.2022Nov07_06:52:24.393708.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS Fix:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2989/overview]

 

Pulled to Groot/Guardian also.

INFO:api-group-aca:Successfully retrieved id f8feb0a2-1915-4a2c-ad87-06af0a2a7072

INFO:ats.aetest:Library group ""aca"" method ""get_sg_by_name"" returned in 0:00:00.145761

'f8feb0a2-1915-4a2c-ad87-06af0a2a7072'

>>>

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/272ea04068b8d0ff3fea9ca283d44369b8139ce9

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/74d41cf8d0068366f1dd2e82b6ddd56112e6c7c2]

  Guardian  2.1.518.72310

The latest release same TC failed for the same reason. We are running with the latest code.

Failed Log:

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8306342&size=492864&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb10_06:58:18.393868.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS

*Previous Build  Pass log:*

*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16333361&size=656614&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_auto_job.2023Jan31_19:05:11.121387.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS*]
  
   Apologies for late reply, do you have logs for tc30 for the last fail run? It looks like sg was not created in tc30. Looking from previous fail logs [TRADe v2 | Logs: env_auto_job (cisco.com)|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13567127&size=124342&archive=env_auto_job.2022Nov07_06:52:24.393708.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS], seems like even when the sg is deleted, later DNAC is giving error saying its still there. If it comes again, you can try manually adding sg to see if the error comes thru UI as well or not. Hi Team,

Ghost P2 RC1 2.3.5.4-70815

The latest release same TC failed for the same reason. We are running with the latest code.

Failed Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10659398&size=695963&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul31_23:48:48.764469.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10659398&size=695963&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul31_23:48:48.764469.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [ENG-SDN / dnac-auto / 5a5cd680c3d - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/5a5cd680c3da0b5cb8ee63bfb23a77c384f5bad8]

added small time delay after deleting before readding sgt, should resolve the issue","['Auton', 'Groot', 'Guardian', 'Issue']",Andrew Chen,Resolved,Avril Bower
SEEN-226,https://miggbo.atlassian.net/browse/SEEN-226,[Auton] : Groot - Test_TC97_wireless_policy_PSK  /   test6_verify_policy_config,"Groot Version : 2.1.560.70297

Script Name :  solution_test_sanityecamb_lan.py 

Latest code pulled on 05/18

Testcases Impacted :  

[Test_TC97_wireless_policy_PSK|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=66981507&size=501969&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-05%2Fenv_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]  /   test6_verify_policy_config  

 

Fail Log :

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=67356213&size=18273&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Description : 

Verify policy config after PSK override is failing with below error,
233821:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/app_policy/group.py"", line 526, in wlan_summary
233822:  if match.group(1) in hirerachy:
233823:  AttributeError: 'NoneType' object has no attribute 'group'
 
 ",2022-05-23T18:53:50.844+0000,"Committed the fix in Groot as well

 

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2968/diff#services/dnaserv/lib/api_groups/Compilance/group.py 

 ","['Auton', 'Groot', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-228,https://miggbo.atlassian.net/browse/SEEN-228,[Auton] : Groot - Test_TC133_verify_wlc_wlan_VNIDs  /   test1_verify_wlans_VNIDs_mapping   ,"Groot Version : 2.1.560.70297

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC133_verify_wlc_wlan_VNIDs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=95583857&size=509676&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-05%2Fenv_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]  /   test1_verify_wlans_VNIDs_mapping

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=95584445&size=508921&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

Error Snip :

 
297300:  SSID CWA_GUEST_SSID_AAAtb3 is not Found on device TB3-DM-WLC
297301:  Failed ON [\{'OPENtb3_TB3-DM-WLC': {'device': 'TB3-DM-WLC', 'GOT': 'WClients_nyc-WirelessVNFB', 'Expected': 'WSClients_nyc-WirelessVNFB'}}, \{'SSIDDot1XIndiatb3_TB3-DM-WLC': {'device': 'TB3-DM-WLC', 'GOT': 'WSClients_nyc-WirelessVNFB', 'Expected': 'WClients_nyc-WirelessVNFB'}}, \{'posturetb3_TB3-DM-WLC': {'device': 'TB3-DM-WLC', 'GOT': 'WSClients_nyc-WirelessVNFB', 'Expected': 'WClients_nyc-WirelessVNFB'}}, \{'Single5KBandtb3_TB3-DM-WLC': {'device': 'TB3-DM-WLC', 'GOT': 'WSClients_nyc-WirelessVNFB', 'Expected': 'WClients_nyc-WirelessVNFB'}}, \{'SSIDDUAL BANDtb3_TB3-DM-WLC': {'device': 'TB3-DM-WLC', 'GOT': 'WSClients_nyc-WirelessVNFB', 'Expected': 'WClients_nyc-WirelessVNFB'}}, \{'TB3-DM-WLC': 'SSID ARUBA_SSIDtb3 Not Found'}, \{'TB3-DM-WLC': 'SSID Guest_webpassthroughtb3 Not Found'}, \{'TB3-DM-WLC': 'SSID Guest_webauthinternaltb3 Not Found'}, \{'TB3-DM-WLC': 'SSID Guest_passthrough_inttb3 Not Found'}, \{'TB3-DM-WLC': 'SSID CWA_GUEST_SSID_AAAtb3 Not Found'}]
297302:  Test returned in 0:01:08.493752
297303:  Failed reason: Failed to verify Sensor SSIDs
297304:  The result of section test1_verify_wlans_VNIDs_mapping is => FAILED",2022-05-24T18:23:02.043+0000,"Merged to Groot

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/4b53153414bb9c91545f0c403bf1eb3cc1975a53]

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9c71dcb43d76bc4b8e441d348e7b8d1985eb5216 **Guardian* *ISO*:*2.1.511.72077

**Polarisis*:*17.8.1a
**Script*:*solution_test_sanityecamb.py
**Impacted Testcases:**Test_TC133_verify_wlc_wlan_VNIDs/test1_verify_wlans_VNIDs_mapping

**Failure log*:*

[test1_verify_wlans_VNIDs_mapping|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=95040165&size=514316&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul27_09:03:10.499995.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

**Snip from Failure Log*:*
312533:  SSID ARUBA_SSIDtb4 is not Found on device TB4-DM-WLC
312538:  SSID Guest_webpassthroughtb4 is not Found on device TB4-DM-WLC
312543:  SSID Guest_webauthinternaltb4 is not Found on device TB4-DM-WLC
312548:  SSID Guest_passthrough_inttb4 is not Found on device TB4-DM-WLC
312607:  SSID CWA_GUEST_SSID_AAAtb4 is not Found on device TB4-DM-WLC
312608:  Failed ON [\{'OPENtb4_TB4-DM-WLC': {'device': 'TB4-DM-WLC', 'GOT': 'WClients_nyc-WirelessVNFB', 'Expected': 'WSClients_nyc-WirelessVNFB'}}, \{'SSIDDot1XIndiatb4_TB4-DM-WLC': {'device': 'TB4-DM-WLC', 'GOT': 'WSClients_nyc-WirelessVNFB', 'Expected': 'WClients_nyc-WirelessVNFB'}}, \{'posturetb4_TB4-DM-WLC': {'device': 'TB4-DM-WLC', 'GOT': 'WSClients_nyc-WirelessVNFB', 'Expected': 'WClients_nyc-WirelessVNFB'}}, \{'Single5KBandtb4_TB4-DM-WLC': {'device': 'TB4-DM-WLC', 'GOT': 'WSClients_nyc-WirelessVNFB', 'Expected': 'WClients_nyc-WirelessVNFB'}}, \{'SSIDDUAL BANDtb4_TB4-DM-WLC': {'device': 'TB4-DM-WLC', 'GOT': 'WSClients_nyc-WirelessVNFB', 'Expected': 'WClients_nyc-WirelessVNFB'}}, \{'TB4-DM-WLC': 'SSID ARUBA_SSIDtb4 Not Found'}, \{'TB4-DM-WLC': 'SSID Guest_webpassthroughtb4 Not Found'}, \{'TB4-DM-WLC': 'SSID Guest_webauthinternaltb4 Not Found'}, \{'TB4-DM-WLC': 'SSID Guest_passthrough_inttb4 Not Found'}, \{'TB4-DM-WLC': 'SSID CWA_GUEST_SSID_AAAtb4 Not Found'}]
312610:  Failed reason: Failed to verify Sensor SSIDs Cherry pick above fix to Guardian. Verified

[test5_verify_wlans_VNIDs_mapping|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11230867&size=482519&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul14_12:46:52.407317.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Groot', 'Guardian', 'Issue']",Tran Lam,Closed,Avril Bower
SEEN-232,https://miggbo.atlassian.net/browse/SEEN-232,[Auton]:Frey-Test_TC23_lan_automation_bringup_devices/test10_verify_lan_automation_completed,"*Frey* *Patch RC 8****:2.1.461.72213*

""Test case  is checking for Level 2 device after Level1 completed""

Note: Sanity branch is upto date with the main branch

*Testcases Impacted :* 

[Test_TC23_lan_automation_bringup_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=329764&size=533664&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F05%2F25%2F12%2F15%2Fenv_auto_job.2022May25_12:15:27.205215.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS] /

[test10_verify_lan_automation_completed|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=713938&size=149317&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F05%2F25%2F12%2F15%2Fenv_auto_job.2022May25_12:15:27.205215.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]
  
 Fail Log:

[https://earms-trade.cisco.com/tradeui/logs/details?ats=%2Fusers%2Frvonmize%2Fpyats-inst&client=web&host=st-ucs-sanity.cisco.com&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May25_03:27:51.441393.zip
]

Snip from Fail Log :
-----------------------

[
  
 4082: Not all devices discovered yet,

4083: currently discovered:['TB2-DM-eCA-BORDER', 'TB2-DM-Transit', 'TB2-DM-WLC'],

4084: List of devices to be found:['TB2-DM-eCA-BORDER', 'TB2-DM-Transit', 'TB2-DM-NF-Switch', 'TB2-DM-WLC']

 ",2022-05-26T05:48:46.354+0000,"Closing the issue, for now, we will re-open if we get Frey","['Auton', 'Frey', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-233,https://miggbo.atlassian.net/browse/SEEN-233,[Auton]:Frey-Test_TC3_generate_dhcp_server_config_on_fusion/test3_ise_cleanup_guest,"*Frey* *Patch 1 RC 8**:2.1.461.72213*
Note: Sanity branch is upto date with the main branch



*Testcase Impacted:*
**

[Test_TC3_generate_dhcp_server_config_on_fusion|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1314327&size=1067573&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F05%2F24%2F14%2F03%2Fenv_auto_job.2022May24_14:03:20.266930.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]/ **

[test3_ise_cleanup_guest|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1995077&size=386643&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F05%2F24%2F14%2F03%2Fenv_auto_job.2022May24_14:03:20.266930.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

Fail Log:

[https://earms-trade.cisco.com/tradeui/logs/details?ats=%2Fusers%2Frvonmize%2Fpyats-inst&client=web&host=st-ucs-sanity.cisco.com&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May24_14:03:20.266930.zip]


Snip from Fail Log :
-----------------------



11957: Error Code: 500 URL:https://10.195.227.32:9060/ers/config/selfregportal/d14f325b-5e7d-4792-aba5-b2f42a6b0ed5 Data:\{'timeout': 60} Headers:\{'Content-Type': 'application/json', 'Accept': 'application/json', 'Authorization': 'Basic YWRtaW46TGFibGFiIzEyMw=='} Message:{

11958: ""ERSResponse"" : {

11959: ""operation"" : ""DELETE-delete-selfregportal"",

11960: ""messages"" : [ {

11961: ""title"" : ""Deleting SelfRegistartion Portal By ID(d14f325b-5e7d-4792-aba5-b2f42a6b0ed5) failed due to com.cisco.epm.edf2.exceptions.EDF2SQLException: ORA-02292: integrity constraint (CEPM.REF_PORTAL_FLOW_PORTAL_TEMP) violated - child record found\n"",

11962: ""type"" : ""ERROR"",

11963: ""code"" : ""CRUD operation exception""

11964: } ],

11965: ""link"" : {

11966: ""rel"" : ""related"",

11967: ""href"" : ""[https://10.195.227.32:9060/ers/config/selfregportal/d14f325b-5e7d-4792-aba5-b2f42a6b0ed5]"",

11968: ""type"" : ""application/xml""

11969: }

11970: }

11971: }

11972: Traceback (most recent call last):

11973: File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Frey/Frey-sanity-common-Multi-job/services/iseserv/client_manager.py"", line 252, in call_api

11974: response.raise_for_status()

11975: File ""/users/rvonmize/pyats-inst/lib/python3.7/site-packages/requests/models.py"", line 941, in raise_for_status

11976: raise HTTPError(http_error_msg, response=self)

{color:#bb0000}11977: requests.exceptions.HTTPError: 500 Server Error: for url: {color}[https://10.195.227.32:9060/ers/config/selfregportal/d14f325b-5e7d-4792-aba5-b2f42a6b0ed5]

11978: ###################################################

{color:#bb0000}11979: #!!!FAILED TO DELETE SELF REG PORTAL in ISE. ERROR 500 Server Error: for url: {color}[https://10.195.227.32:9060/ers/config/selfregportal/d14f325b-5e7d-4792-aba5-b2f42a6b0ed5----#]

11980: ###################################################

11981: Failed to clear guest self reg portal from ISE: GUESTPORTALNEW

 ",2022-05-26T05:59:36.680+0000,"Related:

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-354

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-268

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-233

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-182 Defect: [CSCwc44298 |https://cdetsng.cisco.com/webui/#view=CSCwc44298] Filed a defect, watch out for https://cdetsng.cisco.com/webui/#view=CSCwc44298","['Auton', 'Frey', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-235,https://miggbo.atlassian.net/browse/SEEN-235,[Auton] : Groot - Test_TC22_lan_automation_bringup_devices  /   test1_shut_anyextended_pen_devices,"Groot Version : 2.1.560.70328

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC22_lan_automation_bringup_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9158316&size=957596&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May25_20:30:17.246309.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_shut_anyextended_pen_devices

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9158886&size=101213&archive=env_auto_job.2022May25_20:30:17.246309.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

Description : 

Testcase is not handling yes/no prompt when shutting down the Extended nodes during LAN-A. Testcase shows Passed eventhough it failed

 
38049:  Action: shut interface from EXT/PEN device to avoid onb oarding or VLAN Negotiation..
38050:  connecting to device:SN-FDO2515JDSL
38051:  +++ SN-FDO2515JDSL logfile /home/admin/.pyats/runinfo/env_auto_job.2022May25_20:30:17.246309/SN-FDO2515JDSL-cli-1653540135.log +++
38052:  +++ Unicon plugin iosxe +++
Trying 10.30.0.71...
38054:  +++ connection to spawn: telnet 10.30.0.71 2044, id: 140346982794864 +++
38055:  connection to SN-FDO2515JDSL
Connected to 10.30.0.71. Escape character is '^]'. % Please answer 'yes' or 'no'. Would you like to enter the initial configuration dialog? [yes/no]: no  The enable secret is a password used to protect  access to privileged EXEC and configuration modes.  This password, after entered, becomes encrypted in  the configuration.  -------------------------------------------------  secret should be of minimum 10 characters and maximum 32 characters with  at least 1 upper case, 1 lower case, 1 digit and  should not contain [cisco]  -------------------------------------------------  Enter enable secret: ********* %Password strength validation failed

 ",2022-05-26T11:50:37.790+0000,"Script handling seems working fine. 

>> Would you like to enter the initial configuration dialog? [yes/no]: *{color:#00875a}no{color}*
 * Nothing to do from the script side, the enable password has to be changed to strong one ends with ""!""

example: ""*{color:#de350b}Cisco#123{color}*"" not working >> ""*{color:#00875a}Cisco#123!{color}*"" or ""{color:#00875a}*C1sco#123!*{color}"" works.

This enabled password only needs to be in this format for some cases after clean and reboot the ext node devices. But, ""Cisco#123"" is accepted after already in the config mode. Nothing to add from the script, script works as expected.

Please, file a defect for the enable password strength or change the enable password from the testbeds. Issue is not seen now","['Auton', 'Groot', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-236,https://miggbo.atlassian.net/browse/SEEN-236,[Auton] : Groot - Test_TC136_enable_ICMP_ping_check_AP_reachability  /   test1_enable_icmp_verify_ap_reachability,"Groot Version : 2.1.560.70297

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC136_enable_ICMP_ping_check_AP_reachability|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=106745122&size=420241&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-05%2Fenv_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]  /   test1_enable_icmp_verify_ap_reachability

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=106745710&size=133006&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

Description : 

Testcase is not handling prompt when executing the cli - config interface ap-manager management disable

 

(Cisco Controller) >
352601:  +++ Cisco Controller with via 'a': executing command 'config interface ap-manager management disable' +++
config interface ap-manager management disable Warning! You have no AP manager on this port. The controller behavior will be unpredictable. Disabling the AP-manager interface will reboot AP's connected on this interface. Are you sure you want to continue? (y/n)y (Cisco Controller) > (Cisco Controller) >
352604:  Traceback (most recent call last):
352605:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal_cel7/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 733, in call_service
352606:  self.result = self.get_service_result()
352607:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal_cel7/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 240, in get_service_result
352608:  raise SubCommandFailure(
352609:  unicon.core.errors.SubCommandFailure: ('sub_command failure, patterns matched in the output:', ['^[Ww]arning'], 'service result', ""config interface ap-manager management disable\r\r\n\r\r\n\r\r\nWarning! You have no AP manager on this port.\r\r\nThe controller behavior will be unpredictable.\r\r\nDisabling the AP-manager interface will reboot\r\r\nAP's connected on this interface.\r\r\nAre you sure you want to continue? (y/n) "")
352610: 
352611:  The above exception was the direct cause of the following exception:
 
 
353054: Failed for reason ['Unable to disable WLAN and shut management interface']",2022-05-26T12:49:12.440+0000,should be fixed with changes done via SEEN-1083,"['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity']",Andrew Chen,Closed,Avril Bower
SEEN-237,https://miggbo.atlassian.net/browse/SEEN-237,[Auton] : Groot - Test_TC138_system_health_assurance_checks  /   test1_verify_devices_categorized_health_no_health,"Groot Version : 2.1.560.70297

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC138_system_health_assurance_checks|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=107481165&size=119262&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-05%2Fenv_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]  /   test1_verify_devices_categorized_health_no_health 

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=107481753&size=13935&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Description : 

Total cdp entries displayed : 10 TB3-DM-NF-Switch#
357118:  [('TB3-DM-Fusion', 'Gig 1/0/24', 'I ', 'Ten 1/0/19'), ('SN-JAD23450NLQ.cisco.com', 'Ten 1/1/2', 'I ', 'Ten 1/1/2'), ('SN-JAD23450NLQ.cisco.com', 'Ten 1/1/1', 'I ', 'Ten 1/1/1'), ('TB3-DM-TSIM', 'Gig 1/0/13', 'H ', 'Gig 0/0/2'), ('TB3-DM-TSIM', 'Gig 1/0/14', 'H ', 'Gig 0/0/4'), ('SS-local-Dist', 'Gig 0/0', 'I ', 'Gig 1/0/32'), ('TB3-DM-Transit.cisco.com', 'Gig 1/0/6', 'I ', 'Gig 1/0/4')]
357119:  [('AP502f.a80f.2c80', 'Gig 1/0/7', 'T B I ', 'Gig 0'), ('AP3C41.0EFE.2180', 'Gig 1/0/37', 'R T ', 'Gig 0'), ('TB3-DM-TSIM', 'Gig 1/0/13', 'H ', 'Gig 0/0/2'), ('TB3-DM-TSIM', 'Gig 1/0/14', 'H ', 'Gig 0/0/4'), ('AP7488.BBFD.D8FE', 'Gig 1/0/25', 'R T ', 'Gig 0')]
357120:  []
357121:  []
357122:  Found Following CDP Entries:
357123:  Traceback (most recent call last):
357124:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
357125:  result = testfunc(func_self, **kwargs)
357126:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 9541, in test1_verify_devices_categorized_health_no_health
357127:  if dnac_handle.verify_devices_categorized_health_no_health():
357128:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 6034, in verify_devices_categorized_health_no_health
357129:  if self.services.dnaconfig.testbed.devices[ap].role == ""AP"":
357130:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal_cel7/lib/python3.8/site-packages/pyats/topology/bases.py"", line 100, in __getitem__
357131:  return super().__getitem__(key)
357132:  KeyError: 'AP502f.a80f.2c80'
 ",2022-05-26T13:40:07.559+0000,,"['Auton', 'Groot', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-238,https://miggbo.atlassian.net/browse/SEEN-238,[Auton]:Guardian -Test_TC107_Compliance_verification / test9_verify_POE_compliance,"Guardian_P.1-2.1.511.72077-(2.3.3.1)

*Testcases Impacted :* [Test_TC107_Compliance_verification|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19599086&size=14681856&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May25_08:10:01.512971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] / test9_verify_POE_compliance

Fail Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=33472960&size=129138&archive=env_auto_job.2022May25_08:10:01.512971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Snip from Fail Log :
-----------------------



78377:  Exception:
78378:  Traceback (most recent call last):
78379:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
78380:  result = testfunc(func_self, **kwargs)
78381:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 8611, in test9_verify_POE_compliance
78382:  if dnac_handle.POE_compliance():
78383:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/dnaserv/lib/api_groups/Compilance/group.py"", line 770, in POE_compliance
78384:  power_avail,power_used,power_remain=round(float(match.group(2))),round(float(match.group(5))),round(float(match.group(8)))
78385:  AttributeError: 'NoneType' object has no attribute 'group'",2022-05-26T15:41:39.240+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2935/overview https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2935/overview,"['Auton', 'Groot', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-239,https://miggbo.atlassian.net/browse/SEEN-239,[Auton]:Guardian -Test_TC109_DNAC_maps / test3_import_Ekahau_file,"Guardian_P.1-2.1.511.72077-(2.3.3.1)



*Testcases Impacted :*
[Test_TC109_DNAC_maps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=34557919&size=22427&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May25_08:10:01.512971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] / test3_import_Ekahau_file

Fail Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=34575866&size=4329&archive=env_auto_job.2022May25_08:10:01.512971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Snip from Fail Log :
-----------------------


83133:  Traceback (most recent call last):
83134:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
83135:  result = testfunc(func_self, **kwargs)
83136:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 8720, in test3_import_Ekahau_file
83137:  context_id = dnac_handle.ekahau_archives_import_async(file_path=file_path, site=site)
83138:  TypeError: ekahau_archives_import_async() missing 1 required positional argument: 'error_messages'
83139:  Test returned in 0:00:00.002093
83140:  Errored reason: ekahau_archives_import_async() missing 1 required positional argument: 'error_messages'",2022-05-26T15:53:23.242+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c8bcdf86c72218a274734c87c5c3028b23768b0e]

Added to main scripts in sanity.

Also, update fabric file for map files as below;

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a571be66e84237289bc063dbb8b6f13017c26949#configs/sanity_tb1/solution_sanityeca_SanityTB1.json","['Auton', 'Guardian', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-240,https://miggbo.atlassian.net/browse/SEEN-240,[Auton] : Groot - TC143_system_health_test,"Groot Version : 2.1.560.70297

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[TC143_system_health_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=110178756&size=273224&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-05%2Fenv_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]  /   test2_add_cimc_info

[TC143_system_health_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=110178756&size=273224&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-05%2Fenv_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]  /   test3_generate_ISE_certificate 

 

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=110178756&size=273224&archive=env_auto_job.2022May17_04:39:06.978844.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

Description : 

test2_add_cimc_info - Adding CIMC is failing with below error,

 
359527:  api_switch_call called:
359528:  \{'data': {'key': 'c3e2a6fd-2132-45ee-b7c4-6f0b1e9b25c2', 'dnacaddress': '10.195.227.48', 'cimcaddress': '10.195.227.47', 'password': 'Lablab123', 'username': 'admin'}}
359529:  Resource path full url: [https://10.195.227.48/api/v1/diagnostics/cimc/details/c3e2a6fd-2132-45ee-b7c4-6f0b1e9b25c2]
 
359553:  File ""/auto/pysw/cel7x/python64/3.8.2/lib/python3.8/ssl.py"", line 1241, in recv_into
359554:  return self.read(nbytes, buffer)
359555:  File ""/auto/pysw/cel7x/python64/3.8.2/lib/python3.8/ssl.py"", line 1099, in read
359556:  return self._sslobj.read(len, buffer)
359557:  socket.timeout: The read operation timed out
359558: 
359559:  During handling of the above exception, another exception occurred:
 
 
Generate ISE certificate tc is failing with below error,

359736:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal_cel7/lib/python3.8/site-packages/uniq_adapter/base.py"", line 4, in <module>
359737:  from uniq.common.base import Base as UniqBase
359738:  ModuleNotFoundError: No module named 'uniq.common'",2022-05-26T19:11:36.289+0000,"Checking with Nethra for uniq version in env Added timeout - 
|[)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/users/rmukkama]|[f35fc41973b|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f35fc41973b765204dbb97b7ac43ed3b68059e0e]|","['Auton', 'Groot', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-256,https://miggbo.atlassian.net/browse/SEEN-256,[Auton] : Guardian - Test_TC123_aaa_per_ssid  /   test5_connect_clients_to_ssids,"Guardian Patch.1 Version : 2.1.511.72077-(2.3.3.1)

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC123_aaa_per_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=854215&size=3006065&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May27_01:22:46.130285.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test5_connect_clients_to_ssids

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3826006&size=33208&archive=env_auto_job.2022May27_01:22:46.130285.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

Description : 

 

When connecting to test_AAAtb6 ssid, Authentication is failed,

 

2022-05-27T01:43:28: %SERVICES-INFO: #!!!Client TB6-wireless-client1: Flapped interface LAB-NET-Wireless1!!!#
2022-05-27T01:43:28: %SERVICES-INFO: #############################################
2022-05-27T01:43:48: %SERVICES-INFO: ---Connect wireless clients TB6-wireless-client1 to ssid test_AAAtb6 with 1 retries
2022-05-27T01:43:48: %SERVICES-INFO: ####\{'cmd': 'pythonw C:/ssc_auto/ssc.py config --ssid=test_AAAtb6 --assoc=wpa2 --crypt=aes --ident=aaa_test_user --user=aaa_test_user --pass=aaa_test_pass --nostop'}
2022-05-27T01:43:48: %SERVICES-INFO: ###STart Init Client
2022-05-27T01:43:48: %SERVICES-INFO: ###STart invoke
return output:{'output': '08:34:13.533 OK: Cisco SSC Service Stopped Succesfully\r\n08:34:15.421 OK: Cisco SSC Service Started Succesfully\r\n08:34:15.686 LOG: Starting wifi connection, trying ssid test_AAAtb6 ...\r\n08:34:15.686 LOG: Connection Association Started(wpa2EnterpriseAes)\r\n08:34:20.944 LOG: Network test_AAAtb6: The association timer expired\r\n08:34:20.990 LOG: Starting wifi connection, trying ssid test_AAAtb6 ...\r\n08:34:20.990 LOG: Connection Association Started(wpa2EnterpriseAes)\r\n08:34:26.964 LOG: Network test_AAAtb6: The association timer expired\r\n08:34:27.012 LOG: Starting wifi connection, trying ssid test_AAAtb6 ...\r\n08:34:27.012 LOG: Connection Association Started(wpa2EnterpriseAes)\r\n08:34:27.089 LOG: Connection Authentication Started\r\n08:34:27.089 LOG: Associated to ssid = test_AAAtb6 bssid = 78:bc:1a:88:cf:48.\r\n08:34:29.960 LOG: EAP: Identity requested\r\n08:34:29.976 LOG: Sending unprotected identity = aaa_test_user.\r\n08:34:30.023 LOG: Authentication Failed\r\n08:34:37.635 LOG: Connection Authentication Started\r\n08:34:37.635 LOG: Roaming to ssid = test_AAAtb6 bssid = 78:bc:1a:88:cf:48.\r\n08:34:37.713 LOG: EAP: Identity requested\r\n08:34:37.713 LOG: Sending unprotected identity = aaa_test_user.\r\n08:34:37.729 LOG: *Authentication Failed\r\n08:34:45.013* LOG: Connection Authentication Started\r\n08:34:45.013 LOG: Roaming to ssid = test_AAAtb6 bssid = 78:bc:1a:88:cf:48.\r\n08:34:45.061 LOG: EAP: Identity requested\r\n08:34:45.061 LOG: Sending unprotected identity = aaa_test_user.\r\n08:34:52.378 LOG: Connection Authentication Started\r\n08:34:52.378 LOG: Associated to ssid = test_AAAtb6 bssid = 78:bc:1a:88:cf:48.\r\n08:34:52.517 LOG: EAP: Identity requested\r\n08:34:52.517 LOG: Sending unprotected identity = aaa_test_user.\r\n08:34:52.782 LOG: Authentication Failed\r\n08:35:18.819",2022-05-27T09:03:24.456+0000,"Resolved by Raji:

Refer: https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-573

 ","['Auton', 'Groot', 'Guardian', 'Issue', 'Sanity']",Moe Saeed,Resolved,Avril Bower
SEEN-257,https://miggbo.atlassian.net/browse/SEEN-257,[Auton] : Guardian - Test_TC132_verify_border_edge_kpi  /   test1_verify_border_edge_kpi,"Guardian Patch.1 Version : 2.1.511.72077-(2.3.3.1)

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC132_verify_border_edge_kpi|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=51554282&size=85877&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May25_08:10:01.512971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_border_edge_kpi

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=51554858&size=85137&archive=env_auto_job.2022May25_08:10:01.512971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

Description : 

For Edge device KPI is failing. For other devices, the same overallscore, tcpconnscore was shown but its not failing for those. 

 
122605:  Fabric Control Plane Scores or ISE are not as Expected!! on TB6-SJ-EDGE \{'modificationtime': '1653515700000', 'overallScore': '10', 'tcpConnScore': '-1', 'time': '2022-05-25T21:55:00.000+0000', 'enIseConnScore': '10'}
 
This test keeps failing after we added Edge device to the topology. We dont have a wiki page for this feature to check / debug more",2022-05-27T14:38:16.789+0000,"The production test is failing  https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/877b0bdf5a024ad223dae77b848b399c398f652e Raji, Can you please add this fix in shockwave too ?","['Auton', 'Guardian', 'Issue', 'Shockwave']",Raji Mukkamala,Closed,Avril Bower
SEEN-258,https://miggbo.atlassian.net/browse/SEEN-258,[Auton] : Guardian - Test_TC133_verify_wlc_wlan_VNIDs  /   test1_verify_wlans_VNIDs_mapping,"Guardian Patch.1 Version : 2.1.511.72077-(2.3.3.1)

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC133_verify_wlc_wlan_VNIDs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=51640159&size=202780&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May25_08:10:01.512971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_wlans_VNIDs_mapping 

 

Fail Log :

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=51640735&size=202040&archive=env_auto_job.2022May25_08:10:01.512971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Description : 
123874:  Exception:
123875:  Traceback (most recent call last):
123876:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
123877:  result = testfunc(func_self, **kwargs)
123878:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 9452, in test1_verify_wlans_VNIDs_mapping
123879:  if (dnac_handle.verify_sensor_provisioning()):
123880:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/dnaserv/lib/api_groups/cli_check/group.py"", line 2499, in verify_sensor_provisioning
123881:  aaa_input_fabric = self.services.dnaconfig.testbed.custom[self.services.dnaconfig.sid]['clear_pass']
123882:  KeyError: 'clear_pass'
123883:  The result of section test1_verify_wlans_VNIDs_mapping is => ERRORED
 ",2022-05-27T14:45:52.831+0000,"PR has been raised 

 

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2935/overview Merged","['Auton', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-259,https://miggbo.atlassian.net/browse/SEEN-259,[Auton] : Groot - TC143_system_health_test  /   test3_generate_ISE_certificate,"Guardian Patch.1 Version : 2.1.511.72077-(2.3.3.1)

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[TC143_system_health_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=63977042&size=274590&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May25_08:10:01.512971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test3_generate_ISE_certificate

 

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=63993320&size=15430&archive=env_auto_job.2022May25_08:10:01.512971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

Description : 
183570:  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
183571:  File ""<frozen importlib._bootstrap_external>"", line 783, in exec_module
183572:  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
183573:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/ise3/ise_ui.py"", line 17, in <module>
183574:  from services.commonlibs.ui.uniq_ui_handler import UniqDriver
183575:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/commonlibs/ui/uniq_ui_handler.py"", line 8, in <module>
183576:  from uniq_adapter import Base
183577:  ModuleNotFoundError: No module named 'uniq_adapter'
 ",2022-05-27T15:04:06.887+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f5bd646909f7f8ae34ba08464d91264aba97ce7f]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a5737f2c2b1f417b2ccb06d55980c6e5e4b0ab1f]

Fixed in Guardian and Groot [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a5737f2c2b1f417b2ccb06d55980c6e5e4b0ab1f]

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f5bd646909f7f8ae34ba08464d91264aba97ce7f","['Auton', 'Guardian', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-260,https://miggbo.atlassian.net/browse/SEEN-260,[Auton] : TC151_verify_fabric_360  /   test1_verify_fabric_SD_access,"Guardian Patch.1 Version : 2.1.511.72077-(2.3.3.1)

Groot Version : 2.1.560.70297

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[TC151_verify_fabric_360|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=66594973&size=161747&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May25_08:10:01.512971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_fabric_SD_access 

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=66595549&size=161017&archive=env_auto_job.2022May25_08:10:01.512971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

Description : 

 The device count in Assurance-->SD-Access page and Fabric Sites is matching, but the test is failing still, (Attached screenshots)

 
193316:  Fabric site 'Global/USA/SAN-FRANCISCO' has dont have proper devices count:0 to verify paramaters
193317:  ------------------------------------------------------------------------------------------------------------------------------------------------------
193318:  ******************************************************************************************************************************************************
193319:  prasented Fabric nodes devices are Invalid
193320:  ******************************************************************************************************************************************************
193321:  Test returned in 0:00:02.167295
193322:  Failed reason: Unable to verify_fabric_SD_access parameters
193323:  The result of section test1_verify_fabric_SD_access is => FAILED
 ",2022-05-27T15:34:20.099+0000,"Hi [~603e0198678612006b9f8e30] , it looks like the Assurance -> Health -> SD access counts do not match the count on the Fabric Sites page. Are these counts supposed to be matching in order for this testcase to pass?

!image-2022-06-24-16-14-54-790.png!

!image-2022-06-24-16-15-53-218.png! Update: Device counts are matching now.

!image-2022-06-30-10-26-48-053.png!

!image-2022-06-30-10-27-15-842.png! This test is failing in shockwave too, Need the fix for this test.

 
||[TC151_verify_fabric_360|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=84319529&size=116256&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F03%2F20%2F56%2Fenv_auto_job.2022Jul03_20:56:37.829092.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fhome%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FShockwave%2FShockwave-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=9389&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F03%2F20%2F56%2Fenv_auto_job.2022Jul03_20:56:37.829092.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&from=trade&view=all]|Errored|00:00:05|TC151_verify_fabric_360| |
||[test1_get_fabric_details|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=84319669&size=10932&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F03%2F20%2F56%2Fenv_auto_job.2022Jul03_20:56:37.829092.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fhome%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FShockwave%2FShockwave-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=9390&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F03%2F20%2F56%2Fenv_auto_job.2022Jul03_20:56:37.829092.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&from=trade&view=all]|Passed|00:00:00|test1_get_fabric_details| |
||[test2_verify_fabric_site_health_timeline|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=84330601&size=25602&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F03%2F20%2F56%2Fenv_auto_job.2022Jul03_20:56:37.829092.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fhome%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FShockwave%2FShockwave-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=9401&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F03%2F20%2F56%2Fenv_auto_job.2022Jul03_20:56:37.829092.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&from=trade&view=all]|Errored|00:00:02|test2_verify_fabric_site_health_timeline| |
||[test3_verify_fabric_site_health|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=84356203&size=25526&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F03%2F20%2F56%2Fenv_auto_job.2022Jul03_20:56:37.829092.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fhome%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FShockwave%2FShockwave-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=9411&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F03%2F20%2F56%2Fenv_auto_job.2022Jul03_20:56:37.829092.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&from=trade&view=all]|Errored|00:00:01|test3_verify_fabric_site_health| |
||[test4_verify_fabric_nodes|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=84381729&size=25467&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F03%2F20%2F56%2Fenv_auto_job.2022Jul03_20:56:37.829092.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fhome%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FShockwave%2FShockwave-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=9421&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F03%2F20%2F56%2Fenv_auto_job.2022Jul03_20:56:37.829092.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&from=trade&view=all]|Errored|00:00:01|test4_verify_fabric_nodes| |
||[test5_verify_fabric_assurance_metric|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=84407196&size=28430&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F03%2F20%2F56%2Fenv_auto_job.2022Jul03_20:56:37.829092.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fhome%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FShockwave%2FShockwave-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=9431&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F03%2F20%2F56%2Fenv_auto_job.2022Jul03_20:56:37.829092.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&from=trade&view=all]|Errored|00:00:01|test5_verify_fabric_assurance_metric
 
|

https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul03_20:56:37.829092.zip&reqseq=&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=PYATS This test has a long history and multiple defects. Let's follow the following approaches to handle this.

 

1) Compare the devices summary count from sd-access page with assurance sd-access page 

2) Compare all the devices from sd-access page with assurance sd-access page and print the devices which has mismatch

3) Compare the devices role for each device btw sd-access page with assurance sd-access page

4) The assurance sd-access page devices health should be 7 or above.

 

Thanks,

Raju 

 

 ","['Auton', 'Groot', 'Guardian', 'Issue', 'Shockwave']",Avril Bower,Closed,Avril Bower
SEEN-264,https://miggbo.atlassian.net/browse/SEEN-264,[Auton] : Groot - Test_TC42_ipphone_onboarding_verifications  /   test1_extnode_ap_static_onboarding_verifications,"Groot Version : 2.1.560.70377

Script Name :  solution_test_sanityecamb_lan.py 

Latest code pulled on 05/18

Testcases Impacted :  

[Test_TC42_ipphone_onboarding_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27434833&size=445038&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May30_12:22:14.725117.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_extnode_ap_static_onboarding_verifications

 

Fail Log :

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27435406&size=279093&archive=env_auto_job.2022May30_12:22:14.725117.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Description : 

IP Phone onboarding is failing with below error, but the Tc passed still.

 
106822:  The Schedduled Job failed: with reason \{'id': '9e9ccde6-4b70-4e50-a0bb-fa9482c4f28e', 'triggeredJobTaskId': 'd61c27da-3468-47aa-8b2e-1408bf04af03', 'triggeredTime': 1653951849398, 'status': 'FAILED', 'failureReason': ""NCSP11051: Error occurred while processing the 'modify' request. A different version of the same user intent already exists in the database. Additional info for support: taskId: 'd61c27da-3468-47aa-8b2e-1408bf04af03'. Name: 'TB7-NY-FIAB'. Incoming resourceVersion: '7'. resourceVersion in the database: '8'."", 'triggeredJobId': '9e9ccde6-4b70-4e50-a0bb-fa9482c4f28e'}

  
  ",2022-06-01T17:41:02.722+0000,"Hey [~603e0198678612006b9f8e30],

Did the device IP Phone got onboarded or not?  No issues in the script neither in the logs observed, Nethra might check manually the next run if the device did not get onboarded and reopen it accordingly. ","['Auton', 'Groot', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-266,https://miggbo.atlassian.net/browse/SEEN-266,[Auton] : Groot - TC143_system_health_test  /   test1_check_system_health_prior,"Groot Version : 2.1.560.70297

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[TC143_system_health_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=101203377&size=154740&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May30_12:22:14.725117.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_check_system_health_prior

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=101203950&size=12047&archive=env_auto_job.2022May30_12:22:14.725117.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

Error Snip :

Looks like the description is changed to - 'description': 'Missing Configuration: Cisco IMC is not configured'

 

TC is checking for description - ""Missing Configuration: CIMC""

if ""Missing Configuration: CIMC"" in event['description']:
 found_cimc_warning = True

 ",2022-06-03T16:15:35.721+0000,"[975013ca009|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/975013ca0099ce135051e360317963a094aeed27]

[ea69452915f|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ea69452915f59da160447bc513ba1235142f763b]","['Auton', 'Groot', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-268,https://miggbo.atlassian.net/browse/SEEN-268,Clearing of ISE failed in TC3,"ISE cleanup is failing in ISE 3.0



12808:  ###################################################
12809:  #!!!FAILED TO DELETE SELF REG PORTAL in ISE. ERROR 500 Server Error: for url: [https://10.195.227.32:9060/ers/config/selfregportal/dbefbb2f-02c1-4b2c-939d-6152a93aea1b----#]
12810:  ###################################################
12811:  Failed to clear guest self reg portal from ISE: GUESTPORTAL
12838:  ###################################################
12839:  #!!!FAILED TO DELETE SELF REG PORTAL in ISE. ERROR 500 Server Error: for url: [https://10.195.227.32:9060/ers/config/selfregportal/d84010ad-5a62-4560-80a2-40c16ea9005c----#]
12840:  ###################################################
12841:  Failed to clear guest self reg portal from ISE: GUESTPORTAL2
12903:  Failed reason: Clearing of ISE failed!
 
 

Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1305959&size=379172&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F06%2F04%2F13%2F42%2Fenv_auto_job.2022Jun04_13:42:53.825744.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]

 

 ",2022-06-06T20:23:25.182+0000,"Related:

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-354

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-268

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-233

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-182 Defect id:

https://cdetsng.cisco.com/webui/#view=CSCwc44298 Please follow up with defect, will close this issue for now, Thanks.","['Auton', 'Issue', 'Shockwave']",Moe Saeed,Closed,Avril Bower
SEEN-269,https://miggbo.atlassian.net/browse/SEEN-269,[Groot-Auton] Test_TC29_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision - test2_update_vlan_membership," 
This test is fixed in Groot(https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-195), The same failure in Shockwave please fix this in shockwave too: 

 

 

 
34646:  Encountered unhandled HTTPError in Internal API Call
34647:  Flagging result as FAIL!
34648:  Reason: 500 Server Error: for url: [https://10.195.227.31/api/v1/interface/ab5449f9-bcb4-4e23-bdfd-2e25b221b022?deploymentMode=Deploy]
34649:  Kwargs:
34650:  \{'data': {'vlanId': '30'},
34651:  'method': 'PUT',
34652:  'params': \{'deploymentMode': 'Deploy'},
34653:  'resource_path': '/v1/interface/ab5449f9-bcb4-4e23-bdfd-2e25b221b022'}
34655:  Encountered unhandled HTTPError in group ""inventory"" method ""Update_vlan_membership""!
34656:  Flagging result as FAIL!
34657:  Reason: 500 Server Error: for url: [https://10.195.227.31/api/v1/interface/ab5449f9-bcb4-4e23-bdfd-2e25b221b022?deploymentMode=Deploy]
34658:  Args: (<services.dnaserv.lib.api_groups.inventory.group.Group object at 0x7ff13885de48>,)
34659:  Kwargs:
34660:  {}
34661:  Traceback:
34662:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Shockwave-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
34663:  result = method(*args, **kwargs)
34664:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Shockwave-sanity-common-Multi-job/services/dnaserv/lib/api_groups/inventory/group.py"", line 696, in Update_vlan_membership
34665:  update_response = self.services.api_switch_call(method=""PUT"", resource_path=url, data=payload,params=params)
34666:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Shockwave-sanity-common-Multi-job/services/dnaserv/lib/api_groups/utils/group.py"", line 43, in api_switch_call
34667:  response = self.services.base.NB_API.call_api(**kwargs)
34668:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Shockwave-sanity-common-Multi-job/services/dna_ApicemClientManager/ApicemClientManager.py"", line 552, in call_api
34669:  **kwargs)
34670:  File ""/users/pawansi/pyatsnew/lib/python3.4/site-package
 


https://earms-trade.cisco.com/tradeui/logs/details?ats=%2Fusers%2Fpawansi%2Fpyatsnew&client=web&host=st-ucs-sanity.cisco.com&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun04_13:42:53.825744.zip",2022-06-06T20:24:55.072+0000,"Issue still seen with Guardian P1 

 

Fail log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10469743&size=518661&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun13_16:06:09.092858.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Snip from log :
41495:  \{'data': {'vlanId': '30'}, 'params': \{'deploymentMode': 'Deploy'}}
41496:  Error Caught While Querying the Internal API
41497:  Encountered unhandled HTTPError in group ""inventory"" method ""Update_vlan_membership""!
41498:  Flagging result as FAIL!
41499:  Reason: 500 Server Error: for url: [https://10.195.227.48/api/v1/interface/12bf3e3a-dd84-47ac-819d-6eed4df5934a?deploymentMode=Deploy] This test failed in shockwave too with the Sanity LAN script on TB7.

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1596541&size=162966&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F03%2F20%2F56%2Fenv_auto_job.2022Jul03_20:56:37.829092.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

  Notes for this issue (so I dont forget):

Seems there are (at least) 3 issues with this case. Firstly there is the issue that there are no ethernet vlans to switch to, which has been fixed in groot (this is the issue in shockwave tb7, needs to be pulled to shockwave). Then there is the issue where the role of the device is not ""access"", this can be fixed by either adding the access role or only choosing devices which already have the access role, need to decide how to fix (This is the issue on guardian patch 1 for tb3). Thirdly there is an unknown issue on tb2, which needs further debugging, as there is vlan 30 on the device which is an ethernet vlan, and the device is access role. Issue seen with Groot  2.1.560.70410 

Fail log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12851931&size=148363&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun29_20:54:25.908420.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Snip from fail log:
 54242: api_switch_call called:
 54243: {'data':

{'vlanId': '30'}

, 'params': \{'deploymentMode': 'Deploy'}}
 54244: Resource path full url: [https://10.195.227.48/api/v1/interface/105c8d37-771c-487d-88d3-6ced3f79dd4d]
 54245: Error Code: 500 URL:[https://10.195.227.48/api/v1/interface/105c8d37-771c-487d-88d3-6ced3f79dd4d] Data:{'timeout': 30, 'data': '

{""vlanId"": ""30""}

', 'params': \{'deploymentMode': 'Deploy'}} Headers:\{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2MmJjZjcyOTI5Mjc5ZTM1MWEwZWQ5MDMiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjYyYmNmNzI4MjkyNzllMzUxYTBlZDkwMiJdLCJ0ZW5hbnRJZCI6IjYyYmNmNzI3MjkyNzllMzUxYTBlZDkwMCIsImV4cCI6MTY1NjU3MjI3NSwiaWF0IjoxNjU2NTY4Njc1LCJqdGkiOiJiODAzMjgwYi1jZWJiLTRhZGItOGY5ZC00M2VlNGNkYWVlOTQiLCJ1c2VybmFtZSI6ImFkbWluIn0.bl2ZINhK2RCG2ToFGiUz4-03hcGN-uuexpz7DzbYGtQ8LvXalWv1Rm62mDQtZd-LyI99h_bQFSbbAiPH5fTEtQR85VxYz-NNdBIayXxaXpKHPak3JOVgQrEShXEUlRI7B3JqgBy6n3UMHy3rptE577XcUSDiZs6Y4EexP6cCtGe6jPb5GAGxJMoz9fqNiZcZe43EcSVEd1xcE9rrdBR4OecdcZz9xDKMA41iuGmJVFjGIrYuFs9Lu15zBxjpvQo7NeE5De8h8AQhFNaxWIbMQvRDMlE0fCeviqh9MdlSzn8Ak8b3oN1xTqB6DkugolAHUmzHJPChjQIc0bUDwmQsPQ;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:{""response"":

{""errorCode"":""Unexpected error"",""message"":""Unexpected error"",""detail"":""Port actions are only supported on devices in Access role""}

,""version"":""1.0""} [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3363/overview]

 

Groot pr for using only access role devices.

Tested on tb3: https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=332723&size=231955&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3.2022Aug04_14:35:13.843994.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3415/overview]

-guardian (access role change)

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3414/overview]

-shockwave (create new vlan change)

 

 

 

 

 ","['Auton', 'Frey', 'Groot', 'Guardian', 'Issue', 'Shockwave']",Andrew Chen,Closed,Avril Bower
SEEN-272,https://miggbo.atlassian.net/browse/SEEN-272,Test_TC156_verify_wlc_interface_stats  /   test1_verify_eth_ports_wlcs  (Failed),"The SP interface doesn't have any traffic, Please exclude it the SP and RP interfaces from the validation.

 

log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=116456591&size=78514&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun05_16:06:30.061657.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 
363797:  Traffic rate is Not Seen on Interface SP Interface on Device TB6-DM-WLC
363798:  Verification Failed on Following devices [\{'Missing Or Addition Interface on DNAC': ['Virtual Interface']}, \{'Mismatch on number Of Interfaces': 'TB6-DM-WLC'}, \{'Interface SP Interface Is not having traffic': 'TB6-DM-WLC'}]
 
 
2nd) Not sure why This check is failing, in UI I see all the interface perfectly
 
363646:  Total Interface Count Seen on 7 Device From Inventory Page TB6-DM-WLC
363647:  Interface Not Found on WLC Virtual Interface
363648:  Found TenGigabitEthernet0/0/1 Port Verifying the Status on it
363649:  Interface Status is as Expected TenGigabitEthernet0/0/1
363650:  Interface Not Found on WLC Virtual Interface
363651:  Found TenGigabitEthernet0/0/2 Port Verifying the Status on it
363652:  Interface Status is as Expected TenGigabitEthernet0/0/2
363653:  Interface Not Found on WLC Virtual Interface
363654:  Found TenGigabitEthernet0/0/3 Port Verifying the Status on it
363655:  Interface Status is as Expected TenGigabitEthernet0/0/3
363656:  Interface Not Found on WLC Virtual Interface
363657:  Found TenGigabitEthernet0/0/4 Port Verifying the Status on it
363658:  Interface Status is as Expected TenGigabitEthernet0/0/4
363659:  Interface Not Found on WLC Virtual Interface
363660:  Found RP Interface Port Verifying the Status on it
363661:  Interface Status is as Expected RP Interface
363662:  Interface Not Found on WLC Virtual Interface
363663:  Found SP Interface Port Verifying the Status on it
363664:  Interface Status is as Expected SP Interface
363665:  No Interfaces Does Not Match!! 7, expected 6
 ",2022-06-08T07:32:08.000+0000,"FIle a bug for [https://10.195.227.92/api/v1/interface/network-device/[device_id]] api

 

Showing virtual interface as physical interface in api Hi Andrew ,

Vijay filed one bug:[https://cdetsng.cisco.com/summary/#/defect/CSCwd46971]  for the issue which moved to J
with comments:
""This has been the behaviour in AireOS since Day1 and as AireOS is taking only functionally impacting issue/crash, we are not allowed to do any other changes. And as this is not a functionally impacting issue, we are moving this to J state.""

Thanks,
Anusha Hi Andrew, 

I am seeing the same failure in current Guardian P4 RC3 runs, Can you please update the progress on this ! 
please find the failed log:  [test1_verify_eth_ports_wlcs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2291674&size=76280&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_auto_job.2023Mar03_07:36:02.358910.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/99c4c0453b8fbbae91d9f0d4cc2ac8622517a45d|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/99c4c0453b8fbbae91d9f0d4cc2ac8622517a45d]

Committed fix to guardian and frey, groot onwards already fixed Hi [~accountid:61efa8c457b25b006877eda3] , can you confirm that you have pulled the latest code? Fix is already in, but I dont see it reflected in your logs. Hi [~accountid:63f50bcece6f37e5ed93c87e] 
Closing the Jira as testcase passed after pulling latest code:
Passed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=80416198&size=71254&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May24_23:16:51.523847.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=80416198&size=71254&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May24_23:16:51.523847.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Thanks,
Anusha John","['Auton', 'Frey', 'Groot', 'Guardian', 'Issue', 'assurance']",Andrew Chen,Closed,Avril Bower
SEEN-274,https://miggbo.atlassian.net/browse/SEEN-274,[Auton]:Groot:Test_TC83_verify_neighbor_topology/test1_verify_neighbor_topology,"*Groot : 2.1.560.70345* 
*Guardian: 2.1.512.72100*

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted:

[+Test_TC83_verify_neighbor_topology/+|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=67343124&size=171213&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun05_16:06:30.061657.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [+test1_verify_neighbor_topology
 +**|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=67343697&size=170476&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun05_16:06:30.061657.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Fail log:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=67343697&size=170476&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun05_16:06:30.061657.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Fail log for Guardian:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=29647937&size=72838&archive=env_auto_job.2022Jun04_18:07:22.620939.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Snip from fail log:
 249250: Traceback (most recent call last):
 249251: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 1805, in verify_neighbor_topology
 249252: if fab_devices in fabric_devices:
 249253: TypeError: unhashable type: 'dict'
 249254: Library group ""assurance"" method ""verify_neighbor_topology"" returned in 0:00:04.937658
 249255: Test returned in 0:00:05.080796
 249256: Failed reason: Validation of the Neighbor metrics Failed
 249257: The result of section test1_verify_neighbor_topology is => FAILED

 

 ",2022-06-08T12:12:45.914+0000,"Same issue seen Ghost
*Ghost:2.1.610.70249*

*log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=83368763&size=143662&archive=env_auto_job.2022Jul11_16:57:56.505255.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

** [~61efa8c457b25b006877eda3], [~620b8357878c2f00729881c8] 

Please sync to latest Groot , this has been fixed [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/services/dnaserv/lib/api_groups/assurance/group.py?at=refs%2Fheads%2Fprivate%2FGroot-ms%2Fapi-auto#1800]

 ","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-275,https://miggbo.atlassian.net/browse/SEEN-275,[Auton]:Guardian:TC143_system_health_test/test_4_check_system_health_after_certificate_regen,"*Guardian: 2.1.512.72100*
 Script Name : solution_test_sanityecamb_lan.py

*Groot:2.1.560.70345*

Testcases Impacted:[TC143_system_health_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=119664391&size=160473&archive=env_auto_job.2022Jun04_18:07:22.620939.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [/test_4_check_system_health_after_certificate_regen|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=119697247&size=44092&archive=env_auto_job.2022Jun04_18:07:22.620939.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=119697247&size=44092&archive=env_auto_job.2022Jun04_18:07:22.620939.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Fail log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=119697247&size=44092&archive=env_auto_job.2022Jun04_18:07:22.620939.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Fail log for Groot:

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=111197007&size=86098&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun05_16:06:30.061657.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

Snip from fail log:

374128: Exception:
 374129: Traceback (most recent call last):
 374130: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 374131: result = testfunc(func_self, **kwargs)
 374132: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 9787, in test_4_check_system_health_after_certificate_regen
 374133: assert found_pxgrid_warning, ""Pxgrid warning failed to appear""
 374134: AssertionError: Pxgrid warning failed to appear
  ",2022-06-08T12:36:10.123+0000,This is assert message not script error,"['Auton', 'Groot', 'Guardian', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-276,https://miggbo.atlassian.net/browse/SEEN-276,[Auton]Groot :TC154_apply_custom_profile_issue_on_site_level / test4_verify_cpu_memory_issues,"*Groot : 2.1.560.70345*
Script : solution_test_sanityecamb_lan.py

TC Affected:
 [Test_TC154_apply_custom_profile_issue_on_site_level|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=115074024&size=1335836&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun05_16:06:30.061657.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/test4_verify_cpu_memory_issues

Failed log :
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=115465799&size=918307&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun05_16:06:30.061657.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS




Snip from LOG:
 362958: Failed in Generation Switch CPU Utilization Issue on 204.1.1.72
 363000: Failed in Generation Switch CPU Utilization Issue on 204.1.2.65
 363042: Failed in Generation Switch CPU Utilization Issue on 204.1.1.67
 363043: Verification Failed!!!!
 363045: Failed reason: Unable Verify Configs",2022-06-09T10:35:04.648+0000,"Related:

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-180

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-186

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-276

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-352 This doesn't look like script error. [~61efa8c457b25b006877eda3] can you share more context or check this again? Hi Abhilash,

I have two queries about the Frey feature custom_profile, Could you please clarify.

Once we do the following steps and apply, assume the device cpu and memory value both are one and above. So how long is it going to take to generate an issue for memory and cpu high utilization ?<minimum of 3 violations with in max 35 minutes> Do you have these timings detail committed in bitbucket/testplan to refer the KPI ?
<this is the bitbucket link to know more>
switch memory : [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/THUN/repos/collectors-and-pipelines/browse/assurance-aggregations/src/on-prem/aggregations/snmp_memory_aggregation_trigger.pipeline#355]
switch CPU : [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/THUN/repos/collectors-and-pipelines/browse/assurance-aggregations/src/on-prem/aggregations/snmp_cpu_aggregation_trigger.pipeline#568] [e603e5845f6|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e603e5845f6856701e9570620c75cd87f0493757] Thanks [~63f50bf5e8216251ae4d59cf] for the long pending issue fix: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep02_16:53:47.318846.zip&atstype=ATS]

  Please add this fix in optimized code too Added to optimized sanity [a48f3092db1|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a48f3092db119e607ffbbbaed6c0f9e01b34777b]","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-277,https://miggbo.atlassian.net/browse/SEEN-277,[Auton]:Groot:Test_TC160_cisco_telemetry_broker_as_netflowcollector  /   test4_stream_video_ftp_traffic_and_validate,"*Groot:2.1.560.70345*
 *Guardian:2.1.512.72100*
 Script: solution_test_sanityecamb_lan.py

NOTE : Changes need to  be done on traffic side 

Failed log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=119964942&size=92283&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun05_16:06:30.061657.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Failed log:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=126160963&size=127131&archive=env_auto_job.2022Jun04_18:07:22.620939.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Snip :
 379567: video-over-http Traffic Not Found!!! on Device TB6-DM-eCA-BORDER.cisco.com
 379568: FTP Traffic Not Found!!! on Device TB6-DM-eCA-BORDER.cisco.com
 379584: video-over-http Traffic Not Found!!! on Client Device TB6-wired-client1 00:50:56:BD:14:16
 379585: FTP Traffic Not Found!!! on Client TB6-wired-client1 00:50:56:BD:14:16
 379607: Data is not as exepected. Failed List: ['TB6-DM-eCA-BORDER.cisco.com', 'TB6-wired-client1']
 379615: Failed reason: Failed to verified both Video and FTP traffic",2022-06-09T10:58:33.375+0000,"should be duplicate of https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-219

already started working on parent Jira here is the PR for Ghost:   

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4097/overview]

Groot:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4099/overview]

guardian:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4101/overview] PR is merged for guardian but still TC is failing as it is checking on device side instead of DNAC side
Failed log:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=48142110&size=81670&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov24_09:06:28.368011.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS Fix for:
 * Remove description LAN since its not needed anymore for netflow and might affect netflow in certain scenario.
 * Cover ewlc configuration changes with version >=17.10
 * When lots of video-over-http traffic, it will record as sever ip instead.
 * Add verification for netflow config removed after disabling app telemetry to wait and make sure the disabling process are done before trying to enable app telemetry again.
 * Include extended node

Pass log from sanitytb1: 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Ftranlam-sjc%2Ftestenv%2Fusers%2Ftranlam%2Farchive%2F22-12%2Fsanity_TB1.2022Dec08_20:13:16.271827.zip&atstype=ATS]

(DNAC: 2.3.5.0-70540, 9k: 17.10.1prd10, ewlc: 17.9.1, FlowReplicator-7.1.3-2020.06.16.1331-0.iso)

 

Merged to Ghost, Groot, Guardian

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f4ef33caed9e5f00c75fb7814fc6d3b26eb1d9da]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/48a432902362a3ad2e9f8759e8766dc8fbae8f9b#testcases/forty_eight_hour/solution_test_sanityecamb.py]

 ","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Avril Bower,Resolved,Avril Bower
SEEN-278,https://miggbo.atlassian.net/browse/SEEN-278,[Auton]: Groot: Test_TC150_Client_AP_360  /   test2_verify_AP360_missing_KPI,"*Groot : 2.1.560.70345*
 Script: solution_test_sanityecamb_lan .py

*Guardian:* 2.1.511.72077
Script: solution_test_sanityecamb.py



Failed log:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=113727704&size=24270&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun05_16:06:30.061657.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS


]

Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=99733230&size=24358&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul27_09:03:10.499995.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Snip:
 355899: Exception:
 355900: Traceback (most recent call last):
 355901: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 355902: result = testfunc(func_self, **kwargs)
 355903: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 10381, in test2_verify_AP360_missing_KPI
 355904: if dnac_handle.Verify_AP360_missing_KPI():
 355905: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 6195, in Verify_AP360_missing_KPI
 355906: if x[""powerStatus""]!="""" and x[""ethernetMac""]!="""" and x[""collectionStatus""]!="""" and x[""port""]!="""":
 355907: KeyError: 'port'",2022-06-09T11:02:31.337+0000,"The tested AP does not have the connected switch info (including the port) from the 360 UI.

The seems like a defect, please file a defect for it since I can see the switch info from the physical diagram. 

Please check the attached screenshots:

The missing info:

!image-2022-08-05-11-00-46-090.png!

 

the expected is:

!image-2022-08-05-11-00-03-181.png! Hi Saeed,
 Filed Bug for the same:
 [https://cdetsng.cisco.com/webui/#view=CSCwc64866]

Thanks,
Anusha Bug id:

[https://cdetsng.cisco.com/webui/#view=CSCwc64866]","['Auton', 'Groot', 'Guardian', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-285,https://miggbo.atlassian.net/browse/SEEN-285,[Auton]Groot : Test_TC93_generate_ap_reachability_events / test1_generate_ap_reachability_events,"*Groot ISO: 2.1.560.70360* 

Script: solution_test_sanityecamb_lan.py

*Note:* Behaviour is changed from Groot, change was made to align ap down issue trigger with AP health window to address global ap down per switch feature
 as such the issue is created only if the AP is down for 5 min and resolve in the next window if ap is up

*CDETS LINK:*
https://cdetsng.cisco.com/webui/#view=CSCwc05322

Impacted Testcases:
 [Test_TC93_generate_ap_reachability_events|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19036861&size=86522&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun12_21:41:09.315412.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] / [test1_generate_ap_reachability_events|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19037431&size=85781&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun12_21:41:09.315412.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
 Failed log:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19037431&size=85781&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun12_21:41:09.315412.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS
 ]

Snip:
 70711: AP Reachability issue was not Generated for AP : AP78BC.1A00.4B28config term Enter configuration commands, one per line. End with CNTL/Z. SN-FOC2311Y129(config)# SN-FOC2311Y129(config)#interface GigabitEthernet0/3 SN-FOC2311Y129(config-if)# no shut SN-FOC2311Y129(config-if)#end SN-FOC2311Y129#
 70722: AP Event generation failed on following APs ['APDC8C.3796.20EC', 'AP5CE1.7629.CEF0', 'AP5CE1.7629.C894', 'AP78BC.1A00.4B28']
 70724: Failed reason: No Events has been trigggred",2022-06-14T10:46:19.668+0000,"The production test is failing and it's impacting the quality so please fix it ASAP. Started checking on this change Same issue is also observed in Solution regression.

*Uber ISO Version tested :* Promoted Groot Uber ISO - *2.1.560.70428, Non-FIPS, PUBSUB enabled, CSRF flag marked True*

*Script Name :* solution_assurance_test.py

*Testbed :* MSTB1
  

*Failed logs:* 

1) [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites.2022Jul18_04:36:40.089590.zip&atstype=ATS] -> Refer TC14

2) [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites.2022Jul18_00:11:22.586965.zip&atstype=ATS] -> Refer TC14

**Could you please have a look into this and provide fix?

   Sure Sandeep will check and try to fix as soon as possible. here is the PR

 

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3378/overview Pls fix it in Ghost too Below is the PR for Ghost.

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3384/overview The TC has been verified with the updated code changes. Now its working fine.

 

*Reference Log on MSTB1:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites.2022Aug09_06:31:16.787420.zip&atstype=ATS] -> Refer TC14","['Auton', 'Ghost', 'Groot', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-286,https://miggbo.atlassian.net/browse/SEEN-286,[Auton]:Groot Test_TC97_wireless_policy_PSK / test7_delete_existing_wireless_policy,"Groot *ISO: 2.1.560.70360* 
 Script : solution_test_sanityecamb_lan.py

Impacted Testcase:

Test_TC97_wireless_policy_PSK  /   test7_delete_existing_wireless_policy
| [Test_TC98_tagging_application|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=20044800&size=415657&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun12_21:41:09.315412.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [ /  |https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=46075343&size=962264&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-04%2Fenv_auto_job.2022Apr23_12:04:35.059145.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS] [test5_delete_existing_wireless_policy|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=20447176&size=13122&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun12_21:41:09.315412.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|

Failed log:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=20031518&size=13122&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun12_21:41:09.315412.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Snip:
 73646: Policy undeployment failed:NCSP01001: Error occurred while processing the 'provision' request. Additional info for support: taskId: 'd3e726c7-a96e-437a-b976-1cfb06f00aaa'. Empty 'cfs create, update and delete lists' provided in the request.
 73666: Policy deletion failed:NCSP01001: Error occurred while processing the 'provision' request. Additional info for support: taskId: '06bd8e2a-1ddd-4427-b235-12a5a156dd62'. Empty 'cfs create, update and delete lists' provided in the request.
 73669: Failed reason: Result :: wireless policy at PSK deletion failed",2022-06-14T11:00:51.935+0000,It could be bug.,"['Auton', 'Groot', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-288,https://miggbo.atlassian.net/browse/SEEN-288,TC158 Needs modifications as per defect CSCwb99082," 

params_tmp=\{""cleanConfig"":False}

params = \{""correlationData"": json.dumps(params_tmp)}

print(params)",2022-06-14T18:29:21.898+0000,"Closing this issue for now, please refer to the defect id: CSCwc52435","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-290,https://miggbo.atlassian.net/browse/SEEN-290,[Auton]:Shockwave-Test_TC96_wired_app_policy/test_device_role,"*ShockwaveP4-RC3 2.1.391.72220*

*Testcases Impacted :* [Test_TC96_wired_app_policy|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=69619348&size=277865&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F06%2F14%2F20%2F21%2Fenv_auto_job.2022Jun14_20:21:23.198087.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]/test_device_role

[Test_TC140_device_role|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=96926895&size=38941&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F06%2F14%2F20%2F21%2Fenv_auto_job.2022Jun14_20:21:23.198087.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS] /

[test_device_role|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=96927033&size=38647&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F06%2F14%2F20%2F21%2Fenv_auto_job.2022Jun14_20:21:23.198087.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]

script: solution_test_sanityecamb_lan.py 

Fail Log:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=69619490&size=38648&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F06%2F14%2F20%2F21%2Fenv_auto_job.2022Jun14_20:21:23.198087.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS
]

Fail log for TC-140:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=96926895&size=38941&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F06%2F14%2F20%2F21%2Fenv_auto_job.2022Jun14_20:21:23.198087.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS

Snip from Fail Log :
 -----------------------
 244752: Cannot track test: tracking auth info must be set in order to transfer test tracking data
 244934: failed to change device role for reason ::['failed to change device role for :: and its role :: TB2-DM-Transit']
 244936: Failed reason: Result : Failed to change the device role to ACCESS",2022-06-16T10:40:58.598+0000,"Need a fix for skipping non-EDGENODE or make the conversion for all nodes not only edge nodes Need fix for below script as well on test_device_role.

DNAC Release: Shockwave

Script: dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

Impacted Testcases: 2

Where,
Test_TC96_wired_app_policy/test_device_role
Test_TC140_device_role/test_device_role

Taas Log with (Shockwave (2.1.391.72224) + 17.6.4 (V17_06_04PRD6_FC1)):
https://ngdevx.cisco.com/services/taas/results/b8a724eb-ab1c-4867-a3c6-a390d75e7eee https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3456/overview","['Auton', 'Issue', 'Shockwave']",Moe Saeed,Closed,Avril Bower
SEEN-291,https://miggbo.atlassian.net/browse/SEEN-291,[Auton]:Shockwave-Test_TC96_wired_app_policy/test_deploy_Wired_policy,"*ShockwaveP4-RC3 2.1.391.72220*

*Testcases Impacted :* [Test_TC96_wired_app_policy|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=69619348&size=277865&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F06%2F14%2F20%2F21%2Fenv_auto_job.2022Jun14_20:21:23.198087.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]/

[test_deploy_Wired_policy|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=69687942&size=87217&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F06%2F14%2F20%2F21%2Fenv_auto_job.2022Jun14_20:21:23.198087.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]

 

script: solution_test_sanityecamb_lan.py 

Fail Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=69687942&size=87217&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F06%2F14%2F20%2F21%2Fenv_auto_job.2022Jun14_20:21:23.198087.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]

Snip from Fail Log :
 -----------------------
 245298: Starting Task wait for task:{'response':

{'taskId': 'e9ef806f-b682-42ab-b16f-79f5cca5db1d', 'url': '/api/v1/task/e9ef806f-b682-42ab-b16f-79f5cca5db1d'}

, 'version': '1.0'}
 245316: Library group ""task"" method ""wait_for_task_complete"" returned in 0:00:45.571784
 245317: Custom application deployment failed:NCSP10025: Provisioning failed.
 245318: Library group ""app_policy"" method ""deploy_wired_policy"" returned in 0:00:47.937698
 245319: Test returned in 0:00:48.292410
 245320: Failed reason: Result : Wired application policy deployment failed
 245321: The result of section test_deploy_Wired_policy is => FAILED",2022-06-16T10:48:02.732+0000,"[~620b8357878c2f00729881c8] please update the latest status on this issue  Not a Script issue Not a script issue, passed in latest run

[test_deploy_Wired_policy|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=53129002&size=86944&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F23%2F00%2F42%2Fenv_auto_job.2022Aug23_00:42:19.043785.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Issue', 'Shockwave']",Omkar Sharad Wagh,Closed,Avril Bower
SEEN-292,https://miggbo.atlassian.net/browse/SEEN-292,[Auton]:Shockwave-Test_TC123_aaa_per_ssid/test3_onboard_wireless_segment_for_new_ssid,"*ShockwaveP4-RC3 2.1.391.72220*

*Testcases Impacted :* 

[Test_TC123_aaa_per_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=90270083&size=470001&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F06%2F14%2F20%2F21%2Fenv_auto_job.2022Jun14_20:21:23.198087.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]/

[test3_onboard_wireless_segment_for_new_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=90535591&size=185656&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F06%2F14%2F20%2F21%2Fenv_auto_job.2022Jun14_20:21:23.198087.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]

script: solution_test_sanityecamb_lan.py 

Fail Log:

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=90535591&size=185656&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F06%2F14%2F20%2F21%2Fenv_auto_job.2022Jun14_20:21:23.198087.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS

Snip from Fail Log :
-----------------------

 Requested SSID test_AAAtb2 is not present in :{'response': [{'sessionTimeOut': 1800, 'profileName': 'SSIDDot1XI_Global_F_04e611e9', 'wlanId': 18, 'isEnabledOnDevice': True, 'instanceId': 607787, 'mfpClientprotection': 'OPTIONAL', 'trafficType': 'VOICE_VIDEO', 'resourceVersion': 56, 'wlanOverrideAttrs': [], 'radioPolicy': 0, 'provisioningState': 'UNKNOWN', 'passiveClient': False, 'isStale': False, 'interfaceName': '', 'instanceCreatedOn': 1655280271573, 'isGuestAnchor': False, 'authMode': 'WPA2ENTERPRISE', 'dmsEnable': True, 'isBrownfield': False, 'sleepingClientEnable': False, 'isMacfilteringEnabled': False, 'instanceUpdatedOn': 1655280271573, 'broadcastSSID': 1, 'cfsChangeInfo': [], 'isFlexConnect': False, 'displayName': '1d594268[SSIDDot1XI_Global_F_04e611e9,372e2804-85e7-4f7b-8aa3-f218267b930a]', 'clientExclusionTimeout': 180, 'networkDeviceId': '372e2804-85e7-4f7b-8aa3-f218267b930a', 'type': 'Wlan', 'policyProfile': [{'sessionTimeOut': 1800, 'instanceId': 980003, 'trafficType': 'VOICE_VIDEO', 'isUpnEnabled': False, 'policyProfileName': 'SSIDDot1XI_Global_F_04e611e9', 'interfaceName': '', 'instanceCreatedOn': 1655280271573, 'isGuestAnchor': False, 'accountingListName': 'default', 'siteIds': ['5f40702e-bdc8-4613-ae39-6d62841f8f6a'], 'id': '7689cf1d-d46f-42e0-af2c-0c86d8fdcf7e', 'bssClientIdleTimeout': 300, 'instanceUpdatedOn': 1655280271573, 'isUnicastFilteringEnabled': False, 'displayName':
1298306, 'secAuthServer': '', 'instanceCreatedOn': 1655325909718, 'displayName': '0', 'authServer': '82.1.1.3', 'isOpenAuth': True, 'terAuthServer': '', 'isMacFilteringEnabled': True, 'sixthAuthServer': '', 'instanceVersion': 2, 'instanceUpdatedOn': 1655325909718}, 'managedSites': ['5f40702e-bdc8-4613-ae39-6d62841f8f6a'], 'isRadiusProfilingEnabled': False, 'deployed': False}], 'version': '1.0'}
284949:  Library group ""wlan_segment_onboarding"" method ""onboard_wireless_segment"" returned in 0:00:10.942585
284950:  Library group ""wlan_segment_onboarding"" method ""onboard_wireless_segment_for_ssid"" returned in 0:00:10.942871
284951:  Failed to onboard wireless segment at dev TB2-DM-eCA-BORDER
'8c7fbe41[SSIDDot1XI_Global_F_04e611e9,372e2804-85e7-4f7b-8aa3-f218267b930a]', 'clientExclusionTimeout': 180, 'networkDeviceId': '372e2804-85e7-4f7b-8aa3-f218267b930a', 'settingsSiteId': 'a354504e-c092-4511-815f-5d3cfbab6b3f', 'instanceVersion': 48, 'clientExclusionEnable': True, 'isFabric': True, 'isFlexConnect': False, 'isFastLane': True, 'isRadiusProfilingEnabled': True}], 'clientExclusionEnable': True, 'switchingMode': 'CENTRAL_SWITCH', 'isFastLane': True, 'isSensorPnp': False, 'wlanBandSelectEnable': False, 'fastTransition': 'Adaptive', 'neighborListEnable': True, 'isEnabled': True, 'sleepingClientTimeout': 720, 'bssMaxIdleEnable': True, 'isRandomMacFilterEnabled': False, 'isSeeded': False, 'instanceVersion': 56, 'peerIp': '', 'flexAuthMode': 'CENTRAL_AUTH', 'customProvisions': [], 'id': '0a21793e-ad57-4d97-977f-4a4c259d814c', 'bssClientIdleTimeout': 300, 'l3AuthMode': 'STATIC', 'ssid': 'SSIDDot1XIndiatb2', 'namespace': '4edf505f-dc23-4f43-abb4-0351542157c5', 'wlanType': 'Enterprise', 'mobilityAnchors': [], 'targetIdList': [], 'isSensorBackhaul': False, 'siteId': 'a354504e-c092-4511-815f-5d3cfbab6b3f', 'isFabric': True, 'ftOverDsEnable': True, 'wirelessAuth': {'fifthAuthServer': '', 'fourthAuthServer'",2022-06-16T11:59:18.095+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3416/overview Hi Andrew ,

Even with new code added, the issue still persists. Can you please check

Failed Log:
 TB7 :
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=94182052&size=113246&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F12%2F23%2F35%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS] Same as SEEN-624, fixed here

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/8aa228f28eeaebc7b541af4b3787ed80b567db11#services/dnaserv/lib/api_groups/network_profile/group.py]","['Auton', 'Issue', 'Shockwave']",Andrew Chen,Resolved,Avril Bower
SEEN-293,https://miggbo.atlassian.net/browse/SEEN-293,[Auton] : Guardian - Test_TC102_DNAC_External_Authentication_Radius/test2_enable_external_auth_radius,"*Guardian P1 (2.3.3.3) : 2.1.512.72111*

*Testcases Impacted :* 

[|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=90535591&size=185656&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F06%2F14%2F20%2F21%2Fenv_auto_job.2022Jun14_20:21:23.198087.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS] [test2_enable_external_auth_radius|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1043751&size=103574031&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun16_16:04:21.224230.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

script: solution_test_sanityecamb_lan.py 

Fail Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1043751&size=103574031&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun16_16:04:21.224230.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Description:

Testcase actually failed because with FIPS External Auth is not supported.

But Script got aborted after the tc failed instead of proceeding with the run",2022-06-17T01:13:28.066+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3242/overview Please double commit the fix to Guardian branch as well [04bcbbc8002|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/04bcbbc800240402d0334f1b4ab6239f9c6536e4],"['Auton', 'Guardian', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-295,https://miggbo.atlassian.net/browse/SEEN-295,[Auton] : Guardian -  Test_TC107_Compliance_verification  /   test9_verify_POE_compliance,"Guardian P1 : 2.1.512.72111 

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

 [Test_TC107_Compliance_verification|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=821408&size=7045037&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun16_16:21:36.266718.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test9_verify_POE_compliance

 

Fail Log :
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7308470&size=75029&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun16_16:21:36.266718.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
 

*+Snip from log :+*
14390:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/dnaserv/lib/api_groups/Compilance/group.py"", line 770, in POE_compliance
14391:  self.log.info(""device {} has the POE capability hence proceeding with POE status.device model name is : {}"".format(dev,dev_model_name))
14392:  NameError: name 'dev_model_name' is not defined
 ",2022-06-18T03:41:35.901+0000,"similar to 

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-238

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-218 here is the PR [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3127/overview]

in groot its already committed Merged","['Auton', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-297,https://miggbo.atlassian.net/browse/SEEN-297,[Auton][IBSTE] Test_TC28_DNAC_verify_aaa_lisp_radius_configuration_on_device test2_verify_configuration_on_devices_fabric1s_after_provision,"Testcase:

[Test_TC28_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4802580&size=3825012&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F22-06%2Fsr_ibste.2022Jun17_07:19:38.617011.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&atstype=PYATS]

[test2_verify_configuration_on_devices_fabric1|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8606069&size=7725&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F22-06%2Fsr_ibste.2022Jun17_07:19:38.617011.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Branch: private/Groot-ms/api-auto

 

[Failed Log|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8606069&size=7725&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F22-06%2Fsr_ibste.2022Jun17_07:19:38.617011.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]

script got failed because of get_segment_vn_map got multiple values for argument 'onboard_type'
 !https://wiki.cisco.com/download/attachments/1217932101/image2022-6-21_17-28-30.png?version=1&modificationDate=1655812711000&api=v2|height=250!",2022-06-21T12:42:21.720+0000,"similar to https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-151

The fix should be merged to Groot now. Sathwick Reddy Polamreddy to try again with latest code. Issue is not seen on latest run

 

Passlog : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10974893&size=2131273&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug15_23:17:14.985105.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Groot', 'IBSTE', 'Issue']",Divakar Kumar Yadav,Closed,Avril Bower
SEEN-298,https://miggbo.atlassian.net/browse/SEEN-298,[Auton][IBSTE] Test_TC41_DNAC_EXT_NODE_interface_config_verifications  test10_Verify_Edge_interface_config_before_AEN_onboarding,"Testcase

[Test_TC41_DNAC_EXT_NODE_interface_config_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=31033419&size=41420949&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F22-06%2Fsr_ibste.2022Jun17_10:23:56.142374.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&atstype=PYATS]

[test10_Verify_Edge_interface_config_before_AEN_onboarding|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=39236052&size=8094&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F22-06%2Fsr_ibste.2022Jun17_10:23:56.142374.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Branch: private/Groot-ms/api-auto

 

[Failed Log|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=39236052&size=8094&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F22-06%2Fsr_ibste.2022Jun17_10:23:56.142374.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Script is getting failed because of AttributeError: 'IbsteServices' object has no attribute 'dnaconfig'
 !https://wiki.cisco.com/download/attachments/1217932101/image2022-6-21_17-40-57.png?version=1&modificationDate=1655813458000&api=v2|height=246!",2022-06-21T12:50:33.819+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3390/overview [~63f50bfde8216251ae4d59d8], pls. check and confirm if reported issue is no more observed and this ticket can be marked as ""Closed"" ? Issue is resolved Log : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-92-SDAExtnodeOnboarding&begin=8703885&size=7684&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Foptimized_ibste_job.2022Aug29_07:29:40.889490.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Groot', 'IBSTE', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-301,https://miggbo.atlassian.net/browse/SEEN-301,[Auton]:Guardian: Test_TC6_DNAC_RBAC_create_users_roles/test4_Upload_CA_trusted_certificate,"*Guardian:2.1.511.72077*

Script:solution_test_sanityecamb.py

Failed log:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1717405&size=10485&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F06%2F25%2F09%2F30%2Fenv_auto_job.2022Jun25_09:30:57.946744.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Snip :
 11087: Traceback (most recent call last):
 11088: File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/zARCHIVE_____Guardian-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 11089: result = testfunc(func_self, **kwargs)
 11090: File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/zARCHIVE_____Guardian-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 479, in test4_Upload_CA_trusted_certificate
 11091: if dnac_handle.upload_trusted_certifcate_to_ISE():
 11092: File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/zARCHIVE_____Guardian-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 11093: result = method(*args, **kwargs)
 11094: File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/zARCHIVE_____Guardian-sanity-common-Multi-job/services/dnaserv/lib/api_groups/certifiates/group.py"", line 204, in upload_trusted_certifcate_to_ISE
 11095: if self.services.dnaconfig.iseadminapi.upload_CA_trusted_certifcate(certificate):
 11096: File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/zARCHIVE_____Guardian-sanity-common-Multi-job/services/iseserv/ise_admin_api.py"", line 328, in upload_CA_trusted_certifcate
 11097: response = res.json()[""response""]
 11098: AttributeError: 'NoneType' object has no attribute 'json'",2022-06-27T05:47:39.977+0000,"PR:

skip this test case for ISE < 3.1 since no supported APIs found for this. 

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3185/overview Skipping in latest run if ISE<3.1
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2793235&size=123008&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug29_03:37:15.153801.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Guardian', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-302,https://miggbo.atlassian.net/browse/SEEN-302,[Auton] : Groot:  Test_TC136_enable_ICMP_ping_check_AP_reachability  /   test2_deploy_AP_specific_configs_to_controller,"*Groot : 2.1.560.70345*
**

*Script Name :*  solution_test_sanityecamb_lan.py

Testcases Impacted:

 [Test_TC136_enable_ICMP_ping_check_AP_reachability|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=949823&size=283328&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May12_06:51:07.890467.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test2_deploy_AP_specific_configs_to_controller 

 

*Fail log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=953656&size=279317&archive=env_auto_job.2022May12_06:51:07.890467.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Snip from log :*

Ap config workflow failed with below error,
5402: The Schedduled Job failed: with reason \{'id': '1200d2d3-2178-43c8-94cc-91e506e929ab', 'triggeredJobTaskId': '590e2f0e-c13b-4c75-a863-ae894cc14eac', 'triggeredTime': 1652364089980, 'status': 'FAILED', 'failureReason': 'NCWL10977: 2.4 GHz Radio - Radio Role Assignment of AP1416.9D2E.1FD4 is not compatible with Power Assignment configuration. This configuration is only supported with Serving Radio Role Assignment.', 'triggeredJobId': '1200d2d3-2178-43c8-94cc-91e506e929ab'}

 

This Auton was created to track this issue - [https://cdetsng.cisco.com/webui/#view=CSCwb86231]

 

*Comments from DE :*

Hi,

This is an enhancement done inn AP config workflow to allow the radio role assignment for fixed A & B radios. And there is a validation added to make sure that the dependent items such as power, channel can be configured only if the current radio role assignment is Serving.

2.4 GHz Radio - Radio Role Assignment of AP1416.9D2E.1FD4 is not compatible with Power Assignment configuration. This configuration is only supported with Serving Radio Role Assignment.

The above error is expected since the device will not allow the power configuration unless the role is configured as serving. 
You may change the input json to set the radio role assignment as Serving and configure the power and this will allow the desired power on 2.4 ghz slot. 

 ",2022-06-28T02:24:25.318+0000,"Could be similar to
 https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-189

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-304 had a sycn up with DE got the inputs needs to check manually and add it in script here is the PR

 

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3221/diff issue seen in Guardain 
 Guardian-2.1.514.72142

**

*Script Name :*  solution_test_sanityecamb_lan.py

*Script Name :* solution_test_sanityecamb.py

Testcases Impacted:

 [Test_TC136_enable_ICMP_ping_check_AP_reachability|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=949823&size=283328&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-05%2Fenv_auto_job.2022May12_06:51:07.890467.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test2_deploy_AP_specific_configs_to_controller 

 

*Fail log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=36188664&size=284190&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug05_08:59:26.657110.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Fail log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=97242953&size=151791&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug03_12:08:31.758640.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Snip from log :*
77235: The Schedduled Job failed: with reason \{'id': '8012dbbb-fadf-4b60-a263-aff8b93517bb', 'triggeredJobTaskId': '8485b808-dd92-4cbf-9c8c-7ff70c492599', 'triggeredTime': 1659728416823, 'status': 'FAILED', 'failureReason': 'Invalid Message Code NCSP11000', 'triggeredJobId': '8012dbbb-fadf-4b60-a263-aff8b93517bb'}
77237: AP Config Workflow failed


 306074: The Schedduled Job failed: with reason \{'id': 'c7def5f0-b06f-4c42-bd37-944fd5d68716', 'triggeredJobTaskId': 'a2fc3136-416d-45a7-aec6-a901e4a83e59', 'triggeredTime': 1659610677084, 'status': 'FAILED', 'failureReason': 'Invalid Message Code NCSP11000', 'triggeredJobId': 'c7def5f0-b06f-4c42-bd37-944fd5d68716'}
 306076: AP Config Workflow failed.
 306291: failed to verify AP configs on controllers for reason :: ['failed to verify apMode :: Local after config push for APDC8C.37BF.F3A6, ap mode:Monitor']
 306293: Failed reason: failed to deploy AP specific config on wireless controller
  
 *Snip from log :*
 'lazyLoadedEntities': None}
 77520: failed to verify AP configs on controllers for reason :: ['failed to verify apMode :: Local after config push for AP380E.4D93.6C4E, ap mode:Monitor', 'failed to verify apMode :: Local after config push for AP5CE1.7629.C894, ap mode:Monitor', 'failed to verify apMode :: Local after config push for AP5CE1.7629.CEF0, ap mode:Monitor', 'failed to verify apMode :: Local after config push for AP687D.B45C.2054, ap mode:Monitor', 'failed to verify apMode :: Local after config push for APDC8C.3796.20EC, ap mode:Monitor']
 77521: Test returned in 0:08:29.292410
 77522: Failed reason: failed to deploy AP specific config on wireless controller

   Hi [~620b8357878c2f00729881c8]

Looks like its a valid failure. Please check if it a valid failure or not  by checking ap mode after deployments. You cannot raise Jira ticket for functionality issues. If you feel something change in flow you need to tell what is that.

 

failed to verify AP configs on controllers for reason :: ['failed to verify apMode :: Local after config push for AP380E.4D93.6C4E, ap mode:Monitor', 'failed to verify apMode :: Local after config push for AP5CE1.7629.C894, ap mode:Monitor', 'failed to verify apMode :: Local after config push for AP5CE1.7629.CEF0, ap mode:Monitor', 'failed to verify apMode :: Local after config push for AP687D.B45C.2054, ap mode:Monitor', 'failed to verify apMode :: Local after config push for APDC8C.3796.20EC, ap mode:Monitor'] Its a dnac issue not script issue. Please check with DE and come back to us if there are any changes needed in script

 
306074:  The Schedduled Job failed: with reason \{'id': 'c7def5f0-b06f-4c42-bd37-944fd5d68716', 'triggeredJobTaskId': 'a2fc3136-416d-45a7-aec6-a901e4a83e59', 'triggeredTime': 1659610677084, 'status': 'FAILED', 'failureReason': 'Invalid Message Code NCSP11000', 'triggeredJobId': 'c7def5f0-b06f-4c42-bd37-944fd5d68716'}
306075:  Library group ""schedule-job"" method ""check_status_of_externalScheduled_jobs_with_des"" returned in 0:00:20.373236
306076:  AP Config Workflow failed.","['Auton', 'Groot', 'Guardian', 'Issue']",Omkar Sharad Wagh,Closed,Avril Bower
SEEN-304,https://miggbo.atlassian.net/browse/SEEN-304,"[Auton]Frey, Guardian - Need script enhancement for AP config workflow testcase","Guardian Version : Guardian RC5 #2.1.510.70397

Script Name:

solution_test_3sites_sjc_nyc_sf.py, solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py

Testbed : MSTB1, MSTB2

Testcases Impacted :  

Test_TC58_Verify_DHCP_server_change_on_segments -> test22_deploy_AP_specific_configs_to_controller
  

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=47183394&size=210420&archive=sr_mb2_three_sites.2022Apr26_10:20:07.337201.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]

 

 

Description : 

In our solution testing we observed that the APs provisioning Failed after deploying AP specific configs to the controller. On further debugging along with DE team it was observed that, when the AP provision request is received, the APs were in de-associated state. At this point the issue is as expected. Had further discussion on the use case with DE team in detail and they suggested the scrip flow change.

After changing AP mode from local to monitor using AP config workflow, the devices will go for the reboot and will be de-associated from controller. We need to wait till the devices are joined back to the controller and only after that next AP config workflow to change the AP mode to local has to be performed. Else we will have failure as in current defect. The details about the AP is already collected and available in DNAC inventory after AP config workflow. Even if AP is de-associated, the data is still persistent in DNAC DB. So if we are modifying the config via DNAC, and if the config is pushed to the controller and controller sync is done, you still get the latest data configured, but AP could still be de-associated.

 

As per current Automation implementation, we have only resync of devices after AP config workflow. We do not have check for presence of APs on controllers using ""show ap summary"" after AP config workflow.

 

This check has to be added after both below cases:

a) changing AP mode from local to monitor

b) changing AP mode from monitor to local.
  

Please refer Auton defect - [https://cdetsng.cisco.com/summary/#/defect/CSCwb71685] for more details

 ",2022-06-28T15:19:27.221+0000,"its already committed in Groot and guardian

 

Groot: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/2968/overview]

 

committed in Frey:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3149/overview] ","['Auton', 'Frey', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-305,https://miggbo.atlassian.net/browse/SEEN-305,[Auton] - Fabric health score Verification related Failure,"*Uber ISO Version tested :* 2.1.512.72111- Guardian Patch1

*Script Name :* solution_assurance_test.py

*Testbed :* AWS MSTB, MSTB1

*Testcases Impacted :*  

 [Test_TC50_verify_device_fabric_health_score|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5903975&size=243642&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fsr_cl_ms.2022Jun27_02:03:30.920115.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

  

*Fail Log :*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fsr_cl_ms.2022Jun27_02:03:30.920115.zip&atstype=ATS]

 

*Description :* 

We see Fabric score verification is failing, but not sure why its failing. Tried checking the code but the final else condition print statement is bit confusing. Not sure what is the reason for failure.

 

 ",2022-06-28T16:10:05.535+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/1da4654d721f1d6c553d674e57e1d844fb18f066 https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/1da4654d721f1d6c553d674e57e1d844fb18f066 Hi Raji,

We are still seeing failure during Groot Execution with same reason as before.

Failed log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites.2022Aug09_07:33:27.489592.zip&atstype=ATS] -> Refer TC50

Can we synup sometime during this week to understand about the reason for failure? 

 

Regards
Sandeep S Reopening the issue as the problem is seen across all releases execution still Looks like the this Jira ticket was auto-moved to Resolved state. Reopening the issue as the problem is seen across all releases execution still. Compare it with pass logs from sanity, closing the issue.","['Auton', 'Frey', 'Groot', 'Guardian', 'Issue', 'MSTB1', 'MSTB2', 'Multisite', 'assurance']",Raji Mukkamala,Closed,Avril Bower
SEEN-306,https://miggbo.atlassian.net/browse/SEEN-306,[Auton]:Test_TC115_stream_ftp_traffic  /   test113_stream_ftp_traffic,"Ghost Version :2.1.610.70188

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC115_stream_ftp_traffic|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=33918460&size=51247&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun25_17:44:41.228603.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test113_stream_ftp_traffic

 

Fail Log :
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=33919030&size=30894&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun25_17:44:41.228603.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Snip From Fail log  : 

 
 92421: 
 92422: During handling of the above exception, another exception occurred:
 92423: 
 92424: Traceback (most recent call last):
 92425: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 92426: result = testfunc(func_self, **kwargs)
 92427: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 8843, in test113_stream_ftp_traffic
 92428: if (dnac_handle.stream_ftp_traffic()):
 92429: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 92430: result = method(*args, **kwargs)
 92431: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/appx/group.py"", line 168, in stream_ftp_traffic
 92432: out=con.exec_cmd_stream(i)
 92433: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/utils/winRPCClient.py"", line 224, in exec_cmd_stream
 92434: response = requests.post(
 92435: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/api.py"", line 117, in post
 92436: return request('post', url, data=data, json=json, **kwargs)
 92437: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/api.py"", line 61, in request
 92438: return session.request(method=method, url=url, **kwargs)
 92439: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/sessions.py"", line 529, in request
 92440: resp = self.send(prep, **send_kwargs)
 92441: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/sessions.py"", line 645, in send
 92442: r = adapter.send(request, **kwargs)
 92443: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/adapters.py"", line 519, in send
 92444: raise ConnectionError(e, request=request)
 92445: requests.exceptions.ConnectionError: HTTPConnectionPool(host='10.30.0.76', port=4000): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f562fe49640>: Failed to establish a new connection: [Errno 110] Connection timed out'))
92446:  The result of section test113_stream_ftp_traffic is => ERRORED",2022-06-28T18:00:30.256+0000,Nethra/Omkar Sharad Wagh to check if issue with client/server. Issue with the Client,"['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Nethra Thorali Suthagar,Closed,Avril Bower
SEEN-308,https://miggbo.atlassian.net/browse/SEEN-308,[Frey][Guardian] issue related to WLC interface stats Verification,"*Uber ISO Version tested :* 2.1.512.72111- Guardian Patch1

*Script Name :* solution_assurance_test.py

*Testbed :* AWS MSTB, MSTB1

*Testcases Impacted :*  
 [Test_TC64_verify_wlc_interface_stats|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9186624&size=59843&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fsr_cl_ms.2022Jun27_02:03:30.920115.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Fail Log :*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fsr_cl_ms.2022Jun27_02:03:30.920115.zip&atstype=ATS]

 

*Description :* 

We have some failures related to verification on wlc interface stats. 
Is this expected behavior or a defect? Can you please confirm?",2022-06-29T07:41:58.458+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/99c4c0453b8fbbae91d9f0d4cc2ac8622517a45d|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/99c4c0453b8fbbae91d9f0d4cc2ac8622517a45d]

Committed fix to guardian and frey, groot onwards already fixed","['Auton', 'Frey', 'Groot', 'Guardian', 'Issue', 'MSTB1', 'MSTB2', 'Multisite', 'assurance']",Andrew Chen,Resolved,Avril Bower
SEEN-312,https://miggbo.atlassian.net/browse/SEEN-312,[Auton] :Ghost-Test_TC177_ITSM_ticket_generation_test/test3_approve_SGT_request,"Ghost Version : 2.1.610.70188

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC177_ITSM_ticket_generation_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1092656&size=347773&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun29_08:55:55.838975.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/  test3_approve_SGT_request

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1167678&size=52032&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun29_08:55:55.838975.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Snip from Fail log:

6252: Errored reason: 'in <string>' requires string as left operand, not NoneType
 6253: 
 6254: Exception:
 6255: Traceback (most recent call last):
 6256: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 6257: result = testfunc(func_self, **kwargs)
 6258: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 12834, in test3_approve_SGT_request
 6259: if dnac_handle.approve_latest_event():
 6260: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 6261: result = method(*args, **kwargs)
 6262: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/itsm/group.py"", line 193, in approve_latest_event
 6263: return self.approve_change_request(latest_id, event_name=event_name)
 6264: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 6265: result = method(*args, **kwargs)
 6266: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/itsm/group.py"", line 205, in approve_change_request
 6267: self.log.info(""Status of change request: {}"".format(self.get_integration_event_status(id, event_name=event_name)))
 6268: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 6269: result = method(*args, **kwargs)
 6270: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/itsm/group.py"", line 153, in get_integration_event_status
 6271: if id in event[""ITSMLink""]:
 6272: TypeError: 'in <string>' requires string as left operand, not NoneType
 6273: The result of section test3_approve_SGT_request is => ERRORED

 ",2022-06-29T18:26:48.375+0000,"Same issue seen in Multisite Solution Regression also.

 

DNAC Release : Guardian P1 #2.1.511.72077

Script Name :  solution_test_3sites_sjc_nyc_sf.py

Testcases Impacted : 

[Test_TC165_ITSM_ticket_generation_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1052712&size=4164701&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F05%2F00%2F12%2Fsr_mb2_three_sites.2022Jul05_00:12:37.646805.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

+*Error Snip :*+

!image-2022-07-05-13-15-02-908.png!

!image-2022-07-05-13-16-04-748.png! Spoke with Vijayakumar and Omkar to add the dna controller to servicenow [~63f53512263233e653a96a29] [~620b8357878c2f00729881c8], do we have any update on this ticket? TC is  passed, hence moving  to close state
Guradian Pass Log:
https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-09/env_auto_job.2022Sep21_03:35:21.986447.zip&atstype=ATS","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-322,https://miggbo.atlassian.net/browse/SEEN-322,[Auton] : Test_TC29_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision  /   test3_verify_keywrap_config_on_devices,"ISO : Groot 2.1.560.70410

Script : solution_test_sanityecamb_lan 

Log : 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13000294&size=109700&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun29_20:54:25.908420.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Description :

Keywrap configs are present on the device and validated, but still it shows TC as failed

 ",2022-07-02T01:50:28.265+0000,"Keywrap configs are present on the device and validated, but still it shows TC as failed in IBSTE also

Failed Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4449088&size=1223754&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F22-06%2Fsr_ibste.2022Jun29_22:03:43.374107.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]

  [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ca56240049c02ddf5e8e5d3cb7a477e68ac68c45]

 ","['Auton', 'Groot', 'Guardian', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-332,https://miggbo.atlassian.net/browse/SEEN-332,[Auton][IBSTE]Test_TC97_generate_Worst_Interferers_report,"ISO : Groot 2.1.560.70345

Script : ibste_script.py

Log : [TRADe v2 | Logs: sr_ibste (cisco.com)|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4811185&size=307554&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F22-07%2Fsr_ibste.2022Jul06_06:04:05.001310.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]

Description :

Failing because of script issue

 
18228:  Traceback (most recent call last):
18229:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/ibsteservices.py"", line 1374, in function_call_sequentialOnSites
18230:  function_to_call = getattr(self,function_name)
18231:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/ibsteservices.py"", line 187, in __getattr__
18232:  raise AttributeError(err_msg)
18233:  AttributeError: 'IbsteServices' object has no attribute 'generate_Worst_Interferers_report'",2022-07-06T15:27:50.334+0000,"Not enough time to finish [https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/rmukkama-sjc/pyats/users/andrech3/archive/22-08/ibste.2022Aug04_16:04:29.677667.zip&atstype=ATS]

For tc2 and 3

 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/rmukkama-sjc/pyats/users/andrech3/archive/22-08/ibste.2022Aug04_16:27:39.857011.zip&atstype=ATS]

 

For tc1

 

Still need to confirm when there are all devices. Issue is not seen on latest run

Passlog : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4067890&size=482028&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug22_11:08:47.841848.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

 ","['Auton', 'Groot', 'Guardian', 'IBSTE', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-333,https://miggbo.atlassian.net/browse/SEEN-333,[Auton] : CSRF Cleanup issue - Task-1  /   Test_TC0_dnac_initial_cleanup  /   cleanup_ise_profile,"ISO : Groot , Guardian

Script : solution_test_sanityecamb_lan 

Log : 

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=280501&size=150175&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul03_23:25:30.679713.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Description :

CSRF is enabled in input file, but the ISE version is less than 3.1 during the run. So the ISE cleanup is failing with below error,
2120:  Error: unable to fetch the CSRF Token, token not found!!
2121:  Error Code: 403 URL:https://10.195.227.32:9060/ers/config/authorizationprofile/cbc9bab0-e67c-11ea-b502-ae06feb888ae Data:\{'json': {'AuthorizationProfile': {'id': 'cbc9bab0-e67c-11ea-b502-ae06feb888ae', 'name': 'profile_guest_sf', 'description': '', 'accessType': 'ACCESS_ACCEPT', 'authzProfileType': 'SWITCH', 'trackMovement': False, 'serviceTemplate': False, 'easywiredSessionCandidate': False, 'voiceDomainPermission': False, 'neat': False, 'webAuth': False, 'profileName': 'Cisco', 'link': {'rel': 'self', 'href': 'https://10.195.227.32:9060/ers/config/authorizationprofile/cbc9bab0-e67c-11ea-b502-ae06feb888ae', 'type': 'application/json'}}}, 'timeout': 60} Headers:\{'Content-Type': 'application/json', 'Accept': 'application/json', 'ERS-Media-Type': 'policy.authorizationprofile.1.3', 'Authorization': 'Basic YWRtaW46TGFibGFiIzEyMw==', 'X-CSRF-TOKEN': ''} Message:CSRF nonce validation failed<!DOCTYPE html>
2122:  <html lang=""en"">
2123:  <head>
2124: 
2125:  </head>
2126:  <body>
2127:  <div class=""container"">
2128:  <h1>[ 403 ] </h1>
2129:  <p></p>
2130:  <p></p>
2131:  </div>
2132:  </body>
2133:  </html>
2134:  Traceback (most recent call last):
2135:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/iseserv/client_manager.py"", line 278, in call_api
2136:  response.raise_for_status()
2137:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status
2138:  raise HTTPError(http_error_msg, response=self)
2139:  requests.exceptions.HTTPError: 403 Client Error: for url:  [https://10.195.227.32:9060/ers/config/authorizationprofile/cbc9bab0-e67c-11ea-b502-ae06feb888ae]
2140:  ###################################################
2141:  #!!!FAILED TO UPDATE AUTHORIZATION PROFILE profile_guest_sf in ISE. ERROR 403 Client Error: for url: [https://10.195.227.32:9060/ers/config/authorizationprofile/cbc9bab0-e67c-11ea-b502-ae06feb888ae----#]
2142:  ###################################################
 ",2022-07-06T21:59:28.378+0000,"PR:

This should resolve the issue of CSRF if it cause the issue. Please run again and check if the issue still exists. 

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3131/diff Check the PR.

  Verified the fix in latest code

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=271676&size=1042239&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug23_06:42:05.439330.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS Seeing same issue in Shockwave,
double commit as well Shockwave Branch 
*Branch :*private/Shockwave-ms/sanity_api_auto
*script:*solution_test_sanityecamb_lan.py

*Failed Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=252690&size=8403&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F09%2F25%2F14%2F29%2Fenv_auto_job.2022Sep25_14:29:16.368901.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3762/overview Merged to Shockwave. Need to add the changes in shockwave [~70121:27365d3e-893a-4523-9fca-b89a7e8d0d4c],

Could you please attach the latest logs you have? [~63f50bfce8216251ae4d59d5], pls find the log.

Failed log: [https://earms-trade.cisco.com/tradeui/logs/details?ats=%2Fusers%2Frvonmize%2Fpyats-inst&client=web&host=st-ucs-sanity.cisco.com&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov08_16:13:54.041071.zip]

 

  Hi moe 

TC Failed in Ghost  2.1.610.70530
 *Branch Name:* Ghost-ms/sanity_api_auto

 +
 Failed log+ 
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-11/env_optimized_auto_job.2022Nov13_03:11:08.345784.zip&atstype=ATS] Thanks a lot [~63f50bfce8216251ae4d59d5]

 

Test is working fine in shockwave: [https://earms-trade.cisco.com/tradeui/logs/details?ats=%2Fusers%2Frvonmize%2Fpyats-inst&client=web&host=st-ucs-sanity.cisco.com&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov16_12:59:50.043366.zip]


Team, pls add  in shockwave repo



""enableCSRF"": false,","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Shockwave']",Moe Saeed,Closed,Avril Bower
SEEN-335,https://miggbo.atlassian.net/browse/SEEN-335,[Auton] : Test_TC179_generate_Worst_Interferers_report,"ISO : Groot 2.1.560.70410

Script : solution_test_sanityecamb_lan 

Log : 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1182059&size=275002&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul01_07:23:14.661719.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

+Same issue observed in Multisite-2 Non-DR (On-Prem).+

Log:

[https://earms-trade.cisco.com/tradeui/logs/details?ats=%2Fusers%2Fpawansi%2Fpyatsnew&client=web&host=st-ucs-sanity.cisco.com&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fsr_mb2_three_sites.2022Jun30_03:50:35.391000.zip] 

 

Description :

Executive summary Report is generated in dnac, but the email with report is not received ",2022-07-07T00:48:15.045+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3322/overview Verified the Auton with latest code fix 

  Pass log : 

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1216489&size=1061538&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep08_05:16:36.499915.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-336,https://miggbo.atlassian.net/browse/SEEN-336,[Auton] : Groot - Test_TC138_system_health_assurance_checks / test1_verify_devices_categorized_health_no_health,"Groot Version :2.1.560.70410
 Script Name :  solution_test_sanityecamb_lan.py

Shockwave Version : 2.1.390.72158
 Script Name:  solution_test_sanityecamb.py

 Issue type : Enhancement

Testcases Impacted :

[Test_TC138_system_health_assurance_checks|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=80429924&size=113803&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun29_23:45:27.988506.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] /

[test1_verify_devices_categorized_health_no_health|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=80430500&size=13704&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun29_23:45:27.988506.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
  Fail Log:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun29_23:45:27.988506.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]

Snip Fail Log:
 269890: 
 269891: Exception:
 269892: Traceback (most recent call last):
 269893: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 269894: result = testfunc(func_self, **kwargs)
 269895: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 9623, in test1_verify_devices_categorized_health_no_health
 269896: if dnac_handle.verify_devices_categorized_health_no_health():
 269897: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 6016, in verify_devices_categorized_health_no_health
 269898: if self.services.dnaconfig.testbed.devices[ap].role == ""AP"":
 269899: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/topology/bases.py"", line 100, in __getitem__
 269900: return super().__getitem__(key)
 269901: KeyError: 'AP502f.a80f.2c80'

 ",2022-07-07T18:35:14.241+0000,"[~owagh]/[~rajsaran] 

AP is AP502f.a80f.2c80 not yaml file, please add AP in yaml  

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb3/SanityTB3.yaml?at=refs%2Fheads%2Fprivate%2FGhost-ms%2Fapi-auto]

It is in this file [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb3/SanityTB3_legacy.yaml?at=private/Ghost-ms/api-auto] but not in SanityTB3.yaml

  [~63f50bf5e8216251ae4d59cf] 
We have removed this ap from YAML. since Ap CAP3702E-B-K9( ap502f.a80f.2c80)  not supported the latest device image, can u please do the validation which is ap present in YAML with not with CDP neighbors. To do:
 # Compare the CDP APs and Yaml APs
 # Pick the APs which are in CDP neigbour list and Yaml both for the test. Guardian Fail log :

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2453154&size=14893&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb12_21:35:08.893143.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

   Guardian (2336) version 2.1.517.70045 - HF1
Could you please check..
+*Script:*+solution_test_sanityecamb_lan.py
Failed  Log:

[Test_TC138_system_health_assurance_checks|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11205182&size=149811&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb22_21:45:23.997656.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Script supports only APs in YAML","['Auton', 'Ghost', 'Groot', 'Guardian', 'Halleck', 'Issue', 'Shockwave', 'assurance']",Raji Mukkamala,Closed,Avril Bower
SEEN-337,https://miggbo.atlassian.net/browse/SEEN-337,[Auton] : Groot - TC101_DNAC_Policy_Extended_node/test1_onboard_policy_extended_node_interface,"Groot Version :2.1.560.70410

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[TC101_DNAC_Policy_Extended_node|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=52672530&size=370651&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun29_23:45:27.988506.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/

test1_onboard_policy_extended_node_interface

 

Fail Log:https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=52673106&size=369912&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun29_23:45:27.988506.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS%5D
 
 

Snip from Fail Log:

197075: Exception:
 197076: Traceback (most recent call last):
 197077: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 197078: result = testfunc(func_self, **kwargs)
 197079: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 8324, in test1_onboard_policy_extended_node_interface
 197080: dnac_handle.verify_aaa_pac_key(dev['name']) and \
 197081: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 197082: result = method(*args, **kwargs)
 197083: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/cli_check/group.py"", line 2349, in verify_aaa_pac_key
 197084: return self.verify_cts_pac(device)
 197085: AttributeError: 'Group' object has no attribute 'verify_cts_pac'
  ",2022-07-07T18:57:23.417+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e1c93b72bea856b6e1c0e20d62c27f2a155f6801,"['Auton', 'Groot', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-338,https://miggbo.atlassian.net/browse/SEEN-338,[Auton] : Groot - Test_TC172_syslog_server_event_notification  /   test3_connect_syslog_server_update_rsyslog_config_file,"Groot Version :2.1.560.70410

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC172_syslog_server_event_notification|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1006980&size=1321082&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul08_04:43:09.060876.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test3_connect_syslog_server_update_rsyslog_config_file

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2217931&size=18152&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul08_04:43:09.060876.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Description :

The password for Syslog server is hardcoded in the Library file as 'Maglev123!'

File Name : sftopology.py 

Function : def connect_linux_syslog_server 

 

The password should be taken from the Yaml file from 'syslogser' section 

 ",2022-07-08T18:05:51.682+0000,"We dont need password after running the ""sudo systemctl restart rsyslog"". Removed the passwod in sftopology. Below is the PR for your ref. Log also updated.

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3240/diff]

Make sure your have latest pyats. if you have uniq 19.x you may see issues with connecting the syslog server.

pip list:

unicon 22.6
 unicon.plugins 22.6

python ver: 3.8.2","['Auton', 'Groot', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-341,https://miggbo.atlassian.net/browse/SEEN-341,"[Auton]:Shockwave:""Transit device is taking Border Role""","*Shockwave P3 RC3 :2.1.390.72158*
Summary :
 Policy---->Application QoS--->custom_app_policy------->Transit device failed due to ""Border Role"" with the message ""The device series 'Cisco Catalyst 9300 Series Switches' is only supported in Access or Distribution device role.""",2022-07-12T13:26:04.716+0000,"Please add log and add more details as per 

https://wiki.cisco.com/display/EDPEIXOT/Raising+Jira+Tickets+for+script+issues Hi Tran,
there is no failed log but Transit is taking role as""border"" instead of ""access"".
Attaching paased log:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-07/env_auto_job.2022Jul25_23:58:01.279246.zip&atstype=ATS]

Attached recording and snap shot Pull fix from Groot:

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/2d834c3f01982dee979e22aebcdbca45def166c7 Passing in latest executions","['Auton', 'Guardian', 'Issue', 'Shockwave']",Tran Lam,Closed,Avril Bower
SEEN-342,https://miggbo.atlassian.net/browse/SEEN-342,[Auton] : Groot -Test_TC31_SWIM_UPGRADE_ECA_DEVICE/test4_verify_upload_os_image_mark_golden,"Groot Version :2.1.560.70410

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC31_SWIM_UPGRADE_ECA_DEVICE|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13802075&size=379674&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun29_20:54:25.908420.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/ [test4_verify_upload_os_image_mark_golden

Fail Log:

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13895753&size=82446&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun29_20:54:25.908420.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13895753&size=82446&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-06%2Fenv_auto_job.2022Jun29_20:54:25.908420.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Snip from Fail Log:

57900: Exception:
 57901: Traceback (most recent call last):
 57902: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 57903: result = testfunc(func_self, **kwargs)
 57904: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 2199, in test4_verify_upload_os_image_mark_golden
 57905: if (dnac_handle.swim_image_upload_assign_and_mark_golden(key=""image_dir"", site=""Global"")):
 57906: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 57907: result = method(*args, **kwargs)
 57908: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/swim/group.py"", line 95, in swim_image_upload_assign_and_mark_golden
 57909: result = self.swim_image_upload_url(imageurl=imageurl,key=key)
 57910: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 57911: result = method(*args, **kwargs)
 57912: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/swim/group.py"", line 17, in swim_image_upload_url
 57913: images = self.services.dnaconfig.testbed.custom[key]
 57914: KeyError: 'image_dir'",2022-07-12T19:23:05.516+0000,"Hey [~620b8357878c2f00729881c8], please edit your testbed file and add the correct file dir for swim. Please use this as an example,","['Auton', 'Groot', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-343,https://miggbo.atlassian.net/browse/SEEN-343,[Auton] : Groot -Test_TC172_syslog_server_event_notification/test3_connect_syslog_server_update_rsyslog_config_file,"Groot Version :2.1.560.70410

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC172_syslog_server_event_notification/|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1009085&size=63333&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul08_11:10:30.109461.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [test3_connect_syslog_server_update_rsyslog_config_file|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1011599&size=55719&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul08_11:10:30.109461.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Fail Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1009085&size=63333&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul08_11:10:30.109461.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

+Descrption :+

The TC connects to syslog server, update the syslog config file, restart the syslog server. After restarting the server, we are seeing below failure. But in the pass log mentioned in the wiki, TC just connects to the syslog server and remove the logs from it.

Subtc name :  [Test_TC175_syslog_server_event_notification|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=695108&size=1861206&archive=%2Fusers%2Fpawansi%2Fpyatsnew%2Fusers%2Frakdomma%2Farchive%2F22-04%2Fsanity_TB1.2022Apr23_21:56:15.569814.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=rakdomma&atstype=PYATS]  /   test3_connect_linux_syslog_server_and_clear_logs 

 

Please update latest pass log with updated tc in the wiki as well

 

Snip from Fail Log::

Jul 08 10:42:40 raju-virtual-machine systemd[1]: Started System Logging Service.[m Jul 08 10:42:40 raju-virtual-machine rsyslogd[6577]: rsyslogd's groupid changed to 110[m Jul 08 10:42:40 raju-virtual-machine rsyslogd[6577]: rsyslogd's userid changed to 104[m Jul 08 10:42:40 raju-virtual-machine rsyslogd[6577]: [origin software=""rsyslogd"" swVersion=""8.2001.0"" x-pid=""6577"" x-info=[https://www.rsyslog.com|https://www.rsyslog.com/]] start[m [K[?1l>raju@raju-virtual-machine:~$

5509: Traceback (most recent call last):

5510: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 733, in call_service

5511: self.result = self.get_service_result()

5512: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 240, in get_service_result

5513: raise SubCommandFailure(

5514: unicon.core.errors.SubCommandFailure: ('sub_command failure, patterns matched in the output:', ['^.*?command not found'], 'service result', 'sudo echo \'$WorkDirectory /var/spool/rsyslog\' >> /etc/rsyslog.conf\r\nraju@raju-virtual-machine:~$ sudo echo \'$ActionQueueFileName queue\' >> /etc/rsyslog.conf\r\nsudo systemctl status rsyslog\r\nraju@raju-virtual-machine:~$ sudo echo \'$ActionQueueMaxDiskSpace 1g\' >> /etc/rsyslog.conf\r\nraju@raju-virtual-machine:~$ sudo echo \'$ActionQueueSaveOnShutdown on\' >> /etc/rsyslog.conf\r\nraju@raju-virtual-machine:~$ sudo echo \'$ActionQueueType LinkedList\' >> /etc/rsyslog.conf\r\nraju@raju-virtual-machine:~$ sudo echo \'$ActionResumeRetryCount -1 \' >> /etc/rsyslog.conf\r\nraju@raju-virtual-machine:~$ sudo echo \'$IncludeConfig /etc/rsyslog.d/*.conf\' >> /etc/rsyslog.conf\r\nraju@raju-virtual-machine:~$ cat /etc/rsyslog.conf\r\n\r\nmodule(load=""imuxsock"") \r\nmodule(load=""imudp"")\r\ninput(type=""imudp"" port=""514"")\r\nmodule(load=""imklog"" permitnonkernelfacility=""on"")\r\n$template remote-incoming-logs,""/var/log/syslogserv/serverFri08Jul202210:42:39AMPDT.log""\r\n~*.* ?remote-incoming-logs\r\n$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat\r\n$RepeatedMsgReduction on\r\n$FileOwner syslog\r\n$FileGroup adm\r\n$FileCreateMode 0640\r\n$DirCreateMode 0755\r\n$Umask 0022\r\n$PrivDropToUser syslog\r\n$PrivDropToGroup syslog\r\n$WorkDirectory /var/spool/rsyslog\r\n$ActionQueueFileName queue\r\n$ActionQueueMaxDiskSpace 1g\r\n$ActionQueueSaveOnShutdown on\r\n$ActionQueueType LinkedList\r\n$ActionResumeRetryCount -1 \r\n$IncludeConfig /etc/rsyslog.d/*.conf\r\nraju@raju-virtual-machine:$ sudo systemctl restart rsyslog\r\nraju@raju-virtual-machine:~$ Lablab123\r\nLablab123: command not found\r\nraju@raju-virtual-machine:~$ \r\nraju@raju-virtual-machine:~$ sudo systemctl status rsyslog\r\n\x1b[?1h\x1b=\r\x1b[0;1;32m●\x1b[0m rsyslog.service - System Logging Service\x1b[m\r\n Loaded: loaded (/lib/systemd/system/rsyslog.service; enabled; vendor preset: enabled)\x1b[m\r\n Active: \x1b[0;1;32mactive (running)\x1b[0m since Fri 2022-07-08 10:42:40 PDT; 600ms ago\x1b[m\r\nTriggeredBy: \x1b[0;1;32m●\x1b[0m syslog.socket\x1b[m\r\n Docs: man:rsyslogd(8)\x1b[m\r\n [https://www.rsyslog.com/doc/\x1b[m\r\n|https://www.rsyslog.com/doc/x1b%5bm/r/n] Main PID: 6577 (rsyslogd)\x1b[m\r\n Tasks: 5 (limit: 1087)\x1b[m\r\n Memory: 1.9M\x1b[m\r\n CGroup: /system.slice/rsyslog.service\x1b[m\r\n └─6577 /usr/sbin/rsyslogd -n -iNONE\x1b[m\r\n\x1b[m\r\nJul 08 10:42:40 raju-virtual-machine systemd[1]: Starting System Logging Service...\x1b[m\r\nJul 08 10:42:40 raju-virtual-machine rsyslogd[6577]: imuxsock: Acquired UNIX socket \'/run/systemd/journal/syslog\' (fd 3) from systemd. [v8.2001.0]\x1b[m\r\nJul 08 10:42:40 raju-virtual-machine systemd[1]: Started System Logging Service.\x1b[m\r\nJul 08 10:42:40 raju-virtual-machine rsyslogd[6577]: rsyslogd\'s groupid changed to 110\x1b[m\r\nJul 08 10:42:40 raju-virtual-machine rsyslogd[6577]: rsyslogd\'s userid changed to 104\x1b[m\r\nJul 08 10:42:40 raju-virtual-machine rsyslogd[6577]: [origin software=""rsyslogd"" swVersion=""8.2001.0"" x-pid=""6577"" x-info=[https://www.rsyslog.com|https://www.rsyslog.com/]] start\x1b[m\r\n\r\x1b[K\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x1b[?1l\x1b>raju@raju-virtual-machine:~$ ')

5515:",2022-07-13T04:47:52.097+0000,"Attached is the passlog file for your ref

[https://ngdevx.cisco.com/services/taas/results/fd1f7507-d10a-43d0-80ef-e05c453ce82a/run-results]

Suspecting could be issue with ENV

please make sure you are having latest pyats. check below uniq version. If you are using old uniq like 19.x please update.

pip list:

unicon 22.6
 unicon.plugins 22.6

python ver: 3.8.2

 ","['Auton', 'Groot', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-344,https://miggbo.atlassian.net/browse/SEEN-344,[Auton] : Talos feature - Need to update SSH workflow for AWS,"Guardian Version :2.1.512.80120027

 

Testcases Impacted : 

[Test_TC181_cloud_Talos_anomaly_detection|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1068657&size=86969&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul12_19:36:26.042433.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Fail Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1068657&size=86969&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul12_19:36:26.042433.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Description : 

The SSH workflow is different for the DMZ clusters, can you please handle this workflow through automation ?

Following are the manual steps we do for ssh the DMZ cluster.
 # ssh the jump host with ssh <cec>@[10.195.247.200|https://10.195.247.200/]
 # copy the key file in yr home dir /users/<cec> (One-time procedure)
 # ssh -i /users/<cec>/key-n-california.pem maglev@[172.35.16.150|https://172.35.16.150/] -p 2222

Step 1 and 2 we have done manually and its one-time effort, we need your help to handle the step 3 in automation. We have copied the .pem file (with admin user) in our execution server, can we have this in input file and use it for doing ssh ?

*/home/admin/key-n-california.pem*

 

+Snip from Fail Log::+
 5824: Log started.
 5825: Using persistent connection.
 5826: Calling method `send_cmd`.
 5827: ('172.35.16.150', 'maglev', 'maglev1@3')
 5828: Encountered error on login. Check login details or try again. Error details:
 5829: Could not establish connection to host
 5830: Encountered error during excecution of `send_cmd`
 5831: Could not establish connection to host
 5832: Traceback (most recent call last):
 5833: File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/DMZ-Guardian-sanity-common-Multi-job/services/maglev_cli/utils.py"", line 34, in wrapper
 5834: self.connect()
 5835: File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/DMZ-Guardian-sanity-common-Multi-job/services/maglev_cli/maglevclihandler.py"", line 139, in connect
 5836: if not self.ssh.login(*args, **kwargs):
 5837: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pexpect/pxssh.py"", line 424, in login
 5838: raise ExceptionPxssh('Could not establish connection to host')
 5839: pexpect.pxssh.ExceptionPxssh: Could not establish connection to host
 5840: Last ssh response:
 5841: Could not establish connection to host
 5842: Error in executinh command on DNAC Maglev CLI.
 5843: Test returned in 0:00:00.224627
 5844: Failed reason: Failed to configure cloud URL into DNAC cluster!!",2022-07-13T05:07:34.582+0000,Omkar Sharad Wagh to check. It should be already supported.,"['Auton', 'Guardian', 'Issue']",Omkar Sharad Wagh,Closed,Avril Bower
SEEN-345,https://miggbo.atlassian.net/browse/SEEN-345,TC104_perform_ap_rma_apSwitch,"On our AWS-Multisite setup for RMA feature, the both same model APs are added to NYC site which joins to NYC-FE-9400.After executing TC,one AP of 9120AXE(NYC-AP1-9120AXEB-RMA) model on NYC site  missing in inventory page even though TC got passed .And we observed this on AP.
  
 NYC-AP1-9120AXEB-RMA#[*07/07/2022 22:42:42.3910] Powering down BLE radio
 [*07/07/2022 22:42:55.3270] set cleanair [slot0][band0] enabled
 [*07/07/2022 22:42:55.3300] set cleanair [slot0][band1] enabled
 [*07/07/2022 22:42:55.3520] set cleanair [slot0][band2] disable
 [*07/07/2022 22:42:55.3730] set cleanair [slot1][band1] enabled
 {color:#ffab00}[*07/07/2022 22:43:42.2600] CAC_EXPIRY_EVT: CAC finished on DFS channel 100l[*07/07/2022 22:44:37.7750] AP name NYC-AP1-9120AXEB-RMA change to NYC-AP1-9120AXEB{color}
 Trade Log:
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-07/sr_cl_ms.2022Jul07_15:12:59.328393.zip&atstype=ATS]

Branch: private/Guardian-ms/sanity_api_auto

Source: AWS-Multisite-172.35.16.151(admin/Maglev1@3)
  ",2022-07-13T14:51:09.180+0000,"Could you provide branch with the yaml that has the rma backup? The sanity-api-auto branch does not have the device in the yaml. Hi Andrew, 

Here is the link for the yaml file having all the right parameters. 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sr_mb2/sr_mb2_testbed.yaml?at=refs%2Fheads%2Fprivate%2FGuardian-ms%2Fsanity_api_auto]

We have re-executed the TC104 freshly. 

*Log:* https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fauto_MS_job.2022Jul28_05:09:21.165602.zip&atstype=ATS -> Refer TC104

The TC has passed, but like earlier observation the replaced AP hostname did not change and it did not get assigned to any site and did not get provisioned.  Please refer the attached screenshots for AP state before and after RMA TC execution.

 

*Before RMA TC execution:*

!bc8c5760-667a-438f-beb9-5ab29dca328d.PNG!

 

*After RMA workflow TC Execution:*

*!5064ec36-4c06-4fc2-9887-c4ed027dae28.PNG!*


The same is working fine on MSTB1 only change is AP models are different and the APs joins Aireos Controller on SJ site. 

*Pass log on MSTB1:* https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul15_09:38:16.653780.zip&atstype=ATS -> Refer TC90

 

   Hi Andrew, 

As discussed other day over call, We need to handle other part after AP RMA feature i.e

Assign the site and provision APs. Then interchange the hostnames back and shut/noshut and make sure it joins the Controller back correctly. Could you please have a discussion with Pawan and confirm on this?

Also with respect to MSTB1 (Multisite DR testbed). Here is AP RMA related log:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug02_03:58:39.713917.zip&atstype=ATS] -> Refer TC90

Also please refer the attached AP console logs for both APs. 
[^SJ-AP1-3802E_log1.log][^SJ-AP1-3802E-RMA-log1.log]

As mentioned other day, its going for a false pass. We need to address this as well.

 

  Regards
Sandeep S [^SJ-AP1-3802E_log1.log][^SJ-AP1-3802E-RMA-log1.log] Is this blocking anything? Closing for now as no complaints have come as a result of not having the ap come back up. This has been the behavior since the testcase was written. Please re-open as seen fit.","['AWS_MSTB', 'Auton', 'Guardian', 'Issue', 'MSTB1', 'Multisite']",Andrew Chen,Closed,Avril Bower
SEEN-346,https://miggbo.atlassian.net/browse/SEEN-346,[Auton] : Groot -Test_TC31_SWIM_UPGRADE_ECA_DEVICE/test8_aduit_log_verify,"Groot Version :2.1.560.70410
Swim_test 

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC31_SWIM_UPGRADE_ECA_DEVICE|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=376848&size=11656379&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul12_21:19:41.327620.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] /

[test8_aduit_log_verify|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11487204&size=545861&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul12_21:19:41.327620.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Fail Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=376848&size=11656379&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul12_21:19:41.327620.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Snip from Fail Log::
33737:  Getting Instance ID of parent response
33738:  Count of the audit requestPayload value:: 1
33739:  count of instanceID :: 0
33740:  Retry untill all audit response are verified
33741:  Verify the audit response after retry
33742:  Could not find the expected parent response even after retry :: {'http://172.21.236.183/swim/sanity_image_regr/tb3/cat9k_iosxe.BLD_V176_THROTTLE_LATEST_20220707_151336_V17_6_3_15.SSA.bin': ['The OS image will be uploaded to the Cisco DNA Center swim repository using a url', ['Image import workflow started for url : [http://172.21.236.183/swim/sanity_image_regr/tb3/cat9k_iosxe.BLD_V176_THROTTLE_LATEST_20220707_151336_V17_6_3_15.SSA.bin']]}|http://172.21.236.183/swim/sanity_image_regr/tb3/cat9k_iosxe.BLD_V176_THROTTLE_LATEST_20220707_151336_V17_6_3_15.SSA.bin']]%7D]
33743:  The audit log verification window is : startTime: 1657686374134, endTime: 1657691966230
33744:  The audit log verification window is : startTime: 1657686374134, endTime: 1657691950476
33745:  Test returned in 0:00:37.946693
33746:  Failed reason: Result : Audit response verification failed
33747:  The result of section test8_aduit_log_verify is => FAILED",2022-07-13T17:34:59.418+0000,"Hey Moe, Looks like the input is correct otherwise the images will not be uploaded to the dnac:

 
||[Test_TC31_SWIM_UPGRADE_ECA_DEVICE|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=376848&size=11656379&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul12_21:19:41.327620.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2FGroot-sanity-common-Multi-job@2%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=2153&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul12_21:19:41.327620.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Failed|01:36:25|Test_TC31_SWIM_UPGRADE_ECA_DEVICE| |
||[test1_clear_old_images_from_device_make_flash_space|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=377409&size=41266&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul12_21:19:41.327620.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2FGroot-sanity-common-Multi-job@2%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=2154&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul12_21:19:41.327620.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Passed|00:00:32|test1_clear_old_images_from_device_make_flash_space| |
||[test2_construct_device_image_urls_from_image_dir_avail_files|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=418675&size=13468&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul12_21:19:41.327620.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2FGroot-sanity-common-Multi-job@2%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=2160&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul12_21:19:41.327620.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Passed|00:00:01|test2_construct_device_image_urls_from_image_dir_avail_files| |
||[test3_fetch_version_updates_for_recommendation|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=432143&size=1467&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul12_21:19:41.327620.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2FGroot-sanity-common-Multi-job@2%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=2171&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul12_21:19:41.327620.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Skipped|00:00:00|test3_fetch_version_updates_for_recommendation| |
||[test4_verify_upload_os_image_mark_golden|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=433610&size=10431996&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul12_21:19:41.327620.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2FGroot-sanity-common-Multi-job@2%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=2193&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul12_21:19:41.327620.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Passed|00:15:24|test4_verify_upload_os_image_mark_golden
 
| Please commit the fix in Ghost branch as well 

Currently running Ghost ISO 2.1.610.70338 - I am seeing TC [Test_TC31_SWIM_UPGRADE_ECA_DEVICE|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3775832&size=21363693&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug15_20:13:31.146064.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/[test8_aduit_log_verify] is failing with Error:

Could not find the expected parent response even after retry ::

{'http://172.21.236.183/swim/sanity_image_regr/tb7/cat9k_iosxe.17.09.01.SPA.bin': ['The OS image will be uploaded to the Cisco DNA Center swim repository using a url', ['Image import workflow started for url : [http://172.21.236.183/swim/sanity_image_regr/tb7/cat9k_iosxe.17.09.01.SPA.bin']]}

 

Please find the failed log: 
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24299637&size=839726&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug15_20:13:31.146064.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-348,https://miggbo.atlassian.net/browse/SEEN-348,[Auton] Groot (2.1.560.70448) - Wireless Solution Sanity - TC83_verify_neighbor_topology,"- Release: Groot (2.1.560.70448)
 - Branch: rcdn/Groot-ms/api-auto (Latest Sync to Main branch – 2 days ago)
 - Script file: dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py
 - Testcases Impacted: Test_TC83_verify_neighbor_topology
 - Failed log: [https://ngdevx.cisco.com/services/taas/results/5ff8e30b-9476-4ce8-b601-9a8113033349]
 - Issue details/analysis
 152364: wnbust-ats-26.cisco.com: 2022-07-12T16:43:31: %API-GROUP-ASSURANCE-6-INFO: %[part=152364.1/2][pid=5303][pname=Task-1]: fab_devices :{'TB4-DM1-NF-Switch': ['BORDER', 'MAP-SERVER', 'EDGE'], 'TB4-DM1-9KB1': ['WLC', 'BORDER', 'MAP-SERVER', 'EDGE'], 'SN-FOC2322T3T8': ['EXTENDED-NODE'], 'TB4-DM1-93KB': ['TRANSIT-CP'], 'TB4-DM1-WLC1': ['WLC']} 
 152365: wnbust-ats-26.cisco.com: 2022-07-12T16:43:31: %API-GROUP-ASSURANCE-6-INFO: %[part=152364.2/2][pid=5303][pname=Task-1]: fabric_devices:{'SN-FOC2322T3T8': ['EXTENDED-NODE']}
 152366: wnbust-ats-26.cisco.com: 2022-07-12T16:43:31: %API-GROUP-ASSURANCE-3-ERROR: %[part=152366.1/4][pid=5303][pname=Task-1]: Traceback (most recent call last):
 152367: wnbust-ats-26.cisco.com: 2022-07-12T16:43:31: %API-GROUP-ASSURANCE-3-ERROR: %[part=152366.2/4][pid=5303][pname=Task-1]: File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod1/services/dnaserv/lib/api_groups/assurance/group.py"", line 1804, in verify_neighbor_topology
 152368: wnbust-ats-26.cisco.com: 2022-07-12T16:43:31: %API-GROUP-ASSURANCE-3-ERROR: %[part=152366.3/4][pid=5303][pname=Task-1]: if fab_devices in fabric_devices:
 152369: wnbust-ats-26.cisco.com: 2022-07-12T16:43:31: %API-GROUP-ASSURANCE-3-ERROR: %[part=152366.4/4][pid=5303][pname=Task-1]: TypeError: unhashable type: 'dict'
 152370: 2022-07-12T16:43:31: Library group ""assurance"" method ""verify_neighbor_topology"" returned in 0:00:06.218135
 152371: 2022-07-12T16:43:31: Test returned in 0:00:06.437414
 152372: 2022-07-12T16:43:31: Failed reason: Validation of the Neighbor metrics Failed
 - Testbed info: Testbed is in regression currently
 - Team/Source: Wireless Solution Sanity Regression Testing Team under Loi",2022-07-13T22:31:05.820+0000,Pawan fix: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e3b6df2405915124bce7a0955430c9f7688b5293,"['Auton', 'Groot', 'Issue']",Pawan Singh,Resolved,Avril Bower
SEEN-350,https://miggbo.atlassian.net/browse/SEEN-350,[Auton] : Guardian -  Test_TC57_WIRELESS_VERIFY_ACCESS_TUNNEL_ON_EDGE_ROUTER/test1_verify_access_tunnel_on_edges,"Guardian :2.1.510.70097

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

 [Test_TC57_WIRELESS_VERIFY_ACCESS_TUNNEL_ON_EDGE_ROUTER|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=28639125&size=18969&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul14_02:52:47.340824.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] / [test1_verify_access_tunnel_on_edges

 |https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=28639881&size=9166&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul14_02:52:47.340824.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [Test_TC59_DEV_STRESS_verify_edge_reload_ap_clients_stability     /   test1_verify_access_tunnel_on_edges|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=29107376&size=322086&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul14_02:52:47.340824.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] [

|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=28639881&size=9166&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul14_02:52:47.340824.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Fail Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=28639881&size=9166&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul14_02:52:47.340824.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Snip from fail Log:
 92950: Exception:
 92951: Traceback (most recent call last):
 92952: File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/DMZ-Guardian-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 92953: result = testfunc(func_self, **kwargs)
 92954: File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/DMZ-Guardian-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 4513, in test1_verify_access_tunnel_on_edges
 92955: if not dnac_handle.verify_access_tunnel_on_edges():
 92956: File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/DMZ-Guardian-sanity-common-Multi-job/services/dnaserv/dnaservices.py"", line 309, in __getattr__
 92957: raise AttributeError(err_msg)
 92958: AttributeError: 'DnaServices' object has no attribute 'verify_access_tunnel_on_edges'
  

 

Description : ",2022-07-15T11:39:08.760+0000,"Pull changes from Groot to Guardian:

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3fa45de443a5bacb14624aa7be73562cfb837391#services/dnaserv/lib/api_groups/cli_check/group.py","['Auton', 'Guardian', 'Issue']",Tran Lam,Closed,Avril Bower
SEEN-351,https://miggbo.atlassian.net/browse/SEEN-351,[Auton] -  Test_TC34_check_sgt_port_config  /   test5_add_vip,"Release : Guardian and Groot

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC34_check_sgt_port_config|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=428870&size=828758&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul14_02:52:47.340824.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test5_add_vip 

 

+Description :+

When adding Virtual ip address in ISE with Keywrap enabled, we dont need to have 'encryptionkey' field in the payload. if its mentioned, its throwing 400 error - ""errorCode"":""NCND00017"",""message"":""NCND00017: Update does not support modifying the following attributes: [EncryptionScheme]""

 

Payload to be used :

 \{""accountingPort"":1813,""authenticationPort"":1812,""ciscoISEUrl"":null,""ciscoIseDtos"":[{""description"":"""",""fqdn"":""TB2-DMZ-NON-CC0-32.cisco.com"",""password"":""Lablab123"",""sshkey"":"""",""ipAddress"":""10.12.254.6"",""subscriberName"":""pxgrid_client_1657778128"",""userName"":""admin""}],""ipAddress"":""10.12.254.6"",""pxgridEnabled"":true,""useDnacCertForPxgrid"":false,""isIseEnabled"":true,""port"":49,""protocol"":""RADIUS"",""retries"":1,""role"":""primary"",""sharedSecret"":"""",""timeoutSeconds"":10,""externalCiscoIseIpAddrDtos"":[\{""externalCiscoIseIpAddresses"":[{""externalIpAddress"":""123.1.1.1""}],""type"":""vip""}]}",2022-07-15T22:39:05.696+0000,"[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsanity_jenkins.2022Jul15_17:10:45.763006.zip&atstype=ATS]

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3226/overview] -Groot PR

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3227/overview] - Guardian PR Merged **ISO:**Groot RC5 2.1.560.70513
 **Script:**sanity_TB3_cert.py(optimized run)
 **Branch:**dnac-auto\job\sanity_tb3

*Impacted TC:*

[Task-change_ise_vtp_validate_fabric.py-113-ISEVtpChange|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-change_ise_vtp_validate_fabric.py-113-ISEVtpChange&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug17_04:16:54.085691.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS] /

[test5_add_vip /  test11_delete_vip
  
 *Failed Log:*
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-change_ise_vtp_validate_fabric.py-113-ISEVtpChange&begin=636421&size=42691&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug17_04:16:54.085691.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS]|

*Snip From Fail Log:*

**
 2034: Resource path full url: [https://10.195.227.92/api/v1/aaa/4594b5fe-d35a-48e9-8304-f210b65d324e]
 2035: Error Code: 400 URL:[https://10.195.227.92/api/v1/aaa/4594b5fe-d35a-48e9-8304-f210b65d324e] Data:{'timeout': 30, 'data': '{""ipAddress"": ""10.195.227.93"", ""sharedSecret"": """", ""protocol"": ""RADIUS"", ""role"": ""primary"", ""port"": 49, ""authenticationPort"": 1812, ""accountingPort"": 1813, ""retries"": 1, ""timeoutSeconds"": 10, ""isIseEnabled"": true, ""instanceUuid"": ""4594b5fe-d35a-48e9-8304-f210b65d324e"", ""rbacUuid"": ""f6debe2a-20b8-4f8a-8c51-bce30b7f4990"", ""state"": ""ACTIVE"", ""ciscoIseDtos"": [

{""subscriberName"": ""pxgrid_client_1660729302"", ""description"": """", ""password"": """", ""userName"": ""admin"", ""fqdn"": ""SSTB6-ISE.cisco.com"", ""ipAddress"": ""10.195.227.93"", ""trustState"": ""TRUSTED"", ""instanceUuid"": ""caddb03d-3b10-4de2-adb9-674152b01f34"", ""sshkey"": """", ""type"": ""ISE"", ""failureReason"": null, ""role"": ""PRIMARY""}

], ""externalCiscoIseIpAddrDtos"": [{""type"": ""vip"", ""externalCiscoIseIpAddresses"": [

{""externalIpAddress"": ""123.145.211.111""}

]}], ""encryptionScheme"": ""KEYWRAP"", ""messageKey"": """", ""encryptionKey"": """", ""useDnacCertForPxgrid"": false, ""pxgridEnabled"": true, ""iseEnabled"": true}'} Headers:\{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2MmZjYjc5YTQ0MGYzMTY2MGUwZTc0MWIiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjYyZmM5NDIyNDQwZjMxNjYwZTBlNTFlNSJdLCJ0ZW5hbnRJZCI6IjYyZmM5NDIxNDQwZjMxNjYwZTBlNTFlMyIsImV4cCI6MTY2MDgxOTc3NywiaWF0IjoxNjYwODE2MTc3LCJqdGkiOiIzMDI0YjVlNS1jNzM2LTQ4NWQtYmY1Ny1hNGM2N2UwMjQzNzIiLCJ1c2VybmFtZSI6InN5c2FkbWluIn0.W3Yl3oKAI4vqWQW-VXbnB3J14Hn7tg1Im5aitKALv-D8-vIs6_yZbgMHrs06a4lOTnsgqE949Xmog7aib7copH6P5FVg0j6k3y9DwqvhHLqz7wXlVwzOlF6h7juKbbCKLJar083LxwrqldZxuUwjt6o_yZSQlSl5YnEsj1ZlZHxbqkSfQU3YbQPNrdUoHhIAEhmVCXJK8igAqPnIKKnRtcIqA_Ke5kwZEoQ4VNY2nv4-fPbFHkLbGse0MA6sAIYdXISaQlJrfeXyG1hEe3gwqFCaR8YkeMvSJCAF1JxNkeGQcg9Qa7yjmzKecTR5yLPrYHQRcYzXxsMGSjug3C2QYg;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:{""response"":

{""errorCode"":""NCND00017"",""message"":""NCND00017: Update does not support modifying the following attributes: [EncryptionScheme]"",""href"":""/aaa/4594b5fe-d35a-48e9-8304-f210b65d324e""}

,""version"":""1.0""}
 ** https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3487/overview Please commit the fix for Ghost branch as well 

**ISO:**Ghost ISO 2.1.610.70361  
 **Script:**dnac-auto\job\api_auto\env_optimized_auto_job.py (optimized run)
 **Branch:**private/Ghost-ms/sanity_api_auto

*Impacted Testcase*  : [Test_TC1_check_sgt_port_config|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-change_ise_vtp_validate_fabric.py-113-ISEVtpChange&begin=3976&size=861088&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_optimized_auto_job.2022Aug24_20:54:16.390893.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] /[test5_add_vip|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-change_ise_vtp_validate_fabric.py-113-ISEVtpChange&begin=501995&size=43028&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_optimized_auto_job.2022Aug24_20:54:16.390893.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] , [test6_add_vip_network_settings|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-change_ise_vtp_validate_fabric.py-113-ISEVtpChange&begin=545023&size=86311&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_optimized_auto_job.2022Aug24_20:54:16.390893.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  ,[test10_delete_vip_network_settings|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-change_ise_vtp_validate_fabric.py-113-ISEVtpChange&begin=755715&size=2878&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_optimized_auto_job.2022Aug24_20:54:16.390893.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ,[test11_delete_vip|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-change_ise_vtp_validate_fabric.py-113-ISEVtpChange&begin=758593&size=41919&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_optimized_auto_job.2022Aug24_20:54:16.390893.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Failed log*: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-change_ise_vtp_validate_fabric.py-113-ISEVtpChange&begin=3976&size=861088&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_optimized_auto_job.2022Aug24_20:54:16.390893.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

  [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3508/overview]



PR for ghost ISO:Groot RC5 2.1.560.70513
 **Script:**sanity_TB3_cert.py(optimized run)
 ***Impacted TC:*[Task-change_ise_vtp_validate_fabric.py-113-ISEVtpChange|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-change_ise_vtp_validate_fabric.py-113-ISEVtpChange&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_optimized_auto_job.2022Aug30_22:00:12.306115.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /[Test_TC1_check_sgt_port_config|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-change_ise_vtp_validate_fabric.py-113-ISEVtpChange&begin=3976&size=1114765&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_optimized_auto_job.2022Aug30_22:00:12.306115.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test5_add_vip /test6_add_vip_network_settings  /test11_delete_vip

*Failure Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-change_ise_vtp_validate_fabric.py-113-ISEVtpChange&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_optimized_auto_job.2022Aug30_22:00:12.306115.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]
 *Summary :*
 The issue has again seen, encryption schema and code error, we ran with  lasted code 
 **
 2544: if ""encryption scheme"" in aaa_info:
 2545: NameError: name 'aaa_info' is not defined
 **
   Need the fix asap, failed in ::Groot RC5 2.1.560.70517: 

https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_08:08:22.902987.zip&atstype=ATS Fixed typos in mainline

  Release : Ghost (DMZ-TB)2.1.610.70412


 Script Name: env_optimized_auto_job.py
USECASE: [ISEVtpChange|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-change_ise_vtp_validate_fabric.py-111-ISEVtpChange&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep14_08:24:39.725524.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Failed log:
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-09/env_optimized_auto_job.2022Sep14_08:24:39.725524.zip&atstype=ATS]","['AWS_Sanity', 'Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized']",Andrew Chen,Closed,Avril Bower
SEEN-352,https://miggbo.atlassian.net/browse/SEEN-352,"hardcoded profile names, which is breaking the regression : Test_TC154_apply_custom_profile_issue_on_site_level"," 

There are multiple functions which is using the issue_name and the value of issue_name is not given with proper input so it picks the default hardcoded value. we have requested this change to update the profile name but looks like it is done only in one place, which is breaking the rest of test.

 

Team must verify the test in local setup before changing the code.

 

*def* modify_threshold_cpu_memory_issue(self, tmin=95, profile_id=*None*, issue_name=*""global_profile""*):

 

*def* create_site_issue_profile_assign_site(self, site=*""Global""*, issue_name=*""custom_global_profile""*):

   _""""""_

 

 

_Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=878349&size=58123&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul16_12:23:44.663909.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]_

 ",2022-07-16T23:12:48.876+0000,"The production test is failing needs to be fixed with priority  Related:

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-180

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-186

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-276

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-352

  https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3245/overview Merged to Groot.

Raji, can you also PR to Guardian? Hi Abhilash,

I have two queries about the Frey feature custom_profile, Could you please clarify.

Once we do the following steps and apply, assume the device cpu and memory value both are one and above. So how long is it going to take to generate an issue for memory and cpu high utilization ?<minimum of 3 violations with in max 35 minutes> Do you have these timings detail committed in bitbucket/testplan to refer the KPI ?
<this is the bitbucket link to know more>
switch memory : https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/THUN/repos/collectors-and-pipelines/browse/assurance-aggregations/src/on-prem/aggregations/snmp_memory_aggregation_trigger.pipeline#355
switch CPU : https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/THUN/repos/collectors-and-pipelines/browse/assurance-aggregations/src/on-prem/aggregations/snmp_cpu_aggregation_trigger.pipeline#568 PR merged","['Auton', 'Groot', 'Guardian', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-353,https://miggbo.atlassian.net/browse/SEEN-353,[Auton] - Guardian- Test_TC126_verify_inventory_insights   / test1_verify_speed_duplex_mismatch,"Release : Guardian and Groot

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 
 Test_TC126_verify_inventory_insights / test1_verify_speed_duplex_mismatch

Fail Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15991682&size=71792&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul14_14:52:31.910247.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Snip from Fail log:


44177:  Traceback (most recent call last):
44178:  File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/DMZ-Guardian-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
44179:  result = testfunc(func_self, **kwargs)
44180:  File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/DMZ-Guardian-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 9329, in test1_verify_speed_duplex_mismatch
44181:  if not dnac_handle.verify_speed_duplex_mismatch():
44182:  File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/DMZ-Guardian-sanity-common-Multi-job/services/dnaserv/lib/api_groups/Compilance/group.py"", line 1261, in verify_speed_duplex_mismatch
44183:  out = self.services.dnaconfig.testbed.devices[src_dev].configure(cmd)
44184:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 264, in __call__
44185:  self.call_service(*args, **kwargs)
44186:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 949, in call_service
44187:  self.process_dialog_on_handle(handle, dialog, timeout)
44188:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 986, in process_dialog_on_handle
44189:  self.get_service_result()
44190:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 240, in get_service_result
44191:  raise SubCommandFailure(
44192:  unicon.core.errors.SubCommandFailure: ('sub_command failure, patterns matched in the output:', ['^%\\s*[Ii]nvalid (command|input|number)'], 'service result', ""interface GigabitEthernet1/1 \r\nspeed 100 \r\nspeed 100 \r\n ^\r\n% Invalid input detected at '^' marker.\r\n\r\n"")
44193:  Test returned in 0:00:33.526507
44194:  Errored reason: sub_command failure, patterns matched in the output:
 ",2022-07-18T09:21:44.539+0000,"Related:

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-184

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-241

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-353 Its working fine with Guardian code and in groot dnac Image. We dont have guardian image in automation setup.

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=248673&size=255025&archive=%2Fws%2Frmukkama-sjc%2Fpyats%2Fusers%2Frakdomma%2Farchive%2F22-07%2Fsanity_TB1.2022Jul31_02:59:15.261313.zip&ats=%2Fws%2Frmukkama-sjc%2Fpyats&submitter=rakdomma&from=trade&view=all&atstype=pyATS]

 

Enter configuration commands, one per line. End with CNTL/Z. SN-FOC2416L3YW(config)#interface GigabitEthernet0/1 SN-FOC2416L3YW(config-if)#speed 100 SN-FOC2416L3YW(config-if)#duplex half SN-FOC2416L3YW(config-if)#do wr Building configuration... [OK] 

Please check manually if you are able to configure the speed  manually. Nothing to do with script. let me know if you see the issue will check

SN-FDO2027U0J4(config-if)#speed 100 speed 100 ^

  Its working fine with Guardian code and in groot dnac Image. We dont have guardian image in automation setup.

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=248673&size=255025&archive=%2Fws%2Frmukkama-sjc%2Fpyats%2Fusers%2Frakdomma%2Farchive%2F22-07%2Fsanity_TB1.2022Jul31_02:59:15.261313.zip&ats=%2Fws%2Frmukkama-sjc%2Fpyats&submitter=rakdomma&from=trade&view=all&atstype=pyATS]

 

Enter configuration commands, one per line. End with CNTL/Z. SN-FOC2416L3YW(config)#interface GigabitEthernet0/1 SN-FOC2416L3YW(config-if)#speed 100 SN-FOC2416L3YW(config-if)#duplex half SN-FOC2416L3YW(config-if)#do wr Building configuration... [OK] 

Please check manually if you are able to configure the speed  manually. Nothing to do with script. let me know if you see the issue will check

SN-FDO2027U0J4(config-if)#speed 100 speed 100 ^

 ","['Auton', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-354,https://miggbo.atlassian.net/browse/SEEN-354,[Auton] Groot :Test_TC3_generate_dhcp_server_config_on_fusion /test4_ise_cleanup_guest,"Release : Guardian and Groot

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 
Test_TC3_generate_dhcp_server_config_on_fusion /test4_ise_cleanup_guest



Fail log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3093979&size=322695&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul13_21:26:50.176370.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Snip From Fail log:


22100:  url [https://10.12.254.6:443/admin/rs/uiapi/policytable/radius|https://10.12.254.6/admin/rs/uiapi/policytable/radius]
22101:  headers \{'Referer': 'https://10.12.254.6:443/admin/'}
22102:  <Response [200]>
22103:  Traceback (most recent call last):
22104:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 910, in json
22105:  return complexjson.loads(self.text, **kwargs)
22106:  File ""/auto/pysw/cel8x/python64/3.8.2/lib/python3.8/json/__init__.py"", line 357, in loads
22107:  return _default_decoder.decode(s)
22108:  File ""/auto/pysw/cel8x/python64/3.8.2/lib/python3.8/json/decoder.py"", line 337, in decode
22109:  obj, end = self.raw_decode(s, idx=_w(s, 0).end())
22110:  File ""/auto/pysw/cel8x/python64/3.8.2/lib/python3.8/json/decoder.py"", line 355, in raw_decode
22111:  raise JSONDecodeError(""Expecting value"", s, err.value) from None
22112:  json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
22113: 
22114:  During handling of the above exception, another exception occurred:
22115: 
22116:  Traceback (most recent call last):
22117:  File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/DMZ-Guardian-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
22118:  result = testfunc(func_self, **kwargs)
22119:  File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/DMZ-Guardian-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 288, in test4_ise_cleanup_guest
22120:  if dnac_handle.dnaconfig.cleanup_ise_guest(dnac_handle.input_data[""WIRELESS_GUEST_PORTAL_LIST""]):
22121:  File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/DMZ-Guardian-sanity-common-Multi-job/services/commonlibs/sftopology.py"", line 7516, in cleanup_ise_guest
22122:  if not self.iseadminapi.clear_policies(policy_list, ise_version):
22123:  File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/DMZ-Guardian-sanity-common-Multi-job/services/iseserv/ise_admin_api.py"", line 220, in clear_policies
22124:  table_id = self.no_ui_get_policytable_radius_id_default(session=sesh)
22125:  File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/DMZ-Guardian-sanity-common-Multi-job/services/iseserv/ise_admin_api.py"", line 78, in no_ui_get_policytable_radius_id_default
22126:  for set in resp_tables.json():
22127:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 917, in json
22128:  raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)
22129:  requests.exceptions.JSONDecodeError: [Errno Expecting value] <!doctype html>
22130:  <!--**************************************************-->
22131:  <!-- Copyright (c) 2020 Cisco Systems, Inc.-->
22132:  <!-- All rights reserved.-->
22133:  <!--**************************************************-->",2022-07-18T09:30:35.305+0000,"Related:

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-354

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-268

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-233

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-182 Will validate API chnages  [https://10.12.254.6:443/admin/rs/uiapi/policytable/radius|https://10.12.254.6/admin/rs/uiapi/policytable/radius]

for ISE: 3.2 Please I need ISE 3.2 to test. [~63f50bfce8216251ae4d59d5] do you need ISE for Read-Only access? then please use 10.195.227.49 admin/Lablab123. if you need to modify anything we will let you know when you can use it  Thank you [~70121:27365d3e-893a-4523-9fca-b89a7e8d0d4c] , please let me know when I can use it.

  PR: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3735/diff#services/iseserv/ise_admin_api.py Merged to Guardian, Groot, Ghost. Tc Passed in  Latest Gaudian, 
Moving to close state 
+*pass Log:*+
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1728839&size=589162&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct21_17:11:06.496817.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Groot', 'Guardian', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-355,https://miggbo.atlassian.net/browse/SEEN-355,[Auton] - Guardian- Test_TC23_lan_automation_bringup_devices_on_level2,"Release : Guardian

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 
 Test_TC23_lan_automation_bringup_devices_on_level2

Fail Log:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10282623&size=240392&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul13_21:26:50.176370.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

The issue was, there was not re-try for the test5, so it was failing

 
||[test5_site_verify_device_to_provisioned_lan_automation_inventory|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10515855&size=5251&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul13_21:26:50.176370.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FDMZ-SANITY%2Ftest%2FDMZ-Guardian-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=1641&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul13_21:26:50.176370.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Failed|00:00:00|test5_site_verify_device_to_provisioned_lan_automation_inventory| |
||[test6_stop_lan_automation|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10521106&size=859&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul13_21:26:50.176370.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FDMZ-SANITY%2Ftest%2FDMZ-Guardian-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=1658&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul13_21:26:50.176370.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Blocked|00:00:00|test6_stop_lan_automation| |
||[test7_verify_lan_automation_completed|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10521965&size=871&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul13_21:26:50.176370.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FDMZ-SANITY%2Ftest%2FDMZ-Guardian-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=1673&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul13_21:26:50.176370.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Blocked|00:00:00|test7_verify_lan_automation_completed|

 

 

 ",2022-07-18T09:38:26.505+0000,"Issue is fixed in latest.

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9506680&size=15736&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep01_07:03:55.623805.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 ","['Auton', 'Guardian', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-356,https://miggbo.atlassian.net/browse/SEEN-356,[Auton] : Ghost-Test_TC126_verify_inventory_insights/test2_verify_VLAN_mismatch,"Ghost  Version :2.1.560.70410

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

Test_TC126_verify_inventory_insights/test2_verify_VLAN_mismatch

Fail Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=105563376&size=9499&archive=env_auto_job.2022Jul11_16:57:56.505255.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Snip from Fail Log:


62443:  api_switch_call called:
362444:  {}
362445:  Resource path full url: [https://10.30.0.100/api/v1/network-device]
362447:  Traceback (most recent call last):
362448:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
362449:  result = testfunc(func_self, **kwargs)
362450:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 9372, in test2_verify_VLAN_mismatch
362451:  if not dnac_handle.VLAN_mismatch():
362452:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/lib/api_groups/Compilance/group.py"", line 1364, in VLAN_mismatch
362453:  resource_path = '/assurance/v1/network-device/{}/neighbor-topology'.format(device_ids)
362454:  UnboundLocalError: local variable 'device_ids' referenced before assignment
362455:  Test returned in 0:00:00.250988
362456:  Errored reason: local variable 'device_ids' referenced before assignment",2022-07-18T13:02:57.984+0000,"Here is the PR

Groot:

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3303/overview Ghost:

 

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3304/overview","['Auton', 'Ghost', 'Groot', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-357,https://miggbo.atlassian.net/browse/SEEN-357,[Auton] - Ghost-TC143_system_health_test/test3_generate_ISE_certificate,"Release :Ghost 2.1.610.70249

 

issue: looks like the time module is not imported into the code

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[TC143_system_health_test/|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=112819779&size=199521&archive=env_auto_job.2022Jul11_16:57:56.505255.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[test3_generate_ISE_certificate|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=112859996&size=12343&archive=env_auto_job.2022Jul11_16:57:56.505255.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Fail Log:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=112859996&size=12343&archive=env_auto_job.2022Jul11_16:57:56.505255.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

+Description :+
 snip from fail log:
 381324: Waiting for event ""spinner_element""; timeout: 30
 381325: Event parsed. Time elapsed: 0.36
 381326: Waiting for event ""table_loading_indicator""; timeout: 30
 381327: Event parsed. Time elapsed: 0.04
 381328: Saved screenshot: ""./iseui-20220712-150757.png""
 381329: Test returned in 0:01:33.734158
 381330: Failed reason: module 'time' has no attribute 'clock'
 381331: The result of section test3_generate_ISE_certificate is => FAILED
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=112819779&size=199521&archive=env_auto_job.2022Jul11_16:57:56.505255.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

 ",2022-07-18T15:07:12.765+0000,"Production test is failing [~63f50bcece6f37e5ed93c87e], pls. check on below line and use the appropriate attribute:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/services/ise3/lib/pages/administration/certificates/page.py?at=refs%2Fheads%2Fprivate%2FGroot-ms%2Fsanity_api_auto#14]

  [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f52754ae7b31a4a4dcc3512d44f34babe55bf8db#services/ise3/lib/pages/administration/certificates/page.py]

shockwave

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9f450141f88e014526ca43dfe67a28981f48b3a3#services/ise3/lib/pages/administration/certificates/page.py]

ghost

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/b63e2ed3c41ef93c81e42f53a0066f32efed4089#services/ise3/lib/pages/administration/certificates/page.py]

groot

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/7b8284addf59e6b61fd51b377b073f74aa9628a3#services/ise3/lib/pages/administration/certificates/page.py]

guardian

  TC is  passed latest grrot , hence moving  to the close  state
Groot(DNAC2343)  Pass Log:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10351677&size=18821&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct19_18:08:12.427976.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Ghost', 'Groot', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-358,https://miggbo.atlassian.net/browse/SEEN-358,[Auton] - Ghost-TC143_system_health_test/test6_break_ISE_credentials/test_8_revert_state,"Release :Ghost 2.1.610.70249

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[TC143_system_health_test/|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=112819779&size=199521&archive=env_auto_job.2022Jul11_16:57:56.505255.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[test6_break_ISE_credentials|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=112933168&size=34086&archive=env_auto_job.2022Jul11_16:57:56.505255.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

test_8_revert_state
Fail Log:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul11_16:57:56.505255.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]



+Description :+

When adding Virtual ip address in ISE with Keywrap enabled, we dont need to have 'encryptionkey' field in the payload. if its mentioned, its throwing 400 error - ""errorCode"":""NCND00017"",""message"":""NCND00017: Update does not support modifying the following attributes: [EncryptionScheme]

 

Snip from fail Log:

381982:  Error Code: 400 URL:https://10.30.0.100/api/v1/aaa/2d74c753-7933-4574-b11e-fea896b5710a Data:\{'timeout': 30, 'data': '{""ipAddress"": ""10.30.0.101"", ""sharedSecret"": """", ""protocol"": ""RADIUS"", ""role"": ""primary"", ""port"": 49, ""authenticationPort"": 1812, ""accountingPort"": 1813, ""retries"": 1, ""timeoutSeconds"": 10, ""isIseEnabled"": true, ""instanceUuid"": ""2d74c753-7933-4574-b11e-fea896b5710a"", ""rbacUuid"": ""68032960-c4f0-459a-9a73-12d72550295c"", ""state"": ""ACTIVE"", ""ciscoIseDtos"": [{""subscriberName"": ""pxgrid_client_1657586392"", ""description"": """", ""password"": ""Lablab123"", ""userName"": ""admin"", ""fqdn"": ""SSTB7-ISE.cisco.com"", ""ipAddress"": ""10.30.0.101"", ""trustState"": ""TRUSTED"", ""instanceUuid"": ""17be6d89-d2d3-4071-b3ca-9f1559f6e0db"", ""sshkey"": """", ""type"": ""ISE"", ""failureReason"": null, ""role"": ""PRIMARY""}], ""externalCiscoIseIpAddrDtos"": [], ""encryptionScheme"": ""KEYWRAP"", ""messageKey"": """", ""encryptionKey"": """", ""useDnacCertForPxgrid"": false, ""iseEnabled"": true, ""pxgridEnabled"": true}'} Headers:\{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2MmNjOWZkMmNlZjkxNzdlMzAyMDlhNmEiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjYyY2M5ZmQxY2VmOTE3N2UzMDIwOWE2OSJdLCJ0ZW5hbnRJZCI6IjYyY2M5ZmQwY2VmOTE3N2UzMDIwOWE2NyIsImV4cCI6MTY1NzY2Nzk5OSwiaWF0IjoxNjU3NjY0Mzk5LCJqdGkiOiJhZTViYWUxZC04ZjNjLTQyZjUtYTdjYS0yZGJiZTgzNjdjYWQiLCJ1c2VybmFtZSI6ImFkbWluIn0.kJufIv8bZUMHG3q02Kn4zY-WyAs5Hvet3QtoDuxdKTOS56tLRHfJ_Vlz8bvHhW-yIRDFSU2iXoIpPuefJNkpVq_jTuMgntM_LP9OqN77BhjsotxwvUusmmIzXEHbgUtCk3TLvT7NhTPnOjryzaSKEe5SS_VK9yDqMAOrh8OxlUU3v6FC18E5WMwRdiwe3J978qbY0Mx7ENRqhTSX2pqbUX2RyNSlUag2bBMBgLRZzLJeaZhuHBwRuUUPJp1xsFbTsh3mW0gq6r7nJrtuCtc-EkZJ1AvB9TW3bDC0U29wmZgW8QO9sdnJ-taoIZjkxSkOx4yfEsH776sKoWC8ieUmGA;path=/;secure=true;HttpOnly=true;Expires=Tue, 12-Jul-22 23:19:59 GMT; Secure; HttpOnly; SameSite=Strict'} Message:\{""response"":{""errorCode"":""NCND00017"",""message"":""NCND00017: Update does not support modifying the following attributes: [EncryptionScheme]"",""href"":""/aaa/2d74c753-7933-4574-b11e-fea896b5710a""},""version"":""1.0""}",2022-07-18T15:17:31.816+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3248/overview [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3249/overview] - groot (previous one for guardian)

 ","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-362,https://miggbo.atlassian.net/browse/SEEN-362,"[Groot] - ""DENY_0_FABRIC_PREFIX"" config verification Failure","*Uber ISO Version tested :* Promoted Groot Uber ISO - *2.1.560.70428, Non-FIPS, PUBSUB enabled* 

*Script Name :* solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Testbed :* MSTB1

*Testcases Impacted :*  

 [Test_TC35_DNAC_verifying_configuration_lisp_on_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17604659&size=2508998&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul11_05:25:01.339319.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

-> [test1_verify_configuration_fabric1|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17605415&size=2324336&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul11_05:25:01.339319.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Fail Log :*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul11_05:25:01.339319.zip&atstype=ATS]

 

*Description :* 

We have enabled PUBSUB and we see device config verification failure for “show run | s lisp” with DENY_0_FABRIC_PREFIX after fabric deployment workflow. We are not sure if this is valid failure or false failure as we see the same configs present on the devices (9500 and 9600 – Fabric role as Border+MAPPServer) under “show run | s bgp”

 ",2022-07-18T16:21:02.546+0000,"The configuration should be under router bgp in case of dual borders.

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/b9848e483785628d1fbebc56358373af80e9c87d Hi Tran,

We have verified using the latest Groot code and its working fine now.

 

Reference log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug09_08:14:45.437094.zip&atstype=ATS] -> Refer TC35 Hey Tran,

Looks like Commit is not done for Guardian branch. We are observing the same issue during Guardian Patch2 RC1 execution. Could you please commit the fix to private/Guardian-ms/api-auto ?

*Failed log:* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=21344169&size=2384645&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug28_05:14:30.246780.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS Hey Tran,

We are still observing the same issue during Guardian Patch2 RC2 execution as well. Looks like the still the fix commit is not yet added on Guardian branch. Could you please commit the fix to private/Guardian-ms/api-auto ?

*Failed log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep24_10:49:37.492308.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS] -> Refer TC35 Looks like the this Jira ticket was auto-moved to Resolved state. Reopening the issue as the problem is seen in Guardian. On Guardian P2,2.1.515.70134 ,we have observed same issue while testing on AWS-Multisite testbed.

*Uber ISO Version tested :*  GuardianP2 Uber ISO - *2.1.515.70134, Non-FIPS, PUBSUB enabled* 

*Script Name :* solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Testbed :* AWS-MSTB

*Failure Log:*  

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4755448&size=2269100&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fauto_MS_job.2022Nov14_14:28:44.325878.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  -->Please refer TC 35 Jira ticket was auto-moved to Resolved state due to merge pull initiated by Vijay. Reopening the issue as the problem is seen on Guardian. Hey [~62d2fe9f8afb5805e5d5af49],

We are still observing the same issue during Guardian Patch4 execution as well. Looks like the still the fix commit is not yet added on Guardian branch. Could you please commit the fix to private/Guardian-ms/api-auto ?

*Failed log on Guardian Patch4 RC2 - 2.1.518.72310 :* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fsr_mb_multi_sites_mdnac.2023Feb10_10:26:16.865638.zip&atstype=ATS] -> Refer TC35.1 & TC37.2



*Failed log on Guardian Patch4  Pre-RC2 - 2.1.518.72292 :* 
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fsr_mb_multi_sites_mdnac.2023Jan31_06:23:18.212366.zip&atstype=ATS] -> Refer TC35.1 Cherry picked to Guardian. Issue is no more observed during *Guardian Patch4 RC3 - 2.1.518.72319* testing. 

Pass log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fsr_mb_multi_sites_mdnac.2023Feb24_19:58:20.634450.zip&atstype=ATS] -> Refer TC35.1","['AWS_MSTB', 'Auton', 'Groot', 'Guardian', 'Issue', 'MSTB1', 'MSTB2', 'Multisite']",Tran Lam,Closed,Avril Bower
SEEN-364,https://miggbo.atlassian.net/browse/SEEN-364,[Groot] - Access Tunnel Summary cli check failure on 9400 device,"*Uber ISO Version tested :* Promoted Groot Uber ISO - *2.1.560.70428, Non-FIPS, PUBSUB enabled* 

*Script Name :* solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Testbed :* MSTB1

*Testcases Impacted :*  

[Test_TC55_WIRELESS_VERIFY_ACCESS_TUNNEL_ON_EDGE_ROUTER|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1708073&size=49628&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul12_02:02:41.599576.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

-> [test1_verify_access_tunnel_on_edges|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1708821&size=26679&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul12_02:02:41.599576.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[Test_TC57_DEV_STRESS_verify_edge_reload_ap_clients_stability|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3362096&size=151988&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul12_02:02:41.599576.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

-> [test1_verify_access_tunnel_on_edges|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3487023&size=26871&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul12_02:02:41.599576.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Fail Log :*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul12_02:02:41.599576.zip&atstype=ATS]

 

*Description :* 

We see that Access Tunnel summary check for 9400 device is failing due to incorrect cli being used to query on 9400 device.

 ",2022-07-18T16:32:28.095+0000,"Related: 

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-364

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-371 Same issue seeing on 2.1.560.70467_Airgap iso .

PR raised:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3256/overview]

Trade logs:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19312294&size=22014&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F22-07%2Fthree_sites.2022Jul13_00:16:49.205109.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

Issue:

TB4-DM1-4SLOT-9400#
52423:  Access tunnels foud:12
52424:  +++ TB4-DM1-4SLOT-9400 with via 'a': executing command 'show platform software access-tunnel switch active r0' +++
show platform software access-tunnel switch active r0 show platform software access-tunnel switch active r0  ^ % Invalid input detected at '^' marker. TB4-DM1-4SLOT-9400#

 
 
 
Working scenario:
9300:
======
show access-tunnel summary
show platform software access-tunnel switch active r0
show platform software fed switch active ifm interfaces access-tunnel

9400:
======
show access-tunnel summary
show platform software access-tunnel R0
show platform software fed active ifm interfaces access-tunnel https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0be8645a25ed60d47be1af4cb2f2bedbc4537006 Hi Tran,

We have verified using the latest Groot code and its working fine now.

 

Reference log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug09_08:14:45.437094.zip&atstype=ATS] -> Refer TC55 Issue is resolved and hence closing the JIRA.

Please refer the comments as in 09/Aug/22 9:21 PM for more details.","['Auton', 'Groot', 'Issue', 'MSTB1', 'MSTB2', 'Multisite']",Tran Lam,Closed,Avril Bower
SEEN-365,https://miggbo.atlassian.net/browse/SEEN-365,[Groot] - SVL switchover event TC failure despite event generated,"*Uber ISO Version tested :* Promoted Groot Uber ISO - *2.1.560.70428, Non-FIPS, PUBSUB enabled* 

*Script Name :* solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Testbed :* MSTB1

*Testcases Impacted :*  

 

 [Test_TC56_verify_assurance_health_nw_health_border_edge_wlc_ext_node|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1757701&size=1604395&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul12_02:02:41.599576.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

-> [test6_verify_assurance_SVL_border_edge|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2871216&size=255458&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul12_02:02:41.599576.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Fail Log :*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul12_02:02:41.599576.zip&atstype=ATS]

 

*Description :* 

We see that SVL switchover event is indeed getting generated but the failure reason at the end of the TC reads SVL data failed, which is bit misleading from user perspective.

Actual reason for failure on checking further was due to Device resync for SVL device had taken longer time.

We would want the TC to be enhanced to 

1) To print the actual reason for failure at the end of TC
 2) Total time taken after multiple resync in case of failure and Device manage status at the end of resync Failure. ",2022-07-18T16:43:29.274+0000,"[~62d2fec15d6f5fd2c3db8f9f] can you share previous pass log. Also, is the testbed available to triage? Hi Raji,

Here is the pass log on Cyclops Patch3- [https://earms-trade.cisco.com/tradeui/logs/details?ats=%2Fusers%2Fpawansi%2Fpyatsnew&client=web&host=st-ucs-sanity.cisco.com&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-03%2Fsr_mb_multi_sites_mdnac.2022Mar03_14:41:29.546116.zip] -> Refer TC56.12

There was an issue with SVL switchover event generation DNAC from many releases. It got fixed recently. So recent set of logs would have failures as event itself was not generating.  That is why I have shared you pass log on Cyclops. 

 

However the Failed log specified in this Jira reported has the switchover event - *""NY-CP-9300.cisco.com"" underwent an HA switchover.*

 

Regarding setup availability, we can current moving to Groot RC4. Once we have basic TCs executed will be able to share the setup. Hi Raji,

We are still observing same issue during our Regression testing. Currently when testing on Halleck release, we see same pattern of failure.

*Failed log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May08_03:36:43.138644.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May08_03:36:43.138644.zip&atstype=ATS] → Refer TC56.11

Could you please have a look into this?

  Regards
Sandeep S [~accountid:63f50bf5e8216251ae4d59cf] : As discussed during our syncup for this issue, to start with

a) The failure message needs to be fixed for Managed status fail for device state and switchover event failure.

b) Need change the print message for Device state after every iteration of device resync

Meanwhile, since we had abrupt server down issue we could not debug live on this issue. Will try to repro the issue on ongoing Hulk testing and update.

As requested by you, here is another recent pass log on Ghost Patch1 - 149 :

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fsr_mb_multi_sites_mdnac.2023Mar15_21:17:30.732451.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fsr_mb_multi_sites_mdnac.2023Mar15_21:17:30.732451.zip&atstype=ATS] -> Refer 56.11 Hi,

Same issue is seen on ESXI based execution

Failed log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19749692&size=540450&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul13_23:02:37.104064.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19749692&size=540450&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul13_23:02:37.104064.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Recent device sync changes should resolve this issue [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e0b33aa2a49fcc0835f6b363bdbd8a077453a47f#services/dnaserv/lib/api_groups/device_resync/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e0b33aa2a49fcc0835f6b363bdbd8a077453a47f#services/dnaserv/lib/api_groups/device_resync/group.py]. Incase it still takes longer time  , it should be a product issue.","['Auton', 'Ghost', 'Groot', 'Guardian', 'Halleck', 'Hulk', 'Issue', 'MSTB1', 'MSTB2', 'Multisite', 'assurance']",Raji Mukkamala,Resolved,Avril Bower
SEEN-366,https://miggbo.atlassian.net/browse/SEEN-366,[Groot] - VN Services are shows down for Borders and Transit devices under device 360 page,"*Uber ISO Version tested :* Promoted Groot Uber ISO - *2.1.560.70428, Non-FIPS, PUBSUB enabled* 

*Script Name :* solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Testbed :* MSTB1

*Testcases Impacted :* In general Assurance TCs checking for device 360 page.   

 *Issue Type : Fix*

*Description :* 

 

With PUBSUB enabled, after Fabric deployment workflow we observed health score dropped to 1 and continued to remain in same state for very long period of time on devices with fabric role as Border+MAPP server. The reason for that being multiple sessions down under VN Services under Virtual Network section on device 360 page. Please refer the attachment for the issue.  !bca0bfd2-ee9a-499b-8119-0ae24a3e1b09.PNG!

 

On checking with Nethra, got info that it was a known Auton - *CSCwb83573* which was fixed on Groot. But we are still observing the same issue. P

Could you please have a look into this and confirm?

 

 ",2022-07-18T17:15:00.087+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/1da4654d721f1d6c553d674e57e1d844fb18f066 handling L3 handoff for guest VN Hi Raji,

Same issue was still observed during Groot RC6 and Groot RC7 testing on Multisite profile. This is causing confusion during debugging other Assurance related issues, as it would be difficult to isolate the actual issue if there is health score drop.
 
 I had a syncup with Nethra and for them itseems its fixed in Sanity profile.

Could you please have a look and add fix for this in Multisite as well? 

Please refer below error snip.

  !Health_Score_Drop_issue.png! Looks like the this Jira ticket was auto-moved to Resolved state. Reopening the issue as the problem is seen on Groot. [~accountid:62d2fec15d6f5fd2c3db8f9f] Could you please confirm if the issue is still observed in regression [~accountid:63f50bf5e8216251ae4d59cf] : We have not seen any TC failure due to Health score being 1 on devices in recent regression executions. Actually we do not have TC to validate health score on all devices after Fabric deployment on devices workflow. Also I have not got a chance to check this issue manually at that point. Probably we need to have a TC introduced to check the assurance health score after fabric deployment workflow, to validate on this issue. 

Could you please let me know you thought on this? Are you checking the DNAC UI manually when Fabric testcases are run but Fusion config is not done through script? It is expected behaviour in between TC32 to 40 and the issue should go away when fusion router is configured in TC40/41. Confirm if it is the behaviour or something else. Script is not suppose to align with how you Manually look at the cluster. You can follow this guide to check the issue state.  ","['Auton', 'Groot', 'Issue', 'MSTB1', 'Multisite', 'assurance']",Raji Mukkamala,Closed,Avril Bower
SEEN-367,https://miggbo.atlassian.net/browse/SEEN-367,[Groot] - ISE profile cleanup/deletion Fails with 403 Client error on Author and Reader cluster nodes,"*Uber ISO Version tested :* Promoted Groot Uber ISO - *2.1.560.70428, Non-FIPS, PUBSUB enabled, CSRF flag marked True*

*Script Name :* solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Testbed :* MSTB1
 

*Failed logs:* 
1) [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul07_13:12:05.797376.zip&atstype=ATS]-> Refer TC3.5

2) [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul15_09:38:16.653780.zip&atstype=ATS] -> Refer TCs - 91.3, 91.12, 93.9, 93.21, 93.34, 94.4, 95.12

 

*Description :* 

ISE profile cleanup/update Fails with 403 Client error. 
Please note that CSRF is enabled on ISE during the testing as part of Guardian feature. Could this be due to this? If so, how can it be handled?

 

 ",2022-07-18T17:25:12.276+0000,"Hi Team,

Changing the Severity accordingly as this is affecting mDNAC and DR testcases and causing false failures.

 

  Regards
Sandeep S [~62d2fec15d6f5fd2c3db8f9f], I am looking at it.

thanks. The CSRF flag has to be added for both author and reader json files.  [~62d2fec15d6f5fd2c3db8f9f], pls. confirm if the suggestion from [~63f50bfce8216251ae4d59d5] is in place and you don't see the reported Error any more.  Issue is no more observed after adding the suggested flag. Hence closing the Jira ticket.","['Auton', 'Groot', 'Issue', 'MSTB1', 'Multisite']",Vinoth Kumar Kutty Krishnamoorthy,Closed,Avril Bower
SEEN-368,https://miggbo.atlassian.net/browse/SEEN-368,[Auton] : Need to update condition to check eWLC in ITSM feature,"Release :Guardian, Groot

Script Name :  solution_test_sanityecamb_lan.py

Testcases Impacted : 

[Test_TC177_ITSM_ticket_generation_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1076855&size=372079&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul18_21:01:29.829653.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 

[test8_schedule_dev_deletion|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1367289&size=1668&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul18_21:01:29.829653.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[test9_reject_dev_deletion_request|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1368957&size=30952&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul18_21:01:29.829653.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[test10_schedule_dev_deletion|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1399909&size=1671&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul18_21:01:29.829653.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[test11_approve_dev_deletion_request|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1401580&size=30971&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul18_21:01:29.829653.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[test13_readd_dev|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1443882&size=4884&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul18_21:01:29.829653.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Fail Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1076855&size=372079&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul18_21:01:29.829653.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

+Description :+

In ITSM - device deletion tc, the script is checking only for device with WLC role. The condition has to be updated to check both WLC and eWLC roles.

 

Snip from fail Log:
6564:  Executing testcase Test_TC177_ITSM_ticket_generation_test test 177.8 ""test8_schedule_dev_deletion"".
6565:  wlc device not found
6566:  Library group ""itsm"" method ""create_dev_deletion_event"" returned in 0:00:00.000920
6567:  Test returned in 0:00:00.001925
6568:  Failed reason: Dev deletion from fab scheduling failed
6569:  The result of section test8_schedule_dev_deletion is => FAILED
 ",2022-07-20T00:03:17.169+0000,Added to groot and guardian,"['Auton', 'Groot', 'Guardian', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-369,https://miggbo.atlassian.net/browse/SEEN-369,[Auton] [Groot] Script Unable to Delete the Edge Device from the Fabric,"Hi Tran,

In our solution testing we are trying to integrate the ""Border priority"" feature in our Multisite TB2 (Non-DR) setup. We are observing the below issue,

!image-2022-07-20-22-53-42-716.png!

TC Executed : [TC153_Verify_border_priority_with_fabric_update|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5084665&size=4423009&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb2_three_sites.2022Jul15_08:30:36.679318.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 

Branch : private/Groot-ms/api-auto

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5994220&size=1227176&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb2_three_sites.2022Jul15_08:30:36.679318.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 ",2022-07-20T17:28:36.463+0000,"From configs/sr_mb2/sr_mb2_testbed.yaml, there are TwoGigabitEthernet1/0/20, TwoGigabitEthernet1/0/7, TwoGigabitEthernet1/0/9, TwoGigabitEthernet1/0/4 connected to Clients and the script did clear port as per log.

However from log, seem there was interface TwoGigabitEthernet1/0/15 which was not added in yaml, configured with Closed Authentication.

Can you check which client this interface TwoGigabitEthernet1/0/15 connecting to? And if a valid client, can you add it into yaml?

 

28767: ------------------------------------------------------------
28768: Result: Cleared interface config for edge SJC-FE-9300-1 client:\{'name': 'SJC-IPPHONE-DUMMY1', 'type': 'ipphone', 'intf': <Interface object 'TwoGigabitEthernet1/0/20' on 'SJC-FE-9300-1' at 0x7fad43e71670>, 'sw': 'SJC-FE-9300-1'}
28769: ------------------------------------------------------------

29096: ------------------------------------------------------------
29097: Result: Cleared interface config for edge SJC-FE-9300-1 client:\{'name': 'TSIM-5508-1', 'type': 'tsim', 'intf': <Interface object 'TwoGigabitEthernet1/0/4' on 'SJC-FE-9300-1' at 0x7fad43e71700>}
29098: ------------------------------------------------------------

29441: ------------------------------------------------------------
29442: Result: Cleared interface config for edge SJC-FE-9300-1 client:\{'name': 'AP68CA.E472.5318', 'type': 'sensor', 'intf': <Interface object 'TwoGigabitEthernet1/0/9' on 'SJC-FE-9300-1' at 0x7fad43e716d0>}
29443: ------------------------------------------------------------

29777: ------------------------------------------------------------
29778: Result: Cleared interface config for edge SJC-FE-9300-1 client:\{'name': 'ixia', 'type': 'ixia', 'intf': <Interface object 'TwoGigabitEthernet1/0/7' on 'SJC-FE-9300-1' at 0x7fad43e716a0>, 'ixiaintf': '11/3'}
29779: ------------------------------------------------------------

 

SJC-FE-9300-1:
 interfaces:
 #GigabitEthernet0/0:
 # link: eth-SJC-FE-9300-1-0-SR-MB2-MGMT-1-3
 # type: ethernet
 TenGigabitEthernet1/1/2:
 link: eth-SJC-FE-9300-1-2-SJC-FE-9300-2-2
 type: ethernet
 TenGigabitEthernet1/1/1:
 link: eth-SJC-FE-9300-1-1-SJC-FE-9300-2-1
 type: ethernet
 TwoGigabitEthernet1/0/3:
 link: eth-SJC-IM-9300-1-3-SJC-FE-9300-1-3
 type: ethernet
 TwoGigabitEthernet1/0/20:
 link: eth-SJC-FE-9300-1-20-SJC-ipphone-dummy-1
 type: ethernet
 TwoGigabitEthernet1/0/7:
 link: eth-SJC-FE-9300-1-7-ixia-3
 type: ethernet
 TwoGigabitEthernet1/0/9:
 link: eth-SJC-FE-9300-1-9-sensor
 type: ethernet
 TwoGigabitEthernet1/0/4:
 link: eth-SJC-FE-9300-1-4-TSIM-5508-1-1
 type: ethernet

 

 

 

 

28459: api_switch_call called:
28460: {'data': [{'id': 'a4aaedcc-57d4-4d4e-a698-ef6d85956648', 'instanceId': 663671, 'instanceCreatedOn': 1657688235488, 'instanceUpdatedOn': 1657688235488, 'instanceVersion': 64, 'createTime': 1657688235481, 'deployed': False, 'isSeeded': False, 'isStale': False, 'lastUpdateTime': 1657899968622, 'name': 'SJC-FE-9300-1', 'namespace': 'dcbd424a-d643-44ab-acf6-253c5b4eec4c', 'provisioningState': 'DEFINED', 'resourceVersion': 64, 'targetIdList': [], 'type': 'DeviceInfo', 'cfsChangeInfo': [], 'anchoredManagedSites': [], 'configs': [], 'managedSites': [], 'networkDeviceId': 'b59edcdb-b9e6-4f11-8018-2603aaf49374', 'primaryManagedSites': [], 'rlanList': [], 'roles': ['EDGENODE'], 'saveWanConnectivityDetailsOnly': False, 'secondaryManagedSites': [], 'siteId': '732fea8d-9c67-43d5-8f93-2a053b9b2159', 'tertiaryManagedSites': [], 'akcSettingsCfs': [], 'deviceInterfaceGroupInfo': [], 'deviceInterfaceInfo': [

{'id': '3b1e1dc2-f7c1-487a-a70b-6c81e0f218d3', 'instanceId': 955973, 'instanceCreatedOn': 1657736701502, 'instanceUpdatedOn': 1657736701502, 'instanceVersion': 1, 'apManagementEnabled': False, 'connectedDeviceType': 'USER_DEVICE', 'connectedToSubtendedNode': False, 'description': 'LAN Interface type USER_DEVICE', 'dhcpEnabled': False, 'downloadBW': 0.0, 'interfaceId': '574cbcf0-f710-48a2-8987-bb7df95e9f49', 
'interfaceName': 'TwoGigabitEthernet1/0/15', 
'ipAddressMask': 0, 'isBPDUGuardEnabled': True, 'isTrusted': False, 'memberInterfaceList': [], 'natEnabled': False, 'portNum': 0, 'role': 'LAN', 'uploadBW': 0.0, 'authenticationProfile': \{'id': 'e02d5a30-b0d5-4ffe-98c9-7814cdca324e', 'instanceId': 767774, 'instanceCreatedOn': 1657692229186, 'instanceUpdatedOn': 1657692229186, 'instanceVersion': 1, 'bpduGuardEnabled': True, 'deploymentMode': 'CLOSED', 'dot1xToMabFallbackTimeout': '21', 'hostMode': 'multi_auth', 'name': 'Closed Authentication', 'order': 'dot1x', 'priority': 'dot1x mab', 'profileUuid': 'dcbd424a-d643-44ab-acf6-253c5b4eec4c', 'profileVersion': 1, 'type': 'WIRED_DOT1X', 'wakeOnLan': False, 'webauthBypassMap': [], 'displayName': '18123b3b[Closed Authentication,dcbd424a-d643-44ab-acf6-253c5b4eec4c]'}, 'deviceInterfaceInfoL2SegmentMapping': [], 'deviceInterfaceInfoSegmentMapping': [], 'l2Segment': [], 'segment': [], 'displayName': '8bc32999[574cbcf0-f710-48a2-8987-bb7df95e9f49]'},

{'id': '4ef7bc49-d691-422e-9b8a-814f3d6a89d9', 'instanceId': 955959, 'instanceCreatedOn': 1657703818671, 'instanceVersion': 26, 'apManagementEnabled': False, 'connectedDeviceType': 'USER_DEVICE', 'connectedToSubtendedNode': False, 'description': 'LAN Interface type USER_DEVICE', 'dhcpEnabled': False, 'downloadBW': 0.0, 'interfaceId': 'b5c6b3c4-8b73-4756-a30d-99edb7a9905b', 
'interfaceName': 'TwoGigabitEthernet1/0/9', 
'ipAddressMask': 0, 'isBPDUGuardEnabled': True, 'isTrusted': False, 'memberInterfaceList': [], 'natEnabled': False, 'portNum': 0, 'role': 'LAN', 'uploadBW': 0.0, 'authenticationProfile': \{'id': '6596cc21-e2b8-416a-89cd-b832746fc3f3', 'instanceId': 767771, 'instanceCreatedOn': 1657692229186, 'instanceUpdatedOn': 1657692229186, 'instanceVersion': 1, 'bpduGuardEnabled': True, 'name': 'No Authentication', 'profileUuid': 'dcbd424a-d643-44ab-acf6-253c5b4eec4c', 'profileVersion': 1, 'type': 'WIRED_NOAUTH', 'wakeOnLan': False, 'webauthBypassMap': [], 'displayName': '18123b3b[No Authentication,dcbd424a-d643-44ab-acf6-253c5b4eec4c]'}, 'deviceInterfaceInfoL2SegmentMapping': [], 'deviceInterfaceInfoSegmentMapping': [], 'l2Segment': [\{'idRef': 'a26907d7-4bc0-467d-8c8a-00db25cbfd2e'}], 'segment': [], 'displayName': '8bc32999[b5c6b3c4-8b73-4756-a30d-99edb7a9905b]'},

{'id': 'c001f7b6-3b93-4341-b927-dec941f4bfe1', 'instanceId': 955965, 'instanceCreatedOn': 1657711922498, 'instanceUpdatedOn': 1657711922498, 'instanceVersion': 23, 'apManagementEnabled': False, 'connectedDeviceType': 'ACCESS_POINT', 'connectedToSubtendedNode': False, 'description': 'Interface type ACCESS_POINT', 'dhcpEnabled': False, 'downloadBW': 0.0, 'interfaceId': '75cd2254-d90d-4109-b35d-bf72f988b8f7', 
'interfaceName': 'TwoGigabitEthernet1/0/4',
 'ipAddressMask': 0, 'isBPDUGuardEnabled': True, 'isTrusted': False, 'memberInterfaceList': [], 'natEnabled': False, 'portNum': 0, 'role': 'LAN', 'uploadBW': 0.0, 'authenticationProfile': \{'id': '6596cc21-e2b8-416a-89cd-b832746fc3f3', 'instanceId': 767771, 'instanceCreatedOn': 1657692229186, 'instanceUpdatedOn': 1657692229186, 'instanceVersion': 1, 'bpduGuardEnabled': True, 'name': 'No Authentication', 'profileUuid': 'dcbd424a-d643-44ab-acf6-253c5b4eec4c', 'profileVersion': 1, 'type': 'WIRED_NOAUTH', 'wakeOnLan': False, 'webauthBypassMap': [], 'displayName': '18123b3b[No Authentication,dcbd424a-d643-44ab-acf6-253c5b4eec4c]'}, 'deviceInterfaceInfoL2SegmentMapping': [], 'deviceInterfaceInfoSegmentMapping': [], 'l2Segment': [\{'idRef': 'a32bba9e-6546-4de9-8b33-c2b3adcdd44a'}], 'segment': [], 'displayName': '8bc32999[75cd2254-d90d-4109-b35d-bf72f988b8f7]'},

{'id': '32cdf325-7353-4b46-a029-87ffe4890cab', 'instanceId': 956014, 'instanceCreatedOn': 1657899968641, 'instanceUpdatedOn': 1657899968641, 'instanceVersion': 0, 'apManagementEnabled': False, 'connectedDeviceType': 'USER_DEVICE', 'connectedToSubtendedNode': False, 'description': 'LAN Interface type USER_DEVICE', 'dhcpEnabled': False, 'downloadBW': 0.0, 'interfaceId': '5f9f34e5-2f9d-4362-a635-ac52847b7f4f', 
'interfaceName': 'TwoGigabitEthernet1/0/7', 
'ipAddressMask': 0, 'isBPDUGuardEnabled': True, 'isTrusted': False, 'memberInterfaceList': [], 'natEnabled': False, 'portNum': 0, 'role': 'LAN', 'uploadBW': 0.0, 'authenticationProfile': \{'id': '6596cc21-e2b8-416a-89cd-b832746fc3f3', 'instanceId': 767771, 'instanceCreatedOn': 1657692229186, 'instanceUpdatedOn': 1657692229186, 'instanceVersion': 1, 'bpduGuardEnabled': True, 'name': 'No Authentication', 'profileUuid': 'dcbd424a-d643-44ab-acf6-253c5b4eec4c', 'profileVersion': 1, 'type': 'WIRED_NOAUTH', 'wakeOnLan': False, 'webauthBypassMap': [], 'displayName': '18123b3b[No Authentication,dcbd424a-d643-44ab-acf6-253c5b4eec4c]'}, 'deviceInterfaceInfoL2SegmentMapping': [], 'deviceInterfaceInfoSegmentMapping': [], 'l2Segment': [\{'idRef': '7b4f7de8-e6d5-4ad7-9fae-a5912584efd6'}], 'segment': [], 'displayName': '8bc32999[5f9f34e5-2f9d-4362-a635-ac52847b7f4f]'}],


'deviceInterfaceInfoContainer': \{'idRef': '14debd6e-4b3a-4150-8eea-c8e7004ca688'}, 'deviceWlanSchedulerInfo': [], 'migrationStatus': [], 'networkWideSettings': \{'id': '3aee6c49-ef03-41b9-9735-9dabfd640b05', 'instanceId': 664668, 'instanceCreatedOn': 1657688235488, 'instanceUpdatedOn': 1657688235488, 'instanceVersion': 32, 'aaa': [{'id': '8dc1832e-18e7-4f06-86f1-09d2429d23bf', 'acctPort': 1813, 'authPort': 1812, 'authenticationType': ['NETWORKAUTH'], 'encryptionKey': '$5$STaY8kYkL69mif7novAdOFLBwi6j+CXN0RW6Lrg4R2jl69vf+yFuCv6GpRh4vrZT0f+H6IQD1cd2pgVaK3KG90Tuf5ebmOd72GnnnTuKVzb6vtSBSXBQ+Q==', 'encryptionScheme': 'KEYWRAP', 'ipAddress': {'id': 'd2d5c317-7a33-4fdb-92b3-db474c45ad22', 'address': '204.192.1.136'}, 'isIseEnabled': True, 'messageKey': '$5$kHzeZaMXJuUCybCyp3mYAuY04X1F3FoyASq2LPyjI/clXfZI6hW4Gvx3guNUGMj/S9HtXdywp5gfRNWoRqd/58kIEzhxzwOXn4zUbUZ4799aHAUzkPRCDQ==', 'panIpAddress': ['172.23.241.136', '204.192.1.136'], 'panSharedKey': '$5$hTV8zPLRquSTkeRehc9+a9egA2AX9hcaGkF+XUlq/XwbJCnN4We3TWsiq4ynSjNw/ruv4ypenUeIUHsi0RRKvEjw9snLdEhy', 'protocol': 'RADIUS', 'retries': '1', 'serverPriority': 1, 'sharedKey': '$5$hTV8zPLRquSTkeRehc9+a9egA2AX9hcaGkF+XUlq/XwbJCnN4We3TWsiq4ynSjNw/ruv4ypenUeIUHsi0RRKvEjw9snLdEhy', 'timeout': '10'}, \{'id': '00fd906e-b160-4238-a41a-d6b4a0c4d700', 'acctPort': 1813, 'authPort': 1812, 'authenticationType': ['CLIENTAUTH'], 'encryptionKey': '$5$STaY8kYkL69mif7novAdOFLBwi6j+CXN0RW6Lrg4R2jl69vf+yFuCv6GpRh4vrZT0f+H6IQD1cd2pgVaK3KG90Tuf5ebmOd72GnnnTuKVzb6vtSBSXBQ+Q==', 'encryptionScheme': 'KEYWRAP', 'ipAddress': {'id': 'ccc34c2b-4c3f-41fe-b366-8f97c5589cb0', 'address': '204.192.1.136'}, 'isIseEnabled': True, 'messageKey': '$5$kHzeZaMXJuUCybCyp3mYAuY04X1F3FoyASq2LPyjI/clXfZI6hW4Gvx3guNUGMj/S9HtXdywp5gfRNWoRqd/58kIEzhxzwOXn4zUbUZ4799aHAUzkPRCDQ==', 'panIpAddress': ['172.23.241.136', '204.192.1.136'], 'panSharedKey': '$5$hTV8zPLRquSTkeRehc9+a9egA2AX9hcaGkF+XUlq/XwbJCnN4We3TWsiq4ynSjNw/ruv4ypenUeIUHsi0RRKvEjw9snLdEhy', 'protocol': 'RADIUS', 'retries': '1', 'serverPriority': 1, 'sharedKey': '$5$hTV8zPLRquSTkeRehc9+a9egA2AX9hcaGkF+XUlq/XwbJCnN4We3TWsiq4ynSjNw/ruv4ypenUeIUHsi0RRKvEjw9snLdEhy', 'timeout': '10'}], 'acl': [], 'apjoinprofile': [], 'cmx': [], 'dhcp': [\{'id': '2f2682a9-07eb-4905-8617-1bc614d6357f', 'ipAddress': {'id': '587a2e7f-e628-402d-8a18-e2cbdcf883d7', 'address': '2004:192:3::40'}, 'serverPriority': 0}, \{'id': 'c699c714-d526-416c-ab96-86cb4a6a4778', 'ipAddress': {'id': '74f8cfbc-d5af-4a49-b392-329295922cbc', 'address': '204.192.3.40'}, 'serverPriority': 0}], 'dns': [\{'id': 'b29c13dd-8be5-4445-8034-6442c8981b16', 'domainName': 'cisco.com', 'ip': {'id': '5011042c-8333-40aa-97c9-8f1c8fa74e65', 'address': '171.70.168.183'}, 'secondaryIp': \{'id': 'cdfac35a-b85b-46d7-926b-5b219d05fe31', 'address': '2006:1:1::1'}}], 'ipdtEnabled': True, 'isBridgeModeVm': False, 'ldap': [], 'mesh': [], 'nativeVlan': [], 'netflow': [], 'ntp': [\{'id': '49b35491-db7e-4d89-9bc7-ce125f5057e5', 'ipAddress': {'id': 'b16cc4dc-32a2-46d3-b5da-18796acd444e', 'address': '204.192.3.40'}}], 'snmp': [], 'syslogs': [], 'displayName': '0'}, 'otherDevice': [], 'rlan': [], 'transitNetworks': [], 'virtualNetwork': [], 'wirelessAAAPolicy': [], 'wlan': [], 'displayName': 'f5ba231e[b59edcdb-b9e6-4f11-8018-2603aaf49374]'}]} Moving the ticket to current owner of MS TB2.

[~accountid:63f50bf0e8216251ae4d59ca] , pls. confirm if the suggested change from [~accountid:62d2fe9f8afb5805e5d5af49] has been applied and we do not see the reported issue any more.","['Auton', 'Groot', 'Issue']",Divakar Kumar Yadav,Closed,Avril Bower
SEEN-370,https://miggbo.atlassian.net/browse/SEEN-370,[Guardian][Groot] - Talos cloud parameters failing to read during execution,"*Uber ISO Version tested :* Promoted Groot Uber ISO - *2.1.560.70428, Non-FIPS, PUBSUB enabled, CSRF flag marked True*

*Script Name :* solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Branch used:* private/Groot-ms/api-auto, private/Guardian-ms/api-auto

*Testbed :* MSTB1



*Failed logs:* 
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul20_00:00:08.966553.zip&atstype=ATS] -> Refer TC225

*Testbed files:*

configs/sr_mb/solution_test_input.json
configs/sr_mb/solution_test_input_mdnac.json
configs/sr_mb/sr_mb1-10.195.243.109-local.json ->For Active DR cum Author 
configs/sr_mb/sr_mb1-10.195.243.123-local.json -> For Recovery DR
configs/sr_mb/sr_mb1-10.195.243.182-local.json -> For Reader
configs/sr_mb/sr_mb1_fabric_branch.json
configs/sr_mb/sr_mb1_fabric_branch_mdnac.json

 

*Description :* 

We see that

1) Talos cloud parameters added under testbed local json file is not being read during addition of Talos cloud to DNAC. 

2) Also if basic subTC such as Talos cloud enablement is failing, other subsequent TCs has to be marked as Blocked rather than executing further and marking it failure.

 

These both issues needs to be addressed.

Could you please have a look into this and help to fix ?

  Regards
Sandeep S",2022-07-20T17:35:34.521+0000,"[~62d2fec15d6f5fd2c3db8f9f],

Please put the correct config in the right local file as it is explained in the wiki:

!image-2022-07-21-15-19-15-724.png! https://wiki.cisco.com/display/EDPEIXOT/Talos+Feature https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3270/overview Hi Moe,

Thanks for looking into the issue further. As mentioned by you in Talos feature wiki, we do have the ""cloud"" block in our testbed local josn file. Even then the script is not picking.  Here is the snip as below from the Groot code base:

!image-2022-07-22-10-33-16-772.png!

  Will add handle for clientconfig for mdnac. Please, add the correct config to the right file. Hi Moe,

After adding the Talos cloud block under local json file - configs/sr_mb/sr_mb1-10.195.243.110-local.json, itseems to be picking during execution. But we are seeing another issue now , related to Chrome driver issue. We will be checking further on this Nethra and get back to you. As far as this issue specified as per this Jira ticket, u can close it.

 

*Reference Log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug08_08:01:10.381490.zip&atstype=ATS] -> Refer TC225

 

  No further action needed","['Auton', 'Groot', 'Issue', 'MSTB1', 'Multisite']",Moe Saeed,Closed,Avril Bower
SEEN-371,https://miggbo.atlassian.net/browse/SEEN-371,CSCwc48122 [Auton]:'sh platform software access-tunnel switch active r0' &'show platform software fed switch active ifm interfaces access-tunnel' cmd failed on  Fiab cat9k(C9404R),"*Release*: Ghost and Groot

Script Name :  solution_test_sanityecamb_lan.py

*Testcases Impacted :* 

[Test_TC57_WIRELESS_VERIFY_ACCESS_TUNNEL_ON_EDGE_ROUTER|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12094916&size=120711&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul19_23:21:19.788803.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/

[test1_verify_access_tunnel_on_edges|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12095676&size=104654&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul19_23:21:19.788803.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[Test_TC59_DEV_STRESS_verify_edge_reload_ap_clients_stability|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12663183&size=210445&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul19_23:21:19.788803.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/

[test1_verify_access_tunnel_on_edges|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12768778&size=104660&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul19_23:21:19.788803.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
 *Fail Log:*
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12094916&size=120711&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul19_23:21:19.788803.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Summary:*
Below cli is not applicable for Cat9400 platform, Need to update tc with correct cli for 9400 platform,

+Wrong CLI :+
 artemis_mcln_rck4#$show platform software fed switch active ifm interfaces access-tunnel 
 show platform software fed switch active ifm interfaces access-tunnel
                             ^
 % Invalid input detected at '^' marker.

 

+Right CLI :+

artemis_mcln_rck4#$show platform software fed active ifm interfaces access-tunnel

 

*Snip from Fail log:*

31840: TB7-NY-FIAB
 31841: 
 31842: 
 31843: api_switch_call called:
 31844: {}
 31845: Resource path full url: [https://10.30.0.100/api/v1/file/ad66ad23-31f3-477d-a0f8-f330de807696]
 31848: [
 \{'deviceUuid': '1a908ae0-7d1e-4554-b2a4-715d3a57db28', 'commandResponses': {'SUCCESS': {}, 'FAILURE':

{'show platform software access-tunnel switch active r0': ""Error occurred while executing command : show platform software access-tunnel switch active r0\nshow platform software access-tunnel switch active r0\n ^\n% Invalid input detected at '^' marker.\n\nTB7-NY-FIAB#""}

, 'BLACKLISTED': {}}}]
 31849: Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:02.179931
 31850: Output Error occurred while executing command : show platform software access-tunnel switch active r0
 31851: show platform software access-tunnel switch active r0
 31852: ^
 31853: % Invalid input detected at '^' marker.

31896: TB7-NY-FIAB
 31897: 
 31898: 
 31899: api_switch_call called:
 31900: {}
 31901: Resource path full url: [https://10.30.0.100/api/v1/file/7f7edf23-38b7-4e71-80b7-5421dae97654]
 31904: [
 \{'deviceUuid': '1a908ae0-7d1e-4554-b2a4-715d3a57db28', 'commandResponses': {'SUCCESS': {}, 'FAILURE':

{'show platform software fed switch active ifm interfaces access-tunnel': ""Error occurred while executing command : show platform software fed switch active ifm interfaces access-tunnel\nshow platform software fed switch active ifm interfaces access-tunnel\n ^\n% Invalid input detected at '^' marker.\n\nTB7-NY-FIAB#""}

, 'BLACKLISTED': {}}}]
 31905: Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:02.179692
 31906: Output Error occurred while executing command : show platform software fed switch active ifm interfaces access-tunnel
 31907: show platform software fed switch active ifm interfaces access-tunnel
 31908: ^
 31909: % Invalid input detected at '^' marker.",2022-07-21T05:56:28.449+0000,"Related: 

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-364

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-371

  Watch out for this PR:

By Tran:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0be8645a25ed60d47be1af4cb2f2bedbc4537006]","['Auton', 'Ghost', 'Groot', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-372,https://miggbo.atlassian.net/browse/SEEN-372,[Auton]:Shockwave: Test_TC6_DNAC_Device_Inventory_verify_device_role_correctly_showing_up  /   test1_verify_device_role_correctly_showing_up,"*ISO: Shockwave P4 RC5*
 *Script:* after_upgrade_verify.py

**Note:**In upgrade sanity we are hitting failure for TC6 as transit device is searching for ""Border"" role instead ""access"" role

*Impacted Testcase*: 
 Test_TC6_DNAC_Device_Inventory_verify_device_role_correctly_showing_up / test1_verify_device_role_correctly_showing_up

*Failed log :*
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=390400&size=27385&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F20%2F12%2F38%2Fenv_auto_job.2022Jul20_12:38:13.313434.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]

*Snip from failure:*
 1999: DEVINFO:: ['TB5-DM-eCA-BORDER', 'TB5-DM-Transit', 'TB5-DM-NF-Switch', 'TB5-DM-WLC']
 2000: 
 2001: INFO Obtained:: [

{'role': 'ACCESS', 'name': 'TB5-DM-eCA-BORDER'}

,

{'role': 'ACCESS', 'name': 'TB5-DM-Transit'}

,

{'role': 'ACCESS', 'name': 'TB5-DM-NF-Switch'}

,

{'role': 'ACCESS', 'name': 'TB5-DM-WLC'}

]
 2002: 
 2003: !!! 'BORDER ROUTER', 'name': 'TB5-DM-Transit'}: attribute and value is not as expected!!!
 2004: Library group ""inventory"" method ""verify_device_info"" returned in 0:00:00.775222
 2005: Library group ""inventory"" method ""verify_device_role"" returned in 0:00:00.776834
 2006: Test returned in 0:00:01.166313
 2007: Failed reason: Result: Some or all devices do not have correct device role info
 2008: The result of section test1_verify_device_role_correctly_showing_up is => FAILED

 

 

*After upgrade From Guardian .1 (2.3.3.1) to Guardian P1 (2.3.3.4),* 

test1_verify_device_role_correctly_showing_up is failing with, 
2399:  INFO Obtained:: [\{'name': 'TB4-DM-eCA-BORDER', 'role': 'ACCESS'}, \{'name': 'TB4-DM-Transit', 'role': 'ACCESS'}, \{'name': 'TB4-DM-NF-Switch', 'role': 'ACCESS'}, \{'name': 'TB4-DM-WLC', 'role': 'ACCESS'}]
2400: 
2401:  !!!Device\{'name': 'TB4-DM-eCA-BORDER', 'role': 'BORDER ROUTER'}: attribute and value is not as expected!!!
 
Log :
[Test_TC6_DNAC_Device_Inventory_verify_device_role_correctly_showing_up|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=434069&size=74636&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul28_23:54:43.784472.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&atstype=PYATS]  /   test1_verify_device_role_correctly_showing_up
 
 ",2022-07-21T10:55:38.497+0000,"fix on Groot: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/74eb522c1337393fdaa31d8b3044259de3445382

Fix on Ghost: 
(cherry picked from commit b37a0e929b71f29fb6d1dfe124ff8a7e979fb1a5) (cherry picked from commit 74eb522c1337393fdaa31d8b3044259de3445382)

Fix on Guardian:
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/578e16eb86a7811b0935b46ed5a59aff469b87a6

Shockwave:
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits?until=refs%2Fheads%2Fprivate%2FShockwave-ms%2Fapi-auto See update for fix. Verified in latest Upgrade of Guardian P4 RC2  :
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=266976&size=125604&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb10_09:55:38.777786.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Verified in latest Ghost Pre RC upgrade:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=205240&size=86185&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F02%2F02%2F20%2F49%2Fenv_auto_job.2023Feb02_20:49:53.034205.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 ","['Auton', 'Frey', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Shockwave']",Pawan Singh,Closed,Avril Bower
SEEN-373,https://miggbo.atlassian.net/browse/SEEN-373,Test_TC108_verify_vlan_mapping_interfaces needs changes to handle the new behavior,"The current test case compares the VLAN id value for the interface from Inventory page to the assurance 360 page, but the behavior has been changed the inventory page will have vlan id 1 and the assurance 360 page will have the dynamic vlan id. 

I will share the detailed email for this issue to fix or enhance.

 

 

Failed version: Guardian RC1 2.1.510.70366

 

Fail log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=80126564&size=284744&archive=env_auto_job.2022Apr04_18:37:59.602731.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&from=trade&view=all&atstype=pyATS]

 

 

 

TB2-DM-eCA-BORDER# show  interfaces status  | in Te1/0/45

Te1/0/45     LAN LAN            connected    1030       a-full a-1000 100/1000/2.5G/5G/10GBaseTX

TB2-DM-eCA-BORDER#

 

 

TB2-DM-eCA-BORDER#show  access-session method dot1x

Interface                MAC Address    Method  Domain  Status Fg  Session ID

--------------------------------------------------------------------------------------------

Te1/0/45                 0050.5683.b7ce dot1x   DATA    Auth        0201015B00000107F99B5670

Tw1/0/15                 2c57.4182.26a4 dot1x   DATA    Auth        0201015B000002671D228C50

 

Key to Session Events Blocked Status Flags:

 

  A - Applying Policy (multi-line status for details)

  D - Awaiting Deletion

  F - Final Removal in progress

  I - Awaiting IIF ID allocation

  P - Pushed Session

  R - Removing User Profile (multi-line status for details)

  U - Applying User Profile (multi-line status for details)

  X - Unknown Blocker

 

TB2-DM-eCA-BORDER#sho

TB2-DM-eCA-BORDER#show  run int Te1/0/45

Building configuration...

 

Current configuration : 600 bytes

!

interface TenGigabitEthernet1/0/45

description LAN LAN

switchport mode access

ip flow monitor dnacmonitor input

ip flow monitor dnacmonitor output

ipv6 flow monitor dnacmonitor_v6 input

ipv6 flow monitor dnacmonitor_v6 output

access-session inherit disable interface-template-sticky

access-session inherit disable autoconf

dot1x timeout tx-period 7

dot1x max-reauth-req 3

no macro auto processing

source template DefaultWiredDot1xClosedAuth

spanning-tree portfast

spanning-tree bpduguard enable

service-policy input DNA-MARKING_IN

service-policy output DNA-dscp#APIC_QOS_Q_OUT

end

 

TB2-DM-eCA-BORDER#

 

TB2-DM-eCA-BORDER#show  derived-config interface Te1/0/45

Building configuration...

 

Derived configuration : 866 bytes

!

interface TenGigabitEthernet1/0/45

description LAN LAN

switchport mode access

switchport voice vlan 2046

ip flow monitor dnacmonitor input

ip flow monitor dnacmonitor output

ipv6 flow monitor dnacmonitor_v6 input

ipv6 flow monitor dnacmonitor_v6 output

authentication periodic

authentication timer reauthenticate server

access-session inherit disable interface-template-sticky

access-session inherit disable autoconf

access-session closed

access-session port-control auto

mab

dot1x pae authenticator

dot1x timeout tx-period 7

dot1x timeout supp-timeout 7

dot1x max-req 3

dot1x max-reauth-req 3

no macro auto processing

spanning-tree portfast

spanning-tree bpduguard enable

service-policy type control subscriber PMAP_DefaultWiredDot1xClosedAuth_1X_MAB

service-policy input DNA-MARKING_IN

service-policy output DNA-dscp#APIC_QOS_Q_OUT

end

 

 

TB2-DM-eCA-BORDER#show  vlan

 

VLAN Name                             Status    Ports

---- -------------------------------- --------- -------------------------------

1    default                          active    Tw1/0/6, Tw1/0/8, Tw1/0/10, Tw1/0/11, Tw1/0/12, Tw1/0/13, Tw1/0/14, Tw1/0/16, Tw1/0/18, Tw1/0/19, Tw1/0/20, Tw1/0/21, Tw1/0/22, Tw1/0/23, Tw1/0/24, Tw1/0/25, Tw1/0/26, Tw1/0/27, Tw1/0/28, Tw1/0/29, Tw1/0/30, Tw1/0/31, Tw1/0/32, Tw1/0/33, Tw1/0/34, Tw1/0/35, Tw1/0/36, Te1/0/37, Te1/0/38, Te1/0/39, Te1/0/40, Te1/0/41, Te1/0/42, Te1/0/43, Te1/0/44, Te1/0/46, Te1/0/47, Te1/0/48, Te1/1/2, Te1/1/3, Te1/1/4, Te1/1/5, Te1/1/6, Te1/1/7, Te1/1/8, Ap1/0/1

2    VN7-POOL1_sub-VN7                active    L2LI0:8213,

9    Fabric_VN-sub-Fabric_VN          active    L2LI0:8215,

30   VLAN0030                         active   

50   VLAN0050                         active   

1002 fddi-default                     act/unsup

1003 token-ring-default               act/unsup

1004 fddinet-default                  act/unsup

1005 trnet-default                    act/unsup

1021 SGT_Port_test_sub-SGT_Port_test  active    L2LI0:8188,

1022 WSClients_sub-WirelessVNFB       active    L2LI0:8189,

1023 WClients_sub-WirelessVNFB        active    L2LI0:8190,

1024 EXT_POOL_sub-INFRA_VN            active    L2LI0:8191,

1025 APPool_sub-INFRA_VN              active    L2LI0:8192, Tw1/0/2, Tw1/0/4, Tw1/0/15, Tw1/0/17

1026 SENSORPool_sub-WiredVNStatic     active    L2LI0:8193, Tw1/0/7, Tw1/0/15

1027 64net_sub-WiredVNFB1             active    L2LI0:8194, Tw1/0/9

1028 80net_sub-WiredVNFB1             active    L2LI0:8195, Tw1/0/9

1029 CRITICAL_VLAN                    active    L2LI0:8196,

1030 96net_sub-WiredVNFBLayer2        active    L2LI0:8197, Te1/0/45

1031 112net_sub-WiredVNFBLayer2       active    L2LI0:8198,

1032 GP_sub-WirelessVNFGuest          active    L2LI0:8200,

1033 VN1-POOL2_sub-VN1                active    L2LI0:8201,

1034 VN1-POOL1_sub-VN1                active    L2LI0:8202,

1035 VN2-POOL1_sub-VN2                active    L2LI0:8203,

1036 VN2-POOL2_sub-VN2                active    L2LI0:8204,

1037 VN3-POOL1_sub-VN3                active    L2LI0:8205,

1038 VN3-POOL2_sub-VN3                active    L2LI0:8206,

1039 VN4-POOL2_sub-VN4                active    L2LI0:8207,

1040 VN4-POOL1_sub-VN4                active    L2LI0:8208,

1041 VN5-POOL1_sub-VN5                active    L2LI0:8209,

1042 VN5-POOL2_sub-VN5                active    L2LI0:8210,

1043 VN6-POOL2_sub-VN6                active    L2LI0:8211,

1044 VN6-POOL1_sub-VN6                active    L2LI0:8212,

1045 VN7-POOL2_sub-VN7                active    L2LI0:8214,

1046 AnchorVN1_sub-WiredVNStatic1     active    L2LI0:16188,

1047 AnchorVN2_nyc-WiredVNStatic2     active    L2LI0:16189,

2046 VOICE_VLAN                       active    L2LI0:8199, Tw1/0/12, Tw1/0/15, Te1/0/45

3300 3300                             active   

3301 3301                             active   

3302 3302                             active   

3303 3303                             active   

3304 3304                             active   

3305 3305                             active   

3306 3306                             active   

3307 3307                             active   

3308 3308                             active   

3309 3309                             active   

3310 3310                             active   

3311 3311                             active   

3312 3312                             active   

3313 3313                             active   

3314 3314                             active   

3401 3401                             active   

                                                                                   

 

Device inventory page:

 ",2022-07-21T21:52:28.499+0000,"Production test is failing due to this, Need fix asap Will start working on this Same TC failed  on Groot RC5 
*ISO:* Groot :2.1.560.70513
Branch:private/Groot-ms/sanity_api_auto(optimized Sanity)
Summary : 
TC failed for  AttributeError
*Snip from Failed Log:*
**
905:  Traceback (most recent call last):
906:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Optimized-Deployment_and_Sanity-Job/services/dnaserv/lib/api_groups/assurance/group.py"", line 6023, in verify_vlan_mapping_interfaces
907:  ip=self.services.dnaconfig.testbed.devices[dev['name']].lb_ip
908:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/topology/device.py"", line 559, in __getattr__
909:  raise AttributeError(""'Device' object has no attribute '%s'""
910:  AttributeError: 'Device' object has no attribute 'lb_ip'
*Failure Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-151-SDAFabricAssurance&begin=5638&size=266432&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep02_06:29:19.877807.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS
  [~620b8357878c2f00729881c8] by seeing above error you need to run TC2 also along with your testcase

910: AttributeError: 'Device' object has no attribute 'lb_ip' [~557058:fbb9c502-e858-4224-932a-86d595cefda2]  
 Issue is seen in the optimized run, There is no TC2, Please help to update the test case to work dynamically.
 Also failed in the main script:
 Script Name:solution_test_sanityecamb_lan
 Branch Name:private/Groot-ms/sanity_api_auto
 *ISO*:Groot  RC5 

2.1.560.70513
 *Log:*
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2060694&size=299798&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep04_09:48:22.152294.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Regression team script is passing in latest Groot and shockwave and For automation team it is passing in Ghost.

Omkar will run in latest Guardian and he will update us if there is any issue he will reopen,

Log : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-155-SDAFabricAssurance&begin=106202&size=322602&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep19_18:53:14.285038.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS *Please commit the changes in Reg script also :*

 

*ISO:*Guardian P1RC4-2.1.514.72142
*Script:*solution_test_sanityecamb.py

*Snip from Failed Log:*
241391:  Incorrect Mapping of Interface TwoGigabitEthernet1/0/25 with vlan expected 1 got 1029 on dev TB4-DM-eCA-BORDER
241392:  Incorrect Mapping of Interface TwoGigabitEthernet1/0/26 with vlan expected 1 got 1029 on dev TB4-DM-eCA-BORDER
241410:  [\{'TwoGigabitEthernet1/0/25': {'expected_vlan': '1', 'Dnac_vlan': '1029', 'device': 'TB4-DM-eCA-BORDER'}}, \{'TwoGigabitEthernet1/0/26': {'expected_vlan': '1', 'Dnac_vlan': '1029', 'device': 'TB4-DM-eCA-BORDER'}}]

*Failed Log:*
[test1_verify_vlan_mapping_interfaces|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=70387411&size=316681&archive=env_auto_job.2022Oct12_10:30:18.142391.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [~63f5351b3ec8aa51d3d22dbe], can we have an ETA for this ticket? This issue got passed previously, Now again its giving issue I will check it update you Raise PR for the required change: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4698/overview Changes have been merged to Halleck, Ghost and esxivm_branch.

Will raise separate PR for Groot and Guardian branches. Below two CDETs were raised while fixing the script.

[CSCwe34540|https://cdetsng.cisco.com/webui/#view=CSCwe34540] - Voice and Dynamic Vlan info missing on Inventory

[CSCwe34560|https://cdetsng.cisco.com/webui/#view=CSCwe34560] - Assurance page do not list Voice Vlan info

Once these get fixed, script may require another upgrade. Separate PR raised for Guardian Branch:

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4763/overview Separate PR raised for Groot branch:

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4843/overview Required changes have been merged to Halleck, Ghost, Groot, Guardian and esxivm_branch.

Marking this ticket as ""Done"".","['Auton', 'ESXi', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'Sanity']",Amardeep Kumar,Closed,Avril Bower
SEEN-374,https://miggbo.atlassian.net/browse/SEEN-374,[Auton] : Groot -Test_TC23_delete_all_discoveries/test1_delete_all_discoveries,"Groot Version :2.1.560.70451 FIPS

 

Script Name :  dnac_cleanup_script.py

Testcases Impacted : 

 [Test_TC23_delete_all_discoveries|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=869142&size=14120&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul22_00:45:23.423183.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_delete_all_discoveries

 

Fail Log :
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=869706&size=13394&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul22_00:45:23.423183.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Snip from fail Log:
4222:  Exception:
4223:  Traceback (most recent call last):
4224:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
4225:  result = testfunc(func_self, **kwargs)
4226:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/testcases/mega_topo/dnac_cleanup_script.py"", line 525, in test1_delete_all_discoveries
4227:  if (dnac_handle.discovery.delete_all_discoveries()):
4228:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-sanity-common-Multi-job/services/dnaserv/dnaservices.py"", line 309, in __getattr__
4229:  raise AttributeError(err_msg)
4230:  AttributeError: 'DnaServices' object has no attribute 'discovery'
4231:  The result of section test1_delete_all_discoveries is => ERRORED",2022-07-22T11:16:21.310+0000,,"['Auton', 'Groot', 'Issue']",Omkar Sharad Wagh,Closed,Avril Bower
SEEN-377,https://miggbo.atlassian.net/browse/SEEN-377,[Auton] : Guardian-Test_TC172_syslog_server_event_notification/ test8_connect_linux_syslog_server_check_event_logs,"Guardian Version:2.3.3.3-90120

Script Name:  solution_test_sanityecamb_lan.py

Testcases Impacted : 
 [Test_TC172_syslog_server_event_notification|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1064899&size=2070714&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul25_18:33:09.680313.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/[ test8_connect_linux_syslog_server_check_event_logs
  
 *Failed Log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul25_18:33:09.680313.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]

*Description :*

 

*Seeing issue with [test8_connect_linux_syslog_server_check_event_logs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3130941&size=4499&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul25_18:33:09.680313.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS], where the tc is trying to access the *'/var/log/syslogserv/'* folder and it failed with permission denied err. But in Pass log, I see its accessing different folder and its using more command, Could you please check and update the code with the working one ?

+*Snip from Fail log :*+
 12519: wired mac details ['00.50.56.8D.9C.D0', '00.50.56.8D.A1.4B', '00.50.56.8D.60.8D']
 12520: device names details ['TB2-DMZ-SJ-FIAB-ECA', 'TB2-DMZ-NY-FIAB', 'TB2-DMZ-SJ-FIAB-ECA']
 12521: verify rogue/awips event subscription on linux syslog server
 12522: raju-virtual-machine is already connected
 12523: rogue stime :: Mon25Jul202206:52:23PMPDT
 12524: +++ raju-virtual-machine with via 'linux': executing command 'cat /var/log/syslogserv/serverMon25Jul202206:52:23PMPDT.log' +++
 +cat /var/log/syslogserv/serverMon25Jul202206:52:23PMPDT.log cat: '/var/log/syslogserv/serverMon25Jul202206:52:23PMPDT.log': Permission denied

*Snip from Pass log :*+

9128: wired mac details ['00:50:56:83:86:31', '00:0C:29:3D:81:E6']
 9129: device names details ['TB1-DM-NF-Switch', 'TB1-DM-eCA-BORDER']
 9130: nfs-virtual-machine is already connected
 9131: +++ nfs-virtual-machine: executing command 'more /var/log/10.30.0.90/DNAC.log' +++
 more /var/log/10.30.0.90/DNAC.log Apr 24 05:10:00 10.30.0.90 DNAC {""version"":""1.0.0"",""instanceId"":""5a46576e-1734-4766-a3fb-ddd22cbe647b"",""eventId"":""NETWORK-CLIENTS-4-370"",""namespace"":""ASSURANCE"",""name"":""Clients detected connecting on network"",""description"":null,""type"":""NETWORK"",""category"":""INFO"",""domain"":""Know Your Network"",""subDomain"":""Clients"",""severity"":4,""source"":""DNAC"",""timestamp"":1650777000256,""details"":

{""Type"":""NETWORK"",""As surance Issue Details"":""Client 00:50:56:83:86:31 has connected to Device TB1-DM-NF-Switch.solutionsanity.com at time Sun, 2022-04-24 05:09:59 AM UTC in location Global/USA/New York/BLDNYC"",""Assurance Issue Name"":""Client 00:50:56:83:86:31 has connected to Device TB1-DM-NF-Switch.solutionsanity.com""}

,""ciscoDnaEventLink"":""https://10.195.227.14/dna/assurance/client/details?macAddress=00:50:56:83:86:31 "",""note"":""To programmatically get more info see here - https://<ip-address>/dna/platform/app/consumer-portal/developer-toolkit/apis?apiId=8684-39bb-4e89-a6e4"",""tntId"":""61f9140a3b49ec31dda983a3"",""conte xt"":null,""userId"":null,""i18n"":null,""eventHierarchy"":null,""message"":null,""messageParams"":null,""parentInstanceId"":null,""network"":null,""isSimulated"":false,""startTime"":1650777000260,""dnacIP"":""10.195.227.1 4"",""tenantId"":""SYS0""} Apr 24 05:10:00 10.30.0.90",2022-07-26T13:03:06.024+0000,"checked in multisite, only last tc is failing due to bug#CSCwc68907. DE is still working on this. Will re-open if there is any issues after bug fix

[https://earms-trade.cisco.com/tradeui/logs/details?ats=%2Fusers%2Frvonmize%2Fpyats-inst&client=web&host=st-ucs-sanity.cisco.com&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb2_three_sites.2022Aug07_22:45:04.279339.zip]

About permission issue please check with team its not related to script. please check sathwic server or automation server. ","['Auton', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-378,https://miggbo.atlassian.net/browse/SEEN-378,[Auton]:Guardian:Test_TC9_DNAC_perform_save_on_fabric,"*ISO:2.1.512.72132*
 Polarisis:17.8.1a

*Script Used:* after_upgrade_verify.py

*Impacted Testcase*:
 Test_TC7_DNAC_Device_Re_Provisioning_after_upgrade/test1_verify_provision_the_devices_fabric1
Test_T11_DNAC_verify_device_stauts_after_adding_fabric  /   test2_verify_status_after_adding_to_fabric

 **NOTE*:*
 TC9 is modifying fabric which is causing the issue.

TB5 eca reprovision failed :
 NCSO20323: IP Pool has been updated to IPv6 on the Fabric Site/Zone to which device TB5-DM-eCA-BORDER.cisco.com belongs. Reconfigure Fabric Site/Zone first and then reprovision the device.
 *Edited the Fabric and deployed and tried Re-provision,it went fine*
  
 *Failed Log:*
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-06%2Fenv_auto_job.2022Jun28_23:58:16.774602.zip&atstype=ATS]

*Snip from failure:*
 **
 3700: Config Preview Activity failed with reason: IP Pool has been updated to IPv6 on the Fabric Site/Zone to which device TB5-DM-eCA-BORDER.cisco.com belongs. Reconfigure Fabric Site/Zone first and then reprovision the device.
 3712: Config preview task for provision failed for device:TB5-DM-eCA-BORDER
 3713: Not Valid Activity-ID
 3716: Failed to receive config preview
 4494: The Schedduled Job failed: with reason \{'id': '0448877c-e80b-4cbb-b70d-1871c026efa4', 'triggeredJobTaskId': '1485d9f6-6860-4dd2-866b-d89676b7e63e', 'triggeredTime': 1656486294440, 'status': 'FAILED', 'failureReason': 'IP Pool has been updated to IPv6 on the Fabric Site/Zone to which device TB5-DM-eCA-BORDER.cisco.com belongs. Reconfigure Fabric Site/Zone first and then reprovision the device.', 'triggeredJobId': '0448877c-e80b-4cbb-b70d-1871c026efa4'}
 4496: Provision result for dev TB5-DM-eCA-BORDER is not expected:Success, task status: False
 **",2022-07-27T07:51:53.798+0000,"[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3947864&size=671053&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F29%2F09%2F43%2Fenv_auto_job.2022Jul29_09:43:06.739359.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Issue not seen in execution anymore

 ","['Auton', 'Guardian', 'Issue']",Andrew Chen,Resolved,Avril Bower
SEEN-379,https://miggbo.atlassian.net/browse/SEEN-379,"[Auton]:Shockwave :Post Reprovision TC7 we need to check ""sh wlan sum"" via script","*Script used:*after_upgrade_verify.py

**NOTE*:*Post Reprovision of TC7 [[Test_TC7_DNAC_Device_Re_Provisioning_after_upgrade|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=457294&size=1039602&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F20%2F12%2F38%2Fenv_auto_job.2022Jul20_12:38:13.313434.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]]we need to check ""sh wlan sum"" via script
 *Impacted Testcase*:
 Test_TC19_DNAC_verify_SSID_lan_on_ECA_device/test1_DNAC_verify_SSID_lan_on_ECA_device
 Test_TC42_DNAC_configure_SSID_IP_Pool_Mapping_for_Wireless / test1_device_onboarding_update_virtual_network_pool_and_authentication_fabric1
 Test_TC44_DNAC_verify_SSID_lan_on_ECA_device / test1_DNAC_verify_SSID_lan_on_ECA_device

*Failed log:*
 *1.)*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11412763&size=58654&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F20%2F12%2F38%2Fenv_auto_job.2022Jul20_12:38:13.313434.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS
 ]
 *2.)*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=21552031&size=825646&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F20%2F12%2F38%2Fenv_auto_job.2022Jul20_12:38:13.313434.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS
 ]
 *3.)*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24010214&size=63850&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F20%2F12%2F38%2Fenv_auto_job.2022Jul20_12:38:13.313434.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS

]
 **NOTE:**Manually checking SSID's are present in device",2022-07-27T08:27:48.027+0000,"[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11412763&size=58654&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F20%2F12%2F38%2Fenv_auto_job.2022Jul20_12:38:13.313434.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]

 
ERROR Following line of expected cli output not present on device:
29678:  \d+\s+.*SSIDDot1XIndiaSasitb5\s+UP\s+
 
This SSID is not from the solution input file from bitbucket, actual SSID should be 
 
Please use the input file from here ""SSIDDot1XIndia"" ""Sasi should not be present. Please check if this coming from a local file. [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/upgrade/solution_test_input_upgrade.json?at=private/Guardian-ms/sanity_api_auto]
 
  Hi Raji,
Before Upgrading we used to change  the input json  with ""dnac_input"" : ""./configs/upgrade/solution_test_input_upgrade.json [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/testcases/upgrade/after_upgrade_verify.py?until=563c616dccb10d22720881e8b33d61f85460863c&at=refs%2Fheads%2Fprivate%2FGhost-ms%2Fapi-auto]

 

Committed to Ghost,Groot Guardian. Verified in latest upgrade combination of Shockwave P4RC5 to Guardian P4 RC2:
Pass log:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=272518&size=41563&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb10_10:01:51.586068.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Frey', 'Fury', 'Groot', 'Guardian', 'Issue', 'Shockwave']",Pawan Singh,Closed,Avril Bower
SEEN-380,https://miggbo.atlassian.net/browse/SEEN-380,"[Auton]:Any TC related to SSID fails via script , the script should abort the execution","*Script:* after_upgrade_verify.py
 **Note:**In upgrade sanity when any verification testcase is failed, script should failed and abort.

Example : Recently SSID related TC fails 1st time, but script proceed.


 *Impacted Testcase*:
 1.)Test_TC19_DNAC_verify_SSID_lan_on_ECA_device / test1_DNAC_verify_SSID_lan_on_ECA_device
 2.)Test_TC42_DNAC_configure_SSID_IP_Pool_Mapping_for_Wireless / test1_device_onboarding_update_virtual_network_pool_and_authentication_fabric1
 3.)
 Test_TC44_DNAC_verify_SSID_lan_on_ECA_device / test1_DNAC_verify_SSID_lan_on_ECA_device

*Failed log:*
 *1.)*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11412763&size=58654&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F20%2F12%2F38%2Fenv_auto_job.2022Jul20_12:38:13.313434.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS
 *]*
 *2.)*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=21552031&size=825646&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F20%2F12%2F38%2Fenv_auto_job.2022Jul20_12:38:13.313434.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS
 ]
 *3.)*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24010214&size=63850&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F20%2F12%2F38%2Fenv_auto_job.2022Jul20_12:38:13.313434.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]",2022-07-27T08:43:30.849+0000,"Same as SEEN-379, please check solution_input file for 
ERROR Following line of expected cli output not present on device:
55931:  \d+\s+.*SSIDDot1XIndiaSasitb5\s+UP\s+ Hi Raji,
Before Upgrading we used to change  the input json  with ""dnac_input"" : ""./configs/upgrade/solution_test_input_upgrade.json","['Auton', 'Frey', 'Fury', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Shockwave']",Nethra Thorali Suthagar,Resolved,Avril Bower
SEEN-381,https://miggbo.atlassian.net/browse/SEEN-381,[Auton]:Shockwave: Test_TC32_Compliance_verification/  test1_verify_startup_running_config/test6_verify_Ethernet_port / test7_verify_Fabric_compliance,"*ISO:*2.1.391.72224
*Polarisis:*17.6.4prd3
*Script:*after_upgrade_verify.py
*Impacted Testcases:*
Test_TC32_Compliance_verification/test1_verify_startup_running_config
Test_TC32_Compliance_verification/test6_verify_Ethernet_port 
Test_TC32_Compliance_verification/test7_verify_Fabric_compliance


*Failed log:*1.[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13719779&size=931919&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F20%2F12%2F38%2Fenv_auto_job.2022Jul20_12:38:13.313434.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]

2.[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15289163&size=101089&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F20%2F12%2F38%2Fenv_auto_job.2022Jul20_12:38:13.313434.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS]

3.[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15390252&size=4779&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F20%2F12%2F38%2Fenv_auto_job.2022Jul20_12:38:13.313434.zip&ats=%2Fusers%2Fpawansi%2Fpyatsnew&submitter=admin&from=trade&view=all&atstype=pyATS
]

*Snip from failure:*
*1.Test_TC32_Compliance_verification/test1_verify_startup_running_config*
37872:  Exception:
37873:  Traceback (most recent call last):
37874:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Upgrade-Verify-common-Multi-job/services/commonlibs/test_wrapper.py"", line 195, in wrapper
37875:  result = testfunc(func_self, **kwargs)
37876:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Upgrade-Verify-common-Multi-job/testcases/upgrade/after_upgrade_verify.py"", line 1091, in test1_verify_startup_running_config
37877:  if dnac_handle.Compliance_startup_running_config():
37878:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Upgrade-Verify-common-Multi-job/services/dnaserv/lib/api_groups/Compilance/group.py"", line 127, in Compliance_startup_running_config
37879:  device_config=self.services.input_data[""COMPLIANCE_CONFIG""]
37880:  KeyError: 'COMPLIANCE_CONFIG'

2.*Test_TC32_Compliance_verification/test6_verify_Ethernet_port* 

40082:  Exception:
40083:  Traceback (most recent call last):
40084:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Upgrade-Verify-common-Multi-job/services/commonlibs/test_wrapper.py"", line 195, in wrapper
40085:  result = testfunc(func_self, **kwargs)
40086:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Upgrade-Verify-common-Multi-job/testcases/upgrade/after_upgrade_verify.py"", line 1142, in test6_verify_Ethernet_port
40087:  if dnac_handle.Compliance_ethernetports_check():
40088:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Upgrade-Verify-common-Multi-job/services/dnaserv/lib/api_groups/Compilance/group.py"", line 697, in Compliance_ethernetports_check
40089:  intf_list=self.services.input_data[""INTERFACE_LIST""]
40090:  KeyError: 'INTERFACE_LIST'
3.*Test_TC32_Compliance_verification/test7_verify_Fabric_compliance*
**
40097:  Traceback (most recent call last):
40098:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Upgrade-Verify-common-Multi-job/services/commonlibs/test_wrapper.py"", line 195, in wrapper
40099:  result = testfunc(func_self, **kwargs)
40100:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Upgrade-Verify-common-Multi-job/testcases/upgrade/after_upgrade_verify.py"", line 1151, in test7_verify_Fabric_compliance
40101:  if dnac_handle.fabric_compliance():
40102:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Upgrade-Verify-common-Multi-job/services/dnaserv/lib/api_groups/Compilance/group.py"", line 802, in fabric_compliance
40103:  fabric=self.services.input_data[""Fabric_compliance""]
40104:  KeyError: 'Fabric_compliance'

**",2022-07-27T09:23:27.729+0000,"In recent runs issue is not seen so closed issue:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=60078990&size=7230865&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F03%2F28%2F21%2F10%2Fenv_auto_job.2023Mar28_21:10:59.745529.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=60078990&size=7230865&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F03%2F28%2F21%2F10%2Fenv_auto_job.2023Mar28_21:10:59.745529.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Issue', 'Shockwave']",Faiz Habibbhai Babuna,Closed,Avril Bower
SEEN-383,https://miggbo.atlassian.net/browse/SEEN-383,[Auton] [Groot] [Guardian] Script Unable to Handle SVL Device Login Prompt,"Hi Tran,

       In our SR testing we are observing the issue with handling the SVL device login prompt. Showing below error,

!image-2022-07-28-13-52-46-722.png!

 

Earlier also we observed the same issue, for that we were informed to change the PyATS version to to latest. Now we are running the script in latest pyATS only but still we are seeing this issue.

Can you help us on this ?

 

DNAC ISO Used : Guardian P1 RC3 #2.1.512.72139 

Device Image Used : 17.8.1a

Branch         : private/Guardian-ms/api-auto

 

 

Thanks,

Vijayakumar G.",2022-07-28T08:27:13.839+0000,"[~63f53512263233e653a96a29] please provide trade log. Hi [~63f50be9e76fc61320f4eab3],

            Here is the Failed Trade log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=30076780&size=53781&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fauto_MS_job.2022Jul27_21:23:50.725703.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  Hi [~63f50be9e76fc61320f4eab3],

In AWS-Multisite setup,while testing Ghost-2.1.610.70412,we have observed the same issue ,script unable to handle the SVL device login prompt with below error:

 

*UBER ISO USED:2.1.610.70412*

*DEVICE HAVING ISSUE:*SJC-FB-9500-10.4.2.25 2004 (SVL)
                                                              10.4.2.25 2005 --*Device* *Credentials*:wlcaccess/Lablab#123/Cisco#123

*Execution server details*:st-ds-2.cisco.com(10.195.247.200)(admin/C1sco123)

 
 {color:#de350b}0181: Traceback (most recent call last):{color}
 {color:#de350b}10182: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/statemachine/statemachine.py"", line 737, in go_to{color}
 {color:#de350b}10183: output = transition.do_transitions(){color}
 {color:#de350b}10184: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/statemachine/statetransition.py"", line 484, in do_transitions{color}
 {color:#de350b}10185: m = dialog.process(self.spawn,{color}
 {color:#de350b}10186: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialogs.py"", line 476, in process{color}
 {color:#de350b}10187: return dp.process(){color}
 {color:#de350b}10188: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialog_processor.py"", line 321, in process{color}
 {color:#de350b}10189: if self.expect_eval_statements(pat) is True:{color}
 {color:#de350b}10190: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialog_processor.py"", line 235, in expect_eval_statements{color}
 {color:#de350b}10191: statement._action(){color}
 {color:#de350b}10192: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/statements.py"", line 209, in login_handler{color}
 {color:#de350b}10193: spawn.sendline(context['username']){color}
 {color:#de350b}10194: KeyError: 'username'{color}
 {color:#de350b}10195: {color}
 {color:#de350b}10196: The above exception was the direct cause of the following exception:{color}
 {color:#de350b}10197: {color}
 {color:#de350b}10198: Traceback (most recent call last):{color}
 {color:#de350b}10199: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/commonlibs/test_wrapper.py"", line 301, in wrapper{color}
 {color:#de350b}10200: result = testfunc(func_self, **kwargs){color}
 {color:#de350b}10201: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py"", line 3277, in test1_verify_changing_DHCP_server_verifications{color}
 {color:#de350b}10202: if(dnac_handle.dnaconfig.verify_ip_helper_address(dhcpserver=""8.8.8.8"")):{color}
 {color:#de350b}10203: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/commonlibs/sftopology.py"", line 5775, in verify_ip_helper_address{color}
 {color:#de350b}10204: if verify_dhcp_helper_address(self.testbed.devices[dev[""name""]],dhcpserver):{color}
 {color:#de350b}10205: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/commonlibs/platformlibs.py"", line 313, in verify_dhcp_helper_address{color}
 {color:#de350b}10206: output=oRtr.execute(send_command){color}
 {color:#de350b}10207: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 264, in __call__{color}
 {color:#de350b}10208: self.call_service(*args, **kwargs){color}
 {color:#de350b}10209: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 1885, in call_service{color}
 {color:#de350b}10210: self.result = handle.execute(command,{color}
 {color:#de350b}10211: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 263, in __call__{color}
 {color:#de350b}10212: self.pre_service(*args, **kwargs){color}
 {color:#de350b}10213: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 174, in pre_service{color}
 {color:#de350b}10214: handle.state_machine.detect_state(handle.spawn){color}
 {color:#de350b}10215: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/statemachine/statemachine.py"", line 810, in detect_state{color}
 {color:#de350b}10216: self.go_to('any', spawn, context, prompt_recovery=self.prompt_recovery){color}
 {color:#de350b}10217: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/statemachine/statemachine.py"", line 740, in go_to{color}
 {color:#de350b}10218: raise StateMachineError('Failed while bringing device to ' +{color}
 {color:#de350b}10219: unicon.core.errors.StateMachineError: Failed while bringing device to ""any"" state{color}
 {color:#de350b}10220: Test returned in 0:01:13.067707{color}
 {color:#de350b}10221: Errored reason: Failed while bringing device to ""any"" state{color}
 {color:#de350b}10222: {color}
 {color:#de350b}10223: Exception:{color}
 {color:#de350b}10224: Traceback (most recent call last):{color}
 {color:#de350b}10225: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/statemachine/statemachine.py"", line 737, in go_to{color}
 {color:#de350b}10226: output = transition.do_transitions(){color}
 {color:#de350b}10227: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/statemachine/statetransition.py"", line 484, in do_transitions{color}
 {color:#de350b}10228: m = dialog.process(self.spawn,{color}
 {color:#de350b}10229: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialogs.py"", line 476, in process{color}
 {color:#de350b}10230: return dp.process(){color}
 {color:#de350b}10231: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialog_processor.py"", line 321, in process{color}
 {color:#de350b}10232: if self.expect_eval_statements(pat) is True:{color}
 {color:#de350b}10233: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialog_processor.py"", line 235, in expect_eval_statements{color}
 {color:#de350b}10234: statement._action(){color}
 {color:#de350b}10235: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/statements.py"", line 209, in login_handler{color}
 {color:#de350b}10236: spawn.sendline(context['username']){color}
 {color:#de350b}10237: KeyError: 'username'{color}
 {color:#de350b}10238: {color}
 {color:#de350b}10239: The above exception was the direct cause of the following exception:{color}
 {color:#de350b}10240: {color}
 {color:#de350b}10241: Traceback (most recent call last):{color}
 {color:#de350b}10242: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/commonlibs/test_wrapper.py"", line 301, in wrapper{color}
 {color:#de350b}10243: result = testfunc(func_self, **kwargs){color}
 {color:#de350b}10244: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py"", line 3277, in test1_verify_changing_DHCP_server_verifications{color}
 {color:#de350b}10245: if(dnac_handle.dnaconfig.verify_ip_helper_address(dhcpserver=""8.8.8.8"")):{color}
 {color:#de350b}10246: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/commonlibs/sftopology.py"", line 5775, in verify_ip_helper_address{color}
 {color:#de350b}10247: if verify_dhcp_helper_address(self.testbed.devices[dev[""name""]],dhcpserver):{color}
 {color:#de350b}10248: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/commonlibs/platformlibs.py"", line 313, in verify_dhcp_helper_address{color}
 {color:#de350b}10249: output=oRtr.execute(send_command){color}
 {color:#de350b}10250: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 264, in __call__{color}
 {color:#de350b}10251: self.call_service(*args, **kwargs){color}
 {color:#de350b}10252: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 1885, in call_service{color}
 {color:#de350b}10253: self.result = handle.execute(command,{color}
 {color:#de350b}10254: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 263, in __call__{color}
 {color:#de350b}10255: self.pre_service(*args, **kwargs){color}
 {color:#de350b}10256: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 174, in pre_service{color}
 {color:#de350b}10257: handle.state_machine.detect_state(handle.spawn){color}
 {color:#de350b}10258: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/statemachine/statemachine.py"", line 810, in detect_state{color}
 {color:#de350b}10259: self.go_to('any', spawn, context, prompt_recovery=self.prompt_recovery){color}
 {color:#de350b}10260: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/statemachine/statemachine.py"", line 740, in go_to{color}
 {color:#de350b}10261: raise StateMachineError('Failed while bringing device to ' +{color}
 {color:#de350b}10262: unicon.core.errors.StateMachineError: Failed while bringing device to ""any"" state{color}
 {color:#de350b}10263: The result of section test1_verify_changing_DHCP_server_verifications is => ERRORED{color}
  
  
  
  
 {color:#de350b}Failed Log:{color}
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fauto_MS_job.2022Sep23_02:47:25.544765.zip&atstype=ATS]
  

  
  
  
   assigning to Automation Lead.. Hi Tran

We are still observing this issue. Can you please have some one from you team to have a look into this.

Failed Log : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=35092217&size=36908&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fauto_MS_job.2023Feb01_00:02:40.004891.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS Looks like the reload is taking very long and it is timing out. Need to adjust the timeout for SVL reload. [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3056aec6e5513b98e49c89ef78d8ba4b56f832f8]

 

Committed to groot, ghost, guardian and will be merging to hallack through forward merge. ","['Auton', 'Groot', 'Guardian', 'Issue']",Pawan Singh,Resolved,Avril Bower
SEEN-384,https://miggbo.atlassian.net/browse/SEEN-384,[Auton] : Guardian-Test_TC164_auto_configure_a_new_rep_ring_with_en_or_pen_using_workflow/Test_TC165_monitor_the_rep_ring_via_assurance_and_cut_the_ring/Test_TC166_delete_the_entire_ring,"*ISO:* Guardian Version:2.3.3.3-90120

*Script Name:*  solution_test_sanityecamb_lan.py

*Testcases Impacted :* 

Test_TC164_auto_configure_a_new_rep_ring_with_en_or_pen_using_workflow/Test_TC165_monitor_the_rep_ring_via_assurance_and_cut_the_ring/Test_TC166_delete_the_entire_ring



*Description :*

There was a connection mismatch in the topology section in YAML for the ext nodes, so the rep ring TC couldn't identify the devices to form the ring.. but the test passed..
*Expected:* The test should fail or skip if the extended nodes are not found / the rep ring is not formed. TC should not false pass

 

*Pass Log:*
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul27_17:01:29.356162.zip&atstype=ATS]",2022-07-28T08:27:40.387+0000,"Scripts are written in a way to work for all setups which have or don't have REP rings. 

If we fail then other teams would complaining to have false failure. 

It is testbed's responsibility to make sure of proper connections. and Validate that the testbed is picked up correctly.","['Auton', 'Groot', 'Guardian', 'Issue']",Pawan Singh,Resolved,Avril Bower
SEEN-387,https://miggbo.atlassian.net/browse/SEEN-387,[Auton] : Shockwave - Unable to do WLC provision manually due to Constraint validation error in model config,"Upgraded From,

Shockwave P3 ([2.2.3.5|https://2.2.3.5/]) <> Guardian P1 RC4 ([2.3.3.3|https://2.3.3.3/])

 

When we reprovision WLC manually, in model config we are seeing Constraint validation failure for one of the SSID (SSIDDot1xIndiatb5)

After Editing it and selecting DHCP to YES and Disabling Passive Client Enable, the error is not seen and we can proceed with the provision.

 

Looks like there is a script issue via API for model config editor

 ",2022-07-29T05:30:17.884+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/4a7043751fd75f04d5d2d52947ce66c7364e4e22,"['Auton', 'Guardian', 'Issue', 'Shockwave']",Raji Mukkamala,Closed,Avril Bower
SEEN-388,https://miggbo.atlassian.net/browse/SEEN-388,[Auton]:Guardian: Test_TC100_aca_test  /   test1_cleanup_profile,"*Release : Guardian*
*ISO:2.1.511.72077*

Script Name :  solution_test_sanityecamb.py

Testcases Impacted : 
 [Test_TC100_aca_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=71076493&size=2086774&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul27_09:03:10.499995.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_cleanup_profile

Failure log:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=71077069&size=5207&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul27_09:03:10.499995.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Snip from failure:
 253424: ###################################################
 253425: #!!!FAILED TO UPDATE AUTHORIZATION PROFILE profile_guest_sf in ISE. ERROR local variable 'id' referenced before assignment----#
 253426: ###################################################
 253427: Traceback (most recent call last):
 253428: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/iseserv/iseapi.py"", line 1591, in reset_authprofile
 253429: ""id"": id,
 253430: UnboundLocalError: local variable 'id' referenced before assignment
 253431: 
 253432: During handling of the above exception, another exception occurred:
 253433: 
 253434: Traceback (most recent call last):
 253435: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 7406, in test1_cleanup_profile
 253436: dnac_handle.dnaconfig.iseapi.reset_authprofile(profile_name=p)
 253437: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/iseserv/iseapi.py"", line 1636, in reset_authprofile
 253438: raise Exception(e)
 253439: Exception: local variable 'id' referenced before assignment",2022-07-29T05:41:22.366+0000,"Fixed:
||Commit||Message||Date||Files||
|[a3a2448e9ec|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a3a2448e9ec7976fa735f2561e57d387161fbf73]|SEEN-388 Fixed reset_authprofile|Just now|1 file|
|[6500ac18e08|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6500ac18e080455c860c3325609faefc836a6014]|SEEN-388 Fixed reset_authprofile|6 minutes ago|1 file|
|[68b7702bdba|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/68b7702bdba398b6fd94615f225391a6d9d92df4]|SEEN-388 Fixed reset_authprofile|10 minutes ago|1 file|
|[119fd5a1b12|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/119fd5a1b12e422bc476eb7a03480c8e700aec70]|SEEN-388 Fixed reset_authprofile|16 minutes ago|1 file|

pushed to 4 branches:
||Repository||Branch||
|[dnac-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse]|[private/Frey-ms/api-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits?until=refs/heads/private/Frey-ms/api-auto]|
|[dnac-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse]|[private/Groot-ms/api-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits?until=refs/heads/private/Groot-ms/api-auto]|
|[dnac-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse]|[private/Guardian-ms/api-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits?until=refs/heads/private/Guardian-ms/api-auto]|
|[dnac-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse]|[private/Shockwave-ms/api-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits?until=refs/heads/private/Shockwave-ms/api-auto]| Hi Phan,

   We are still seeing this issue in sanity execution.

 

*Branch Used :* private/Shockwave-ms/sanity_api_auto (Latest code pulled from main branch - private/Shockwave-ms/api-auto)

*DNAC ISO Used :* DNAC Release Used : Shockwave P3 RC3 #2.1.390.72158

*ISE Version Used :* 3.1 P3

*Failed Log :* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=52354688&size=5971&archive=env_auto_job.2022Sep14_07:51:47.661486.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS

*Error Snip :* 

!image-2022-09-19-18-08-52-259.png! Vijay,

The additional issue was discussed with Nethra and the action item to check ISE auth profiles are still pending. Can you check ISE and confirm?

  !image-2022-09-19-17-17-25-371.png|width=738,height=322!

Regards,

Phan After creating the missing ISE authorization profile, this TC passed.  After creating the missing ISE authorization profile, the TC passed.","['Auton', 'Guardian', 'Issue']",Phan Nguyen,Closed,Avril Bower
SEEN-389,https://miggbo.atlassian.net/browse/SEEN-389,[Auton]:Shockwave :TC151_verify_fabric_360,"*ISO:* Shockwave#2.1.391.72224

*Script Name:*  solution_test_sanityecamb_lan.py

*Testcases Impacted :* 

[TC151_verify_fabric_360|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=25566981&size=115447&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F27%2F14%2F54%2Fenv_auto_job.2022Jul27_14:54:47.129499.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Fail Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=25566981&size=115447&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F27%2F14%2F54%2Fenv_auto_job.2022Jul27_14:54:47.129499.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Snip from Fail log:*
**
 api_switch_call called:
58463:  \{'method': 'POST', 'resource_path': '/assurance/v1/network-device/fabric-sites', 'data': {'type': 'ALL', 'endTime': 1658976951019, 'startTime': 1658890550835, 'currentTime': 1658977851194}}
58464:  Encountered unhandled HTTPError in Internal API Call
58465:  Flagging result as FAIL!
58466:  Reason: 405 Client Error: Method Not Allowed for url: [https://10.30.0.100/api/assurance/v1/network-device/fabric-sites]
58467:  Kwargs:
58468:  {'data': {'currentTime': 1658977851194,
58469:  'endTime': 1658976951019,
58470:  'startTime': 1658890550835,
58471:  'type': 'ALL'},
58472:  'method': 'POST',
58473:  'resource_path': '/assurance/v1/network-device/fabric-sites'}
58474:  Error Caught While Querying the Internal API
58475:  Traceback (most recent call last):
58476:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Shockwave-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 195, in wrapper
58477:  result = testfunc(func_self, **kwargs)
58478:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Shockwave-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 9448, in test4_verify_fabric_nodes
58479:  result = dnac_handle.verify_fabric_nodes()
58480:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Shockwave-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance_fabric/group.py"", line 549, in verify_fabric_nodes
58481:  fabric_response = self.services.api_switch_call(method=""POST"", resource_path=fabric_url, data=payload)
58482:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Shockwave-sanity-common-Multi-job/services/dnaserv/lib/api_groups/utils/group.py"", line 43, in api_switch_call
58483:  response = self.services.base.NB_API.call_api(**kwargs)
58484:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Shockwave-sanity-common-Multi-job/services/dna_ApicemClientManager/ApicemClientManager.py"", line 552, in call_api
58485:  **kwargs)
58486:  File ""/users/rvonmize/pyats-inst/lib/python3.7/site-packages/uniq/apis/client_manager.py"", line 232, in call_api
58487:  raise e
58488:  File ""/users/rvonmize/pyats-inst/lib/python3.7/site-packages/uniq/apis/client_manager.py"", line 227, in call_api
58489:  response.raise_for_status()
58490:  File ""/users/rvonmize/pyats-inst/lib/python3.7/site-packages/requests/models.py"", line 941, in raise_for_status
58491:  raise HTTPError(http_error_msg, response=self)
58492:  requests.exceptions.HTTPError: 405 Client Error: Method Not Allowed for url: [https://10.30.0.100/api/assurance/v1/network-device/fabric-sites]
58493:  Test returned in 0:00:00.995726
58494:  Errored reason: 405 Client Error: Method Not Allowed for url: [https://10.30.0.100/api/assurance/v1/network-device/fabric-sites]
**

**

 

 ",2022-07-29T11:00:57.458+0000,"RAkesh, please check with Venkat why he made this changes from Frey. And check whether the changes need for Shockwave too??

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/35a0b2ac1a7d5f546b19832571a6b49e2051c6fb#testcases/forty_eight_hour/solution_test_sanityecamb.py] Need fix for below script as well

DNAC Release: Shockwave

Script: dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

Impacted Testcases: 1

Where,
TC151_verify_fabric_360

Taas Log with (Shockwave (2.1.391.72224) + 17.6.4 (V17_06_04PRD6_FC1)):
https://ngdevx.cisco.com/services/taas/results/b8a724eb-ab1c-4867-a3c6-a390d75e7eee [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py?until=7fd55dab22293e98e14526d8f52889f71babec5a&at=refs%2Fheads%2Fprivate%2FShockwave-ms%2Fapi-auto]

Fixed on both scripts. [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py?until=7fd55dab22293e98e14526d8f52889f71babec5a&at=refs%2Fheads%2Fprivate%2FShockwave-ms%2Fapi-auto]

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/12bbd0e3b6f1b075ab0e572bf57454ed8f359ec5]

  Issue is not seen after the fix

Adding latest log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=50326338&size=123331&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F06%2F19%2F11%2F40%2Fenv_auto_job.2023Jun19_11:40:15.159702.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=50326338&size=123331&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F06%2F19%2F11%2F40%2Fenv_auto_job.2023Jun19_11:40:15.159702.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Issue', 'Shockwave']",Pawan Singh,Closed,Avril Bower
SEEN-390,https://miggbo.atlassian.net/browse/SEEN-390,Script Unable to Delete the Edge Devices in Border Priority TC,"Hi Tran,

    In our SR Multisite2 TB we are trying to integrate Border Priority TC but it is blocked with below mentioned reason,

!image-2022-07-29-22-41-38-904.png!

 

DNAC ISO Used : Guardian P1 RC3 #2.1.512.72139 

Device Image Used : 17.8.1a

Failed Log (TC152-TC153): https://earms-trade.cisco.com/tradeui/logs/details?ats=%2Fusers%2Fpawansi%2Fpyatsnew&client=web&host=st-ucs-sanity.cisco.com&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb2_three_sites.2022Jul29_05:37:17.137529.zip

 

Thanks,

Vijayakumar G.",2022-07-29T17:14:21.815+0000,"Both  SJC-FE-9300-1 and SJC-FE-9300-2 have Extended node connected as per log based on yaml.

The usecase was skipped because there was no Edge that have no extended node connected that was expected. This is as expected since it required an edge without extended node connected.","['AWS_MSTB', 'Auton', 'Issue', 'Multisite']",Tran Lam,Resolved,Avril Bower
SEEN-391,https://miggbo.atlassian.net/browse/SEEN-391,"[Auton] : Upgrade - Script is checking for Posture SSID after upgrade verify, but its removed after Initial Sanity run","Cluster Upgraded From,

{color:#6b778c}Shockwave P3 ({color}[2.2.3.5|https://2.2.3.5/]{color:#6b778c}) <> Guardian P1 RC4 ({color}[2.3.3.3|https://2.3.3.3/]{color:#6b778c}) {color}

{color:#6b778c}Solution Input file used for After_upgrade script : dnac-auto\configs\upgrade\solution_test_input_upgrade.json{color}

 

+*{color:#6b778c}Impacted TC's :{color}*+

[Test_TC19_DNAC_verify_SSID_lan_on_ECA_device|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15524662&size=102415&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F29%2F09%2F43%2Fenv_auto_job.2022Jul29_09:43:06.739359.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

[Test_TC44_DNAC_verify_SSID_lan_on_ECA_device|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=34938802&size=104891&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F29%2F09%2F43%2Fenv_auto_job.2022Jul29_09:43:06.739359.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 [Test_TC42_DNAC_configure_SSID_IP_Pool_Mapping_for_Wireless|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=31228539&size=2162670&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul29_09:43:06.739359.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&atstype=PYATS]

 

+*{color:#6b778c}Description :{color}*+

{color:#6b778c}In Shockwave P3 Sanity, we do not create 'POSTURE' SSID {color}

{color:#6b778c}But After upgrading the cluster to Guardian, in after_upgrade_verify script, the script is checking for 'POSTURE' SSID which is wrong.{color}

 

+*{color:#6b778c}SHOW WLAN SMMARY o/p after Shockwave Sanity :{color}*+

TB5-DM-eCA-BORDER#sh wlan summary

 

Number of WLANs: 14

 

ID   Profile Name                     SSID                             Status Security                                                                                            

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

17   SSIDDUALBA_Global_F_02231af4     SSIDDUAL BANDtb5                 UP     [WPA2][802.1x][FT + 802.1x][AES],[FT Enabled]                                                       

18   SSIDDot1XI_Global_F_afe5b29e     SSIDDot1XIndiatb5                UP     [WPA2][802.1x][AES]                                                                                 

19   Guest_weba_Global_F_b0ab54a7     Guest_webauthinternaltb5         UP     [WPA2][802.1x][AES],MAC Filtering,[Web Auth]                                                        

20   GUEST2tb5_Global_F_8ea917c5      GUEST2tb5                        UP     [open],MAC Filtering                                                                                

21   Random_mac_Global_F_f81aeb1a     Random_mactb5                    UP     [WPA2][802.1x][AES]                                                                                 

22   test_AAAtb_Global_F_605298da     test_AAAtb5                      UP     [WPA2][802.1x][AES]                                                                                 

23   Guest_webp_Global_F_646b0e2c     Guest_webpassthroughtb5          UP     [open],MAC Filtering,[Web Auth]                                                                     

24   GUESTtb5_Global_F_15bba114       GUESTtb5                         UP     [open],MAC Filtering                                                                                

25   ARUBA_SSID_Global_F_61bdc7d2     ARUBA_SSIDtb5                    UP     [open],[Web Auth]                                                                                   

26   Guest_pass_Global_F_7c2eb3f9     Guest_passthrough_inttb5         UP     [open],MAC Filtering,[Web Auth]                                                                     

 

 

Number of WLANs: 14

 

ID   Profile Name                     SSID                             Status Security                                                                                            

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

27   OPENtb5_Global_F_fa39d8b7        OPENtb5                          UP     [open]                                                                                              

28   DUALBANDwi_Global_F_8a1789b0     DUALBANDwithBANDSelecttb5        UP     [open]                                                                                              

29   Radius_ssi_Global_F_9ed3d838     Radius_ssidtb5                   UP     [WPA2][802.1x][AES]                                                                                 

30   Single5KBa_SAN JO_F_17bb4cf5     Single5KBandtb5                  UP     [WPA2][PSK][FT + PSK][AES],[FT Enabled],MAC Filtering                                               

 

TB5-DM-eCA-BORDER#

 ",2022-07-30T02:27:56.016+0000,Nethra to use different solution input for different upgrade path as discussed. Created New input files for Guardian and Groot Upgrades Created Release specific input json and removed unwanted SSID's. ,"['Auton', 'Guardian', 'Issue']",Nethra Thorali Suthagar,Closed,Avril Bower
SEEN-392,https://miggbo.atlassian.net/browse/SEEN-392,[Auton]:Ghost-Test_TC1_DNAC_Prime_Migration_DNAC_Server_Settings_NTP_DHCP_AAA_DNS_settings /test8_aduit_log_verify ,"*ISO:* Ghost (2350) 2.1.610.70269

*Script Name:*  job/sanity_tb3/sanity_TB3_cert.py ( Optimized job: TB3 )

*Testcases Impacted :* [Task-ise_integration_verification_aca_sync.py-32-ISEIntegration|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_integration_verification_aca_sync.py-32-ISEIntegration&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsanity_TB3_cert.2022Jul25_13:56:40.502942.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS] /[Test_TC1_DNAC_Prime_Migration_DNAC_Server_Settings_NTP_DHCP_AAA_DNS_settings|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_integration_verification_aca_sync.py-32-ISEIntegration&begin=4128&size=388059&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsanity_TB3_cert.2022Jul25_13:56:40.502942.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS] /[test8_aduit_log_verify|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_integration_verification_aca_sync.py-32-ISEIntegration&begin=189837&size=196281&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsanity_TB3_cert.2022Jul25_13:56:40.502942.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Description :*

According the scrubnotes in bug (CSCwc54566), need to update common settings messages as mentioned below:

Old

A request to create common settings is successful

A request to create common settings has failed

New

A request for desired common settings operation is successful

A request for desired common settings operation has failed

--------------------------------------------------------------------------------------------------

Old

Created Common Settings successfully.

Create common settings failed

New

Desired Common Settings operation successful.

Desired Common Settings operation failed

--------------------------------------------------------------------------------------------------

Please find the failed logs: 
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsanity_TB3_cert.2022Jul25_13:56:40.502942.zip&reqseq=&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=PYATS] 

 

 ",2022-08-01T07:03:46.437+0000,"Need fix for below script as well on audit log verification.

DNAC Release: Ghost

Script: dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

Impacted Testcases: 6

Where,

TC7 (test21) ; TC18 (test2) ; TC19 (test3) ; TC21 (test2) ; TC23 (test2) ; TC24 (test2)

Taas Log: [https://ngdevx.cisco.com/services/taas/results/71d03b9e-7ecd-4990-9a62-c4a2608bede3]  *Iso#*Ghost -2.1.610.70299

*Script Name:* solution_test_sanityecamb_lan.py 

*Testcases Impacted :*
 **

[Test_TC13_DNAC_configure_aaa_settings_site_level|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3611152&size=122408&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug01_19:02:10.172233.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/[test2_aduit_log_verify
 
[Test_TC14_DNAC_Wireless_SSID_creation_open_enterprise|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3733560&size=311552&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug01_19:02:10.172233.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/[test2_aduit_log_verify
 
[Test_TC16_DNAC_wireless_sensor_setting|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4127924&size=65612&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug01_19:02:10.172233.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/[test2_aduit_log_verify
 
[Test_TC18_DNAC_verify_creating_wireless_guest_portal|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4306612&size=2997541&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug01_19:02:10.172233.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/[test2_aduit_log_verify
 
[Test_TC19_DNAC_select_credentials_at_site_level|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7304153&size=83352&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug01_19:02:10.172233.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/[test2_aduit_log_verify
 
*Note :* please commit ghost branch as well.

**Failed  Log:*
 *[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug01_19:02:10.172233.zip&atstype=ATS]

 

 

  [ca28a90144a|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ca28a90144aa546f0ffd24717ff1a0f6170e4f61]","['Auton', 'Ghost', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-393,https://miggbo.atlassian.net/browse/SEEN-393,[Auton]:Guardian: Test_TC32_Compliance_verification  /   test7_verify_Fabric_compliance,"Cluster Upgraded From,

Shockwave P3 ([2.2.3.5|https://2.2.3.5/]) <> Guardian P1 RC4 ([2.3.3.3])

*ISO:*2.1.514.72142
 *Polarisis:*17.6.4prd7

 

*Observations:*
The path:services/dnaserv/lib/api_groups/Compilance/group.py
in this method, these lines are commented,

#self.log.info(""Remove VRF config"")

#cmd = ""no vrf definition {} \n"".format(fabric[""Vrf""][1][0])

#out=self.services.dnaconfig.testbed.devices[dev].configure(cmd)



 

*Need to fix in both scripts:*
 *Script:*after_upgrade_verify.py
  *Script:*solution_test_sanityecamb.py

Solution Input file used for After_upgrade script : dnac-auto\configs\upgrade\solution_test_input_upgrade.json

**NOTE:**
 Tried running compliance continuosly for 5 times, no issues seen manually. Please increase retries in the script

*Impacted Testcases in Upgrade Sanity:*
 Test_TC32_Compliance_verification/test7_verify_Fabric_compliance

*Impacted Testcases in Solution Sanity:*

Test_TC107_Compliance_verification / test7_verify_Fabric_compliance

*Failed log Upgrade:*
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=25130319&size=194043&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F29%2F09%2F43%2Fenv_auto_job.2022Jul29_09:43:06.739359.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Failed log Sanity:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=72522143&size=172674&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep19_04:01:44.578962.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Snip from failure:*
 *Test_TC32_Compliance_verification/test7_verify_Fabric_compliance*

63634: fabric compliance verification failed [""Failed to verify few attributes ['Vrf', 'IpHelperAddressSettings', 'VLANInterfaceExtended', 'LispProcess'] :: TB5-DM-eCA-BORDER"", ""Failed to verify few attributes ['Vrf', 'IpHelperAddressSettings', 'VLANInterfaceExtended', 'LispProcess'] :: TB5-DM-NF-Switch""]
 63636: Failed reason: Failed to verify fabric compliance",2022-08-01T07:32:53.290+0000,"same issue is seen in Lan script 
**ISO:**2.1.514.72142

*Script Name* :  solution_test_sanityecamb_lan.py

*Testcases Impacted* : [Test_TC107_Compliance_verification|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1412434&size=16077774&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug05_08:59:26.657110.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test7_verify_Fabric_compliance
*Fail Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15671494&size=303935&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug05_08:59:26.657110.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS Hi Raji 

I am seeing same failure in Guardian P4 RC3 2.1.518.72319 -> Can you please give a update on the fix

Please find the log: [test7_verify_Fabric_compliance|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8480532&size=205930&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_auto_job.2023Mar03_01:38:40.695796.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  we do not support it anymore from test as it causes unsupported issues in the network which does not get restored by reprovision as fabric thinks it is expected config change Hi [~63f50bf5e8216251ae4d59cf] 

For the fabric compliance issue is there any workaround or any other way to fix this as we are hitting this continuosly on Guardain Clusters Guardian Patch 4 -2.1.518.72323 RC4 
 Failed Log:

[test7_verify_Fabric_compliance|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=83329411&size=287925&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_auto_job.2023Mar06_10:56:12.503676.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Guardian P4RC4-2.1.518.72328
Failed Log:
[ [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=90843676&size=293741&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr18_10:03:40.142479.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=90843676&size=293741&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr18_10:03:40.142479.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ]","['Auton', 'Guardian', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-394,https://miggbo.atlassian.net/browse/SEEN-394,[Auton][IBSTE] : Test_TC97_generate_Worst_Interferers_report test1_generate_Worst_Interferers_CSV_report,"Affected testcase:

Test_TC97_generate_Worst_Interferers_report
 test1_generate_Worst_Interferers_CSV_report

Branch : private/Groot-ms/api-auto

Failed Log : 

Testcase is failing due to ""generate_Worst_Interferers_report"" attribute error
 24632: Traceback (most recent call last):
 24633: File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/ibsteservices.py"", line 1374, in function_call_sequentialOnSites
 24634: function_to_call = getattr(self,function_name)
 24635: File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/ibsteservices.py"", line 187, in _getattr_
 24636: raise AttributeError(err_msg)
 24637: AttributeError: 'IbsteServices' object has no attribute 'generate_Worst_Interferers_report'
 24638:
 24639: During handling of the above exception, another exception occurred:
 24640:
 24641: Traceback (most recent call last):
 24642: File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/client_manager.py"", line 295, in call_api
 24643: response.raise_for_status()
 24644: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status
 24645: raise HTTPError(http_error_msg, response=self)
 24646: requests.exceptions.HTTPError: 500 Server Error: Request failed. for url: [https://10.195.247.188/api/dnacaap/v1/daas/core/data-set]
 24647: Encountered unhandled HTTPError in Internal API Call
 24648: Flagging result as FAIL!
  
 Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5419147&size=139518&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_ibste.2022Jul31_22:48:44.660022.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2022-08-01T09:00:21.640+0000,"Issue is not seen with latest run

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4067890&size=482028&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug22_11:08:47.841848.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Groot', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-395,https://miggbo.atlassian.net/browse/SEEN-395,[Auton][IBSTE] : TC99_Verify_Global_Level_Events test1_verify_global_level_events,"Testcase affected : 

TC99_Verify_Global_Level_Events

test1_verify_global_level_events

 *Issue type* : Fix

Branch : private/Groot-ms/api-auto

Failed Log : 
 1659336661044, ""query"": {""queryType"": ""trend"", ""target"": ""events"", ""fields"": [], ""groupBy"": [""deviceFamily""], ""filters"": [

{""key"": ""siteHierarchyGraphId"", ""value"": ""6d42ad28-3a75-4604-aa48-024e46d58cbf"", ""operator"": ""like""}

]}}'} Headers:\{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2MmM2ZjdkNmQyYjc5NTFmYzgwZTI2ZWMiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjYyYzZmN2Q2ZDJiNzk1MWZjODBlMjZlYiJdLCJ0ZW5hbnRJZCI6IjYyYzZmN2Q1ZDJiNzk1MWZjODBlMjZlOSIsImV4cCI6MTY1OTMzOTc0NiwiaWF0IjoxNjU5MzM2MTQ2LCJqdGkiOiIxNGNlOTI1OS05ZjE4LTQyYTEtODM0NS1iOGUyOWQ4NzI3YjMiLCJ1c2VybmFtZSI6ImFkbWluIn0.mFB6RnbQAjIrgPl49Fs6Sbebhp-Tfi4A3nTHwhx0rxqTaF2APhAm06iuDMN4yDNbCMeMCaiRhNsN0fOr4VmQ1VP6wnqoNuPPrscVBR3B7OjJUn_dZVgqYknW8Hde2WA6xB8Yz_Xkg1YCGgEpPYCQo63WECaMQUekofqGUubr6CIzxaVGobYhKAncRHhGEzfGtKKfZIN4ofInF9iZR3cHtSdZxZN3QYG0GNBqvPfHl3tlItAKey4kRCwa21SAYINEgN8diXOnnuveiGYf898Jgtd5ahHG9Yab_2whvRc8ERa4f-XAB4rwmg4z4sugE6beVQe7XY-1ShM3o0D3gbRyBg;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:\{""response"":{""errorCode"":5000,""message"":""An internal has error occurred while processing this request."",""detail"":null}}
 25208: Traceback (most recent call last):
 25209: File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/ibsteservices.py"", line 1374, in function_call_sequentialOnSites
 25210: function_to_call = getattr(self,function_name)
 25211: File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/ibsteservices.py"", line 187, in __getattr__
 25212: raise AttributeError(err_msg)
 25213: AttributeError: 'IbsteServices' object has no attribute 'verify_global_events'
 25214: 
 25215: During handling of the above exception, another exception occurred:
  
 Failed Log :  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5711343&size=26952&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_ibste.2022Jul31_22:48:44.660022.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]",2022-08-01T09:06:27.777+0000,"Related : [https://miggbo.atlassian.net/browse/SEEN-648|https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-648]  Hi Team,

 

Can we please get closure for the ticket.

 

Thanks,

Divakar Moving reporter name to [~accountid:63f50bfde8216251ae4d59d8] as he is the current owner.

[~accountid:63f50bfde8216251ae4d59d8] , do we still have this issue?  Hi [~accountid:63f50bfde8216251ae4d59d8], do you still have this issue?

I just run this test case with the latest code on the Groot branch, and it gets passed.

Trade log link: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-07/sanity_TB7.2023Jul24_20:34:12.814947.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-07/sanity_TB7.2023Jul24_20:34:12.814947.zip&atstype=ATS]

If you did not still get this issue anymore, I will close the ticket. Thanks.","['Auton', 'Groot', 'IBSTE', 'Issue']",NhanHuu Nguyen,Closed,Avril Bower
SEEN-396,https://miggbo.atlassian.net/browse/SEEN-396,[Auton]:Shockwave:TC71_DNAC_extended_node_link_failover_test/test70_reassign_ext_node_ip,"**ISO*:*2.1.390.72158

**Polarisis*:*17.6.4prd7
**Script*:*solution_test_sanityecamb.py

**NOTE:** IP is changed but script failed due to timeout issue. Please increase retries / wait time

**Impacted Testcases:**TC71_DNAC_extended_node_link_failover_test/test70_reassign_ext_node_ip

**Failure log*:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=38455010&size=135542&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F26%2F23%2F54%2Fenv_auto_job.2022Jul26_23:54:09.596172.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS


*Snip from failure log:*
**
134677:  Device Connectivity issue, not recoverable through resyn, check why device in this state.
134678:  ERROR: Device SN-FOC2311T18E.cisco.com is not in managed state, current state:Partial Collection Failure, errorDescription:NCIM12013: SNMP timeouts are occurring with this device. Either the SNMP credentials are not correctly provided to Cisco DNA Center or the device is responding slow and SNMP timeout is low. If it’s a timeout issue, Cisco DNA Center will attempt to progressively adjust the timeout in subsequent collection cycles to get device to managed state. User can also run discovery again only for this device using the discovery feature after adjusting the timeout and SNMP credentials as required. Or user can update the timeout and SNMP credentials as required using update credentials., retrying
**


  ",2022-08-02T05:30:06.836+0000,"Product issue. In First resync attempt itself, it should rediscover and update IP in inventory, it did not do it even in 4 attempts. Raise a product defect, on sda why device is not rediscoverd at resync.","['Auton', 'Issue', 'Shockwave']",Pawan Singh,Resolved,Avril Bower
SEEN-397,https://miggbo.atlassian.net/browse/SEEN-397,[Auton]:Shockwave: Test_TC68_configure_policy/test2_aduit_log_verify,"**ISO*:*2.1.390.72158

**Polarisis*:*17.6.4prd7
 **Script*:*solution_test_sanityecamb.py

**NOTE:** ""Application registry"" is repeated twice in the response that’s the reason validation failed.

**Impacted Testcases:**Test_TC68_configure_policy/test2_aduit_log_verify

**Failure log*:*:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=36834344&size=78614&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F26%2F23%2F54%2Fenv_auto_job.2022Jul26_23:54:09.596172.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

**Snip from failure log:**
**

Custom application custom_classfier_UDP_30 was added to application set general-misc.']], 'UDP_31': ['Received Scalable Group Provisioning/ Application registry request for UDP_31', ['Service request processing completed successfully.', 'Application registry: Custom application UDP_31 was added to application set general-misc.']]}

128014: Failed reason: Result : Audit response verification failed

**",2022-08-02T05:35:40.319+0000,"Need fix for below script as well

DNAC Release: Shockwave

Script: dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

Impacted Testcases: 1

Where,
Test_TC68_configure_policy/test2_aduit_log_verify

Taas Log with (Shockwave (2.1.391.72224) + 17.6.4 (V17_06_04PRD6_FC1)):
https://ngdevx.cisco.com/services/taas/results/b8a724eb-ab1c-4867-a3c6-a390d75e7eee https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/5a21ed58828cca47ffd5a2606565e64ddb9d1599 Verified the fix","['Auton', 'Issue', 'shockwave']",Nethra Thorali Suthagar,Closed,Avril Bower
SEEN-398,https://miggbo.atlassian.net/browse/SEEN-398,[Auton]:Shockwave: Test_TC70_DNAC_verify_assurance_health_data_after_traffic_run/test1_verify_assurance_health_nw_health_ext_node/test1_verify_assurance_health_nw_health_border_node,"**ISO*:*2.1.390.72158

**Polarisis*:*17.6.4prd7
 **Script*:*solution_test_sanityecamb.py

**NOTE:** 

Need to avoid the time stamp when critical VLAN TC was executed or move this TC before critical VLAN TC

**Impacted Testcases:**
Test_TC70_DNAC_verify_assurance_health_data_after_traffic_run/test1_verify_assurance_health_nw_health_ext_node/test1_verify_assurance_health_nw_health_border_node

**Failure Log:**
*Log1:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=38005955&size=191308&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F26%2F23%2F54%2Fenv_auto_job.2022Jul26_23:54:09.596172.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]*[|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=38005955&size=191308&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F26%2F23%2F54%2Fenv_auto_job.2022Jul26_23:54:09.596172.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]*
*Log2:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=38286108&size=103753&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F26%2F23%2F54%2Fenv_auto_job.2022Jul26_23:54:09.596172.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

**Snip from failure:**

**

131886: No Values received

131887: Generating the Time in the ISO Format

131888: Using the URL /assurance/v1/time

**",2022-08-02T05:43:21.143+0000,"[~61efa8c457b25b006877eda3],

do you still see this issue? Hi [~63f50bfce8216251ae4d59d5] 

We are not hiting this issue now 
I will close the Jira

Thanks,
Anusha John Hi Moe
I am hitting that issue again:
Failed log1:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=38481369&size=106975&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F01%2F31%2F09%2F46%2Fenv_auto_job.2023Jan31_09:46:37.424457.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

Failed log2:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=38268296&size=124140&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F01%2F31%2F09%2F46%2Fenv_auto_job.2023Jan31_09:46:37.424457.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

Thanks,
Anusha Shockwave Fix: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5153/diff#services/dnaserv/lib/api_groups/assurance/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5153/diff#services/dnaserv/lib/api_groups/assurance/group.py]","['Auton', 'Issue', 'Sanity', 'Shockwave', 'assurance']",Moe Saeed,Resolved,Avril Bower
SEEN-399,https://miggbo.atlassian.net/browse/SEEN-399,[Auton]:Shockwave: Test_TC94_generate_link_flap_issues/test1_generate_link_flap_issues,"**ISO*:*2.1.390.72158

**Polarisis*:*17.6.4prd7
**Script*:*solution_test_sanityecamb.py

**NOTE:** If the connected interface is not up before the trigger, connect to IXIA reserve the ports and make it up

**Impacted Testcases:**Test_TC94_generate_link_flap_issues/test1_generate_link_flap_issues

**Failure log*:*:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=53727933&size=77755&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F26%2F23%2F54%2Fenv_auto_job.2022Jul26_23:54:09.596172.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

**Snip from Failure:**
TB5-DM-NF-Switch#
188986:  Link down Event not foundconfig term Enter configuration commands, one per line. End with CNTL/Z. TB5-DM-NF-Switch(config)# TB5-DM-NF-Switch(config)#interface GigabitEthernet1/0/24 TB5-DM-NF-Switch(config-if)# no shut TB5-DM-NF-Switch(config-if)#end TB5-DM-NF-Switch#
188996:  Unable retrieve output after multiple retries
189001:  Failed reason: No Events has been trigggred",2022-08-02T05:49:33.597+0000,"Passed in latest run:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=48668265&size=39784&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F14%2F23%2F56%2Fenv_auto_job.2022Aug14_23:56:05.552097.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS Seeing same issue in Guardian P2 (2.3.3.5)#2.1.515.70134

Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=34345274&size=70660&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov24_09:06:28.368011.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Script used:
solution_test_sanityecamb.py hi [~61efa8c457b25b006877eda3]

 

i try to run this TC but i catch another issue. it's difference from your failed log

my Tradelog: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/22-12/sanity_TB5.2022Dec13_00:31:06.903454.zip&atstype=ATS]

 

Can you provide me which branch did you run this TC? also is script file, TB files?... everything related to this issue

 

Thanks Hi [~63f50bcf4e86f362d39acde5] ,

I used ""private/Guardian-ms/sanity_api_auto"" and ""private/Shockwave-ms/sanity_api_auto""

**Script*:*solution_test_sanityecamb.py

You are hitting different issue because you missed to  run loopback TC 

Thanks,
 Anusha Passed in latest run:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=48668265&size=39784&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F14%2F23%2F56%2Fenv_auto_job.2022Aug14_23:56:05.552097.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Will reopen if issue is seen again","['Auton', 'Guardian', 'Issue', 'Shockwave']",QuangVinh Nguyen,Closed,Avril Bower
SEEN-400,https://miggbo.atlassian.net/browse/SEEN-400,[Auton]:Shockwave:TC101_DNAC_Policy_Extended_node/test1_onboard_policy_extended_node_interface,"**ISO*:*2.1.390.72158

**Polarisis*:*17.6.4prd7
**Script*:*solution_test_sanityecamb.py

**NOTE:** First clear the interface port assignment and then deploy with new configuration

**Impacted Testcases:**TC101_DNAC_Policy_Extended_node/test1_onboard_policy_extended_node_interface

**Failure log*:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=57801382&size=200658&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F26%2F23%2F54%2Fenv_auto_job.2022Jul26_23:54:09.596172.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS

**Snip from failure:**
**

199620: The Schedduled Job failed: with reason \{'id': 'db37b476-5ceb-4912-a87b-7b056f7b608d', 'triggeredJobTaskId': '2dbd9b25-803e-4ff7-aecc-f2011e9a32e5', 'triggeredTime': 1658937971615, 'status': 'FAILED', 'failureReason': 'NCSP10250: Error During persistence (modify) of CFS & SerializedSnapshot (name: SN-FCW2307G03S type: DeviceInfo qualifier: null)', 'triggeredJobId': 'db37b476-5ceb-4912-a87b-7b056f7b608d'}

199625: Failed reason: all the static IXIA interfaces are provisioned by dnac

**",2022-08-02T05:53:30.253+0000,"This does not look like script error. Please verify on DNAC, this doesn't look like script error Hi Raji,

  we get serialized snapshot error when the interfaces are not cleared and still if we try to onboard again. Please clear the interface before onboarding it again. To pull from Groot to Shockwave for this testcases to add clearing onboarding before onboarding.

Please also check other branches too if needed. https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/64abe046fe759dfae9b16ffee275bf1d63cc1490 Verified in Shockwave","['Auton', 'Issue', 'Shockwave']",Nethra Thorali Suthagar,Closed,Avril Bower
SEEN-401,https://miggbo.atlassian.net/browse/SEEN-401,[Auton]:Shockwave:Test_TC107_Compliance_verification/test10_verify_overall_compliance_status_for_devices,"**ISO*:*2.1.390.72158

**Polarisis*:*17.6.4prd7
 **Script*:*solution_test_sanityecamb.py

**NOTE:** Manually ran compliance consecutively 5 times, but the issue is not seen. Only via script its seen, Maybe need to add retries

**Impacted Testcases:**Test_TC107_Compliance_verification/test7_verify_Fabric_compliance/test10_verify_overall_compliance_status_for_devices

**Failure log*:*:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=65140977&size=53757&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F26%2F23%2F54%2Fenv_auto_job.2022Jul26_23:54:09.596172.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

**Snip from failure log:**

208801: Device TB5-DM-WLC, is non complianet :\{'deviceUuid': '252f210e-0e8c-47fa-98b9-9b979ecc0cc5', 'complianceType': 'IMAGE', 'status': 'NOT_APPLICABLE', 'state': 'SUCCESS', 'lastSyncTime': 1658906799291, 'lastUpdateTime': 1658939837338, 'sourceInfoList': [], 'message': 'Golden image is not available', 'additionalDataURL': '/api/v2/device-image/device?id=252f210e-0e8c-47fa-98b9-9b979ecc0cc5'}",2022-08-02T05:57:07.945+0000,"PR raised in below branches

Groot :

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3710/overview]

 

Guardian :

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3711/overview]

 

Ghost :

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3712/overview]

 

Shockwave

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3713/overview]","['Auton', 'Guardian', 'Issue', 'Shockwave']",Faiz Habibbhai Babuna,Resolved,Avril Bower
SEEN-402,https://miggbo.atlassian.net/browse/SEEN-402,[Auton]:Shockwave:Test_TC133_verify_wlc_wlan_VNIDs/test1_verify_wlans_VNIDs_mapping,"**ISO*:*2.1.390.72158

**Polarisis*:*17.6.4prd7
 **Script*:*solution_test_sanityecamb.py

**Impacted Testcases:**Test_TC133_verify_wlc_wlan_VNIDs/test1_verify_wlans_VNIDs_mapping

**Failure log*:*:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=76994778&size=520111&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F26%2F23%2F54%2Fenv_auto_job.2022Jul26_23:54:09.596172.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Snip from Failure Log:*
**

236232: SSID CiscoSensorProvisioning is not Found on device TB5-DM-WLC

236233: Failed ON [\{'TB5-DM-WLC': 'SSID CiscoSensorProvisioning Not Found'}]

236235: Failed reason: Failed to verify Sensor SSIDs

 

 ",2022-08-02T06:19:44.388+0000,"This is known product issue. 
(Cisco Controller) >config wlan ssid 1 CiscoSensorProvisioning

 WLAN not created.
======
The Sensor provisioning SSID need to be manually enabled once in the WLC.  Then in subsequent run it will get picked up in automation.

======= Manual steps 1 time:=====
Login to affected airos wlc.
Create the Sensor ssid using cli: config wlan create 1 CiscoSensorProvisioning
Once create use the assurance CLI to delete it:  config network assurance sensor provisioning disable
Resync the wlc and re-provision it.  and you should see the CiscoSensorProvisioning being configured by DNAC.
========== We have done this workaround long back, Not sure how this is coming up again. 
Can we fail tc3 if ssid cleanup is not done, so we can do it manually Passed in latest Run:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=72525496&size=541143&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F14%2F23%2F56%2Fenv_auto_job.2022Aug14_23:56:05.552097.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Issue', 'Shockwave']",Unassigned,Closed,Avril Bower
SEEN-403,https://miggbo.atlassian.net/browse/SEEN-403,[Auton]:Shockwave: Test_TC150_Client_AP_360/test2_verify_AP360_missing_KPI,"**ISO*:*2.1.390.72158

**Polarisis*:*17.6.4prd7
**Script*:*solution_test_sanityecamb.py
**NOTE**:For 4800 AP, slot 2 will be in monitor mode, hence we are hitting this failure. Script has to exclude 4800 AP for this TC

**Impacted Testcases:* Test_TC150_Client_AP_360/test2_verify_AP360_missing_KPI

**Failure log*:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=81839107&size=20868&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F26%2F23%2F54%2Fenv_auto_job.2022Jul26_23:54:09.596172.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

**Snip from Failure Log:**
**

254413: Failed to validate Missing KPIs on AP 360 for reason :: ['AP channel details is not showing up under connectivity under Radio 2 :: APDC8C.3756.9BA8', 'AP Current channelwidth details is not showing up under Connectivity Radio 2 :: APDC8C.3756.9BA8']

254415: Failed reason: Failed to verify AP360 missing KPI info

**",2022-08-02T06:24:05.902+0000,"[~603e0198678612006b9f8e30], please have the AP added to TB yaml file. We'll control it from the code. Added the AP in the yaml given slot as [0,1] in yaml, but still facing the same error 



Log : 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=500269&size=31982&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F18%2F13%2F46%2Fenv_auto_job.2022Aug18_13:46:39.323830.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 
2862:  Failed to validate Missing KPIs on AP 360 for reason :: ['AP channel details is not showing up under connectivity under Radio 2 :: APDC8C.3756.9BA8', 'AP Current channelwidth details is not showing up under Connectivity Radio 2 :: APDC8C.3756.9BA8']
2863:  Test returned in 0:00:02.774371
2864:  Failed reason: Failed to verify AP360 missing KPI info
 
!image-2022-08-18-13-56-10-049.png! There was an another Jira:
https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-865

where the issue was fixed and after pulling latest code issue was not seen.

Recently Passed Log:



[test1_AP_CDP_neighbor_info|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=20257040&size=238007&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F03%2F01%2F01%2F33%2Fenv_auto_job.2023Mar01_01:33:10.838037.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Issue', 'Shockwave', 'assurance']",Phan Nguyen,Closed,Avril Bower
SEEN-404,https://miggbo.atlassian.net/browse/SEEN-404,[Auton]:Shockwave:Test_TC43_DNAC_EXT_NODE_interface_config_verifications/test9_verify_extended_node_onboarding,"**ISO*:*2.1.390.72158

**Polarisis*:*17.6.4prd7
**Script*:*solution_test_sanityecamb.py
**NOTE** :Device  is UP and NON-COMPLIANT

 

**Impacted Testcases:**Test_TC43_DNAC_EXT_NODE_interface_config_verifications/test9_verify_extended_node_onboarding

**Failure log*:*:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19945227&size=31125&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F26%2F23%2F54%2Fenv_auto_job.2022Jul26_23:54:09.596172.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS%27]


**Snip from Failure Log:**
**

76506: Device Connectivity issue, not recoverable through resyn, check why device in this state.

76507: ERROR: Device SN-FCW2307G03S is not in managed state, current state:Partial Collection Failure, errorDescription:NCIM12021: Inventory data collection is aborted for this device. It could happen if a configuration push has been initiated for the device when the inventory data collection was in progress. Cisco DNA Center will attempt to resync this device in sometime., retrying

76518: Failed reason: FAILED: one or more extended node device has not been learnd through pnp

 ",2022-08-02T06:37:11.632+0000,"The device is in pcp, raise a dnac bug for partial collection failure. 

Collect RCA and show pnp history in device. 
Also sh log from device console. Hi Pawan,
Filed bug for the same:
[https://cdetsng.cisco.com/webui/#view=CSCwc64067]
Thanks,

Anusha Hi Pawan,
We had a discussion with Engineer([sha1@cisco.com|mailto:Engineer%C2%A0sha1@cisco.com]) for the bug:

[https://cdetsng.cisco.com/summary/#/defect/CSCwc64067]

As per the RCA ,TC 43.9 is taking more than 3 minutes to resync but in script execution time is 00:02:30 min 
As per the suggestion of Shashank we need to increase resync time.

Attaching the RCA Workflow log:

Please find below RCA logs:
*1.PNP Provision snip:*
2022-09-14 16:23:35,944 | INFO | SimpleAsyncTaskExecutor-1 | | c.c.e.n.m.ZtdDeviceProvisionedMessageHandler | Received pnp provisioned message for extended node NwOrchSubtendedNodeData\{pnpDeviceId='6321ff1acc89790d049c351e', pnpDeviceSerialNumber='FCW2307G03S', inventoryDeviceId='null', connectivityDomain='6be51a5f-1c6b-42ed-b49f-8ee3c0ac6d15', namespace='6be51a5f-1c6b-42ed-b49f-8ee3c0ac6d15', siteId='3cebb426-4035-4411-a674-e209828f915e', devicePlatformId='C9300-24P', extendedPoolVlan='1024', extendedPoolCIDR='204.1.33.0/24', extendedSegment='f5a551a2-60de-4df0-94c3-dfa622f66a61', resetInterfaces=[], trunkInterfaces=[tengigabitethernet1/1/1, tengigabitethernet1/1/2], extendedPoolId='05051a0f-ae46-4488-8c4f-4d3b5eb0ddf6', edgeDeviceInterfaceInfo={907c4e01-49e5-475f-9178-2bc121f3a852={TenGigabitEthernet1/1/2=TenGigabitEthernet1/1/2, TenGigabitEthernet1/1/1=TenGigabitEthernet1/1/1}}, retryCount=0, nwOrchSubtendedNodeRuntimeData=com.cisco.ef.networkorchestration.NwOrchSubtendedNodeData$NwOrchSubtendedNodeRuntimeData@45a88365, workflowData=ExtDevWorkflowData\{id='a45f9e36-13f3-4875-ae14-dab9205bac15', stage=PNP provision started}} |

 

 

*2.Inventory Log:*
2022-09-14 16:26:29,012 | TRACE | SimpleAsyncTaskExecutor-1 | | c.c.e.n.m.InventorySyncMessageHandler | Received InventorySyncMessage for NetworkDevice[apManagerInterfaceIp=,associatedWlcIp=,bootDateTime=2022-09-14 16:18:28,collectionInterval=Global Default,collectionIntervalValue=01:30:00,collectionStatus=Managed,description=Cisco IOS Software [Cupertino], Catalyst L3 Switch Software (CAT9K_IOSXE), Version 17.9.1, RELEASE SOFTWARE (fc8) Technical Support: http://www.cisco.com/techsupport Copyright (c) 1986-2022 by Cisco Systems, Inc. Compiled Sun 31-Jul-22 14:36 by mcpre netconf enabled,deviceSupportLevel=Supported,family=Switches and Hubs,hostname=SN-FCW2307G03S,interfaceCount=0,inventoryStatusDetail=<status><general code=""SUCCESS""/></status>,lastUpdateTime=2022-09-14 16:26:28.125,lastUpdated=2022-09-14 16:26:28,lineCardCount=0,lineCardId=,macAddress=d0:ec:35:9c:b2:80,managedAtleastOnce=false,managementIpAddress=204.1.33.150,managementState=Managed,memorySize=NA,paddedMgmtIpAddress=204. 1. 33.150,platformId=C9300-24P,reachabilityFailureReason=,reachabilityStatus=Reachable,role=UNKNOWN,roleSource=AUTO,serialNumber=FCW2307G03S,series=Cisco Catalyst 9300 Series Switches,snmpContact=,snmpLocation=,softwareType=IOS-XE,softwareVersion=17.9.1,tagCount=0,type=Cisco Catalyst 9300 Switch,upTime=0:08:41.54,uptimeSeconds=481,instanceUuid=7cca005b-2bb8-43de-963c-f5f67486fe70,instanceId=288293,authEntityId=288293,authEntityClass=-927529445,instanceTenantId=SYS0,_orderedListOEIndex=<Integer>,_creationOrderIndex=<Integer>,_isBeingChanged=<Boolean>,deployPending=<DeployPendingEnum>,instanceVersion=0], serial number:FCW2307G03S

 

*3.Workflow ID:*
a45f9e36-13f3-4875-ae14-dab9205bac15 The device state is PCF. Device should not go to PCF state at all if resync is in progress. 
Device going to PCF is not acceptable. The inventory state should show in progress if sync is in progress not PCF. Reopen the defect with these input, should be debugged more from DNAC/Device side.
Also these intermittent issue happen in customer setup and should be debugged. don;t let them go to  U simply.","['Auton', 'Issue', 'Shockwave']",Pawan Singh,Closed,Avril Bower
SEEN-405,https://miggbo.atlassian.net/browse/SEEN-405,[Auton]:Shockwave: TC151_verify_fabric_360/test2_verify_fabric_site_health_timeline/test3_verify_fabric_site_health/test4_verify_fabric_nodes/test5_verify_fabric_assurance_metric,"**ISO*:*2.1.390.72158

**Polarisis*:*17.6.4prd7
**Script*:*solution_test_sanityecamb.py

**NOTE:** First clear the interface port assignment and then deploy with new configuration

**Impacted Testcases:*TC151_verify_fabric_360/test2_verify_fabric_site_health_timeline/test3_verify_fabric_site_health/test4_verify_fabric_nodes/test5_verify_fabric_assurance_metric

**Failure log*:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=81946956&size=116336&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F26%2F23%2F54%2Fenv_auto_job.2022Jul26_23:54:09.596172.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Snip from Failure Log*:

{color:#bb0000}254941: requests.exceptions.HTTPError: 405 Client Error: Method Not Allowed for url: {color}[https://10.195.227.80/api/assurance/v1/network-device/fabric-sites]",2022-08-02T07:46:56.988+0000,"[~accountid:61efa8c457b25b006877eda3] , pls. double check if the reported issue got resolved with the fix provided  for [https://miggbo.atlassian.net/browse/SEEN-389|https://miggbo.atlassian.net/browse/SEEN-389|smart-link].



If yes, pls. mark this ticket as “Resolved”. Closing as issue was not seen after fix
Attaching latest log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=50326338&size=123331&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F06%2F19%2F11%2F40%2Fenv_auto_job.2023Jun19_11:40:15.159702.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=50326338&size=123331&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F06%2F19%2F11%2F40%2Fenv_auto_job.2023Jun19_11:40:15.159702.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Issue', 'Shockwave']",Pawan Singh,Closed,Avril Bower
SEEN-406,https://miggbo.atlassian.net/browse/SEEN-406,[Auton]:Upgrade_Guardian: Test_TC51_verify_post_upgrade_migration_status/test_check_poe_post_upgrade_migration_status,"Cluster Upgraded From,

{color:#6b778c}Shockwave P3 ({color}[2.2.3.5|https://2.2.3.5/]{color:#6b778c}) <> Guardian P1 RC4 ({color}[2.3.3.3|https://2.3.3.3/]{color:#6b778c}) {color}

{color:#6b778c}Solution Input file used for After_upgrade script : dnac-auto\configs\upgrade\solution_test_input_upgrade.json{color}

{color:#6b778c}Impacted TC's :{color}*{color:#6b778c}
{color}*{color:#6b778c}[Test_TC51_verify_post_upgrade_migration_status
|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=35818587&size=38466&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F29%2F09%2F43%2Fenv_auto_job.2022Jul29_09:43:06.739359.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]{color}

*{color:#6b778c}{color:#172b4d}Failed Log:{color}
{color}*{color:#6b778c}https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=35818587&size=38466&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F29%2F09%2F43%2Fenv_auto_job.2022Jul29_09:43:06.739359.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS{color}*{color:#6b778c}
{color}*

*Snip from Failed Log*

{color:#bb0000}86279: Errored reason: 404 Client Error: Not Found for url: {color}[https://10.195.227.80/api/v1/dna/telemetry/provision/subscription/getMigrationStatus]

 ",2022-08-02T08:02:47.395+0000,"FIxed on Guardian Also.
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/services/dnaserv/lib/api_groups/upgrade/group.py?until=15d83068dbbe9f2cda34d9ef43aac1154fee6fa3&at=refs%2Fheads%2Fprivate%2FGuardian-ms%2Fapi-auto
","['Auton', 'Issue']",Pawan Singh,Resolved,Avril Bower
SEEN-407,https://miggbo.atlassian.net/browse/SEEN-407,[Auton]:Upgrade-Test_TC51_verify_post_upgrade_migration_status/test_check_poe_post_upgrade_migration_status,"Cluster Upgraded From,

{color:#6b778c}Shockwave P3 ({color}[2.2.3.5|https://2.2.3.5/]{color:#6b778c}) <> Guardian P1 RC4 ({color}[2.3.3.3|https://2.3.3.3/]{color:#6b778c}) {color}

{color:#6b778c}Solution Input file used for After_upgrade script : dnac-auto\configs\upgrade\solution_test_input_upgrade.json{color}

 

+*{color:#6b778c}Impacted TC's :{color}*+

[Test_TC51_verify_post_upgrade_migration_status|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=35818587&size=38466&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F29%2F09%2F43%2Fenv_auto_job.2022Jul29_09:43:06.739359.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Failure Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=35818587&size=38466&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F07%2F29%2F09%2F43%2Fenv_auto_job.2022Jul29_09:43:06.739359.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS

*Snip from Failed Log*

{color:#bb0000}86279: Errored reason: 404 Client Error: Not Found for url: {color}[https://10.195.227.80/api/v1/dna/telemetry/provision/subscription/getMigrationStatus]",2022-08-02T08:06:05.884+0000,"Fixed in Guardian and Groot.
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/services/dnaserv/lib/api_groups/upgrade/group.py?until=15d83068dbbe9f2cda34d9ef43aac1154fee6fa3&at=refs%2Fheads%2Fprivate%2FGuardian-ms%2Fapi-auto

Will merge to Ghost from Groot.","['Auton', 'Guardian', 'Issue']",Pawan Singh,Resolved,Avril Bower
SEEN-408,https://miggbo.atlassian.net/browse/SEEN-408,[Auton]:Guardian: Test_TC29_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision/test1_verify_configuration_on_devices_fabric1,"**ISO*:*2.1.511.72077

**Polarisis*:*17.8.1a
 **Script*:*solution_test_sanityecamb.py

**NOTE:** DNAC Networking settings was selected ""Tacacs-Tacacs"" instead of ""Tacacs-Radius "" , Manually changed and proceeded

**Impacted Testcases:**
 1.Test_TC29_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision/test1_verify_configuration_on_devices_fabric1
 2.Test_TC37_DNAC_verifying_configuration_lisp_on_devices/test1_verify_configuration_fabric1
 3.Test_TC39_DNAC_configure_wireless_multicast/test1_verify_pim_igmp_config_on_fabric_devices
 4.Test_TC58_verify_assurance_health_nw_health_border_edge_wlc_ext_node_and_ipv6_migration/test7_verify_assurance_health_nw_health_border_node
 5.Test_TC70_DNAC_verify_assurance_health_data_after_traffic_run/test1_verify_assurance_health_nw_health_border_node
 6.TC72_verify_assurance_data_after_extnode_ip_change/test1_verify_assurance_health_nw_health_border_node
 7.TC73_DNAC_verify_aaa_lisp_radius_configuration_on_devices_through_scheduled_job/test2_verify_configuration_on_devices_fabric1/test3_verify_configuration_on_devices_fabric1
 8.Test_TC82_verify_wlc_device_assurance_metrics/test1_verify_wlc_device_assurance_metrics

**Failure log*:*
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8635699&size=642681&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul27_09:03:10.499995.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

**Snip from Failure Log*:*
 33221: TB4-DM-NF-Switch,TB4-DM-Transit,TB4-DM-eCA-BORDER
 33222: \{'result': True, 'output': 'show run | s tacacs\naaa group server tacacs+ dnac-client-tacacs-group\n server name dnac-tacacs_85.1.1.3\naaa authentication login dnac-cts-list group dnac-client-tacacs-group local\naaa authentication login VTY_authen group dnac-network-tacacs-group local\naaa authentication dot1x default group dnac-client-tacacs-group\naaa authorization exec VTY_author group dnac-network-tacacs-group local if-authenticated \naaa authorization network default group dnac-client-tacacs-group \naaa authorization network dnac-cts-list group dnac-client-tacacs-group \naaa accounting identity default start-stop group dnac-client-tacacs-group\naaa accounting exec default start-stop group dnac-network-tacacs-group\nip tacacs source-interface Loopback0 \ntacacs server dnac-tacacs_85.1.1.3\n address ipv4 85.1.1.3\n key xxxxxxxx\n timeout 10\nTB4-DM-NF-Switch#'}
 33484: Failed reason: Result: Config Verifications failed",2022-08-02T13:40:15.294+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/60ddafefdaf81c1ae028255d8438440abe11cb31#services/dnaserv/lib/api_groups/common_settings/group.py Passed in latest Run:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10050627&size=318397&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug29_03:37:15.153801.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Guardian', 'Issue']",Unassigned,Closed,Avril Bower
SEEN-409,https://miggbo.atlassian.net/browse/SEEN-409,[Auton]:Guardian:TC101_DNAC_Policy_Extended_node/test1_onboard_policy_extended_node_interface,"**ISO*:*2.1.511.72077

**Polarisis*:*17.8.1a
**Script*:*solution_test_sanityecamb.py

**Impacted Testcases:**
 TC101_DNAC_Policy_Extended_node/test1_onboard_policy_extended_node_interface

**Failure log*:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=73163267&size=359908&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul27_09:03:10.499995.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

**Snip from Failure Log*:*
260644:  Exception:
260645:  Traceback (most recent call last):
260646:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
260647:  result = testfunc(func_self, **kwargs)
260648:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 8217, in test1_onboard_policy_extended_node_interface
260649:  dnac_handle.verify_aaa_pac_key(dev['name']) and \
260650:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
260651:  result = method(*args, **kwargs)
260652:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/dnaserv/lib/api_groups/cli_check/group.py"", line 2349, in verify_aaa_pac_key
260653:  return self.verify_cts_pac(device)
260654:  AttributeError: 'Group' object has no attribute 'verify_cts_pac'",2022-08-02T13:48:56.098+0000,"[c739a6d8730|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c739a6d873053c7b29e23f2de2289af9767d87f0] Passed in latest Run:
**

[TC101_DNAC_Policy_Extended_node|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16920442&size=378271&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug31_12:07:23.329936.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Guardian', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-410,https://miggbo.atlassian.net/browse/SEEN-410,[Auton]:Guardian: TC167_wireless_posture_url_filter/test1_create_posture_filter/test2_add_posture_filter_to_ssid/test4_verify_url_filter_on_device/test5_prepare_client_install_anyconnect,"**ISO*:*2.1.511.72077

**Polarisis*:*17.8.1a
**Script*:*solution_test_sanityecamb.py

**Impacted Testcases:**
TC167_wireless_posture_url_filter/test1_create_posture_filter/test2_add_posture_filter_to_ssid/test5_prepare_client_install_anyconnect

**Snip from Failed Log*:*
*1.TC167_wireless_posture_url_filter/test1_create_posture_filter*


360436:  Exception:
360437:  Traceback (most recent call last):
360438:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
360439:  result = testfunc(func_self, **kwargs)
360440:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 11819, in test1_create_posture_filter
360441:  acl_info = dnac_handle.dnaconfig.testbed.custom[dnac_handle.dnaconfig.sid][""Posture_ACL""]
360442:  KeyError: 'Posture_ACL'
360443:  The result of section test1_create_posture_filter is => ERRORED

*2.TC167_wireless_posture_url_filter/test2_add_posture_filter_to_ssid / test5_prepare_client_install_anyconnect* 

360457: 
360458:  Exception:
360459:  Traceback (most recent call last):
360460:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
360461:  result = testfunc(func_self, **kwargs)
360462:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 11828, in test2_add_posture_filter_to_ssid
360463:  ssid_info = dnac_handle.dnaconfig.testbed.custom[dnac_handle.dnaconfig.sid][""Wireless_Posture""][""ssid""]
360464:  KeyError: 'Wireless_Posture'
360465:  The result of section test2_add_posture_filter_to_ssid is => ERRORED

3.TC167_wireless_posture_url_filter/test4_verify_url_filter_on_device 

362987:  TB4-DM-eCA-BORDER
362988:  \{'result': True, 'output': 'show running-config | s posture-filter\nTB4-DM-eCA-BORDER#'}
362995:  Failed reason: Result: Posture url filter cannot be verified on device",2022-08-02T14:18:22.993+0000,"Please follow wiki to integrate the feature:

https://wiki.cisco.com/display/EDPEIXOT/Posture+URL+Filter https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/29998f3e571efee7df06ce386c2b8135bfed454e","['Auton', 'Guardian', 'Issue']",Anusha John,Closed,Avril Bower
SEEN-411,https://miggbo.atlassian.net/browse/SEEN-411,[Groot] Assigning AI RF Profile to site fails with Internal Error,"*Uber ISO Version tested :* Promoted Groot RC1 Uber ISO - *2.1.560.70463, Non-FIPS, PUBSUB enabled* 

*Script Name :* solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Testbed :* MSTB1

*Testcases Impacted :*  

 [Test_TC51_DNAC_provision_all_aps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1027725&size=932216&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul27_03:51:39.928547.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
 -> [test4_assign_AI_profile_site|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1781079&size=178701&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul27_03:51:39.928547.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Fail Log :*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1781079&size=178701&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul27_03:51:39.928547.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Description :* 

We see that after enabling Kairos and creating AI RF profile, we see that assigning location for AI RF profile fails with Internal Error. However on checking the Cluster GUI, we see the assign of location is successful with no errors.

Attached is the relevant screenshot:
 !image-2022-08-02-21-33-46-163.png!

 

Could this be timing issue and needs to be checked in loop for some more time? OR Is it a genuine defect? 

 

 ",2022-08-02T15:46:53.518+0000,"Issue is till observed during Guardian Patch2 RC2 and Groot RC6/RC7 testing.

*Failed logs:*

*On Guardian Patch2 RC2:* 
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep24_16:07:54.401830.zip&atstype=ATS] -> Refer TC51, sub TC3

*On Groot RC7:*
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep21_01:28:52.576535.zip&atstype=ATS] -> Refer TC51, sub TC3 [~accountid:62d2fec15d6f5fd2c3db8f9f] Please check if the cloud connectivity is up and valid, and if it is not cloud connectivity issue, try to reach to DE team and check with them it could be a product issue. [~accountid:62d2fec15d6f5fd2c3db8f9f] , please let me know so I can close tis ticket.  Closing this ticket for now. Please add more info to this ticket. This seems to me a product issue. ","['Auton', 'Groot', 'Issue', 'MSTB1', 'MSTB2', 'Multisite']",Moe Saeed,Closed,Avril Bower
SEEN-416,https://miggbo.atlassian.net/browse/SEEN-416,[Groot] - Disable of ICAP Fails under ICAP section,"*Uber ISO Version tested :* Promoted Groot RC1 Uber ISO - *2.1.560.70463, Non-FIPS, PUBSUB enabled* 

*Script Name :* solution_assurance_test.py

*Testbed :* MSTB1

*Testcases Impacted :*  

[Test_TC35_disable_icap_response|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6571003&size=81633&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites.2022Jul30_11:38:14.469079.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 

 

*Fail Log :*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites.2022Jul30_11:38:14.469079.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS] -> Refer TC35

 

*Description :* 

Disabling of ICAP from ICAP session is giving 504 Server error.  Is this expected or do we need to add more time by looping for some more time?",2022-08-02T17:36:08.173+0000,"Issue is still observed during Guardian Patch4 testing.

Failed log - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fsr_mb_multi_sites.2023Feb27_09:49:23.926708.zip&atstype=ATS] -> Refer TC35 PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5214/diff#services/dnaserv/lib/api_groups/icap/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5214/diff#services/dnaserv/lib/api_groups/icap/group.py] [~accountid:63f50bfce8216251ae4d59d5] : Issue was still observed even after increasing the timeout to 300 seconds.

*Failed log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites.2023May08_10:38:02.000646.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites.2023May08_10:38:02.000646.zip&atstype=ATS] → Refer Task-3, TC35 

*Branch used for execution* - private/Halleck-ms/api-auto [~accountid:62d2fec15d6f5fd2c3db8f9f] ,

 Looks like the timeout is still not enough, this time with 300s timeout, 7 sessions got removed. However, before we only had 4 sessions got removed. I will double check to see what is the right timeout needed here. I believe it will need at least 360s for 8 sessions.  [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5674/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5674/overview]

Updated the session based on the observation, it worked for me with 300 timeout. But based on the logs, the timeout needed at least 360 for 8 sessions.

Please let me know if this did not work.","['Auton', 'Ghost', 'Groot', 'Guardian', 'Halleck', 'Hulk', 'Issue', 'MSTB1', 'Multisite', 'assurance']",Moe Saeed,Resolved,Avril Bower
SEEN-417,https://miggbo.atlassian.net/browse/SEEN-417,[Auton] - Ghost-Test_TC0_dnac_initial_cleanup/test1dnac_initial_cleanup,"*Release:* Ghost2.1.610.70299

*Script Name*:  solution_test_sanityecamb_lan.py

*Testcases Impacted :*
Ghost-Test_TC0_dnac_initial_cleanup/test1dnac_initial_cleanup 
*Fail Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=345643&size=109354&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug01_19:02:10.172233.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*snip from Fail log:*
2048:  Resource path full url: [https://10.30.0.100/api/v1/onboarding/pnp-settings]
2049:  Error Code: 500 URL:https://10.30.0.100/api/v1/onboarding/pnp-settings Data:\{'timeout': 30} Headers:\{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2MmU4MGEyMTc1ZGZmODQ3Y2QyY2Q0M2QiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjYyZTgwYTIwNzVkZmY4NDdjZDJjZDQzYyJdLCJ0ZW5hbnRJZCI6IjYyZTgwYTIwNzVkZmY4NDdjZDJjZDQzYSIsImV4cCI6MTY1OTQxMDU5OCwiaWF0IjoxNjU5NDA2OTk4LCJqdGkiOiJlNGJiN2I1YS0wMDRmLTRkYzUtYWEyMy00ZGM4NDc1MTg4YjIiLCJ1c2VybmFtZSI6ImFkbWluIn0.BUYR2U06iKp7kVzCS3b9Anyi-EuGq6V4zRQwm_sODmcv0CXedsLpgqfwWOBPD49Z6sOwdXUf12NayFYG0FITVC4ObUiHC4QMKQc25m_kNH4ZC79oKF3-j_eOG9uAHFQVhEdZ21IHAh9XQr-ajeX11G76K0C0dPL5iqJ-M-srSCIGLO6NhITw0aM7TZZoB0tGtEr0i6qUrCR5SHjUn71FAUrMQ8l3ZLe630AutC44JH0-SXFRDSBQx7nvLeYagmEWClL9yMht7Py8nlMWr17DeWDss1CL1lQ-PSHyib8191GXOWiDmvIES-ke0LyXIzyyROyMCJrpRPsOZTdja8yOug;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:\{""response"":{""errorCode"":""ERROR_CODE_N/A"",""message"":""com.cisco.pnp.api.exception.ZtdException: NCOB01257: Could not determine proxy enablement info."",""detail"":""NCOB01257: Could not determine proxy enablement info."",""href"":""/onboarding/pnp-settings""},""version"":""1.0""}
2050:  Traceback (most recent call last):
2051:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 295, in call_api
2052:  response.raise_for_status()
2053:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status
2054:  raise HTTPError(http_error_msg, response=self)
2055:  requests.exceptions.HTTPError: 500 Server Error: Server Error for url: [https://10.30.0.100/api/v1/onboarding/pnp-settings]
2056:  Encountered unhandled HTTPError in Internal API Call
2057:  Flagging result as FAIL!
2058:  Reason: 500 Server Error: Server Error for url: [https://10.30.0.100/api/v1/onboarding/pnp-settings]
2059:  Kwargs:
2060:  {}
2061:  Error Caught While Querying the Internal API
2062:  Encountered unhandled HTTPError in group ""common_settings"" method ""get_pnp_settings""!
2063:  Flagging result as FAIL!
2064:  Reason: 500 Server Error: Server Error for url: [https://10.30.0.100/api/v1/onboarding/pnp-settings]
2065:  Args: (<services.dnaserv.lib.api_groups.common_settings.group.Group object at 0x7fb64a4b8ee0>, 'authorizeDevices')
2066:  Kwargs:
2067:  {}",2022-08-02T20:37:21.658+0000,"Hey [~620b8357878c2f00729881c8],

Please file a defect for this, nothing to fix here.

thank you. Issue has not seen the latest run,
hence moving close state:  
PassLog:
https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fenv_optimized_auto_job.2022Dec05_14:19:41.378148.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS","['Auton', 'Ghost', 'Issue']",Omkar Sharad Wagh,Closed,Avril Bower
SEEN-418,https://miggbo.atlassian.net/browse/SEEN-418,[Auton]:Guardian: Test_TC52_dnac_all_config_cleanup  /   test1dnac_initial_cleanup,"Cluster Upgraded From,

{color:#6b778c}Guardian P1 ({color}[2.3.3.1|https://2.2.3.5/]{color:#6b778c}) <> Guardian P1 RC4 ({color}[2.3.3.3|){color:#6b778c} {color}

{color:#6b778c}Solution Input file used for After_upgrade script : dnac-auto\configs\upgrade\solution_test_input_upgrade.json{color}

 *{color:#6b778c}Impacted TC's :{color}*
Test_TC52_dnac_all_config_cleanup / test1dnac_initial_cleanup 

*Failure Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=306809&size=6886259&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F03%2F06%2F05%2Fenv_auto_job.2022Aug03_06:05:01.093289.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Snip from Failed Log***
24742:  Exception:
24743:  Traceback (most recent call last):
24744:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-Upgrade-Verify-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
24745:  result = testfunc(func_self, **kwargs)
24746:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-Upgrade-Verify-common-Multi-job/testcases/upgrade/after_upgrade_verify.py"", line 2145, in test1dnac_initial_cleanup
24747:  if(dnac_handle.cleanup_all_config(config_cleanup=False)):
24748:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-Upgrade-Verify-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
24749:  result = method(*args, **kwargs)
24750:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-Upgrade-Verify-common-Multi-job/services/dnaserv/lib/api_groups/cleanup/group.py"", line 274, in cleanup_all_config
24751:  r=self.services.unassociate_site_profile(issue_name=self.services.input_data[""issue_name""])
24752:  KeyError: 'issue_name'",2022-08-03T19:04:06.423+0000,"Pawan has a fix in a personal branch, and will merge. Closing this issue.","['Auton', 'Guardian', 'Issue']",Phan Nguyen,Resolved,Avril Bower
SEEN-419,https://miggbo.atlassian.net/browse/SEEN-419,Overlapping pools test need to fix to test the traffic for both the clients.,"The current test doesn't set up the traffic validation for both the client. 

we can do the following test to validate the traffic for the client after confirming with Tran/Fangfang.

we can ping the following IP's from the client.

 

fusion configs:

FUsion VLAN and IP

 

Vlan3305               [204.1.16.22|https://204.1.16.22/]     YES NVRAM  up                    up      VN2

Vlan3306               [204.1.16.26|https://204.1.16.26/]     YES NVRAM  up                    up    VN1

====

 

 

TB6-DM-eCA-BORDER#ping vrf VN1 [204.1.16.26|https://204.1.16.26/] source [204.1.176.1|https://204.1.176.1/]

Type escape sequence to abort.

Sending 5, 100-byte ICMP Echos to [204.1.16.26|https://204.1.16.26/], timeout is 2 seconds:

Packet sent with a source address of [204.1.176.1|https://204.1.176.1/]

!!!!!

Success rate is 100 percent (5/5), round-trip min/avg/max = 1/1/1 ms

 

======

TB6-DM-eCA-BORDER#ping vrf VN2 [204.1.16.22|https://204.1.16.22/] source [204.1.176.1|https://204.1.176.1/]

Type escape sequence to abort.

Sending 5, 100-byte ICMP Echos to [204.1.16.22|https://204.1.16.22/], timeout is 2 seconds:

Packet sent with a source address of [204.1.176.1|https://204.1.176.1/]

!!!!!

Success rate is 100 percent (5/5), round-trip min/avg/max = 1/1/1 ms

TB6-DM-eCA-BORDER#

 

 



Fail log:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug01_18:13:08.474516.zip&atstype=ATS]

 

 ",2022-08-04T08:08:33.351+0000,"PR: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3506/overview Is this fix merged in Groot too, Test failed with latest code in Groot: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep02_12:00:35.661678.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]

  Hey Raju,

It has not merged yet on Guardian. Once it get merged will do cherry pick for groot. Meanwhile, you can check the PR and pull the changes from my branch and run it. 

!image-2022-09-07-10-18-01-815.png! Thanks for the update [~63f50bfce8216251ae4d59d5] Issue still observed during Ghost testing on MSTB2 and AWS MS. Please commit the fix on Groot and Ghost as well. Hey [~62d2fec15d6f5fd2c3db8f9f], 

this issue fix has not merged in the fist place, it is still in PR. 

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3506/overview Hey [~63f50bfce8216251ae4d59d5] / [~62ab7a399cd13c0068b18fe0],

We see the fix has been merged to Guardian. Currently mostly of all our testbeds or on either Groot Patch1 or Ghost. Could you please commit the fix on these branches?
private/Groot-ms/api-auto
private/Ghost-ms/api-auto

 

 Regards

Sandeep S Moe, Can you chery pick or PR to other branches too? merged to all branches Hold on this Ticket, until this defect issue resolved:

https://cdetsng.cisco.com/webui/#view=CSCwd23162","['AWS_MSTB', 'Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'MSTB2', 'Multisite', 'sanity']",Moe Saeed,Closed,Avril Bower
SEEN-420,https://miggbo.atlassian.net/browse/SEEN-420,[Auton]Guardian :Test_TC23_delete_all_discoveries/test1_delete_all_discoveries,"*Release*: Shockwave, Guardian & Ghost

*Testbed* :  DMZ_TB2

*Script name*: dnac_cleanup_script.py 

*Testcases Impacted*: Test_TC23_delete_all_discoveries/test1_delete_all_discoveries 

 

Deletion of all discoveries from DNAC is failed with reason as ""AttributeError: 'DnaServices' object has no attribute 'discovery'""  

 

Please find the failed log :
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1070257&size=19474&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug03_22:00:03.234480.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Please commit the fix for Groot and Ghost ",2022-08-04T10:11:39.786+0000,"The fix is working fine in Groot. 
Need the fix for Shockwave, Guardian & Ghost 

Can you please share the commit or PR where the fix is committed ","['Auton', 'Ghost', 'Guardian', 'Issue', 'Shockwave']",Phan Nguyen,Closed,Avril Bower
SEEN-421,https://miggbo.atlassian.net/browse/SEEN-421,[Auton] : Guardian - Test_TC14_DNAC_Wireless_SSID_creation_open_enterprise / test1_ssid_creation_custome_rf_profile," 

*Guardian Uber ISO* : 2.1.512.72132

*Script* : ** sdwan_ibste_multi_site_script.py

*Impacted Test cases* : Test_TC14_DNAC_Wireless_SSID_creation_open_enterprise

*Testbed* : SDWAN [[https://wiki.cisco.com/display/EDPEIXOT/S.R.+SDWAN+Testbed+Inventory]]

 

*Full Log* : [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_sdwan.2022Aug03_23:44:05.765193.zip&atstype=ATS]

 

*Failure Log* :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2522214&size=93637&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_sdwan.2022Aug03_23:44:05.765193.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Failure Snip* :

Message:{""response"":

{""errorCode"":""NCND00050"",""message"":""NCND00050: An internal error occurred while processing the request""}

,""version"":""1.0""}
 13244: Traceback (most recent call last):
 13245: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SD-WAN/SDWAN-common-Multi-job/services/dnaserv/client_manager.py"", line 295, in call_api
 13246: response.raise_for_status()
 13247: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status
 13248: raise HTTPError(http_error_msg, response=self)
 13249: requests.exceptions.HTTPError: 500 Server Error: Server Error for url: [https://10.195.247.212/api/v1/commonsetting/wlan/-1]

 ===============================

13468: Adding wireless SSID FAILED for [{'instanceType': 'rfprofile', 'groupUuid': '-1', 'namespace': 'wlan', 'type': 'rfprofile.setting', 'key': 'rfprofile.info', 'value': [

{'rfProfileName': 'LOW', 'isARadioType': True, 'isBRadioType': True, 'parentProfileA': 'LOW', 'parentProfileB': 'LOW', 'channelWidth': '20', 'aRadioChannels': '36,40,44,48,52,56,60,64,149,153,157,161', 'bRadioChannels': '1,6,11', 'dataRatesA': '6,9,12,18,24,36,48,54', 'mandatoryDataRatesA': '6,12,24', 'dataRatesB': '1,2,5.5,6,9,11,12,18,24,36,48,54', 'mandatoryDataRatesB': '1,2,5.5,11', 'isCustom': False, 'isBrownField': False, 'defaultRfProfile': False, 'powerThresholdV1A': -60, 'powerThresholdV1B': -65, 'rxSopThresholdB': 'LOW', 'rxSopThresholdA': 'LOW', 'minPowerLevelA': -10, 'maxPowerLevelA': 30, 'minPowerLevelB': -10, 'maxPowerLevelB': 30}

, \{'rfProfileName': 'HIGH', 'isARadioType': True, 'isBRadioType': True, 'parentProfileA': 'HIGH', 'parentProfileB': 'HIGH', 'channelWidth': '20', 'aRadioChannels': '36,40,44,48,52,56,60,64,149,153,157,161', 'bRadioChannels': '1,6,11', 'dataRatesA': '12,18,24,36,48,54', 'mandatoryDataRatesA': '12,24', 'dataRatesB': '9,12,18,24,36,48,54', 'mandatoryDataRatesB': '12', 'isCustom': False, 'isBrownField': False, 'defaultRfProfile': False, 'powerThresholdV1A': -65, 'powerThresholdV1B': -70, 'rxSopThresholdB': 'MEDIUM', 'rxSopThresholdA': 'MEDIUM', 'minPowerLevelA': 7, 'maxPowerLevelA': 30, 'minPowerLevelB': 7, 'maxPowerLevelB': 30}, \{'rfProfileName': 'TYPICAL', 'isARadioType': True, 'isBRadioType': True, 'parentProfileA': 'TYPICAL', 'parentProfileB': 'TYPICAL', 'channelWidth': '20', 'aRadioChannels': '36,40,44,48,52,56,60,64,149,153,157,161', 'bRadioChannels': '1,6,11', 'dataRatesA': '6,9,12,18,24,36,48,54', 'mandatoryDataRatesA': '6,12,24', 'dataRatesB': '9,12,18,24,36,48,54', 'mandatoryDataRatesB': '12', 'isCustom': False, 'isBrownField': False, 'defaultRfProfile': True, 'powerThresholdV1A': -70, 'powerThresholdV1B': -70, 'rxSopThresholdB': 'AUTO', 'rxSopThresholdA': 'AUTO', 'minPowerLevelA': -10, 'maxPowerLevelA': 30, 'minPowerLevelB': -10, 'maxPowerLevelB': 30}, \{'rfProfileName': 'testProfile', 'isARadioType': True, 'isBRadioType': True, 'parentProfileA': 'LOW', 'parentProfileB': 'HIGH', 'channelWidth': '20', 'aRadioChannels': '36,40,44,48,52,56,60,64,149,153,157,161', 'bRadioChannels': '1,6,11', 'dataRatesA': '6,9,12,18,24,36,48,54', 'mandatoryDataRatesA': '6', 'dataRatesB': '9,12,18,24,36,48,54', 'mandatoryDataRatesB': '9', 'isCustom': True, 'isBrownField': False, 'defaultRfProfile': False, 'powerThresholdV1A': -60, 'powerThresholdV1B': -70, 'rxSopThresholdB': 'MEDIUM', 'rxSopThresholdA': 'LOW', 'minPowerLevelA': -10, 'maxPowerLevelA': 30, 'minPowerLevelB': 7, 'maxPowerLevelB': 30}, \{'rfProfileName': 'custom_profile', 'isARadioType': True, 'isBRadioType': True, 'parentProfileA': 'LOW', 'parentProfileB': 'HIGH', 'channelWidth': '20', 'aRadioChannels': '36,40,44,48,52,56,60,64,149,153,157,161', 'bRadioChannels': '1,6,11', 'dataRatesA': '6,9,12,18,24,36,48,54', 'mandatoryDataRatesA': '6', 'dataRatesB': '9,12,18,24,36,48,54', 'mandatoryDataRatesB': '9', 'isCustom': True, 'isBrownField': False, 'defaultRfProfile': False, 'powerThresholdV1A': -60, 'powerThresholdV1B': -70, 'rxSopThresholdB': 'MEDIUM', 'rxSopThresholdA': 'LOW', 'minPowerLevelA': -10, 'maxPowerLevelA': 30, 'minPowerLevelB': 7, 'maxPowerLevelB': 30}]}]

 

 ",2022-08-05T06:51:35.054+0000,"Hi Andrew,

 

Seems we need to get this change as below ticket in the file configs/sr_sdwan/solution_input/solution_test_input.json

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-150

 

Thanks,

Srikanth [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3695/overview]

 

Also added to groot and ghost As per the latest run log on the local branch this is fixed.

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2475666&size=97584&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_sdwan.2022Sep21_17:51:40.466662.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Guardian', 'Issue']",Andrew Chen,Resolved,Avril Bower
SEEN-422,https://miggbo.atlassian.net/browse/SEEN-422,[Auton]:Guardian MS2 TB2: Syslog - Instead of skipping the step if already event notification is created its failing.,"While trying to integrate Syslog Feature in Multisite TB2, Script is failing while checking event notifications. 
*test5_syslog_event_notifications_creation* –  Instead of skipping the step if already event notification is created its failing. Looks like api output has been changed in latest Image.
Log :  https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb2_three_sites.2022Aug02_23:39:28.932283.zip&reqseq=&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=PYATS",2022-08-05T07:03:27.899+0000,"There is no issue with script already communicated to user as mentioned in below.

I re-checked your logs and below are my inputs.

 

*test5_syslog_event_notifications_creation* –  Instead of skipping the step if already event notification is created its failing. Looks like api output has been changed in latest Image. Will fix it tomorrow.

 

I don’t see any issue with the script. From api response, I could see that name and description is having expected ip but inside 'syslogConfig',  I see that ip you have changed to diff ip which is causing the error.  Please re-run by correct the ip or remove the current notification and re-run multiple times, It should work.

 

[{

                'version': None,

                'subscriptionId': '0c2825f1-ece4-4710-9e74-781594695ff0',

                'name': '172.23.241.139',

                'description': '172.23.241.139',

                'subscriptionEndpoints': [{

                                'instanceId': 'e20796b4-6395-4da8-b938-db1f5c372e21',

                                'subscriptionDetails': {

                                                'connectorType': 'SYSLOG',

                                                'instanceId': 'e20796b4-6395-4da8-b938-db1f5c372e21',

                                                'name': 'sys',

                                                'description': 'test',

                                                'syslogConfig': {

                                                                'version': '1.0',

                                                                'tenantId': '62e12b3abd66326895a970b0',

                                                                'configId': 'e20796b4-6395-4da8-b938-db1f5c372e21',

                                                                'name': 'sys',

                                                                'description': 'test',

                                                                'host': '10.192.251.4',

                                                                'port': 514,

                                                                'protocol': 'TCP'

 

https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Frmukkama-sjc%2Fpyats%2Fusers%2Frakdomma%2Farchive%2F22-08%2Fsanity_TB1.2022Aug07_09:11:02.231222.zip&atstype=ATS","['Auton', 'Guardian', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-423,https://miggbo.atlassian.net/browse/SEEN-423,Remove the prime check from fabric ,"Already discussed with Andrew, This ticket is to track the  remove the prime check from fabric ",2022-08-08T21:41:03.744+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e576a8ae21aedd75ccaaba16f309239e9d1b8e60]

 

Guardian Fix is available in Groot as well as Guardian Branch 

 ","['Groot', 'Guardian', 'Issue', 'auton']",Andrew Chen,Closed,Avril Bower
SEEN-424,https://miggbo.atlassian.net/browse/SEEN-424,Guardian-Test_TC42_generate_ap_impersonation -test2_add_linux_syslog_server,"*Uber ISO Version tested :* 2.1.512.72139-Guardian P1

*Script Name :* solution_assurance_test.py

*Testbed :* AWS MS

*Branch code is running on*:private/Guardian-ms/sanity_api_auto

*Testcases Impacted :*  

[Test_TC42_generate_ap_impersonation|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=610726&size=2831896&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fauto_MS_job.2022Aug05_01:02:41.332962.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Failed Log:
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fauto_MS_job.2022Aug05_01:02:41.332962.zip&atstype=ATS]

 

*Description:*
 In [Test_TC42_generate_ap_impersonation|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=610726&size=2831896&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fauto_MS_job.2022Aug05_01:02:41.332962.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Tc,this Tc [test2_add_linux_syslog_server|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=640460&size=2526&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fauto_MS_job.2022Aug05_01:02:41.332962.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] is failing. When we checked ,we observed that while verifying syslog feature Tc , the syslog server was already configured on DNAC.So it's failing here when we are trying to add linux_syslog_server again.
 The script should say already syslog server exists right, if already configured.
  ",2022-08-09T07:08:16.529+0000,"We can send a GET for the existing syslog configuration, and skip adding the same if it exists. PR has been raised for guardian,groot,ghost

 

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3635/diff#testcases/mega_topo/solution_assurance_test.py here is the right PR

 

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3650/overview","['AWS_MSTB', 'Auton', 'Guardian', 'Issue', 'Multisite']",Avril Bower,Closed,Avril Bower
SEEN-425,https://miggbo.atlassian.net/browse/SEEN-425,prime testcases needs xpath changes,"Few of the xpaths are not working in DMZ prime, need help to update.

 

log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug09_09:46:58.588494.zip&atstype=ATS]

 

 ",2022-08-09T23:48:34.519+0000,"xpath is not working, when we have the ISE integrated.","['Auton', 'Groot', 'Guardian', 'Issue', 'auton']",Andrew Chen,Resolved,Avril Bower
SEEN-426,https://miggbo.atlassian.net/browse/SEEN-426,[Auton]:Guardian: Test_TC44_ext_node_ipphone_onboarding_verifications/test1_extnode_ap_static_onboarding_verifications,"<This cluster is upgraded from Guardian P1 RC4 (2334) <> Groot RC4>

Sanity Script Name :   solution_test_sanityecamb.py

Script Name :  after_upgrade_verify.py

*Release :*Guardian P1 RC4 

*ISO:*2.1.514.72142
 *Polarisis:*17.8.1a

 

*Release :*Groot RC4 

*ISO:*2.1.560.70508
 *Polarisis:*17.8.1a

 

*Description:*

The script is onboarding Interface Twice 
 Onboarding failed on Ext node for AP - Gig 0/3 , Manually worked

 

*Testcases Impacted:*

Test_TC44_ext_node_ipphone_onboarding_verifications/test1_extnode_ap_static_onboarding_verifications (Sanity) Test_TC36_ext_node_ipphone_onboarding_verifications / test1_extnode_ipphone_static_onboarding_verifications(Upgrade Sanity) 

 

*Error from DNAC:*
 ++++
 Failure Reason: NCSP11108: Error occurred while processing the request. Additional info for support: taskId: '7b2b4cdd-ad1d-430f-8439-13a128cae258'. CFS persistence failed for (operation: 'modify' name: 'TB5-DM-eCA-BORDER.cisco.com' type: 'DeviceInfo' qualifier: 'null'). Error: 'org.springframework.dao.DataIntegrityViolationException: Could not execute JDBC batch update; SQL [insert into PolicyProfile (INSTANCE_VERSION, InstanceUuid, displayName, PolicyProfileName, NetworkDeviceId, InterfaceName, IsGuestAnchor, IsFlexConnect, AuthMode, SessionTimeOut, ClientExclusionEnable, IsFastLane, IsFabric, IsUpnEnabled, IsUnicastFilteringEnabled, BssClientIdleTimeout, ClientExclusionTimeout, TrafficType, AccountingListName, IsRadiusProfilingEnabled, SettingsSiteId, AuthAclName, AuthAclIpv6Name, UrlFilterName, AaaPolicyName, AuthEntityId, AuthEntityClass, InstanceOrigin, tenantIntSegment, tenantLongSegment, ID) values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]; constraint [policyprofile_bk]; nested exception is org.hibernate.exception.ConstraintViolationException: Could not execute JDBC batch update'.

*Sanity Failed Log:*
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=28135304&size=269091&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul27_09:03:10.499995.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Upgrade Failed Log:*

*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=265908&size=324425&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F11%2F21%2F51%2Fenv_auto_job.2022Aug11_21:51:47.556683.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS*]

*Snip from Failed Log:*

103373: The Schedduled Job failed: with reason \{'id': 'f35eec79-60a7-42ae-a166-a4059d452b51', 'triggeredJobTaskId': '2d4c692d-ce2e-44aa-9ff1-c22ea7f546f2', 'triggeredTime': 1658951907599, 'status': 'FAILED', 'failureReason': 'Failure reason is too large to be displayed. Please refer to log file for module: PROVISION', 'triggeredJobId': 'f35eec79-60a7-42ae-a166-a4059d452b51'}
 103381: Failed reason: Some of the TSIM onboarding failed or not TSIM client present.

 

 

 

Assurance Health of EXT.node Passed Log:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13559118&size=406400&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F09%2F16%2F41%2Fenv_auto_job.2022Aug09_16:41:27.723900.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 ",2022-08-10T08:58:24.797+0000,"Script: dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

Observed similar failure on solution sanity regression with Guardian/Groot/Ghost.

Either of or more than one among TC42,TC44,TC46 will fail due to it.

Pawan did script fix and fix works fine with latest groot workspace sync

Failed on Aug19-2022 with Guardian Branch. Looks like fix commit is missing. Have pinged Pawan to commit fix on Guardian branch as well Pawan fixed the issue:


 # [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3c45de45941f258507a82336c684e2ee7690bc42], https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d449e89e363b378dfa349029118f2a3e44ea7f21) Passed in latest Guardian Execution:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=474855&size=290327&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug31_12:07:23.329936.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS Closing the issue, as per pass log from [~61efa8c457b25b006877eda3]

 ","['Auton', 'Groot', 'Guardian', 'Issue', 'Sanity']",Pawan Singh,Closed,Avril Bower
SEEN-427,https://miggbo.atlassian.net/browse/SEEN-427,[Auton]:Guardian: Test_TC44_ext_node_ipphone_onboarding_verifications/test2_dnac_ext_node_verify_edge_interface_config_after_onboarding,"*Release :*Guardian P1 RC4 

*ISO:*2.1.514.72142
*Polarisis:*17.8.1a

 

Impacted TC's:
Test_TC46_ext_node_onboarding_verifications/test2_dnac_ext_node_verify_edge_interface_config_after_onboarding
TC71_DNAC_extended_node_link_failover_test/test70_identify_devices_and_interfaces
Test_TC79_hitless_authentication/test1_start_dot1x_auth_ixia_ipv4/test5_verify_template/test6_verify_dot1x_auth_ixia_ipv4
Test_TC126_verify_inventory_insights/test1_verify_speed_duplex_mismatch/test2_verify_VLAN_mismatch
Test_TC150_Client_AP_360/test1_AP_CDP_neighbor_info

 

*Failed Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=29735457&size=38583&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug03_12:08:31.758640.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Snip From Failed Log:*

 
102304:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/connections/manager.py"", line 453, in connect
102305:  output = connection.connect()
102306:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/connection.py"", line 772, in connect
102307:  raise ConnectionError('failed to connect to %s\n%s' %
102308:  unicon.core.errors.ConnectionError: failed to connect to SN-FOC2350L1BR
102309:  Failed while bringing device to ""any"" state
 ",2022-08-10T09:10:59.448+0000,"Fixed Line speed issue for Extended nodes in testbed Passed in latest run:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=18632555&size=363194&archive=env_auto_job.2022Oct12_10:30:18.142391.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Guardian', 'Issue']",Nethra Thorali Suthagar,Closed,Avril Bower
SEEN-428,https://miggbo.atlassian.net/browse/SEEN-428,[Auton]:Guardian: Test_TC136_enable_ICMP_ping_check_AP_reachability/test1_enable_icmp_verify_ap_reachability,"*Release :*Guardian P1 RC4 

*ISO:*2.1.514.72142
 *Polarisis:*17.8.1a

Script Name :  solution_test_sanityecamb.py

*Failed Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=97131446&size=111507&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug03_12:08:31.758640.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 
 *Snip from Failure Log:*
 305178: raise SubCommandFailure(""Command execution failed"", err) from err
 305179: unicon.core.errors.SubCommandFailure: ('Command execution failed', SubCommandFailure('sub_command failure, patterns matched in the output:', ['^[Ww]arning'], 'service result', ""config interface ap-manager management disable\r\r\n\r\r\n\r\r\nWarning! You have no AP manager on this port.\r\r\nThe controller behavior will be unpredictable.\r\r\nDisabling the AP-manager interface will reboot\r\r\nAP's connected on this interface.\r\r\nAre you sure you want to continue? (y/n) ""))
 y
 305547: Failed for reason ['Unable to disable WLAN and shut management interface']
 305550: Failed reason: Failed to verify AP reachability after it is disassociated from controller",2022-08-10T09:21:42.101+0000,Might be Related to https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-236 should be fixed with changes done via SEEN-1083,"['Auton', 'Groot', 'Guardian', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-429,https://miggbo.atlassian.net/browse/SEEN-429,Need to clear the line if console is locked [Optimized code],"This issue is seen only when we run the optimized code and ran the usecase from 6 and 7.

 
514:  Group ipPoolCidr for pool:underlay_sub is 204.1.1.0/24
515:  +++ TB3-DM-eCA-BORDER logfile /home/admin/.pyats/runinfo/sanity_TB3_cert.2022Aug10_16:10:28.263364/TB3-DM-eCA-BORDER-cli-1660173145.log +++
516:  +++ Unicon plugin iosxe (unicon.plugins.iosxe) +++
Trying 172.19.186.119...
518:  +++ connection to spawn: telnet 172.19.186.119 2038, id: 140114853328824 +++
519:  connection to TB3-DM-eCA-BORDER
telnet: connect to address 172.19.186.119: Connection refused
521:  Traceback (most recent call last):
522:  File ""src/unicon/eal/backend/pty_backend.py"", line 496, in unicon.eal.backend.pty_backend.RawPtySpawn._read
523:  OSError: [Errno 5] Input/output error
524: 
525:  The above exception was the direct cause of the following exception:
526: 
 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-underlay_automation_and_validations.py-71-LANAutomation&begin=80736&size=42696&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug10_16:10:28.263364.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS]

 ",2022-08-10T23:48:04.437+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits?until=refs%2Fheads%2Fprivate%2FGroot-ms%2Fapi-auto

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a76c4beaf75cb2ca1a190d1f57a179f80d6a9fad Fix is in Ghost & Groot
Fix is not available in Guardian","['Groot', 'Guardian', 'Issue', 'auton']",Pawan Singh,Closed,Avril Bower
SEEN-430,https://miggbo.atlassian.net/browse/SEEN-430,[Guardian] [Groot] [Auton] - Telemetry should not be enabled on Cat4k/Cat6k devices,"*Uber ISO Version tested :* Promoted Guardian RC3 Uber ISO - *2.1.510.70395*

*Script Name :* quake.py

*Testbed :* Cat6k LAN Automation Testbed

*Testcases Impacted :* TCs 31 and TC32

 

 

*Fail Log :*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Froot%2F.pyats%2Farchive%2F22-04%2Fquake.2022Apr18_09:57:18.462444.zip&reqseq=&ats=%2Fws%2Fsmounasw-sjc%2FPyats&submitter=root&from=trade&view=all&atstype=PYATS] -> Refer TC31 & TC32

 

*Description :* 
 
During Guardian testing, Provision of devices for cat4k and cat6k devices failed. When checked with Telemetry team and they confirmed, Telemetry doesn't support cat4k/cat6k. Hence should not be enabled on Cat4k/Cat6k devices.


Please refer below Auton for more details - 
*CSCwb66956* - [SR] Telemetry team confirmed: ""App telemetry does n't support on cat4k and cat6k devices ",2022-08-11T10:48:27.551+0000,"There is no change in this script. From log, all our tasks triggered by script were successful. Our script didnot enable App Telemetry.  The failed task 'taskId': 'd109f4f5-3bf9-46bb-af68-07841fe88e81' was not found in any test in the shared log. This is not Autons issue.

Telemetry should be triggered by DNAC system itself. Please open/reopen defect for it.

 

  Close this ticket since Sandeep didnot have any new update for a long time.","['Auton', 'Cat6k', 'Groot', 'Guardian', 'Issue']",SANDEEP SHIVARAMAREDDY,Closed,Avril Bower
SEEN-432,https://miggbo.atlassian.net/browse/SEEN-432,ISE ACA APi failed,"Hi Team,

The ISE aca API is failing.

ISE version: 3.1

Branch: private/Groot-ms/sanity_api_auto

 

Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-dnac_cleanup.py-11-dnacCleanup&begin=4235&size=66131&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug08_12:38:35.334983.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS]

 

 

 
4:  {}
195:  Resource path full url: [https://10.195.227.92/api/v1/aca-controller-service/platform/details]
196:  Error Code: 404 URL:https://10.195.227.92/api/v1/aca-controller-service/platform/details Data:\{'timeout': 30} Headers:\{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2MmYwZDQyZmVkMGRiMTE5ZTQyYzNlMWUiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjYyZjBkNDJlZWQwZGIxMTllNDJjM2UxZCJdLCJ0ZW5hbnRJZCI6IjYyZjBkNDJkZWQwZGIxMTllNDJjM2UxYiIsImV4cCI6MTY1OTk5MTEzMCwiaWF0IjoxNjU5OTg3NTMwLCJqdGkiOiI3NjFlNGYxMS03Yzc2LTQzNGQtOWQ5My1jMGIzMGFmNTZiOTMiLCJ1c2VybmFtZSI6ImFkbWluIn0.xWsAFY8je9qIVovfMRVwPY2kflMIoOtC3BWZ1eqM1r3i-C_EzAZBSw-U1paRYtfOiBbO8c6N-cgJUmMDvfGBzjAz9FgzbWNykp0eIWpbIrBVwKmMwRe_qa3xJY9hs9RPiSskwTeV1IXgvrEd1QPXSn9Uiv9jqdcGac8VW_FQ9pQjvqYGT8oYfW5IvVTZMEPGN6RmSqeDHjLpB49QaxkZiWDGEKzDwAKV53lmsGjsOL3gOIaLdIMgbHrjqPVYgfK4aAbuSV5IyROWzPfpDi4ioOoZ6KSbUBuMd1LMXj8iaf6xhMk1HE3_MBe9SzykJzsIAJRFxcn_Po6-MvRMAzXj4w;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:Not Found
197:  Traceback (most recent call last):
198:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/optimizedsanity1/services/dnaserv/client_manager.py"", line 295, in call_api
199:  response.raise_for_status()
200:  File ""/ws/sjc-it/pawan/pyatsds3/lib64/python3.6/site-packages/requests/models.py"", line 960, in raise_for_status
201:  raise HTTPError(http_error_msg, response=self)
202:  requests.exceptions.HTTPError: 404 Client Error: Not Found for url: [https://10.195.227.92/api/v1/aca-controller-service/platform/details]
203:  Encountered unhandled HTTPError in Internal API Call
204:  Flagging result as FAIL!
205:  Reason: 404 Client Error: Not Found for url: [https://10.195.227.92/api/v1/aca-controller-service/platform/details]
 ",2022-08-11T19:59:17.352+0000,"Did the cluster have 'Access Control Application' package installed? Seem like the package was missing. This is happening due to the parallel execution(packages installation is in progress), can we do error handling if the package installation is in-progress, we can skip this check ? We shouldnot do the package installation in parallel with Cleanup. I found this code, fresh_install value is not set, if that's defined, we will not hit this issue



def cleanup_all_config(self, force=False, skip_aca=False, config_cleanup=False):
 '''
 skip_aca = True: uses in ACA regression to keep brownfield data
 force = True: will cleanup all regardless dnac role AUTHOR or READER
 '''
 result = PASS

 thread_list = []
 try:
 #self.services.delete_all_system_health_analzer_wf()
 t = MultiUserThreadWithReturn(target=self.services.delete_all_system_health_analzer_wf)
 t.start()
 thread_list.append(t)
 except:
 result = FAIL
 if self.services.fresh_install:
 self.log.warn(""Cluster is freshly installed no need of any cleanup"")
 self.services.fresh_install = False
 return True","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Nethra Thorali Suthagar,Resolved,Avril Bower
SEEN-436,https://miggbo.atlassian.net/browse/SEEN-436,[Auton] : Groot - Need to skip/remove test2_validate_AppEx_parameters tc in optimised sanity,"Groot RC4 #2.1.560.70508

Impacted TC : Optimized sanity #

 [Task-app_traffic_application_telemetry_dashboard.py-135-applicationTelemetryAppTraffic|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-app_traffic_application_telemetry_dashboard.py-135-applicationTelemetryAppTraffic&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug09_05:58:15.294052.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&atstype=PYATS]  /   [Test_TC6_stream_ftp_traffic|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-app_traffic_application_telemetry_dashboard.py-135-applicationTelemetryAppTraffic&begin=603504&size=19022&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug09_05:58:15.294052.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&atstype=PYATS]  /   test2_validate_AppEx_parameters

 

Description :

Due to memory limitation, this feature has been removed by the Dev Team (Attached mail conversation with DE)

So we need to skip / remove the test in Optimized Sanity until its been implemented again. 

 

Note : This has been removed from LAN-A Sanity script already - solution_test_sanityecamb_lan.py

 ",2022-08-11T21:45:46.733+0000,"PR raised for Groot , Ghost, Guardian

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3633/overview] Verified, tc is commented for now","['Auton', 'Groot', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-437,https://miggbo.atlassian.net/browse/SEEN-437,"[Auton]Wired application traffic is not supported for clients, check need to be removed","Defect ID : [https://cdetsng.cisco.com/webui/#view=CSCwb44057]

 

We need to skip the application/appx verification for the clients 

1865: Result Generated is []
 1866: video-over-http Traffic Not Found!!! on Client Device TB6-wired-client1 00:50:56:BD:14:16
 1867: FTP Traffic Not Found!!! on Client TB6-wired-client1 00:50:56:BD:14:16
 1868: TB6-DM-eCA-BORDER
 1869: 
 log: 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-app_traffic_application_telemetry_dashboard.py-135-applicationTelemetryAppTraffic&begin=623330&size=192022&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug09_05:58:15.294052.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS]

 

 ",2022-08-11T22:28:29.608+0000,"here is the PR for Ghost:   

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4097/overview]

Groot:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4099/overview]

guardian:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4101/overview] Hitting this Issue again in Shockwave sanity branch:

Script Used:

solution_test_sanityecamb.py


Failed log:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1481192&size=101349&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F01%2F24%2F18%2F58%2Fenv_auto_job.2023Jan24_18:58:34.225400.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS Shockwave is too old, we are not fixing in shockwave. there won't regression requests on shockwave anymore. ","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Shockwave']",Pawan Singh,Resolved,Avril Bower
SEEN-438,https://miggbo.atlassian.net/browse/SEEN-438,Test Validate_sda_multicast_external_apis  always fails in full run,"[Validate_sda_multicast_external_apis|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-sda_multicast_external_api.py-144-SDAmulticastExternalAPIs&begin=95340&size=531586&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug09_22:32:57.908150.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&atstype=PYATS]   test always fails in a full run or first try, but passed in the re-run, Could 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-sda_multicast_external_api.py-144-SDAmulticastExternalAPIs&begin=96069&size=58362&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug09_22:32:57.908150.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS]

 ",2022-08-12T03:29:10.095+0000,"API response s taking more than 30 Secs. For All Extenal APIs the response should come within 30 sec. Raise a product defect. Similar product defect: CSCwd44285 SDA-API: hostonboarding/access-point, pending reponse need to optimized to <30 sec  V-Verified Product issue.","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Pawan Singh,Closed,Avril Bower
SEEN-440,https://miggbo.atlassian.net/browse/SEEN-440,ISE CA certificate upload to be skipped for 2.6 P11,"we are testing the 2.6 P11 ISE after a long time in Sanity, Is there current API will not work for 2.6 P11 ?

 

start Uploading CA PKI certificate to ISE!

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-rback_roles_users_configs.py-31-systemUserAndRoleback&begin=129350&size=163614&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug10_12:00:44.659475.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Moe: 

 

Hey Raju, We skip the CA certificates upload for ISE < 3.0 since no API found. I will reach out to ISE team check if there is an API we can use for older versions. I already pushed the changes to Groot to skip it for now: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ae1864c8d9c23b835640c92b066faa812892c0e6]

 

This ticket is track until we get the API and fix may need for all the branches we use this code",2022-08-13T19:21:33.024+0000,"Moe, Can you pls add this fix to Ghost branch as well: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ae1864c8d9c23b835640c92b066faa812892c0e6]

  [~63f50bfce8216251ae4d59d5] Please add this workaround for other scripts and Ghost branch and in Optimized code [~63f50bfce8216251ae4d59d5] did you get any response from ISE team to handle this ? Hey [~70121:27365d3e-893a-4523-9fca-b89a7e8d0d4c],

I am still in contact with them. I am planning to have a meeting with them and will see. No API supported at the moment for older versions.","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Shockwave']",Moe Saeed,Closed,Avril Bower
SEEN-444,https://miggbo.atlassian.net/browse/SEEN-444,[Auton]:Guardian MS2 TB2: Test_TC3_generate_dhcp_server_config_on_fusion/test3_ise_cleanup_guest  - ISE profile cleanup guest failed,"*Uber ISO Version tested :* Groot RC4 Uber ISO - *2.1.560.70508, FIPS,  CSRF flag marked True*

*ISE Version :* 3.2

*Script Name :* solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB2

*Branch :*  private/Groot-ms/api-auto


 

*Failed Log :* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4024856&size=325104&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fauto_MS_job.2022Aug16_00:59:01.623327.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Description :* Ise cleanup guest fails with Json Decoder issue.
28194:  json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
Assuming it has something to do with 3.2 ISE version. API calls or json output might have changed.
Could you please look into the issue and help me on how to handle this?",2022-08-16T11:21:07.478+0000,"[~63f50bfde8216251ae4d59d8],

To fix this issue, I need ISE 3.2. Please me know once you have it.

Thank you! Hi Moe,
Sure, As of now we are using 3.1 P4 for regressions. I will let you know once we switch back to 3.2 Merged to Guardian, Groot, Ghost

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3735/diff#services/iseserv/ise_admin_api.py Issue was fixed and verified in Groot Pre RC 2.1.563.70111
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4595065&size=732114&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_MS_job.2022Oct10_05:03:11.058228.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 ","['Auton', 'Fips', 'Groot', 'Issue', 'MSTB2', 'Multisite']",Moe Saeed,Closed,Avril Bower
SEEN-445,https://miggbo.atlassian.net/browse/SEEN-445,Edge device is not added in cleanup list for fabric devices,"While running the test on Groot it failed to delete the control plane from the SJ site, in SJ site we have the EDGE device which is connected to the FIAB, Edge device must be deleted before attempting to delete the FIAB device.

 

Another enhancement: This cleanup function doesn't print which devices are being deleted remove_devices_from_fabric.

 


Branch: private/Groot-ms/sanity_api_auto

[Log: https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5436289&size=380208&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug16_14:34:47.189082.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5436289&size=380208&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug16_14:34:47.189082.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

 ",2022-08-16T23:37:55.237+0000,"Please retry after adding the entry of EDGE device in respective testbed's .json file under ""*devicelist: []*"".

At the moment, it's not there so it got skipped. marking this ticket as ""Done"" as nothing there to work on.","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'shockwave']",Amardeep Kumar,Closed,Avril Bower
SEEN-446,https://miggbo.atlassian.net/browse/SEEN-446,LAN Automation is not able to associate right IP Pool for third Site,"While executing the LAN Automation on Groot using TB9 set-up, it failed to associate third site to right IP Pool.

Branch: private/Groot-ms/sanity_api_auto

*[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug15_23:05:02.030782.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS*]

 [Test_TC28_DNAC_Device_Provisioning|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=696235&size=508000&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug15_23:05:02.030782.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_adding_devices_to_the_site  (Failed)

 LAN Automation Workflow is currently supporting for two sites - SJ and NY.

We need to add additional workflow for new site(s).

 ",2022-08-17T01:00:27.851+0000,"PR raised: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3723/overview]

Pass log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=319112&size=580171&archive=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats%2Fusers%2Famardkum%2Farchive%2F22-09%2Fsanity_TB9.2022Sep23_17:40:32.120643.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats&submitter=amardkum&from=trade&view=all&atstype=pyATS] Closing this Jira ticket as the required improvement is getting covered along with more coverage via [SEEN-546-https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-546], [SEEN-547|https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-547] & [SEEN-548|https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-548]. ","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'shockwave']",Amardeep Kumar,Closed,Avril Bower
SEEN-448,https://miggbo.atlassian.net/browse/SEEN-448,"[Auton]:Guardian,Groot:  Test_TC50_verify_assurance_health_nw_health_border_edge_wlc_ext_node  /   test6_verify_assurance_health_nw_health_border_node","<This cluster is upgraded from Guardian P1 RC4 (2334) <> Groot RC4>

Script Name :  after_upgrade_verify.py

*ISO:* 2.1.560.70508

*Polarisis:* 17.8.1a

*Description:*

 VN Services after upgrade from Older releases like Shockwave P3 RC3 and Guardian P.1 to Guardian / Groot is going down state  due to which device health is going to 1.

 

*Related Bug ID:* 

[https://cdetsng.cisco.com/webui/#view=CSCwb83573] 

As per new Assurance feature added from Groot- VN services(Default Route Registration)/KPI. we are fixing our topology to re-distribute bgp routes using default route/VN.

 

*Impacted Testcases:*

Test_TC24_verify_assurance_health_nw_health_border_edge_wlc_ext_node / test6_verify_assurance_health_nw_health_border_node

Test_TC50_verify_assurance_health_nw_health_border_edge_wlc_ext_node / test6_verify_assurance_health_nw_health_border_node

 

*Failure Log*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=32317551&size=100859&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F11%2F10%2F18%2Fenv_auto_job.2022Aug11_10:18:22.134078.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14630089&size=100857&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F11%2F10%2F18%2Fenv_auto_job.2022Aug11_10:18:22.134078.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Snip From Failed Log:*
 72755: Health Score is below expected. using metric management ip:204.1.1.65, score received:\{'modificationtime': '1660249800000', 'overallScore': '1', 'time': '2022-08-11T20:30:00.000+0000'}, score expected:7.0
 72756: Health Score is below expected. using metric management ip:204.1.1.65, score received:\{'modificationtime': '1660249500000', 'overallScore': '1', 'time': '2022-08-11T20:25:00.000+0000'}, score expected:7.0
  ",2022-08-17T06:22:49.296+0000,"It should be defect. Pull Pawan fix from Groot to Shockwave.

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/647590fba6e11c98bb9a1c874ef307048bd49c43 Passed in latest Groot RC5:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=33962786&size=411688&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F19%2F13%2F14%2Fenv_auto_job.2022Aug19_13:14:55.292366.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Groot', 'Issue']",Vijayakumar Govindaraju,Closed,Avril Bower
SEEN-449,https://miggbo.atlassian.net/browse/SEEN-449,[Auton]:Shockwave:TC101_DNAC_Policy_Extended_node  /   test1_onboard_policy_extended_node_interface,"**ISO*:*2.1.390.72158

**Polarisis*:*17.6.4prd7
 **Script*:*solution_test_sanityecamb.py

**NOTE:** Script should clear the existing interface onboarding before onboarding it again

*Testcases Impacted:*
 TC101_DNAC_Policy_Extended_node /test1_onboard_policy_extended_node_interface

*Failure Log:*
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=53451556&size=218208&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F14%2F23%2F56%2Fenv_auto_job.2022Aug14_23:56:05.552097.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Snip from Failure Log:*
 166633: The Schedduled Job failed: with reason \{'id': '56e33cf9-23af-4635-9d47-065598366edf', 'triggeredJobTaskId': '225d956a-3e88-4276-90a6-b41b2ad49f1e', 'triggeredTime': 1660579654567, 'status': 'FAILED', 'failureReason': 'NCSP10250: Error During persistence (modify) of CFS & SerializedSnapshot (name: SN-FCW2307G03S type: DeviceInfo qualifier: null)', 'triggeredJobId': '56e33cf9-23af-4635-9d47-065598366edf'}
 166638: Failed reason: all the static IXIA interfaces are provisioned by dnac",2022-08-18T06:29:01.295+0000,"Note: Committed a PR, but needs more testing with shockwave ","['Auton', 'Issue', 'Shockwave']",Moe Saeed,Resolved,Avril Bower
SEEN-450,https://miggbo.atlassian.net/browse/SEEN-450,[Auton] - Groot- Test_TC45_DNAC_TSIM_static_onboarding_verifications/ test8_wait_ap_to_be_provisioned,"*ISO:*Groot RC5 2.1.560.70513  (Non-Fips)

*Script Name:*  solution_test_sanityecamb_lan.py

*Testcases Impacted :* [Test_TC45_DNAC_TSIM_static_onboarding_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=908687&size=5010373&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug17_18:36:56.298857.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] /[test8_wait_ap_to_be_provisioned
  
 *Note**:* All APs Have Provisioned successfully, but still TC is failed, 
 Please find attached the snapshot ** 
 +*Failure Log:*+
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5511658&size=65162&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug17_18:36:56.298857.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS



*Snip from Fail Log:*

**
 15515: Calculating If the required result found in the Response!!
 15516: Result Generated is ['TB6-SJ-EDGE', 'TB6-DM-Transit', 'TB6-DM-NF-Switch', 'SN-FDO2502J6N9', 'SN-FDO2453J12H', 'SN-FOC2336Y17B', 'SN-FDO2505J8SL', 'AP70F3.5A7A.02B8', 'SN-JAE24040C8K']
 15517: Few of the APs Haven't Provisioned Succsesfully : \{'SN-FOC2336Y17B', 'SN-FDO2453J12H', 'SN-FDO2502J6N9', 'TB6-DM-Transit', 'TB6-SJ-EDGE', 'SN-JAE24040C8K', 'SN-FDO2505J8SL', 'TB6-DM-NF-Switch', 'AP70F3.5A7A.02B8'}
 15518: 
 15519: 
 15520: api_switch_call called:
 15521: \{'params': {'state': 'Error', 'sort': 'onbState', 'sortOrder': 'asc', 'offset': 0, 'limit': 25}}
 15522: Resource path full url: [https://10.195.227.92/api/v1/onboarding/pnp-device]
 15523: Error Response []
 15524: Test returned in 0:01:59.698192
 15525: Failed reason: Claiming of AP failed!!
 15526: The result of section test8_wait_ap_to_be_provisioned is => FAILED
  ",2022-08-18T07:34:44.303+0000,Pawan: Fixed it: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0bd3d38aba240d8f83e093dee4f9f0fe9c1d9fb4 Fix:  https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0bd3d38aba240d8f83e093dee4f9f0fe9c1d9fb4,"['Auton', 'Ghost', 'Groot', 'Issue', 'Sanity']",Pawan Singh,Closed,Avril Bower
SEEN-451,https://miggbo.atlassian.net/browse/SEEN-451,[Auton]:Groot MS2 TB2:  Test_TC50_DNAC_verify_SSID_lan_on_ECA_device/test1_DNAC_verify_SSID_lan_on_ECA_device,"*Uber ISO Version tested :* Groot RC4 Uber ISO - *2.1.560.70508, FIPS*

*ISE Version :* 3.2

*Script Name :* solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB2

*Branch :*  private/Groot-ms/api-auto

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=30665756&size=154554&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fauto_MS_job.2022Aug17_23:25:27.889455.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Description :* CiscoSensorProvisioning SSID is supported only for Aireos And 9800 EWLC. We see its being verified on ECA devices and hence getting failed. ECA devices needs to be skipped for this SSID validation.


 51494: NYC-FE-9400
 51495: 
 51496: 
 51497: api_switch_call called:
 51498: {}
 51499: Resource path full url: [https://172.23.241.114/api/v1/file/4fe61a17-aecc-48f1-bcde-6d148333e663]
 51500: [{'deviceUuid': 'ffed4c73-8943-4092-a31a-a553b1038ac2', 'commandResponses': {'SUCCESS':

{'show wlan summary ': 'show wlan summary\n\nNumber of WLANs: 14\n\nID Profile Name SSID Status Security \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n17 posture_profile postureMS2 UP [WPA2][802.1x][AES] \n18 OPEN_profile OPENMS2 UP [open] \n19 Radius_ssid_profile Radius_ssidMS2 UP [WPA2][802.1x][AES] \n20 Random_mac_profile Random_macMS2 UP [WPA2][802.1x][AES] \n21 GUEST_profile GUESTMS2 UP [open],MAC Filtering \n22 Guest_webpassthrough_profile Guest_webpassthroughMS2 UP [open],MAC Filtering,[Web Auth] \n23 SSIDDUAL BAND_profile SSIDDUAL BANDMS2 UP [WPA2][802.1x][FT + 802.1x][AES],[FT Enabled] \n24 Guest_webauthinternal_profile Guest_webauthinternalMS2 UP [WPA2][802.1x][AES],MAC Filtering,[Web Auth] \n25 Single5KBand_New_PSK Single5KBandMS2 UP [WPA2][PSK][FT + PSK][AES],[FT Enabled],MAC Filtering \n26 CWA_GUEST_SSID_AAA_profile CWA_GUEST_SSID_AAAMS2 UP [WPA2][802.1x][AES] \n\n\nNumber of WLANs: 14\n\nID Profile Name SSID Status Security \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n27 Guest_passthrough_int_profile Guest_passthrough_intMS2 UP [open],MAC Filtering,[Web Auth] \n28 ARUBA_SSID_profile ARUBA_SSIDMS2 UP [open],[Web Auth] \n29 GUEST2_profile GUEST2MS2 UP [open],MAC Filtering \n30 SSIDDot1XIndia_profile SSIDDot1XIndiaMS2 UP [WPA2][802.1x][AES] \n\nNYC-FE-9400#'}

, 'FAILURE': {}, 'BLACKLISTED': {}}}]
 51501: Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:02.190801
 51502: 
 51503: ERROR Following line of expected cli output not present on device:
 51504: \d+\s+.*CiscoSensorProvisioning\s+UP\s+",2022-08-18T14:06:42.287+0000,"One more Skip condition for ECA and CiscoSensorProvisioning is required.

Raised PR with required change: 
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4105/overview [~63f50bfde8216251ae4d59d8], PR got approved and merged into Groot and Ghost branches. Please validate the changes and confirm on the closure. Hi [~62ab7a399cd13c0068b18fe0] 

We see that CiscoSensorProvisioning SSID is skipped for ECA devices and script is getting passed now.
We have verified it on Groot P1 RC2 - 2.1.563.70154. 
Pass Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=42464914&size=96902&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fauto_MS_job.2022Nov11_03:27:00.158136.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Regards,
Sathwick. Moving the close state, Script is passing now after adding check for Ignoring CiscoSensorProvisioning for ECA.","['Auton', 'Groot', 'Guardian', 'Issue', 'MSTB1', 'MSTB2', 'Multisite']",Amardeep Kumar,Closed,Avril Bower
SEEN-452,https://miggbo.atlassian.net/browse/SEEN-452,[Auton] Groot : Test_TC51_DNAC_provision_all_aps/test2_enable_kairos_cs_details,"*Uber ISO Version tested :* Groot RC4 Uber ISO - *2.1.560.70508, FIPS*

*Script Name :* solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB2

*Branch :*  private/Groot-ms/api-auto

 

*Failed Log :* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=31721603&size=96923&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fauto_MS_job.2022Aug17_23:25:27.889455.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Description :* We are seeing 403 error code while accessing AI Analytics page. Seems it is not supported on FIPS. Attaching the GUI page screenshot

 

*!image-2022-08-18-19-54-20-615.png|width=386,height=251!*

 

In case of fips enabled DNAC cluster, the script has to be handled accordingly.

 ",2022-08-18T14:27:25.607+0000,[86e0a35f217|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/86e0a35f217712555601d8b6ecf6a8b26312c4f0],"['Auton', 'Groot', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-453,https://miggbo.atlassian.net/browse/SEEN-453,[Auton] : Shockwave : Unsupported dot1x CLI Executed on Extended Node,"Hi Tran,

   In our upgrade sanity testing we observed one of the dot1x CLI 'show authentication sessions method dot1x' not working on the extended node. Seems this CLI is not supported on the extended node. To check the dot1x sessions on the extended node we tried the CLI 'show access-session method dot1x' which is working. Kindly check this.

DNAC ISO Used : Shockwave P3 #2.1.390.72158

Release Branch : private/Shockwave-ms/sanity_api_auto

Extended Node(FOC2311T18E) Device Image Used : 15.2(7)E0s

Script Used : solution_test_sanityecamb.py

Impacted TC : [Test_TC66_DNAC_dot1x_onboarding_ixia_scale|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=337440&size=1940160&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F18%2F02%2F49%2Fenv_auto_job.2022Aug18_02:49:21.769830.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS] / [test3_subtest2_dot1x_auth_ixia_ipv4|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2127904&size=97738&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F18%2F02%2F49%2Fenv_auto_job.2022Aug18_02:49:21.769830.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2127904&size=97738&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F18%2F02%2F49%2Fenv_auto_job.2022Aug18_02:49:21.769830.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 *Issue Type: Fix* 

Error Snip :

 

!image-2022-08-18-22-56-58-716.png!",2022-08-18T17:28:28.553+0000,"@[~63f53512263233e653a96a29]  what is the device model? Also, do you know what command is supported on this device? Hi Raji,

Device model is  WS-C3560CX-12PC-S 

Below command works, 

SN-FOC2311T18E#show access-session method dot1x 
No sessions match supplied criteria. [d6957d3546f|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d6957d3546fe7e2d8e63d2062cb096530c43442c] Hi Raji,

    Its failing in Ghost branch also.

Branch Used : private/Ghost-ms/sanity_api_auto

Main Branch : private/Ghost-ms/api-auto

 

Please commit these changes to following branches as well,

private/Guardian-ms/api-auto

private/Groot-ms/api-auto

private/Ghost-ms/api-auto

private/Hellack-ms/api-auto Passed in Shockwave P3 #2.1.390.72158.

 

Pass Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=353049&size=1378383&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F08%2F04%2F00%2Fenv_auto_job.2022Nov08_04:00:07.946595.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]   In our upgrade sanity testing we observed one of the dot1x CLI 'show authentication sessions method dot1x' not working on the extended node. Seems this CLI is not supported on the extended node. To check the dot1x sessions on the extended node we tried the CLI 'show access-session method dot1x' which is working. Kindly check this.

DNAC ISO Used : Guardian 2.1.515.70134

Release Branch : private/Guardian-ms/sanity_api_auto

Extended Node(FOC2311Y129) Device Image Used : 15.2(7)E6

Script Used : solution_test_sanityecamb_lan

Impacted TC :   [Test_TC66_DNAC_dot1x_onboarding_ixia_scale|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=41246473&size=1924148&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov30_17:56:21.540596.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] /   test3_subtest2_dot1x_auth_ixia_ipv4

 Failed Log : 

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=42886896&size=205453&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov30_17:56:21.540596.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

 *Issue Type: Fix* 

Error Snip :","['Auton', 'Guardian', 'Issue', 'Shockwave']",Raji Mukkamala,Closed,Avril Bower
SEEN-454,https://miggbo.atlassian.net/browse/SEEN-454,[Auton] : Shockwave : Test_TC139_Enhance_RCA_AAA_Issue  /   test2_Enhance_RCA_AAA_Issue,"*ISO*: 2.1.390.72158

*Script*: solution_test_sanityecamb.py
*Impacted Testcases:* [Test_TC139_Enhance_RCA_AAA_Issue|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=74683944&size=55181&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug14_23:56:05.552097.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&atstype=PYATS]  /   test2_Enhance_RCA_AAA_Issue

*Failure log*:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=74701570&size=19913&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F14%2F23%2F56%2Fenv_auto_job.2022Aug14_23:56:05.552097.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Description :*

**Tc runs only for ~2 seconds and fails with keyerror,
211735:  api_switch_call called:
211736:  \{'method': 'GET', 'resource_path': '/assurance/v1/time'}
211738:  Current DNAC time is as follows \{'version': '1.0', 'response': [{'timeType': 'GLOBAL', 'time': 1660589880000}, \{'timeType': 'CLIENT', 'time': 1660590360000}, \{'timeType': 'NETWORK', 'time': 1660589880000}, \{'timeType': 'CURRENT', 'time': 1660590561924}, \{'timeType': 'POE', 'time': 1660589940000}]}
211739:  Time in the Unix Time format 1660590561924
211740:  entityId:00:E0:4C:29:C5:58
211743:  \{'version': '1.0'}
211744:  Traceback (most recent call last):
211745:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Shockwave-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 195, in wrapper
211746:  result = testfunc(func_self, **kwargs)
211747:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Shockwave-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 8334, in test2_Enhance_RCA_AAA_Issue
211748:  if (dnac_handle.Enhance_RCA_AAA_Issue()):
211749:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Shockwave-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 4184, in Enhance_RCA_AAA_Issue
211750:  for i in response['response']:
211751:  KeyError: 'response'",2022-08-18T20:54:01.344+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/5e0981fc5a40d9bada612b507ab84873bd5dd83a,"['Auton', 'Ghost', 'Guardian', 'Issue', 'Shockwave']",Raji Mukkamala,Closed,Avril Bower
SEEN-456,https://miggbo.atlassian.net/browse/SEEN-456,"[Auton][IBSTE] :   Test_TC40_ipphone_onboarding_verifications, test1_edge_node_static_onboarding_ipphone_clear_interface_using_schedular","Testcase affected:

[Test_TC40_ipphone_onboarding_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=44956477&size=4517633&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug16_04:18:09.048558.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 

test1_edge_node_static_onboarding_ipphone_clear_interface_using_schedular 

 

Branch : private/Groot-ms/api-auto

Failed Log : 
 Scheduling for ""Interface onboarding on Device: CUB-FE-9300-2"" started successfuly
148981:  Library group ""schedule-job"" method ""schedule_a_job_externalSchedule"" returned in 0:00:00.642329
148982:  Traceback (most recent call last):
148983:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/api_groups/wired_interface_onboarding/group.py"", line 471, in onboard_device_clear_interface
148984:  result1 &= self.services.check_status_of_externalScheduled_jobs()
148985:  UnboundLocalError: local variable 'result1' referenced before assignment
148986:  Deploying interface in onboarding in ERROR !!!
 
Log : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=46487276&size=1433171&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug16_04:18:09.048558.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS",2022-08-19T05:03:57.736+0000,"Hi,

 

We have passlog for the testcase on Groot#2.1.560.70345

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=26399888&size=1702162&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F22-06%2Fsr_ibste.2022Jun17_10:23:56.142374.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS On latest optimized script, issue is not seen anymore Passlog : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-92-SDAExtnodeOnboarding&begin=12826869&size=146257&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Foptimized_ibste_job.2022Aug29_07:29:40.889490.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS Issue would be hit when ""schedule"" parameter is set to ""True"".

PR to avoid this Exception:
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3578/overview With the PR merged into Groot branch, marking this issue as ""Closed"" ","['Auton', 'Groot', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-457,https://miggbo.atlassian.net/browse/SEEN-457,[Auton][IBSTE] : Test_TC42_ext_node_ipphone_onboarding_verifications test1_extnode_ipphone_onboarding_verifications,"Testcase affected:

 [Test_TC42_ext_node_ipphone_onboarding_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4171017&size=287919&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug16_10:13:04.190689.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 

test1_extnode_ipphone_onboarding_verifications

 

Branch : private/Groot-ms/api-auto

Failed Log : 
18985:  Traceback (most recent call last):
18986:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/commonlibs/test_wrapper.py"", line 301, in wrapper
18987:  result = testfunc(func_self, **kwargs)
18988:  File ""/auto/dna-sol/ws/sr-ibste/groot/testcases/ibste/ibste_script.py"", line 1294, in test1_extnode_ipphone_onboarding_verifications
18989:  if ibste_handle.AEN_pnp_onboarding_preparation():
18990:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/ibstelibs/decorators.py"", line 32, in wrapper
18991:  result = method(*args, **kwargs)
18992:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/ibstelibs/feature_groups/pnp/group.py"", line 50, in AEN_pnp_onboarding_preparation
18993:  if handle.AEN_Enabling_on_virtual_segment_mapping():
18994:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/decorators.py"", line 32, in wrapper
18995:  result = method(*args, **kwargs)
18996:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/api_groups/onboarding/group.py"", line 210, in AEN_Enabling_on_virtual_segment_mapping
18997:  if site[""location""] == self.services.dnaconfig.testbed.devices[dev['name']].site:
18998:  KeyError: 'location'
18999:  Test returned in 0:00:25.997154
19000:  Errored reason: location
 
Failed Log : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4171587&size=117661&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug16_10:13:04.190689.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS",2022-08-19T05:26:57.831+0000,"Issue is not seen on IbsteOptimized script execution

 

Passlog : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-92-SDAExtnodeOnboarding&begin=12827565&size=82115&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Foptimized_ibste_job.2022Aug29_07:29:40.889490.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Groot', 'Issue']",Divakar Kumar Yadav,Closed,Avril Bower
SEEN-458,https://miggbo.atlassian.net/browse/SEEN-458,[Auton][IBSTE] :  Test_TC54_TSIM_verify_tsim_clients_on_wlc  test1_verify_tsim_clients,"Testcase affected: 

 

[Test_TC54_TSIM_verify_tsim_clients_on_wlc|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=49907251&size=122006&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug16_10:13:04.190689.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 

test1_verify_tsim_clients 

Branch : private/Groot-ms/api-auto

Failed Log : 
73773:  Traceback (most recent call last):
73774:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/commonlibs/test_wrapper.py"", line 301, in wrapper
73775:  result = testfunc(func_self, **kwargs)
73776:  File ""/auto/dna-sol/ws/sr-ibste/groot/testcases/ibste/ibste_script.py"", line 1528, in test1_verify_tsim_clients
73777:  if(ibste_handle.verify_clients_joined_on_wlc_eca()):
73778:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/ibstelibs/decorators.py"", line 32, in wrapper
73779:  result = method(*args, **kwargs)
73780:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/ibstelibs/feature_groups/wireless/group.py"", line 129, in verify_clients_joined_on_wlc_eca
73781:  tsimclients1=int(tsimclients/len(self.services.all_sites_dna_handles[site].dnaconfig.wlc))
73782:  ZeroDivisionError: division by zero
73783:  Test returned in 0:01:58.990049
73784:  Errored reason: division by zero
 
Failed log : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=49907821&size=121264&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug16_10:13:04.190689.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS",2022-08-19T06:22:29.015+0000,"Issue is seen with latest Guardian Run#2.1.517.70033

 

Failed Log : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=49641582&size=133505&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F22-11%2Fsr_ibste.2022Nov21_04:41:31.252087.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS  Raised PR with the logic to skip the iteration for SPAIN site explicitly and failing the use-case if any other Site that do not have at least one device as ECA or WLC or eWLC:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4278/overview] Issue is fixed. Fix works fine

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-101-FEWAccessPointAndCLients&begin=1886283&size=64211&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Foptimized_ibste_job.2023Jan05_07:25:35.414185.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Ghost', 'Groot', 'Guardian', 'IBSTE', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-459,https://miggbo.atlassian.net/browse/SEEN-459,[Auton][IBSTE] :  Test_TC56_verify_assurance_health_nw_health_border_edge_wlc_ext_node  test1_verify_assurance_health_nw_health_update_inv_data_all_dev,"Testcase affected: 

 

 [Test_TC56_verify_assurance_health_nw_health_border_edge_wlc_ext_node|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4669684&size=3042017&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug16_22:06:58.869358.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 

 test1_verify_assurance_health_nw_health_update_inv_data_all_dev

Branch : private/Groot-ms/api-auto

Failed Log : 
 24366: Traceback (most recent call last):
 24367: File ""/auto/dna-sol/ws/sr-ibste/groot/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 24368: result = testfunc(func_self, **kwargs)
 24369: File ""/auto/dna-sol/ws/sr-ibste/groot/testcases/ibste/ibste_script.py"", line 1566, in test1_verify_assurance_health_nw_health_update_inv_data_all_dev
 24370: if not ibste_handle.update_device_data_from_inventory() :
 24371: File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/ibstelibs/decorators.py"", line 32, in wrapper
 24372: result = method(*args, **kwargs)
 24373: File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/ibstelibs/feature_groups/inventory/group.py"", line 20, in update_device_data_from_inventory
 24374: result &= self.services.all_sites_dna_handles[site].update_device_data_from_inventory(self.services.all_sites_dna_handles[site].dnaconfig.ext_nodes)
 24375: File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 24376: result = method(*args, **kwargs)
 24377: File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/api_groups/cli_check/group.py"", line 608, in update_device_data_from_inventory
 24378: result &= self.update_device_data_from_inventory(dev)
 24379: File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 24380: result = method(*args, **kwargs)
 24381: File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/api_groups/cli_check/group.py"", line 622, in update_device_data_from_inventory
 24382: params={""managementIpAddress"":self.services.dnaconfig.testbed.devices[name].lb_ip}
 24383: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/topology/device.py"", line 559, in __getattr__
 24384: raise AttributeError(""'Device' object has no attribute '%s'""
 24385: AttributeError: 'Device' object has no attribute 'lb_ip'
 24386: Test returned in 0:00:02.002718
 24387: Errored reason: 'Device' object has no attribute 'lb_ip'
  
 Failed log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5048631&size=51704&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug16_22:06:58.869358.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Testcases seeing similar issue : 

 [Test_TC67_verify_assurance_health_restored_after_restoring_connection|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9078702&size=1968890&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_assurance_health_nw_health_update_inv_data_ext_nodes

[Test_TC67_verify_assurance_health_restored_after_restoring_connection|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9078702&size=1968890&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_assurance_health_nw_health_update_inv_data_all_dev

[Test_TC72_verify_nw_assurance_after_dnac_reload|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=22664164&size=1873394&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_assurance_health_nw_health_update_inv_data_ext_nodes 

[Test_TC72_verify_nw_assurance_after_dnac_reload|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=22664164&size=1873394&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_assurance_health_nw_health_update_inv_data_all_dev

[Test_TC75_verify_nw_assurance_after_backupcreation|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24599213&size=2061386&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_assurance_health_nw_health_update_inv_data_ext_nodes

 [Test_TC75_verify_nw_assurance_after_backupcreation|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24599213&size=2061386&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_assurance_health_nw_health_update_inv_data_all_dev

[Test_TC78_verify_assurance_after_dnac_reload_backupcreation|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=36654274&size=1978363&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_assurance_health_nw_health_update_inv_data_ext_nodes

[Test_TC78_verify_assurance_after_dnac_reload_backupcreation|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=36654274&size=1978363&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_assurance_health_nw_health_update_inv_data_all_dev

 ",2022-08-19T06:45:31.184+0000,"[~63f50bf0e8216251ae4d59ca], based on discussion with [~5f3c6ae932360700388f7b4b] on this, TC2 is not required to execute the test-cases after TC20.

Please execute the failing TC individually and share the result. Issue is not seen with IBSTEOptimized script execution

 

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_360_nw_health.py-103-assuranceHealth360&begin=451665&size=63824&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Foptimized_ibste_job.2022Aug29_09:36:08.870061.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Groot', 'Issue']",Divakar Kumar Yadav,Closed,Avril Bower
SEEN-460,https://miggbo.atlassian.net/browse/SEEN-460,[Auton][IBSTE] :  Test_TC65_configure_policy_aca_migration_test test1_cleanup_profile,"Testcase affected:  

 [Test_TC65_configure_policy_aca_migration_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4285946&size=199892&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 

test1_cleanup_profile

Branch : private/Groot-ms/api-auto

Failed Log : 
Action: Unconfigure security groups for ise authoriation profiles
19339:  Traceback (most recent call last):
19340:  File ""/auto/dna-sol/ws/sr-ibste/groot/testcases/ibste/ibste_script.py"", line 1935, in test1_cleanup_profile
19341:  sg_profile_map = dnac_handle.dnaconfig.testbed.custom[dnac_handle.dnaconfig.sid]['ISE_PROFILE_LIST']
19342:  KeyError: 'ISE_PROFILE_LIST'
 
Log : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4286516&size=2276&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS",2022-08-19T07:01:41.665+0000,"After making latest changes as per below wiki, issue is not seen anymore

 

https://wiki.cisco.com/display/EDPEIXOT/ACA+integration+guideline+v2","['Auton', 'Groot', 'Issue']",Divakar Kumar Yadav,Closed,Avril Bower
SEEN-461,https://miggbo.atlassian.net/browse/SEEN-461,[Auton][IBSTE] :  Test_TC65_configure_policy_aca_migration_test test1_aca_create_sg,"Testcase affected:  

[Test_TC65_configure_policy_aca_migration_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4285946&size=199892&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 

test1_aca_create_sg

Branch : private/Groot-ms/api-auto

Failed Log : 
19845:  During handling of the above exception, another exception occurred:
19846: 
19847:  Traceback (most recent call last):
19848:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/commonlibs/test_wrapper.py"", line 301, in wrapper
19849:  result = testfunc(func_self, **kwargs)
19850:  File ""/auto/dna-sol/ws/sr-ibste/groot/testcases/ibste/ibste_script.py"", line 2162, in test1_aca_create_sg
19851:  dnac_handle.dnaconfig.testbed.custom[dnac_handle.dnaconfig.sid]['SG_LIST'], e))
19852:  KeyError: 'SG_LIST'
19853:  Test returned in 0:00:00.006881
19854:  Errored reason: SG_LIST
 
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4465168&size=6734&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS
 ",2022-08-19T07:13:30.917+0000,"After following below wiki, and updating fabric file issue is not seen anymore

 

https://wiki.cisco.com/display/EDPEIXOT/ACA+integration+guideline+v2","['Auton', 'Groot', 'IBSTE', 'Issue', 'Sanity']",Divakar Kumar Yadav,Closed,Avril Bower
SEEN-462,https://miggbo.atlassian.net/browse/SEEN-462,[Auton][IBSTE] :  Test_TC65_configure_policy_aca_migration_test test2_aca_create_contract,"Testcase affected:  

[ |https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4285946&size=199892&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] [Test_TC65_configure_policy_aca_migration_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4285946&size=199892&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] [ test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4285946&size=199892&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

[2_aca_create_contract|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4285946&size=199892&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

Branch : private/Groot-ms/api-auto

Failed Log : 
File ""/auto/dna-sol/ws/sr-ibste/groot/testcases/ibste/ibste_script.py"", line 2172, in test2_aca_create_contract
19879:  dnac_handle.dnaconfig.testbed.custom[dnac_handle.dnaconfig.sid]['CONTRACT_LIST']]
19880:  KeyError: 'CONTRACT_LIST'
19881:  Traceback (most recent call last):
19882:  File ""/auto/dna-sol/ws/sr-ibste/groot/testcases/ibste/ibste_script.py"", line 2172, in test2_aca_create_contract
19883:  dnac_handle.dnaconfig.testbed.custom[dnac_handle.dnaconfig.sid]['CONTRACT_LIST']]
19884:  KeyError: 'CONTRACT_LIST'
19885: 
19886:  During handling of the above exception, another exception occurred:
 
Log : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4471902&size=6954&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS",2022-08-19T07:17:06.997+0000,"After adding V2 changes issue got resolved

 

https://wiki.cisco.com/display/EDPEIXOT/ACA+integration+guideline+v2","['Auton', 'Groot', 'Issue']",Divakar Kumar Yadav,Closed,Avril Bower
SEEN-463,https://miggbo.atlassian.net/browse/SEEN-463,[Auton][IBSTE] :  Test_TC65_configure_policy_aca_migration_test test3_aca_create_policy,"Testcase affected:  

[Test_TC65_configure_policy_aca_migration_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4285946&size=199892&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 

test3_aca_create_policy

Branch : private/Groot-ms/api-auto

Failed Log : 
During handling of the above exception, another exception occurred:
19928: 
19929:  Traceback (most recent call last):
19930:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/commonlibs/test_wrapper.py"", line 301, in wrapper
19931:  result = testfunc(func_self, **kwargs)
19932:  File ""/auto/dna-sol/ws/sr-ibste/groot/testcases/ibste/ibste_script.py"", line 2231, in test3_aca_create_policy
19933:  dnac_handle.dnaconfig.testbed.custom[dnac_handle.dnaconfig.sid]['POLICY_LIST'], e))
19934:  KeyError: 'POLICY_LIST'
19935:  Test returned in 0:00:00.002487
19936:  Errored reason: POLICY_LIST
 
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4478856&size=6806&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS",2022-08-19T08:34:09.277+0000,"After adding V2 changes in Fabric Json issue got resolved

 

[https://wiki.cisco.com/display/EDPEIXOT/ACA+integration+guideline+v2]","['Auton', 'Groot', 'Issue']",Divakar Kumar Yadav,Closed,Avril Bower
SEEN-464,https://miggbo.atlassian.net/browse/SEEN-464,[Auton][IBSTE] : Test_TC84_DNAC_static_onboarding_ixia_scale  test1_onboard_all_ixia,"Testcase affected:  

[Test_TC84_DNAC_static_onboarding_ixia_scale|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=22542583&size=2820277&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_19:21:46.735349.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

[test1_onboard_all_ixia|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4285946&size=199892&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_07:12:59.642854.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

Branch : private/Groot-ms/api-auto

Failed Log : 
Config Preview Activity failed with reason: Vlan/IP Pool must be specified for Port Assignment of Open Authentication or Low Impact on port TwoGigabitEthernet1/0/7 of device CUB-FE-9300-1.cisco.local. Please select Vlan/IP Pool and try again.
105011:  Config preview task for provision failed for device:CUB-FE-9300-1
105012:  Not Valid Activity-ID
105014:  Failed to receive config preview
105030:  onboard_device_on_interface result is failed for site:Cuba
 
Log : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24494249&size=830356&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_19:21:46.735349.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS",2022-08-19T08:54:13.169+0000,"Hi Team,

 

Can we please get any closure ETA for the ticket?

 

Thanks,

Divakar [~63f50bf0e8216251ae4d59ca], 
do you still see this issue?

 Hi [~63f50bfce8216251ae4d59d5] 

Still issue is seen

 

Latest fail log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=25356140&size=850446&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F23-02%2Fsr_ibste.2023Feb06_22:41:48.084558.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Thanks,

Divakar This Tc failed since it depends on TC82 to onboard segments. But since TC82 failed, this is expected to fail. I am closing this ticket for now. Please check the segment onboarding use case first.","['Auton', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-465,https://miggbo.atlassian.net/browse/SEEN-465,[Auton][IBSTE] :  Test_TC5_verify_neighbor_topology  /   test1_verify_neighbor_topology,"Testcase affected:  

[Test_TC5_verify_neighbor_topology|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6632167&size=3051939&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug18_02:37:36.638283.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] [ |https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=22542583&size=2820277&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_19:21:46.735349.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

[test1_verify_neighbor_topology
 
Branch : private/Groot-ms/api-auto

Script : solution_assurance_test

Failed Log : 
 Traceback (most recent call last):
 27910: File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/api_groups/assurance/group.py"", line 1804, in verify_neighbor_topology
 27911: if fab_devices in fabric_devices:
 27912: TypeError: unhashable type: 'dict'
  
 Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6632737&size=3051206&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug18_02:37:36.638283.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2022-08-19T09:14:08.582+0000,"* PR link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5185/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5185/overview]
* Test Cases Summary: add exception to check device is none or not
* Trade log link: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-03/sr_ibste.2023Mar31_00:30:23.750475.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-03/sr_ibste.2023Mar31_00:30:23.750475.zip&atstype=ATS]","['Auton', 'Groot', 'Issue', 'assurance']",QuangVinh Nguyen,Resolved,Avril Bower
SEEN-466,https://miggbo.atlassian.net/browse/SEEN-466,[Auton][IBSTE] : Test_TC27_verify_vlan_mapping_interfaces  /   test1_verify_vlan_mapping_interfaces,"Testcase affected:  

[Test_TC27_verify_vlan_mapping_interfaces|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16274697&size=1171998&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug18_02:37:36.638283.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 

test1_verify_vlan_mapping_interfaces

 
Branch : private/Groot-ms/api-auto

Script : solution_assurance_test

Failed Log : 
Traceback (most recent call last):
53068:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/api_groups/assurance/group.py"", line 5694, in verify_vlan_mapping_interfaces
53069:  vlan=self.map_interface_utilization(dev)
53070:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/api_groups/assurance/group.py"", line 5650, in map_interface_utilization
53071:  int=self.conversion_of_interface_abb(i[0]+i[1])
53072:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/api_groups/assurance/group.py"", line 1105, in conversion_of_interface_abb
53073:  interfaces=inttypes[(ints.group(1))]+ints.group(2)
53074:  KeyError: 'Fif'
53103:  Traceback (most recent call last):
53106:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/api_groups/assurance/group.py"", line 5694, in verify_vlan_mapping_interfaces
53107:  vlan=self.map_interface_utilization(dev)
53108:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/api_groups/assurance/group.py"", line 5650, in map_interface_utilization
53109:  int=self.conversion_of_interface_abb(i[0]+i[1])
53110:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/api_groups/assurance/group.py"", line 1105, in conversion_of_interface_abb
53111:  interfaces=inttypes[(ints.group(1))]+ints.group(2)
53112:  KeyError: 'Fou'
 
Log : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16275267&size=1171258&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug18_02:37:36.638283.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS",2022-08-19T09:27:43.920+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4191/overview]

Here is my PR Merged to Groot.

PLease PR to Ghost too. I created PR for Ghost:

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4416/overview","['Auton', 'Groot', 'IBSTE', 'Issue']",ThangQuoc Tran,Resolved,Avril Bower
SEEN-467,https://miggbo.atlassian.net/browse/SEEN-467,[Auton][IBSTE] : Traffic cases are failing due to unable to start traffic,"Most of traffic cases are failing due to below error

 

Error:
 Send traffic interfaces
28050:  Exception in starting traffic
28051:  None
28052:  Test returned in 0:15:45.774192
28053:  Failed reason: Result: Failed to start traffic
28054:  The result of section test1_static_ixia_noauth_traffic_convergence_test is => FAILED
 
Failed log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7649187&size=182771&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug17_03:29:31.728858.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
 
Below environment I'm using to run the script
source /auto/dna-sol/ws/rajsaran/sanity/pyats_internal/env.sh
export NO_IXIA_ENV=""True""
export PYTHONPATH=""${PYTHONPATH}:./""
export NOUNIQENV=""True""
export PYATS_CONFIGURATION=""./pyats.conf""
cd /auto/dna-sol/ws/sr-ibste/groot/
easypy ./job/sr_ibste/sr_ibste.py -mailto [divayada@cisco.com|mailto:divayada@cisco.com]
 
All IXIA ports are showing as down on IXIA server",2022-08-19T15:05:45.137+0000,"Divakar, traffic is configured correctly. looks like you have a product issue for arp not resolving.. Debug on setup and raise a product issue. YOu can check on Edge if why arp is no resolved. [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/56ca2f788f0a24cbb7adfffa91081f9a1e37810d]

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/configs/sr_ibste/testbed/sr-ibste-tb.yaml?until=383b8d70910a4d6f5e3235b356fa4208e179fdf2&at=refs%2Fheads%2Fprivate%2FGhost-ms%2Fapi-auto]

 ","['Auton', 'Ghost', 'Groot', 'IBSTE', 'Issue', 'MSTB2', 'Multisite']",Pawan Singh,Resolved,Avril Bower
SEEN-468,https://miggbo.atlassian.net/browse/SEEN-468,[Auton] : Groot - Test_TC32_Compliance_verification  /   test2_verify_SW_image_version,"This issue is seen after upgrade from Shockwave P3 to Groot

+*Versio**n :*+ Groot 2.1.560.70513

+*Script :*+ after_upgrade_verify.py 

+*Testcase Impacted :*+ 

[Test_TC32_Compliance_verification|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15240997&size=7085343&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug19_13:14:55.292366.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&atstype=PYATS]  /   test2_verify_SW_image_version

 

+*Log :*+

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19352077&size=120085&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F19%2F13%2F14%2Fenv_auto_job.2022Aug19_13:14:55.292366.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 

+*Error from Log :*+
46163:  Compliance status
46164: 
46165: 
46166:  api_switch_call called:
46167:  {}
46168:  Resource path full url: [https://10.195.227.80/api/v2/data/compliance/eb5c7a08-8949-492d-a3a8-fa950d3c497c/detail]
46169:  Traceback (most recent call last):
46170:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Upgrade-Verify-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
46171:  result = testfunc(func_self, **kwargs)
46172:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Upgrade-Verify-common-Multi-job/testcases/upgrade/after_upgrade_verify.py"", line 1088, in test2_verify_SW_image_version
46173:  if dnac_handle.Compliance_Software_Image():
46174:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Upgrade-Verify-common-Multi-job/services/dnaserv/lib/api_groups/Compilance/group.py"", line 409, in Compliance_Software_Image
46175:  if x[""status""]==status:
46176:  UnboundLocalError: local variable 'status' referenced before assignment",2022-08-20T02:31:42.206+0000,[c968a4717fe|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c968a4717fe2a4fe21f4c650a70171327397c398],"['Auton', 'Groot', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-469,https://miggbo.atlassian.net/browse/SEEN-469,"[Auton] [Optimized Sanity ]Interface onboarding failed, due to concurrent task","Looks like there are two tasks running at the same time, need to handle it

 

The following two sub tests failed:
||[Test_TC2_DNAC_TSIM_static_onboarding_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding&begin=1001528&size=5691473&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug17_04:16:54.085691.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2Foptimizedsanity1%2Ftestcases%2Fsanityusecases%2FSDAwiredHostOnboarding%2Faccess_wired_ipphone_tsim_ap_hostOnboarding.py&lineno=189&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug17_04:16:54.085691.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&from=trade&view=all]|Failed|01:19:13|Test_TC2_DNAC_TSIM_static_onboarding_verifications| |
||[test1_tsim_static_onboarding_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding&begin=1002308&size=1656764&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug17_04:16:54.085691.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2Foptimizedsanity1%2Ftestcases%2Fsanityusecases%2FSDAwiredHostOnboarding%2Faccess_wired_ipphone_tsim_ap_hostOnboarding.py&lineno=190&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug17_04:16:54.085691.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&from=trade&view=all]|Failed|00:13:27|test1_tsim_static_onboarding_verifications| |
||[test2_onboard_all_wired_clients|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding&begin=2659072&size=1405385&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug17_04:16:54.085691.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2Foptimizedsanity1%2Ftestcases%2Fsanityusecases%2FSDAwiredHostOnboarding%2Faccess_wired_ipphone_tsim_ap_hostOnboarding.py&lineno=234&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug17_04:16:54.085691.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&from=trade&view=all]|Failed|00:10:19|test2_onboard_all_wired_clients
 
|

Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding&begin=1001528&size=5691473&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug17_04:16:54.085691.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS]

 

79: The Schedduled Job failed: with reason \{'id': '06b017a5-7541-4dc5-b878-0d174c22c1d6', 'triggeredJobTaskId': '2d4ce4de-1727-4e9c-bcb0-1aac6f01f0ca', 'triggeredTime': 1660739482507, 'status': 'FAILED', 'failureReason': ""NCSP11033: Error occurred while processing the 'modify' request. Additional info for support: taskId: '2d4ce4de-1727-4e9c-bcb0-1aac6f01f0ca'. Version being provisioned '47' in namespace 'cc95c06c-05a3-4386-87fb-5378e6bd6615' is older than the current version '48' configured by task 'b1523d72-83c0-456c-8099-a0a19182f506' in the device."", 'triggeredJobId': '06b017a5-7541-4dc5-b878-0d174c22c1d6'}

 

 ",2022-08-20T03:56:47.830+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/services/dnaserv/lib/api_groups/wired_interface_onboarding/group.py?until=3c45de45941f258507a82336c684e2ee7690bc42&at=refs%2Fheads%2Fprivate%2FGroot-ms%2Fapi-auto

Will merge groot and ghost, it will got to ghost part of the merge. Passed in Groot RC6 2.1.560.70517

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding&begin=463833&size=4558842&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_08:08:22.902987.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

Passed in Ghost 2.1.610.70375

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding&begin=448825&size=6284876&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_optimized_auto_job.2022Aug30_19:49:25.486750.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized']",Pawan Singh,Closed,Avril Bower
SEEN-470,https://miggbo.atlassian.net/browse/SEEN-470,Auton: Latest 17.9.1 onwards needs certificate for TSIM AP,"TSIM AP's are not joining to ECA and EWLC due to the following error.  No issue with real AP

 

May 26 05:05:12.989: %PKI-3-CERTIFICATE_INVALID_EXPIRED: Certificate chain validation has failed.  The certificate (SN: 3B55AFE8000000144842) has expired.    Validity period ended on 2021-12-29T03:11:41Z

May 26 05:05:14.126: %PKI-3-CERTIFICATE_INVALID_EXPIRED: Certificate chain validation has failed.  The certificate (SN: 3B55AFE8000000144842) has expired.    Validity period ended on 2021-12-29T03:11:41Z

 

Workaround: To be added in the script if device version is 17.9 and above, These commands can be added after adding the wireless role in fabric

 

You have to configure below policy under ‘Trustpool’ (bundle of trusted CA certs).


(config)#
*crypto pki certificate map map1 1*
*issuer-name co cisco manufacturing ca*
crypto pki certificate map map1 2
issuer-name co act2 sudi ca

 


*crypto pki trustpool policy*
*match certificate map1 allow expired-certificate*

 

 

*More info: [https://wiki.cisco.com/pages/viewpage.action?pageId=1258043173]*

 ",2022-08-21T20:44:52.214+0000,"Please fix this issue ASAP, as it is affecting production testcases [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/01a7c0ffc29de456e9825c45f91196df125fc415] for groot. Groot:  [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/01a7c0ffc29de456e9825c45f91196df125fc415]

Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c967c44f3f2a3d3bbe57a64756436f238a62f945]

Guardian: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/890fcaedfc5a89edf8ed60c93d8de290f3b7c9e8 Fixed ib all imp branches.

 

 ","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Shockwave']",Pawan Singh,Closed,Avril Bower
SEEN-471,https://miggbo.atlassian.net/browse/SEEN-471,[Auton] [Optimized Sanity ] Command-runner being used by multiple processes,"Test failed due to command runner being used by two processes.

: cli failed.\{'result': False, 'output': 'This device is already under process by command runner in another session,try with other device'}

 

Branch: private/Groot-ms/sanity_api_auto

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_360_nw_health.py-133-assuranceHealth360&begin=95502&size=93044&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug17_19:39:56.770492.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS]

 

 

 ",2022-08-21T23:19:56.397+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/49425c0042d9d565a3f12c92961af51c177fa7b1#services/dnaserv/lib/api_groups/command_runner/group.py Passed in Ghost: 2.1.610.70375

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_360_nw_health.py-133-assuranceHealth360&begin=104235&size=77211&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_optimized_auto_job.2022Aug31_05:25:35.935189.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

Passed in Groot-RC6 2.1.560.70517
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_360_nw_health.py-131-assuranceHealth360&begin=100777&size=77209&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep04_00:27:43.233174.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Pawan Singh,Closed,Avril Bower
SEEN-472,https://miggbo.atlassian.net/browse/SEEN-472,"[Auton] : AP PNP test, Script failed to login in ap console","The script failed to login in to ap console due to multiple errors with the latest parts and unicon version.

 

 

Branch: private/Groot-ms/sanity_api_auto

Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding&begin=5190519&size=71426&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug17_04:16:54.085691.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS]

 

unicon 22.4

'prefix': '/ws/sjc-it/pawan/pyatsds3',

 ",2022-08-22T01:20:19.963+0000,"Use 22.10 pyats. the unicon is fixed in 22.10/

Older version keeps sending the same password and does not try a different password [from the three passwords] set for it. Can you use pyats version 22.10? pyats env issue.","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Raju Saran,Closed,Avril Bower
SEEN-473,https://miggbo.atlassian.net/browse/SEEN-473,"[Auton]  Test_TC94_generate_link_flap_issues, wrong permission RBAC roles","This test needs higher permission while re-running the test with other testcases failed due to wrong permission, This test needs to have higher permission

branch: private/Groot-ms/sanity_api_auto

Failed due to assurance user.

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1553531&size=56471&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug18_03:41:26.148594.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

 ",2022-08-22T02:26:18.719+0000,"Found the problem, Please review and add it to the main branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/579e5b8af2f2252df8bd3082c286f9b6b768ef04]

  https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0d21799d6e8e0266237dbdea403986d8f0ede5ba","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-474,https://miggbo.atlassian.net/browse/SEEN-474,[Auton] - Groot-Test_TC96_wired_app_policy/test_verify_device_configuration,"**ISO:**Groot RC5 2.1.560.70513  (Non-Fips)

*Script Name:*  solution_test_sanityecamb_lan.py

*Testcases Impacted* 
  [Test_TC96_wired_app_policy|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1612771&size=310067&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug18_03:41:26.148594.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test_verify_device_configuration 
  
 *Note**:* Test doesn’t print what the expected config on the interface instead it just prints the list of interfaces

* 
 +*Failure Log:*
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1736354&size=186328&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug18_03:41:26.148594.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS+]

*Snip from Fail Log:*
 7489: Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:02.206367
 7490: Validation of wired application policy config failed for reason :: ['Class-map and policy-map configuration verification failed ::TB6-DM-eCA-BORDER reason :: []', ""Failed to push any of the service policy config (DNA-dscp#APIC_QOS_Q_OUT, input DNA-MARKING_IN, DNA-APIC_QOS_IN) on interfaces: ['interface TenGigabitEthernet1/0/1', 'interface TenGigabitEthernet1/0/3', 'interface TenGigabitEthernet1/0/4', 'interface TenGigabitEthernet1/0/7', 'interface TenGigabitEthernet1/0/8', 'interface TenGigabitEthernet1/0/13', 'interface TenGigabitEthernet1/0/14', 'interface TenGigabitEthernet1/0/15', 'interface TenGigabitEthernet1/0/16', 'interface TenGigabitEthernet1/0/17', 'interface TenGigabitEthernet1/0/19', 'interface TenGigabitEthernet1/0/23', 'interface TwentyFiveGigE1/1/1', 'interface AppGigabitEthernet1/0/1', 'interface AppGigabitEthernet1/0/2']\n"", 'Class-map and policy-map configuration verification failed ::TB6-SJ-EDGE reason :: []', ""Failed to push any of the service policy config (DNA-dscp#APIC_QOS_Q_OUT, input DNA-MARKING_IN, DNA-APIC_QOS_IN) on interfaces: ['interface GigabitEthernet1/0/11', 'interface AppGigabitEthernet1/0/1', 'interface AppGigabitEthernet1/0/2']\n""]
 7491: Test returned in 0:00:19.978366
 7492: Failed reason: Result : Failed to push policy on device
 7493: The result of section test_verify_device_configuration is => FAILED",2022-08-22T05:58:34.963+0000,"[~70121:27365d3e-893a-4523-9fca-b89a7e8d0d4c],

 Please review the ticket and leave some suggestions if you can.

Thank you,

Moe 7490:  Validation of wired application policy config failed for reason :: ['Class-map and policy-map configuration verification failed ::TB6-DM-eCA-BORDER reason :: []', ""Failed to push any of the service policy config (DNA-dscp#APIC_QOS_Q_OUT, input DNA-MARKING_IN, DNA-APIC_QOS_IN) on interfaces: ['interface TenGigabitEthernet1/0/1', 'interface TenGigabitEthernet1/0/3', 'interface TenGigabitEthernet1/0/4', 'interface TenGigabitEthernet1/0/7', 'interface TenGigabitEthernet1/0/8', 'interface TenGigabitEthernet1/0/13', 'interface TenGigabitEthernet1/0/14', 'interface TenGigabitEthernet1/0/15', 'interface TenGigabitEthernet1/0/16', 'interface TenGigabitEthernet1/0/17', 'interface TenGigabitEthernet1/0/19', 'interface TenGigabitEthernet1/0/23', 'interface TwentyFiveGigE1/1/1', 'interface AppGigabitEthernet1/0/1', 'interface AppGigabitEthernet1/0/2']\n"", 'Class-map and policy-map configuration verification failed ::TB6-SJ-EDGE reason :: []', ""Failed to push any of the service policy config (DNA-dscp#APIC_QOS_Q_OUT, input DNA-MARKING_IN, DNA-APIC_QOS_IN) on interfaces: ['interface GigabitEthernet1/0/11', 'interface AppGigabitEthernet1/0/1', 'interface AppGigabitEthernet1/0/2']\n""]

 

 

 

DNA-dscp#APIC_QOS_Q_OUT, input DNA-MARKING_IN, DNA-APIC_QOS_IN

 

 

These are the configs which are missing from the listed interface it also part of the print.","['Auton', 'Groot', 'Issue']",Moe Saeed,Resolved,Avril Bower
SEEN-475,https://miggbo.atlassian.net/browse/SEEN-475,[Auton][Optimized Sanity ] - Groot-Test_TC4_add_ssid/test1_add_ssid,"**ISO:**Groot RC5 2.1.560.70513  (Non-Fips)

*Branch Name:* private/Groot-ms/sanity_api_auto  (optimized run)

*Testcases Impacted :* 
Test_TC4_add_ssid/test1_add_ssid
*Failure Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_edits_special_char.py-173-ssidEditSpecialChar&begin=320633&size=206763&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsanity_TB3_cert.2022Aug17_21:39:21.833714.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS]


+*Snip from Failed Log:*+

'NCSP11051', 'serviceType': 'NCSP', 'lastUpdate': 1660808791158, 'isError': True, 'failureReason': ""NCSP11051: Error occurred while processing the 'modify' request. A different version of the same user intent already exists in the database. Additional info for support: taskId: '7a4ccf0e-da94-4143-a791-95c8d542e533'. Name: 'TB6-DM-WLC'. Incoming resourceVersion: '53'. resourceVersion in the database: '54'."", 'instanceTenantId': '62fc9421440f31660e0e51e3', 'id': '7a4ccf0e-da94-4143-a791-95c8d542e533'}

1230: Provisioning of device failed for reason:NCSP11051: Error occurred while processing the 'modify' request. A different version of the same user intent already exists in the database. Additional info for support: taskId: '7a4ccf0e-da94-4143-a791-95c8d542e533'. Name: 'TB6-DM-WLC'. Incoming resourceVersion: '53'. resourceVersion in the database: '54'.

1231: [{'id': '01c51221-8f17-4115-9820-0e29b9a808a5', 'instanceId': 503684, 'instanceCreatedOn': 1660733323016, 'instanceUpdatedOn': 1660733323016, 'instanceVersion': 107, 'createTime': 1660733323011, 'deployed': False, 'isSeeded': False, 'isStale': False, 'lastUpdateTime': 1660807986408, 'name': 'TB6-DM-eCA-BORDER.cisco.com', 'namespace': 'cc95c06c-05a3-4386-87fb-5378e6bd6615', 'provisioningState': 'DEFINED', 'resourceVersion': 107, 'targetIdList': [], 'type': 'DeviceInfo', 'cfsChangeInfo': [], 'customProvisions': [], 'anchoredManagedSites': [], 'configs': [], 'managedSites': [], 'networkDeviceId': '7dabb4b4-1b3a-4a0c-82c8-751badf147fd', 'primaryManagedSites': ['067301bb-bfb5-4e91-aa27-a86bddcf93d1'], 'profileId': '51be24c2-70e7-4bbd-a8e1-dd69c62c7206', 'profileVersion': '17', 'rlanList': [], 'roles': ['EDGENODE', 'BORDERNODE', 'MAPSERVER', 'ENDPOINT_PROXY",2022-08-22T08:35:44.556+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/79fccbd70a2e3af220f9abb3ce0aeee355a93d6c
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c112eed4ab3d8d91c4aafe42f57d3ee9de2b2996
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f991127a658527631820def6b230b63feac4aa78
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d7933f14096803ea648197d11768eb62f8165105","['Auton', 'Groot', 'Issue', 'Optimized', 'Sanity']",Pawan Singh,Closed,Avril Bower
SEEN-476,https://miggbo.atlassian.net/browse/SEEN-476,[Auton][IBSTE] : Test_TC98_ITSM_ticket_generation_test test3_approve_SGT_request,"Affected testcase:

[Test_TC98_ITSM_ticket_generation_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4441075&size=1865591&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug22_00:00:13.816034.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 

test3_approve_SGT_request

 test6_reject_SGT_request

test9_reject_dev_deletion_request

test11_approve_dev_deletion_request

Branch : private/Groot-ms/api-auto

Failed Log : 
Library group ""itsm"" method ""get_integration_events"" returned in 0:00:00.100321
19903:  Traceback (most recent call last):
19904:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/commonlibs/test_wrapper.py"", line 301, in wrapper
19905:  result = testfunc(func_self, **kwargs)
19906:  File ""/auto/dna-sol/ws/sr-ibste/groot/testcases/ibste/ibste_script.py"", line 3868, in test3_approve_SGT_request
19907:  if dnac_handle.approve_latest_event():
19908:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/decorators.py"", line 32, in wrapper
19909:  result = method(*args, **kwargs)
19910:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/api_groups/itsm/group.py"", line 193, in approve_latest_event
19911:  return self.approve_change_request(latest_id, event_name=event_name)
19912:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/decorators.py"", line 32, in wrapper
19913:  result = method(*args, **kwargs)
19914:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/api_groups/itsm/group.py"", line 205, in approve_change_request
19915:  self.log.info(""Status of change request: {}"".format(self.get_integration_event_status(id, event_name=event_name)))
19916:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/decorators.py"", line 32, in wrapper
19917:  result = method(*args, **kwargs)
19918:  File ""/auto/dna-sol/ws/sr-ibste/groot/services/dnaserv/lib/api_groups/itsm/group.py"", line 153, in get_integration_event_status
19919:  if id in event[""ITSMLink""]:
19920:  TypeError: 'in <string>' requires string as left operand, not NoneType
19921:  Test returned in 0:01:43.746424
19922:  Errored reason: 'in <string>' requires string as left operand, not NoneType
 
Failed Log : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4531360&size=30400&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug22_00:00:13.816034.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS",2022-08-22T09:47:30.935+0000,"Setup issues Setup issues resolved, working now

 ","['Auton', 'Groot', 'Issue']",Andrew Chen,Closed,Avril Bower
SEEN-478,https://miggbo.atlassian.net/browse/SEEN-478,[Auton]: Test enhancement -Test_TC94_generate_link_flap_issues /test1_generate_link_flap_issues,"Ghost  ISO 2.1.610.70338 

Script Name :  solution_test_sanityecamb_lan.py

Please enhance the Testcase *Test_TC94_generate_link_flap_issues /test1_generate_link_flap_issues* by checking whether the port is up or not, before flapping the port

 

Failed log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=48340907&size=57454&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug16_13:03:43.653275.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Passed log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=706330&size=34180&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug23_05:03:37.884504.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ",2022-08-23T12:24:54.622+0000,"hi Tran, i've create the PR. Please check it out

Auton Info:
===============
1. PR link: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4463/overview
2. Test Case: https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-478
3. Script files: solution_test_sanityecamb_lan.py
4. Testbed: TB7
5. Trade log: https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/22-12/sanity_TB7.2022Dec20_01:31:03.664865.zip&atstype=ATS
6. Any related issues/bugs: SEEN-399, SEEN-781","['Auton', 'Issue', 'assurance']",QuangVinh Nguyen,Resolved,Avril Bower
SEEN-479,https://miggbo.atlassian.net/browse/SEEN-479,[Auton]: Test enhancement-Test_TC45_DNAC_TSIM_static_onboarding_verifications/test8_wait_ap_to_be_provisioned,"Ghost  ISO 2.1.610.70338 

Script Name :  solution_test_sanityecamb_lan.py

Please enhance the Testcase: Test_TC45_DNAC_TSIM_static_onboarding_verifications/test8_wait_ap_to_be_provisioned  

The testcase should not perform AP-pnp if the ISE version is lower 

 

Failed log where AP pnp failed with dot1x authentication while ISE version used it 2.6 P11

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6305331&size=95056&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug16_13:03:43.653275.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

 ",2022-08-23T12:41:44.502+0000,PR: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3665/overview,"['Auton', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-480,https://miggbo.atlassian.net/browse/SEEN-480,[Auton][IBSTE] : Test_TC87_Verify_border_priority test1_verify_border_priority_config_before_update,"Affected testcase:

[Test_TC87_Verify_border_priority|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4029727&size=282191&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug22_20:42:09.935958.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] [ |https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4441075&size=1865591&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug22_00:00:13.816034.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

[test1_verify_border_priority_config_before_update|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4441075&size=1865591&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug22_00:00:13.816034.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

Branch : private/Groot-ms/api-auto

Failed Log : 

Currently script is checking for only SJ site if it has dual border, this check has failed due to Dual border is present on CUBA site on IBSTE.

 

Expected Behaviour : Dual border verification should be done on all site before script fails.

 

Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4030297&size=277998&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_ibste.2022Aug22_20:42:09.935958.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2022-08-23T13:03:02.079+0000,"Hi

We have run this testcase for first time on IBSTE

Regards,

Divakar Hi Team,

 

Can I get ETA for the ticket raised

 

Regards,

Divakar Since Cuba is your Internet site, please update INTERNET_SITE as Cuba in global_site.yaml. The script will use your INTERNET_SITE  info.

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sr_ibste/sites/global_site.yaml?at=refs%2Fheads%2Fprivate%2FGroot-ms%2Fapi-auto#55 After adding INTERNET_SITE: Cuba in ""ibste_global.yaml"" issue is not seen","['Auton', 'Groot', 'Issue']",Tran Lam,Closed,Avril Bower
SEEN-482,https://miggbo.atlassian.net/browse/SEEN-482,Prime initial config setup,"This ticket we are creating to track the initial config setup for Prime usecase.

 

we will discuss with Andrew, what is the best way to set up and move forward.

 

1) One approch is to setup the config manually on ECA,Transit,Prime sw and reload them with a defined file name

2) 2nd option, Move the test after the TC23, configure isis between Tranist and prime sw and run the prime test (Best approach)

 ",2022-08-23T20:40:35.353+0000,,"['Auton', 'Guardian', 'Issue']",Andrew Chen,Cancelled,Avril Bower
SEEN-483,https://miggbo.atlassian.net/browse/SEEN-483,add use-case for adding NTP config on FUSION device,"During TB9 validation via *solution_test_sanityecamb_lan.py* script, *Test_TC40/test3* had failed. Upon debug, FUSION device didn’t had required NTP config.

Since it’s a pre-requisite, request the Automation team to add a use-case under TC40 to provision the FUSION with NTP config.

Below is what I propose:
 # Introduce another key-value pair “NEXTHOP” or “NEXTHOP_FOR_NTP” under NTP section inside Config’s yaml file.
 # Make use of below config template to provision Fusion Device as pre-requisite before failing Test_TC40_DNAC_edit_border_attributes  /   test3_ntp_sync_check_on_fusion_after_bgp_config.
 # If the required variable is not defined, print the statement about the same so that executioner know what might be missing.

 

Below is the configuration sample required:
{code:java}
TB9-Fusion#show running-config | section ntp
ntp source Loopback0
ntp master
ntp server vrf Mgmt-vrf $NTP_IP
ntp server $NTP_IP

TB9-Fusion#show running-config | include ip route
ip route vrf Mgmt-vrf $NTP_IP 255.255.255.255 $NEXTHOP

TB9-Fusion#show ntp status | include Clock
Clock is synchronized, stratum 8, reference is 127.127.1.1
{code}",2022-08-24T02:15:25.722+0000,"The reachability to NTP could be set up differently on different testbeds, based on uplink connectivity. Since we don't want to force Uplink config, so we skip implementing it.

It is listed as part of the sanity testbed steps on how to setup NTP.

 

 ","['Auton', 'Ghost', 'Groot', 'Guardian', 'Improvement']",Pawan Singh,Closed,Avril Bower
SEEN-484,https://miggbo.atlassian.net/browse/SEEN-484,Script is making connection to the incorrect device with recently pulled Groot and Ghost sanity code,"*Script and the TC* : solution_test_sanityecamb.py /Test_TC45_DNAC_TSIM_static_onboarding_verifications  /   test1_tsim_static_onboarding_verifications

*Branch* : private/Groot-ms/api-auto

The issue is seen after I pulled code for both Groot and Ghost in the first week of August, before raising Jira I have tried pulling code yesterday (Aug 23rd) and still see the issue. Issue across multiple testbeds.

 

*Issue* : The script was supposed to connect to the device ""sda-9k-118-FiaB"" and perform some commands on specific interface, but the script was connecting to the device ""SN-FOC2341V4NT"" which is wrong, due to this the specific interface is not found and throwing an error which I mentioned in the logs. However if I re-run the same TC45 the script was connecting to correct device. I have provided both the failed log and the passed log (it passed in the re-run). This issue noticed only after pulling the sanity code recently and seen both with Groot and Ghost sanity codes. Below is sample errored log:

 

SN-FOC2341V4NT(config)#end SN-FOC2341V4NT#
 335108: Prompt Recovery has commenced. Total timeout occurs in 50 seconds.
 335109: Sending prompt recovery command: b'\r' SN-FOC2341V4NT#
 335111: Sending prompt recovery command: b'\x15'
 335112: Sending prompt recovery command: b'\x1a'^Z SN-FOC2341V4NT#
 335114: Sending prompt recovery command: b'\r' SN-FOC2341V4NT#
 335116: Sending prompt recovery command: b'\x1e'
 335118: Traceback (most recent call last):
 335119: File ""/home/dnac/pyATS5/lib/python3.6/site-packages/unicon/statemachine/statemachine.py"", line 719, in go_to
 335120: output = transition.do_transitions()
 335121: File ""/home/dnac/pyATS5/lib/python3.6/site-packages/unicon/statemachine/statetransition.py"", line 220, in do_transitions
 335122: timeout=self.timeout)
 335123: File ""/home/dnac/pyATS5/lib/python3.6/site-packages/unicon/eal/dialogs.py"", line 460, in process
 335124: return dp.process()
 335125: File ""/home/dnac/pyATS5/lib/python3.6/site-packages/unicon/eal/dialog_processor.py"", line 304, in process
 335126: self.timeout_handler()
 335127: File ""/home/dnac/pyATS5/lib/python3.6/site-packages/unicon/eal/dialog_processor.py"", line 799, in timeout_handler
 335128: super().timeout_handler()
 335129: File ""/home/dnac/pyATS5/lib/python3.6/site-packages/unicon/eal/dialog_processor.py"", line 763, in timeout_handler
 335130: repr(self.spawn.buffer)))
 335131: unicon.core.errors.TimeoutError: Prompt recovery timeout occurred:
 335132: timeout value: 50

 

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=94054381&size=34590&archive=%2Fhome%2Fdnac%2FpyATS5%2Fusers%2Fapic%2Farchive%2F22-08%2FSanity_TB4.2022Aug08_20:27:19.593958.zip&ats=%2Fhome%2Fdnac%2FpyATS5&submitter=ksinigam&from=trade&view=all&atstype=pyATS]

 

Passed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=763421&size=1047563&archive=%2Fhome%2Fdnac%2FpyATS5%2Fusers%2Fapic%2Farchive%2F22-08%2FSanity_TB4.2022Aug09_03:29:15.810202.zip&ats=%2Fhome%2Fdnac%2FpyATS5&submitter=ksinigam&from=trade&view=all&atstype=pyATS]

 

Note : Since I dont have access to Jira to open a case, I have updated the issue in the wiki ""https://wiki.cisco.com/display/EDPEIXOT/Groot"" few weeks back. But yesterday I got access to Jira so opened a Jira ticket for the same.

 ",2022-08-24T07:21:38.956+0000,"The issue is with pyats unicon environment. if it is script issue it will fail in rerun also with same issue as nothing changes from script side.  Also other teams are not hitting any such issue with same script. 

Wireless team earlier had the same issue, which was  resolved after moviing to pyats latest version for unicon.

Update your environment to latest pyats/unicon or raise the issue with pyats team.

I issue case the device 9K is pointing to ext node console.

335149:  +++ sda-9k-118-FiaB: configure +++. <--------Pyats device object is (fiab)

SN-FOC2341V4NT#. <----------Prompt shows console of EXT node.
335151:  Prompt Recovery has commenced. Total timeout occurs in 50 seconds.
335152:  Sending prompt recovery command: b'\r'


--Pawan

","['Auton', 'Ghost', 'Groot', 'Issue']",Pawan Singh,Closed,Avril Bower
SEEN-485,https://miggbo.atlassian.net/browse/SEEN-485,"[Auton]: [Auton]:  Groot/Guardian Swim upgradation  TC failed due to config, not present devices ""IP domain lookup""","*Release*: Groot & Guardian 

*Script Name:* solution_test_sanityecamb_cert_lan.py+(FQDN)

*Summary* :
IP domain lookup, cli not present on devices , that's why TC Swim upgradation failed, Kindly fix that issue 
+*Before :*+
TB2-DM-NF-Switch#show run | in domain
no ip domain lookup
ip domain lookup source-interface Loopback0
ip domain name tb2sanity.com
domain-password CISCO
800 deny udp any any eq domain
TB2-DM-NF-Switch# 

Not working 
+*After add cli*  :+
8/23/2022 10:47 PM • TB2-DM-NF-Switch#conf t
Enter configuration commands, one per line. End with CNTL/Z.
*TB2-DM-NF-Switch(config)#ip domain lookup*
TB2-DM-NF-Switch(config)#end
TB2-DM-NF-Switch#ping tb2.tb2sanity.com
Type escape sequence to abort.
Sending 5, 100-byte ICMP Echos to 82.1.1.2, timeout is 2 seconds:
!!!!!
Success rate is 100 percent (5/5), round-trip min/avg/max = 1/1/1 ms
TB2-DM-NF-Switch#

Fail log: https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug23_06:42:05.439330.zip&atstype=ATS",2022-08-24T08:24:04.917+0000,PR: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3539/overview,"['Auton', 'Groot', 'Gurad', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-486,https://miggbo.atlassian.net/browse/SEEN-486,[Auton] [Groot] Config verification on Extended node fails with string error,"*Uber ISO Version tested :* 

Promoted Groot RC4 Uber ISO - *2.1.560.70508, FIPS*
 Promoted Groot RC5 Uber ISO - *2.1.560.70.513, Non-FIPS, PUBSUB enabled*

*Script Name:*
 solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB1, MSTB2
  

*Failed logs:* 
 1) [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug19_03:18:06.979631.zip&atstype=ATS] -> Refer TC42

2) [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fauto_MS_job.2022Aug17_23:25:27.889455.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS] -> Refer TC42

 

*Description :* 

We see Configuration verification on some of the Ext node devices are failing with string concatenation errors. Please check both the Failed logs above to see the devices.

 

 ",2022-08-24T09:26:16.426+0000,"PR has been raised for the required changes: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3961/overview [~62d2fec15d6f5fd2c3db8f9f], the changes have been merged to Groot branch.

Please validate the failure against the fix and share the result. Hi Amardeep,

Issue is no more observed during Groot Patch1 RC2 Testing.

Uber ISO used - 2.1.610.70540

*Pass log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1004485&size=434358&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fsr_mb_multi_sites_mdnac.2022Nov08_20:12:47.334846.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] -> TC42

 

But we are still having issue on Ghost as well as in Guardian Patch2. Hence will reopen the JIRA. Could you please commit the fixes on both these branches?

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1321574&size=83152&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fsr_mb_multi_sites_mdnac.2022Nov17_19:50:25.358843.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] -> Refer TC42, Failed log on Ghost

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1431468&size=105308&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep24_16:07:54.401830.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] -> Refer TC42, Failed log on Guardian Patch2 RC2 Please refer above latest comment for more details. Cherry picked the fix for Ghost branch
and
have raised PR for Guardian: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4267/overview] due to merge conflict. [~62d2fec15d6f5fd2c3db8f9f], required change have got merged to Guardian branch as well.

Let us know if we can conclude on this ticket. PR raised for Shockwave branch as well.

Marking this ticket as ""Done"".","['Auton', 'Groot', 'Issue', 'MSTB1', 'MSTB2', 'Multisite']",Amardeep Kumar,Closed,Avril Bower
SEEN-487,https://miggbo.atlassian.net/browse/SEEN-487,Logs improvement for test Test_TC183_check_system_health_with_validation_tool,"This test just prints one line during the failure and doesn't provide detailed info about the test failure, Can we please improve it so it can help in regression for debugging?

 

Branch: private/Groot-ms/sanity_api_auto

Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11686538&size=16384&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug17_07:15:49.851946.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

 
42350:  Resource path full url: [https://10.30.0.100/api/v1/diagnostics/validation/workflows/8562a24c-1471-4679-a9e6-d2a7ffbc8a58]
42351:  Status of validation run is Critical
42352:  Library group ""system_health"" method ""check_system_health_with_validation_tool"" returned in 0:01:30.961509
42353:  Test returned in 0:01:30.963164
42354:  Failed reason: System issue found with validation tool, please check errors.",2022-08-24T22:40:45.134+0000,"Execution log with the changes pushed to Groot ms branch:
https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/ws/amardkum/pyats/users/amardkum/archive/22-08/sanity_TB9.2022Aug24_18:52:21.485276.zip&atstype=ATS
  
 {color:#ff8b00}5757: Overall Status of validation run is Critical{color}
 {color:#ff8b00}5758: 1. ValidationTitle: Check and verify DNS server configuration requirements{color}
 {color:#ff8b00}5759: Status: Critical{color}
 {color:#ff8b00}5760: Message: Maglev service is not available. Cannot retrieve data. Please wait and then retry{color}
 {color:#ff8b00}5761: Remedy: Maglev service is not available. Cannot retrieve data. Please wait and then retry
 {color}
 {color:#ff8b00}5762: 2. ValidationTitle: Cassandra service status{color}
 {color:#ff8b00}5763: Status: Critical{color}
 {color:#ff8b00}5764: Message: Maglev service is not available. Cannot retrieve data. Please wait and then retry{color}
 {color:#ff8b00}5765: Remedy: Maglev service is not available. Cannot retrieve data. Please wait and then retry
 {color}
 {color:#ff8b00}5766: 3. ValidationTitle: Cluster node(s) status{color}
 {color:#ff8b00}5767: Status: Critical{color}
 {color:#ff8b00}5768: Message: Maglev service is not available. Cannot retrieve data. Please wait and then retry{color}
 {color:#ff8b00}5769: Remedy: Maglev service is not available. Cannot retrieve data. Please wait and then retry{color} PR for the required change is pending for review and approval:
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3502/overview Reviewed the PR and merged to private/Groot-ms/api-auto

There may be some sync from Groot-ms to Ghost-ms as we won't have features for Ghost-ms until next week or so. That means we may not need to double commits or PRs to both branches. The fix is available in Groot RC6 2.1.560.70517

Passed log: where i can see detailed info

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=32995281&size=21016&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep05_01:43:16.015757.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ","['Auton', 'Ghost', 'Groot', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-488,https://miggbo.atlassian.net/browse/SEEN-488,TC35 test1 requires generalization in terms of looking for ECA devices,"Currently Test_TC35_enable_fabric_wireless_eca / test1_enable_fabric_wireless_eca is hard-coded to consider ECA devices from SJ and NY.

The same needs to be generalized.

Also, observed a sleep time of 120 seconds. in below use-cases, which is not required:

1. test3_disable_fabric_wireless

2. test4_read_fabric_wireless

Hence, remove those sleep time related statements as well.",2022-08-25T02:01:57.598+0000,"Raised PR for the required change:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3501/overview] PR merged into private/Groot-ms/api-auto branch.

Marking this ticket as ""Closed"".","['Auton', 'Groot', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-489,https://miggbo.atlassian.net/browse/SEEN-489,"Test Failed : Verify_all_the_device_events_in_one_location, Full feature recording is missing too"," 

This feature is failing on Groot and Ghost both, looks like the API is changed.

The current webex recording doesn't explain what is implemented in the automation, This recording is about the Q/A.

Can you please fix the test case and also add what are we validating in this api ""[https://10.195.227.92/api/assurance/v2/analytics]"" and others, which are not covered 

Groot/ghost branch

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1138036&size=24275&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug24_22:18:17.367806.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

 ",2022-08-25T06:41:15.784+0000,"Hi [~70121:27365d3e-893a-4523-9fca-b89a7e8d0d4c],

I was run this test case on Groot and Ghost branches. Use Sanity and Multisite testbed. But all are passed.

I think there was a problem with the API before. But now it's back to normal.

Please check the trade log link below:

private/Ghost-ms/api-auto - Sanity TB1
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-02/sanity_TB1.2023Feb14_23:35:54.205839.zip&atstype=ATS]

private/Ghost-ms/api-auto - Multisite:
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-02/three_sites.2023Feb15_23:34:54.670519.zip&atstype=ATS]

private/Groot-ms/api-auto - Sanity TB1
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-02/sanity_TB1.2023Feb15_00:52:42.233850.zip&atstype=ATS]

private/Groot-ms/api-auto Multisite:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-02/three_sites.2023Feb16_19:45:07.733438.zip&atstype=ATS]

Thanks.

 ","['Auton', 'Ghost', 'Groot', 'Issue']",NhanHuu Nguyen,Resolved,Avril Bower
SEEN-491,https://miggbo.atlassian.net/browse/SEEN-491,[Auton] [Guardian] Attribute error during ISE cleanup for reader,"*Uber ISO Version tested :* 

Promoted Guardian Patch2 RC1 Uber ISO - *2.1.515.70102,* *Non-FIPS, PUBSUB enabled, CSRF enabled*

*Script Name:*
 solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py

*Testbed :* MSTB1
  

*Failed logs:* 
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug26_01:06:45.965578.zip&atstype=ATS] -> Refer TC 0.3 and TC3.5

 

*Description :* 

We are observing attribute error during execution at ISE cleanup proc on the reader node. It has to be handled from script side. Better we can have a seperate sub TC to check for CSRF eligibility for both Author and reader nodes.

 

*Error snip:*  

{{37092:  Traceback (most recent call last):}}{{37093:  File ""/auto/dna-sol/ws/sr-mb1_Guardian/services/commonlibs/test_wrapper.py"", line 301, in wrapper}}{{37094:  result = testfunc(func_self, **kwargs)}}{{37095:  File ""/auto/dna-sol/ws/sr-mb1_Guardian/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py"", line 556, in cleanup_ise_profile_for_reader}}{{37096:  r_dnac_handle_list.check_CSRF_eligibility()}}{{37097:  AttributeError: 'list' object has no attribute 'check_CSRF_eligibility'}}

 ",2022-08-26T14:28:50.807+0000,"Hey Moe,

Looks like this ticket has been marked Resolved by mistake. We still see the corresponding code is not changed. Could you please have a look once?

!image-2022-09-07-19-34-51-204.png! Hey [~62d2fec15d6f5fd2c3db8f9f],

Please follow the PR as the link attached, it has not merged yet. --> Please instead of open the ticket, check the PR if it is merged yet or not Yet. the status is shown down as ""Open""  !image-2022-09-07-10-15-06-646.png! Hey Moe,

Thanks for sharing the info!

Sorry I was not aware of Development field in Automation JIRA. I was looking in History section or All section for the PR link as usually it was being updated here. Since PR link was not there I thought it was closed by mistake. Going forward will look into that. With respect to this will wait for PR to be merged. 

 

Regards

Sandeep S @[~63f50bfce8216251ae4d59d5]: We are unable to view the merge request/ commit link. Could you please share the commit link? Issue is no more observed during Guardian Patch2 RC2 testing. 

*Pass log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep23_08:03:13.954061.zip&atstype=ATS] -> Refer TC0","['Auton', 'Guardian', 'Issue', 'MSTB1', 'Multisite']",Moe Saeed,Closed,Avril Bower
SEEN-492,https://miggbo.atlassian.net/browse/SEEN-492,[Auton]:Ghost MS2 TB2:Test_TC2_DNAC_configure_multicast_on_sites/cleanup_existing_multicast,"*Uber ISO Version tested :* Ghost Uber ISO - *2.1.610.70338, FIPS*

*Script Name :* solution_test_3sites_multicast.py

*Testbed :* MSTB2

*Branch :*  private/Ghost-ms/api-auto

*Failed Log :* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2917376&size=574821&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F28%2F22%2F01%2Fsr_mb2_three_sites.2022Aug28_22:01:09.587297.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Previous Pass Log :* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2431687&size=992939&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F19%2F05%2F38%2Fsr_mb2_three_sites.2022Aug19_05:38:52.051951.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Description :*  **  We are seeing 404 error code issue while trying to clear Multicast RP from fabric site. But when we check Dnac UI, we didn't see any failed tasks over there. But while running script it is getting 404 error code.

 
 11212: Error Code: 404 URL:[https://172.23.241.114/api/v1/orchestration-engine/activities/b3b1e32e-6dbc-473c-8578-9c41d1059f57/devices] Data:\{'timeout': 30} Headers:\{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2MzA1YTRjOGM4Y2NlNzNkZTk0ZmIyOGIiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjYzMDVhNGM4YzhjY2U3M2RlOTRmYjI4YSJdLCJ0ZW5hbnRJZCI6IjYzMDVhNGM3YzhjY2U3M2RlOTRmYjI4OCIsImV4cCI6MTY2MTc1NzcxNywiaWF0IjoxNjYxNzU0MTE3LCJqdGkiOiIxYTQwM2NmNi0xMmM4LTQwYTYtODJkNS05ZGYxM2IxYTMxZDkiLCJ1c2VybmFtZSI6ImFkbWluIn0.Kj7zM_dXl_r9kPuQqRBCy0P3LdVMv5LaXfl1QK2xmaSt3cK4PSe3DT8JfXMmCchy7Yun5CQyQHQ_tJyyejRiv2wV4_YoCJczU76V5HgcRfksPNXfCMQHRO1b0rQrn4MpjsWf2XzoDtiLZ3HIS5wb3mAk14WeAnag5jUcqCKgRo73Ja_luSbzYwckCqcJld-GfiiZ9k7R3vaYByZFH8yTm92X7_wG8By6VsSWfvc7zUOqvfgx1pHbRv5gBtxWhtD-w_c7BvosowIe6ZAusO-4WpyOJ6b9LsW3Qd7o6hq83A5BleVPuRVvv8Lb5QQqqSHdPKNaHTIcwmaiqsMMx2mLOA;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:
 11213: Traceback (most recent call last):
 11214: File ""/auto/dna-sol/ws/sr-mb2/Ghost/code_commit/dnac-auto/services/dnaserv/client_manager.py"", line 295, in call_api
 11215: response.raise_for_status()
 11216: File ""/users/rvonmize/pyats-inst/lib/python3.7/site-packages/requests/models.py"", line 941, in raise_for_status
 11217: raise HTTPError(http_error_msg, response=self)
 11218: requests.exceptions.HTTPError: 404 Client Error: Not Found for url: [https://172.23.241.114/api/v1/orchestration-engine/activities/b3b1e32e-6dbc-473c-8578-9c41d1059f57/devices]
 11219: Encountered unhandled HTTPError in Internal API Call
 11220: Flagging result as FAIL!
 11221: Reason: 404 Client Error: Not Found for url: [https://172.23.241.114/api/v1/orchestration-engine/activities/b3b1e32e-6dbc-473c-8578-9c41d1059f57/devices]
 11222: Kwargs:
 11223: {}
 11226: No response for activity ID: ""b3b1e32e-6dbc-473c-8578-9c41d1059f57""
 11228: Failed to receive config preview
 11549: !!!!!!!! Clear Multicast RP failed from fabric site Global/USA/SAN_JOSE_US_SJ_Fabric1 !!!!!!!!
 12950: Failed reason: Failed to cleanup multicast configuration!",2022-08-29T11:12:07.395+0000,"I don't se the issue on other testbed. Do you still see the issue again with latest Ghost? Seems, it might be a defect or some issue during the execution of that Ghost image.
In Later Regressions, we are not seeing this issue.","['Auton', 'Ghost', 'Issue', 'MSTB2', 'Multisite']",Tran Lam,Closed,Avril Bower
SEEN-497,https://miggbo.atlassian.net/browse/SEEN-497,CLONE - [Auton] <TC Name> <Sub test> <Headline>," 

Reporter Analysis: 

Description:  The error from log or more info 

Branch Name:

Script file: 

Source Team:  Sanity

Issue Seen first time or day0 issue:

Fail Log:

Pass Log:

Testbed details: Only if available for debugging ",2022-08-31T23:32:47.127+0000,Just for testing,"['Auton', 'Issue']",Unassigned,Closed,Avril Bower
SEEN-498,https://miggbo.atlassian.net/browse/SEEN-498,[Auton] Test_TC45_DNAC_TSIM_static_onboarding_verifications AP PNP test needs to be skipped," 

Reporter Analysis: The AP PNP feature is only supported from ISE 3.1 onwards, So we need to check if the ISE version is 3.1 or higher before executing the following subtest, otherwise we need to skip them.
||[test5_onboard_ap_dot1x|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=497458&size=496886&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug31_17:08:27.454155.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2FGroot-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=3930&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug31_17:08:27.454155.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Passed|00:04:34|test5_onboard_ap_dot1x| |
||[test6_prepare_aps_dot1x|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=994344&size=30516&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug31_17:08:27.454155.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2FGroot-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=3956&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug31_17:08:27.454155.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Passed|00:01:19|test6_prepare_aps_dot1x| |
||[test7_pnp_onboard_dot1x_ap|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1024860&size=180092&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug31_17:08:27.454155.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2FGroot-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=3965&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug31_17:08:27.454155.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Passed|00:00:02|test7_pnp_onboard_dot1x_ap| |
||[test8_wait_ap_to_be_provisioned|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1204952&size=131531&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug31_17:08:27.454155.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2FGroot-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=3974&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug31_17:08:27.454155.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Failed|00:16:20|test8_wait_ap_to_be_provisioned| |
||[test9_aduit_log_verify|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1336483&size=1376&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug31_17:08:27.454155.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2FGroot-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=3983&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug31_17:08:27.454155.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Skipped|00:00:00|test9_aduit_log_verify
  |

Description:  NA

Branch Name: NA

Script file:  solution_test_sanityecamb_lan, solution_test_sanityecamb, solution_test_sanityecamb_cert_lan , optimized code use case: (SDAwiredHostOnboarding)

Source Team:  Sanity

Issue Seen first time or day0 issue: day0 issue

Fail Log: NA

Pass Log: NA

Testbed details: NA",2022-09-01T00:59:08.474+0000,"Sev2, because when we run this test, with a lower ISE version it fails PR: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3665/overview","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity', 'Shockwave']",Moe Saeed,Closed,Avril Bower
SEEN-499,https://miggbo.atlassian.net/browse/SEEN-499,[Auton] Cleanup Script Remove the Golden image tag,"Reporter Analysis: The Cleanup script, does not remove the golden tag for the images marked golden,  This is affecting the re-run, where we don't want to update the image during LAN-A

 

Description:  No FAILED log

Branch Name: All the main branches from Shockwave onwards 

Script file: dnac_cleanup_script.py

Source Team:  Sanity

Issue Seen first time or day0 issue:

Fail Log:  

*Log 1-17*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-08/env_auto_job.2022Aug29_04:58:59.062129.zip&atstype=ATS]

*Log18-19*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-08/env_auto_job.2022Aug29_22:40:12.004169.zip&atstype=ATS]

*Log 18-31*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-08/env_auto_job.2022Aug29_23:39:50.082670.zip&atstype=ATS]

Pass Log: NA

Testbed details: NA",2022-09-01T06:41:34.799+0000,"PR: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3765/overview Merged to Guardian Moe, can you Cherry pick or PR to other branches too? Already added to other branches, moving it to resolved. ","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity', 'Shockwave']",Moe Saeed,Resolved,Avril Bower
SEEN-500,https://miggbo.atlassian.net/browse/SEEN-500,[Auton]-Groot-Test_TC160_cisco_telemetry_broker_as_netflowcollector/test3_verify_netflow_config_on_devices/test4_stream_video_ftp_traffic_and_validate/test7_verify_netflow_config_on_devices,"*Reporter Analysis:*  Devices are unable to configure expected data, so sub-TC failed for anticipated configuration not present 
  .Also, include the test cases in optimized sanity code

*Description*: No Full PassLog
 Cisco Telemery broker(Full Flexible NetFlow TC)   

*Branch Name:* Groot-ms/sanity_api_auto

**Script* [*file:**|file://%2A/] solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:

**Fail Log:**ISo:Groot: RC5 2.1.560.70513
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=985674&size=5589184&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug23_22:58:59.082161.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:* Not having  past full pass_log 

 ",2022-09-01T09:31:25.279+0000,"Failed in :: Groot RC6 2.1.560.70517: 

Failed Log:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=941559&size=5087511&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep05_23:32:18.113562.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS Please provide the full pass log passlog file is updated in wiki after completion of automation of this usecase.

 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Froot%2F.pyats%2Farchive%2F21-11%2Fsanity_TB1.2021Nov03_04:57:57.866400.zip&atstype=ATS]

 

[https://wiki.cisco.com/display/EDPEIXOT/CTB]  TC Failed in  Guardian_Patch2_RC2=2.1.515.70134

Failed Log:
https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-09/env_auto_job.2022Sep21_22:34:10.474521.zip&atstype=ATS Hi Team,

 

Can I get ETA for the ticket raised

 

Regards,

Omkar Hi Omkar, i know its been pending but there were some issues in automation setup on wired client and traffic part and some provision issues

 

to validate this usecase we need wired clients and traffic part too

 

let me check  TC Failed with same error  in  Ghost-AWS=2.1.610.70454
 Failed Log:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1254616&size=6558097&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct19_01:55:51.200229.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
h6.   [~557058:fbb9c502-e858-4224-932a-86d595cefda2], [~62d2fe9f8afb5805e5d5af49], can we have some quick update on this ticket? It's been more than a month now. [~62ab7a399cd13c0068b18fe0] no update sofar was working on other autons and closing ..busy with Ghost use cases let me see if we can close this week on this. Had a discussion with [~63f53512263233e653a96a29] by seeing logs its clearing saying some of the lines are missing in some of the devices. please file a bug and check with DE what is the issue. We cannot take the feature if it's not working, Please report the defect and If dev team agrees to fix the defect, Testcase will be accepted otherwise the test case integration goes in backlog moving ticket to [~62d2fe9f8afb5805e5d5af49] for inputs as Rakesh is no more with the Org. Fix for:
 * Remove description LAN since its not needed anymore for netflow and might affect netflow in certain scenario.
 * Cover ewlc configuration changes with version >=17.10
 * When lots of video-over-http traffic, it will record as sever ip instead.
 * Add verification for netflow config removed after disabling app telemetry to wait and make sure the disabling process are done before trying to enable app telemetry again.
 * Include extended node

Pass log from sanitytb1: 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Ftranlam-sjc%2Ftestenv%2Fusers%2Ftranlam%2Farchive%2F22-12%2Fsanity_TB1.2022Dec08_20:13:16.271827.zip&atstype=ATS]

(DNAC: 2.3.5.0-70540, 9k: 17.10.1prd10, ewlc: 17.9.1, FlowReplicator-7.1.3-2020.06.16.1331-0.iso)

 

Merged to Ghost, Groot, Guardian

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f4ef33caed9e5f00c75fb7814fc6d3b26eb1d9da]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/48a432902362a3ad2e9f8759e8766dc8fbae8f9b#testcases/forty_eight_hour/solution_test_sanityecamb.py]

  When executed on MSTB2 testbed, it prolonged to 4 hrs.+:
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/SR-CL-MS/job/MSTB2_Multijob_Solution_Regression_Git/259/console]

This should get investigated and fix on early abortion if test-step's result is not as expected. To be specific, test4_stream_video_ftp_traffic_and_validate is getting stuck with MSTB2:
https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/SR-CL-MS/job/MSTB2_Multijob_Solution_Regression_Git/286/ We need to enhance our script to timeout when failed to connect RPC to clients in case clients don’t have proper RPC setup. GHost:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits?until=refs%2Fheads%2Fprivate%2FGhost-ms%2Fapi-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits?until=refs%2Fheads%2Fprivate%2FGhost-ms%2Fapi-auto]

Groot:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/53503a1a1977ec79f54071d420c40989d67d4729|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/53503a1a1977ec79f54071d420c40989d67d4729]

Halleck:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/fb3d6f8fb9821f39f11deaea8a9b9c06a924aeec|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/fb3d6f8fb9821f39f11deaea8a9b9c06a924aeec]","['AWS_Sanity', 'Auton', 'Ghost', 'Groot', 'Issue', 'Optimized', 'Sanity']",Moe Saeed,Resolved,Avril Bower
SEEN-501,https://miggbo.atlassian.net/browse/SEEN-501,[Auton] Test_TC28_DNAC_Device_Provisioning test5_verify_provision_the_devices_fabric1 Error with Design API,"Reporter Analysis: Error observed with ""[api/v2/named-capability/design|https://172.19.168.121/api/v2/named-capability/design]"" API

Description:  The error from log or more info 

Branch Name: Groot-ms/sanity_api_auto

Script file: solution_test_sanityecamb_lan.py / Test_TC28_DNAC_Device_Provisioning / test5_verify_provision_the_devices_fabric1

Source Team:  Sanity

Issue Seen first time or day0 issue: Not sure.

Fail Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=481844&size=545392&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug31_15:09:56.888676.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Pass Log: (False PASS) [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=452677&size=512778&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug16_13:44:22.781698.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Testbed used: TB9",2022-09-01T20:58:20.407+0000,"With change pushed via [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e4c92be9f9098371570187a48914239b21bb1f2c,] error is no more observed.

Execution log with above change:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=442528&size=573978&archive=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats%2Fusers%2Famardkum%2Farchive%2F22-09%2Fsanity_TB9.2022Sep01_13:22:08.416606.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats&submitter=amardkum&from=trade&view=all&atstype=pyATS

However, the library should be enhanced further to avoid false pass. [acc516fce7d|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/acc516fce7d246669713c7396ff38a764f5044cb]","['Auton', 'Issue', 'Sanity']",Raji Mukkamala,Closed,Avril Bower
SEEN-502,https://miggbo.atlassian.net/browse/SEEN-502,"[Auton] Test_TC158_DNAC_AEN_ext_nodes_assurance, Need to cleanup AEN from ISE"," 

Reporter Analysis: During Sanity test we only cleanup once the Network devices from ISE. In the AEN onboarding, we remove the PEN device and onboard it as AEN(*SDAExtnodeOnboarding*). Later we remove it from AEN and add it back as PEN(*SDAExtendednodePEN*).

Description:  none, Shared details in email

Branch Name: private/Groot-ms/sanity_api_auto

Script file:    solution_test_sanityecamb_lan, solution_test_sanityecamb, solution_test_sanityecamb_cert_lan , optimized code use case: (SDAExtendednodePEN)

Source Team:  Sanity

Issue Seen first time or day0 issue:

Fail Log: 

Pass Log: 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug31_18:17:59.934758.zip&atstype=ATS]

Testbed details: not available ",2022-09-01T21:05:10.770+0000,"PR: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3724/overview Need to PR to Guardian, Groot too. Merged to Ghost.

Need to PR to Guardian, Groot too.","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity']",Moe Saeed,Resolved,Avril Bower
SEEN-503,https://miggbo.atlassian.net/browse/SEEN-503,[Auton]Groot:Task-extNode_onboarding.py-122-SDAExtnodeOnboarding/Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding,"Reporter Analysis: Got error , unsupported operand types failed verification test case 

Description: 

ERROR: TypeError: unsupported operand type(s) for &=: 'bool' and 'str'

 Branch Name: private/Groot-ms/sanity_api_auto

Script file: solution_test_sanityecamb_lan.py

Source Team:  Sanity

Issue Seen first time or day0 issue:

Fail Log:1)[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-122-SDAExtnodeOnboarding&begin=91071&size=27189&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_optimized_auto_job.2022Aug30_22:00:12.306115.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
2)https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding&begin=4164057&size=56144&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_optimized_auto_job.2022Aug30_22:00:12.306115.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

Pass Log:

Testbed details: Only if available for debugging ",2022-09-02T12:05:52.675+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/8879aef25561a644bc8b07bf8c3949c737292b00,"['Auton', 'Groot', 'Issue', 'Optimized', 'Sanity']",Tran Lam,Closed,Avril Bower
SEEN-504,https://miggbo.atlassian.net/browse/SEEN-504,[Auton][IBSTE] : Guardian and Groot feature which are integrated recently needs to be double commited to IBSTEOptimized script,"Hi 

New features that got integrated recently are not pushed onto IBSTE_optimized script. Below are the feature which needs to be committed
 # Rep Ring
 # Worst interferers
 # ITSM
 # Eox Scan

Testcase Name :

Test_TC89_auto_configure_a_new_rep_ring_with_en_or_pen_using_workflow

Test_TC90_monitor_the_rep_ring_via_assurance_and_cut_the_ring

Test_TC91_delete_the_entire_ring

Test_TC97_generate_Worst_Interferers_report

Test_TC98_ITSM_ticket_generation_test

Test_TC101_accept_cisco_cx_cloud_eula_for_scanned_devices",2022-09-02T14:17:38.496+0000,"Except Test_TC98_ITSM_ticket_generation_test, rest all have been included for Optimized IBSTE Execution list.
PR for the same: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3585/overview]

 

Test_TC98_ITSM_ticket_generation_test is already present but commented out.

Checking with [~5f3c6ae932360700388f7b4b] for the same. [~63f50bf0e8216251ae4d59ca], the required PR has been merged to private/Groot-ms/api-auto branch.
Please pull the latest, test the execution of the mentioned test-cases and share the result. Will be validating the fix on next Groot or Ghost Run Script is available on Groot.

 

Passlog :

https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Foptimized_ibste_job.2022Oct11_08:45:11.607681.zip&atstype=ATS","['Auton', 'Groot', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-505,https://miggbo.atlassian.net/browse/SEEN-505,[Auton]:Groot:TC12_verify_fabric_360/test2_verify_fabric_site_health_timeline/test3_verify_fabric_site_health/test4_verify_fabric_nodes/test5_verify_fabric_assurance_metric,"Reporter Analysis: TC failed attribute error, no any pass log in Optimized Run  

*Description:*  
 4721: Exception:
 4722: Traceback (most recent call last):
 4723: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Optimized-Deployment_and_Sanity-Job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 4724: result = testfunc(func_self, **kwargs)
 4725: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Optimized-Deployment_and_Sanity-Job/testcases/sanityusecases/SDAFabricAssurance/assurance_fabric_assurance.py"", line 287, in test5_verify_fabric_assurance_metric
 4726: result = dnac_handle.verify_fabric_assurance_metric()
 4727: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Optimized-Deployment_and_Sanity-Job/services/dnaserv/dnaservices.py"", line 309, in __getattr__
 4728: raise AttributeError(err_msg)
 4729: AttributeError: 'DnaServices' object has no attribute 'verify_fabric_assurance_metric'
 4721: Exception:
 4722: Traceback (most recent call last):
 4723: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Optimized-Deployment_and_Sanity-Job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 4724: result = testfunc(func_self, **kwargs)
 4725: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Optimized-Deployment_and_Sanity-Job/testcases/sanityusecases/SDAFabricAssurance/assurance_fabric_assurance.py"", line 287, in test5_verify_fabric_assurance_metric
 4726: result = dnac_handle.verify_fabric_assurance_metric()
 4727: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Optimized-Deployment_and_Sanity-Job/services/dnaserv/dnaservices.py"", line 309, in __getattr__
 4728: raise AttributeError(err_msg)
 4729: AttributeError: 'DnaServices' object has no attribute 'verify_fabric_assurance_metric'

*Branch Name:* private/Groot-ms/sanity_api_auto(optimized )
 *Source Team:*  Sanity

Issue Seen first time or day0 issue:

Fail Log:[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-151-SDAFabricAssurance&begin=758806&size=48837&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep02_06:29:19.877807.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Pass Log:no pass log in optimized run 

Testbed details: Only if available for debugging 

 

 

following functions are not defined 
||[test2_verify_fabric_site_health_timeline|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-155-SDAFabricAssurance&begin=1032139&size=14685&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2FGroot-Optimized-Deployment_and_Sanity-Job@2%2Ftestcases%2Fsanityusecases%2FSDAFabricAssurance%2Fassurance_fabric_assurance.py&lineno=253&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Errored|00:00:00|test2_verify_fabric_site_health_timeline| |
||[test3_verify_fabric_site_health|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-155-SDAFabricAssurance&begin=1046824&size=6476&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2FGroot-Optimized-Deployment_and_Sanity-Job@2%2Ftestcases%2Fsanityusecases%2FSDAFabricAssurance%2Fassurance_fabric_assurance.py&lineno=263&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Errored|00:00:00|test3_verify_fabric_site_health| |
||[test4_verify_fabric_nodes|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-155-SDAFabricAssurance&begin=1053300&size=6417&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2FGroot-Optimized-Deployment_and_Sanity-Job@2%2Ftestcases%2Fsanityusecases%2FSDAFabricAssurance%2Fassurance_fabric_assurance.py&lineno=273&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Errored|00:00:00|test4_verify_fabric_nodes| |
||[test5_verify_fabric_assurance_metric|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-155-SDAFabricAssurance&begin=1059717&size=6516&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FGroot%2FGroot-Optimized-Deployment_and_Sanity-Job@2%2Ftestcases%2Fsanityusecases%2FSDAFabricAssurance%2Fassurance_fabric_assurance.py&lineno=283&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Errored|00:00:00|test5_verify_fabric_assurance_metric|",2022-09-02T14:20:11.865+0000,"Attribute error is resolved, 
Log :
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-151-SDAFabricAssurance&begin=899611&size=169708&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct21_09:22:09.327068.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS [56ee0b7729c|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/56ee0b7729c9adf932c62d26ba8ceabfd3e14b29] - Ghost Sanity branch

[57fa106979e|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/57fa106979ec4b2b56e26985b207b3ca9c84f141] - Groot Sanity branch Pls ignore previous comment, below are correct Commit id's

[39f44099c41|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/39f44099c41167b2e6b655dc76f73d2dab3088d6] - Ghost Sanity branch

[e0e0f057d2c|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e0e0f057d2c002d5cc349e49b0b8d1be5a74876e] - Groot Sanity branch","['AWS_Sanity', 'Auton', 'Ghost', 'Groot', 'Issue', 'Optimized', 'sanity']",Nethra Thorali Suthagar,Closed,Avril Bower
SEEN-506,https://miggbo.atlassian.net/browse/SEEN-506,[Auton]  Test_TC159_configure_the_external_sftp  /   test3_read_configs_from_the_remote_server : Config archive work flow,"Test worked fine : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=991673&size=34919&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep02_17:50:42.884899.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 ",2022-09-03T00:27:49.485+0000,,"['Auton', 'Issue', 'Sanity']",Raju Saran,Closed,Avril Bower
SEEN-507,https://miggbo.atlassian.net/browse/SEEN-507,[Auton] Test_TC1_DNAC_testbed_topology_discovery_through_cdp Console to be cleared," 

Reporter Analysis: Need to add a method to clear the console before collecting the cdp data

Description:  

Branch Name: private/Groot-ms/sanity_api_auto

Script file/usecase: testbedTopologyGenerate

Source Team:  Sanity

Issue Seen first time or day0 issue:

Fail Log: https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-testbed_cdp_links_discovery.py-12-testbedTopologyGenerate&begin=4829&size=88956&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep02_20:48:11.923911.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

Pass Log: NA

Testbed NA",2022-09-04T01:34:41.622+0000,"Fix : [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e44e017fe2b475675d0004a7656c13ca4b2cea5c]

 ","['Auton', 'Ghost', 'Groot', 'Issue', 'Sanity']",Raju Saran,Closed,Avril Bower
SEEN-508,https://miggbo.atlassian.net/browse/SEEN-508,[Auton] Test_TC1_DNAC_Connectivity_domain_creation_in_dnac  /   test1_setup_SGT sgt cleanup to be added," 

Reporter Analysis: SGT cleanup is not done 

Description:  
Test returned in 0:00:06.415696
222:  Failed reason: Failed because could not create scalable group due to : NCCS16037: Security Group with the name sgt_port_test_sg already exists
 
Branch Name: private/Groot-ms/sanity_api_auto

Script file/usecase:  [SDAFabricCDVirtualNetworksAndSegmentOnboarding|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-virtual_networks_segments.py-72-SDAFabricCDVirtualNetworksAndSegmentOnboarding&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep02_20:48:11.923911.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  

Source Team:  Sanity

Issue Seen first time or day0 issue: day0

Fail Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-virtual_networks_segments.py-72-SDAFabricCDVirtualNetworksAndSegmentOnboarding&begin=5291&size=95342&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep02_20:48:11.923911.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Pass Log:

Testbed details: Only if available for debugging ",2022-09-04T02:22:02.955+0000,"Production test fails, need fix asap https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3785/diff#testcases/sanityusecases/SDAFabricCDVirtualNetworksAndSegmentOnboarding/virtual_networks_segments.py [167de6a007d|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/167de6a007d277dd68249ab18f03015b8736f071] -ghost commit Test passed in Groot : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-virtual_networks_segments.py-72-SDAFabricCDVirtualNetworksAndSegmentOnboarding&begin=4508&size=1020234&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct11_16:35:33.976536.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS Test passed in ghost: 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct06_19:00:32.109699.zip&atstype=ATS]

  Thanks Andrew for the fix :) ","['Auton', 'Ghost', 'Groot', 'Issue', 'Optimized', 'Sanity']",Andrew Chen,Closed,Avril Bower
SEEN-509,https://miggbo.atlassian.net/browse/SEEN-509,[Auton] Test_TC31_SWIM_UPGRADE_ECA_DEVICE  /   test8_aduit_log_verify RBAC permission issue," 

Reporter Analysis:  audit log is secure, need correct permission to view it.

Description:  Branch Name:  Groot main branch 

Script file: 

Source Team:  Sanity

Issue Seen first time or day0 issue: day0 issue

Fail Log: https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15471891&size=555489&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep03_01:49:15.591795.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

Pass Log:

Testbed details: Only if available for debugging ",2022-09-04T02:58:56.510+0000,[9edfe9f093b|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9edfe9f093b32aae9bf3d55b0719cf34b9e5be83],"['Auton', 'Groot', 'Issue', 'Sanity']",Raji Mukkamala,Closed,Avril Bower
SEEN-510,https://miggbo.atlassian.net/browse/SEEN-510,[Auton]   Test_TC4_DNAC_verify_anchorvn_withIXIA_clients_and_traffic_noauth  /   verify_anchorvn_withIXIA  - Test fails in optimized code," 

Reporter Analysis: Script issue, looks like variable is not set

Description: 
8671:  Setting Port Media and AutoNegSpeed
28672:  RESULT: IXIA Successfuly connected to IXIA ports
28673:  Traceback (most recent call last):
28674:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Optimized-Deployment_and_Sanity-Job@2/testcases/sanityusecases/SDAFabricAnchorvn/anchor_vn_validations.py"", line 483, in verify_anchorvn_withIXIA
28675:  count=IXIA_STATIC_ENDPOINT)
28676:  NameError: name 'IXIA_STATIC_ENDPOINT' is not defined
28677:  Exception while checking IXIA traffic for AnchorVN WiredVNStatic2: name 'IXIA_STATIC_ENDPOINT' is not defined
28678:  Verification for some Anchoredvn ['WiredVNStatic1', 'WiredVNStatic2'] with IXIA clients and traffic failed
28679:  Test returned in 0:33:03.217864
 
*Branch Name:* private/Groot-ms/sanity_api_auto

*Script file/usecase:*  SDAFabricAnchorvn

*Source Team:*  Sanity

Issue Seen first time or day0 issue: day0

Fail Log: https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-anchor_vn_validations.py-161-SDAFabricAnchorvn&begin=5816287&size=3829819&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

Pass Log:

Testbed details: NA",2022-09-04T20:52:01.697+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/78e15f867d3f41a17726e6b04ad6863f7619e12e,"['Auton', 'Ghost', 'Groot', 'Issue', 'Optimized', 'Sanity']",Tran Lam,Closed,Avril Bower
SEEN-511,https://miggbo.atlassian.net/browse/SEEN-511,[Auton] Task-assurance_fabric_assurance.py-155-SDAFabricAssurance  /   Test_TC7_generate_ap_reachability_events  : Console is accessed by multiple thread," 

*Reporter Analysis:* The interface ([TB7-NY-FIAB.cisco.com|https://10.30.0.100/dna/assurance/device/details?id=c3bec6b8-f127-449e-8927-4cb479e4c8da&endTime=1662262222955&startTime=1662175822955&timeLabel=24%20hours], GigabitEthernet1/0/14) connected to AP AP5CE1.7629.CEF0 was not 'un shut' after the test, due to the console access was taken by another thread.  Can we use the rest API to shut/no shut for this ?
 
*Description:  The error from log or more info* 
2043: 2022-09-03T19:07:24: %UNICON-6-INFO:  +++ TB7-NY-FIAB with via 'a': configure +++
config term Enter configuration commands, one per line. End with CNTL/Z. TB7-NY-FIAB(config)# TB7-NY-FIAB(config)#interface GigabitEthernet1/0/14 TB7-NY-FIAB(config-if)# shut TB7-NY-FIAB(config-if)#end TB7-NY-FIAB#
2052: 2022-09-03T19:07:25: %SERVICES-6-INFO: 
2053: 2022-09-03T19:07:25: %SERVICES-6-INFO:  interface GigabitEthernet1/0/14
2054: 2022-09-03T19:07:25: %SERVICES-6-INFO:  shut
2055: 2022-09-03T19:07:25: %API-GROUP-ASSURANCE-6-INFO:  Waiting for the Issues to be generated
2056: 2022-09-03T19:09:25: %API-GROUP-ASSURANCE-6-INFO:  Using the URL /assurance/v1/time
 
*While accessing the console to un shut, it was in config prompt.*
 
TB7-NY-FIAB#
2306: 2022-09-03T19:14:24: %UNICON-6-INFO:  +++ TB7-NY-FIAB with via 'a': configure +++config term Enter configuration commands, one per line. End with CNTL/Z. TB7-NY-*FIAB(config)*#no logging console TB7-NY-FIAB(config)#
2311: 2022-09-03T19:14:35: %SERVICES-3-ERROR:  Traceback (most recent call last):
2312: 2022-09-03T19:14:35: %SERVICES-3-ERROR:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 969, in process_dialog_on_handle
2313: 2022-09-03T19:14:35: %SERVICES-3-ERROR:  cmd_result = dialog.process(
2314: 2022-09-03T19:14:35: %SERVICES-3-ERROR:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialogs.py"", line 476, in process
2315: 2022-09-03T19:14:35: %SERVICES-3-ERROR:  return dp.process()
 
*issue:*
2193: 2022-09-03T19:12:10: %UNICON-6-INFO:  +++ connection to spawn: telnet 10.30.0.71 2003, id: 139910875007584 +++
2194: 2022-09-03T19:12:10: %UNICON-6-INFO:  connection to TB7-NY-FIABtelnet: connect to address 10.30.0.71: Connection refused
2196: 2022-09-03T19:12:21: %SERVICES-4-WARNING:  Failed login through primary , trying clearing console and retry
2197: 2022-09-03T19:12:21: %UNICON-6-INFO:  Trying 10.30.0.71...
2198: 2022-09-03T19:12:21: %UNICON-6-INFO: 
2199: 2022-09-03T19:12:21: %UNICON-6-INFO:  Connected to 10.30.0.71.
2200: 2022-09-03T19:12:21: %UNICON-6-INFO: 
2201: 2022-09-03T19:12:21: %UNICON-6-INFO:  Escape character is '^]'.
2202: 2022-09-03T19:12:21: %UNICON-6-INFO: 
2203: 2022-09-03T19:12:21: %UNICON-6-INFO: 
2204: 2022-09-03T19:12:21: %UNICON-6-INFO: 
2205: 2022-09-03T19:12:21: %UNICON-6-INFO:  User Access Verification
2206: 2022-09-03T19:12:21: %UNICON-6-INFO: 
2207: 2022-09-03T19:12:21: %UNICON-6-INFO:  Password:
2208: 2022-09-03T19:12:21: %UNICON-6-INFO: 
2209: 2022-09-03T19:12:21: %UNICON-6-INFO:  DM-TS4>
2210: 2022-09-03T19:12:22: %UNICON-6-INFO:  enable
2211: 2022-09-03T19:12:22: %UNICON-6-INFO:  Password:
2212: 2022-09-03T19:12:22: %UNICON-6-INFO: 
2213: 2022-09-03T19:12:22: %UNICON-6-INFO:  DM-TS4#
2214: 2022-09-03T19:12:22: %UNICON-6-INFO: 
2215: 2022-09-03T19:12:22: %UNICON-6-INFO:  DM-TS4#
2216: 2022-09-03T19:12:22: %UNICON-6-INFO:  clear line 3
2217: 2022-09-03T19:12:22: %UNICON-6-INFO:  [confirm]
2218: 2022-09-03T19:12:22: %UNICON-6-INFO: 
2219: 2022-09-03T19:12:22: %UNICON-6-INFO:  [OK]
2220: 2022-09-03T19:12:22: %UNICON-6-INFO:  DM-TS4#*Trying 10.30.0.71... telnet: connect to address 10.30.0.71: Connection refused*
2222: 2022-09-03T19:12:53: %UNICON-6-INFO:  +++ connection to spawn: telnet 10.30.0.71 2003, id: 139910875012736 +++
2223: 2022-09-03T19:12:53: %UNICON-6-INFO:  connection to TB7-NY-FIAB
2224: 2022-09-03T19:13:04: %SERVICES-4-WARNING:  Failed login through primary , trying clearing console and retry
2225: 2022-09-03T19:13:04: %UNICON-6-INFO:  Trying 10.30.0.71...
2226: 2022-09-03T19:13:04: %UNICON-6-INFO: 
2227: 2022-09-03T19:13:04: %UNICON-6-INFO:  Connected to 10.30.0.71.
2228: 2022-09-03T19:13:04: %UNICON-6-INFO: 
2229: 2022-09-03T19:13:04: %UNICON-6-INFO:  Escape character is '^]'.
2230: 2022-09-03T19:13:04: %UNICON-6-INFO: 
2231: 2022-09-03T19:13:04: %UNICON-6-INFO: 
2232: 2022-09-03T19:13:04: %UNICON-6-INFO: 
2233: 2022-09-03T19:13:04: %UNICON-6-INFO:  User Access Verification
2234: 2022-09-03T19:13:04: %UNICON-6-INFO: 
2235: 2022-09-03T19:13:04: %UNICON-6-INFO:  Password:
2236: 2022-09-03T19:13:05: %UNICON-6-INFO: 
2237: 2022-09-03T19:13:05: %UNICON-6-INFO:  DM-TS4>
2238: 2022-09-03T19:13:06: %UNICON-6-INFO:  enable
2239: 2022-09-03T19:13:06: %UNICON-6-INFO:  Password:
2240: 2022-09-03T19:13:06: %UNICON-6-INFO: 
2241: 2022-09-03T19:13:06: %UNICON-6-INFO:  DM-TS4#
2242: 2022-09-03T19:13:06: %UNICON-6-INFO: 
2243: 2022-09-03T19:13:06: %UNICON-6-INFO:  DM-TS4#
2244: 2022-09-03T19:13:06: %UNICON-6-INFO:  clear line 3
2245: 2022-09-03T19:13:06: %UNICON-6-INFO:  [confirm]
2246: 2022-09-03T19:13:06: %UNICON-6-INFO: 
2247: 2022-09-03T19:13:06: %UNICON-6-INFO:  [OK]
2248: 2022-09-03T19:13:06: %UNICON-6-INFO:  DM-TS4#Trying 10.30.0.71...
2250: 2022-09-03T19:13:37: %UNICON-6-INFO:  +++ connection to spawn: telnet 10.30.0.71 2003, id: 139910874828272 +++
2251: 2022-09-03T19:13:37: %UNICON-6-INFO:  connection to TB7-NY-FIABt*elnet: connect to address 10.30.0.71: Connection refused*
 
*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* [SDAFabricAssurance|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-155-SDAFabricAssurance&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-155-SDAFabricAssurance&begin=263571&size=539737&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:*

*Testbed details:* NA",2022-09-05T03:38:36.096+0000,"This is some unicon issue. The connection is going through but not successful due to some bad unicon state. 

you can raise a pyats ticket with below logs:
cisco C9404R (X86) processor (revision V01) with 1833804K/6147K bytes of memory.
Processor board ID FXS2418Q0KU
14 Virtual Ethernet interfaces
24 Gigabit Ethernet interfaces
40 Ten Gigabit Ethernet interfaces
4 Forty Gigabit Ethernet interfaces
32768K bytes of non-volatile configuration memory.
15993800K bytes of physical memory.
10444800K bytes of Bootflash at bootflash:.
1638400K bytes of Crash Files at crashinfo:.
234430023K bytes of SATA hard disk at disk0:.
234430023K bytes of SATA hard disk at disk0-1-1:.
1638400K bytes of Crash Files at crashinfo-1-1:.
10444800K bytes of Bootflash at bootflash-1-1:.

Base Ethernet MAC Address          : 70:61:7b:ea:36:80
Motherboard Assembly Number        : 4AAE
Motherboard Serial Number          : FXS2417001E
Model Revision Number              : V02
Motherboard Revision Number        : 1
Model Number                       : C9404R
System Serial Number               : FXS2418Q0KU

TB7-NY-FIAB#
3253:  +++ TB7-NY-FIAB with via 'a': configure +++
config term
Enter configuration commands, one per line.  End with CNTL/Z.
TB7-NY-FIAB(config)#no logging console
TB7-NY-FIAB(config)#
3256:  Traceback (most recent call last):
3257:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 969, in process_dialog_on_handle
3258:      cmd_result = dialog.process(
3259:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialogs.py"", line 476, in process
3260:      return dp.process()
3261:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialog_processor.py"", line 321, in process
3262:      if self.expect_eval_statements(pat) is True:
3263:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialog_processor.py"", line 235, in expect_eval_statements
3264:      statement._action()
3265:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 852, in config_state_change
3266:      invalid_state_change_action(
3267:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 64, in invalid_state_change_action
3268:      raise StateMachineError(msg)
3269:  unicon.core.errors.StateMachineError: Expected device to reach 'config' state, but landed on 'enable' state.
3270: 
3271:  The above exception was the direct cause of the following exception:
3272: 
3273:  Traceback (most recent call last):
3274:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/connection.py"", line 765, in connect
3275:      output = self.connection_provider.connect()
3276:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/connection_provider.py"", line 229, in connect
3277:      self.init_handle()
3278:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/connection_provider.py"", line 298, in init_handle
3279:      self.execute_init_commands()
3280:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/connection_provider.py"", line 125, in execute_init_commands
3281:      con.configure(self.init_config_commands, prompt_recovery = self.prompt_recovery)
3282:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 264, in __call__
3283:      self.call_service(*args, **kwargs)
3284:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 949, in call_service
3285:      self.process_dialog_on_handle(handle, dialog, timeout)
3286:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 976, in process_dialog_on_handle
3287:      raise SubCommandFailure('Configuration failed', err) \
3288:  unicon.core.errors.SubCommandFailure: ('Configuration failed', StateMachineError(""Expected device to reach 'config' state, but landed on 'enable' state.""))
3289: 
3290:  The above exception was the direct cause of the following exception:
3291: 
3292:  Traceback (most recent call last):
3293:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Optimized-Deployment_and_Sanity-Job@2/services/commonlibs/sftopology.py"", line 5016, in send_device_cmd_no_shut_port
3294:      config_device_timezone(self.testbed.devices[dev])
3295:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Optimized-Deployment_and_Sanity-Job@2/services/commonlibs/platformlibs.py"", line 1573, in config_device_timezone
3296:      oRtr.configure(command)
3297:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 263, in __call__
3298:      self.pre_service(*args, **kwargs)
3299:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 872, in pre_service
3300:      super().pre_service(*args, **kwargs)
3301:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 167, in pre_service
3302:      self.connection.connect()
3303:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/connection.py"", line 772, in connect
3304:      raise ConnectionError('failed to connect to %s\n%s' %
3305:  unicon.core.errors.ConnectionError: failed to connect to TB7-NY-FIAB
3306:  ('Configuration failed', StateMachineError(""Expected device to reach 'config' state, but landed on 'enable' state.""))

===========
 See comments","['Auton', 'Issue', 'Optimized', 'Sanity']",Pawan Singh,Closed,Avril Bower
SEEN-512,https://miggbo.atlassian.net/browse/SEEN-512,[Auton]  Test_TC103_DNAC_External_Authentication  /   test2_enable_tacacs_attributes : Fails through script," 

*Reporter Analysis:* This tests fail through the script in the first test but works manually, Looks like there is a problem with the ''encryptionScheme': 'KEYWRAP',' Is it a defect or we need to remove the 'encryptionScheme': 'KEYWRAP', from payload ?

*Description:*  Error Code: 400 URL:[https://10.30.0.100/api/v1/aaa/7a1c970a-7dd3-4187-99ca-7a97e9d57ed6] Data:{'timeout': 30, 'data': '{""ipAddress"": ""10.30.0.101"", ""sharedSecret"": """", ""protocol"": ""RADIUS_TACACS"", ""role"": ""primary"", ""port"": 49, ""authenticationPort"": 1812, ""accountingPort"": 1813, ""retries"": 1, ""timeoutSeconds"": 10, ""isIseEnabled"": true, ""instanceUuid"": ""7a1c970a-7dd3-4187-99ca-7a97e9d57ed6"", ""rbacUuid"": ""526a8fe1-c627-461c-8a4a-4b0dd2e7605d"", ""state"": ""ACTIVE"", ""ciscoIseDtos"": [

{""subscriberName"": ""pxgrid_client_1662178406"", ""description"": """", ""password"": """", ""userName"": ""admin"", ""fqdn"": ""SSTB7-ISE.cisco.com"", ""ipAddress"": ""10.30.0.101"", ""trustState"": ""TRUSTED"", ""instanceUuid"": ""74d80b60-bdd5-1ed8-cf9a-0b60d8a4ba6b"", ""sshkey"": null, ""type"": ""ISE"", ""failureReason"": null, ""role"": ""PXGRID""}

, \{""subscriberName"": ""pxgrid_client_1662178406"", ""description"": """", ""password"": """", ""userName"": ""admin"", ""fqdn"": ""SSTB7-ISE.cisco.com"", ""ipAddress"": ""10.30.0.101"", ""trustState"": ""TRUSTED"", ""instanceUuid"": ""cccd38ec-061f-4f85-9240-85c5774ddcb9"", ""sshkey"": """", ""type"": ""ISE"", ""failureReason"": null, ""role"": ""PRIMARY""}], ""externalCiscoIseIpAddrDtos"": [], ""encryptionScheme"": ""KEYWRAP"", ""messageKey"": """", ""encryptionKey"": """", ""useDnacCertForPxgrid"": false, ""iseEnabled"": true, ""pxgridEnabled"": true}'} Headers:\{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2MzEyYjVkNmQ1ZGIxYTQyMzdmNzk1ZTEiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjYzMTJiNWQ1ZDVkYjFhNDIzN2Y3OTVlMCJdLCJ0ZW5hbnRJZCI6IjYzMTJiNWQ0ZDVkYjFhNDIzN2Y3OTVkZSIsImV4cCI6MTY2MjI4MjkwNiwiaWF0IjoxNjYyMjc5MzA2LCJqdGkiOiJmYmRkMmFkNi03MGM3LTQ0MGYtOGFhYS03ZTU1ZTMzOTJlY2IiLCJ1c2VybmFtZSI6ImFkbWluIn0.1krVTiyvMdA2AEKFmRqzLoSSQ9zIqAmBYLMuf_SBhZEZ_4IpL3YApDhmrABW1v_3MrEeNVCZnoUa-K-XE4HiaSK-4WPBxh34rXuQEf6UBzUGI0HQCLa0Gj5UMDWIrX328o4TtAVGN0qfB8VcoEDWnHgmiGXh6em-aOmJGTh_jM0fS3XwsQ1iATKZAYzkrHFzbQsuk2OdtM7kpm3NH8Ap47QdhaG02sJQZxRCVJNylDzKEvEp-9ZwhDWgnW3j8JyorHwA_Eh5ps70xewQI2Doo1IEi9hgxwoQr_21lt4hpy2Dr5e3-xa_P2hmHknbOcOQRZhRpGZyLSQuy59sKOfxAA;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:{""response"":

{""errorCode"":""NCND00017"",""message"":""NCND00017: Update does not support modifying the following attributes: [EncryptionScheme]"",""href"":""/aaa/7a1c970a-7dd3-4187-99ca-7a97e9d57ed6""}

,""version"":""1.0""}

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* All the sanity scripts

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0*

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2154512&size=22954&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep04_00:49:35.805391.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:*

*Testbed details:* NA",2022-09-05T04:11:44.616+0000,"Issue is seen in latest log too with Guardian P2: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=78694760&size=37440&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep18_16:11:21.732010.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

  [cc0d36df4d8|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/cc0d36df4d898605df1c21149f4f9492fd85752b]","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity']",Raji Mukkamala,Closed,Avril Bower
SEEN-513,https://miggbo.atlassian.net/browse/SEEN-513,[Auton]test3_import_Ekahau_file test is not running due to aetest.loop," 

*Reporter Analysis:*  The test test3_import_Ekahau_file is not running, looks like an issue with following code 

@aetest.loop(ekahau_type=['ekahau_without_lat_long', 'ekahau_with_lat_long', 'ekahau_without_lat_long'],
 expect_failure=[True, False, False])

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep04_22:51:31.169329.zip&atstype=ATS]*

 *Issue Type : Fix* 

*Pass Log:*

*Testbed details:* NA",2022-09-05T06:12:58.279+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ebcb444eb3dc344852c94923cc34f8d80cfb3c4f,"['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'Sanity']",Raji Mukkamala,Closed,Avril Bower
SEEN-514,https://miggbo.atlassian.net/browse/SEEN-514,[Auton] [Optimized]assurance_fabric_assurance.py-155-SDAFabricAssurance  /   Test_TC11_verify_SDA_fabric_issue  Loopback ip missing," 

*Reporter Analysis:*  Loopback ip missing in optimized code for [SDAFabricAssurance|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-155-SDAFabricAssurance&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  usecase

*Description:*  
 4053: +------------------------------------------------------------------------------+
 4054: | Starting section test2_verify_fabric_dhcp_issue |
 4055: +------------------------------------------------------------------------------+
 4056: Cannot track test: tracking auth info must be set in order to transfer test tracking data
 4057: Executing testcase Test_TC11_verify_SDA_fabric_issue test 11.2 ""test2_verify_fabric_dhcp_issue"".
 4058: Verifying Fabric Issues on the all the Border
 4059: Traceback (most recent call last):
 4060: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Optimized-Deployment_and_Sanity-Job@2/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 4061: result = testfunc(func_self, **kwargs)
 4062: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Optimized-Deployment_and_Sanity-Job@2/testcases/sanityusecases/SDAFabricAssurance/assurance_fabric_assurance.py"", line 232, in test2_verify_fabric_dhcp_issue
 4063: if not dnac_handle.verify_fabric_issues_health_scores():
 4064: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Optimized-Deployment_and_Sanity-Job@2/services/dnaserv/lib/api_groups/assurance_fabric/group.py"", line 102, in verify_fabric_issues_health_scores
 4065: deviceip = self.services.dnaconfig.testbed.devices[dev].lb_ip
 4066: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/topology/device.py"", line 559, in __getattr__
 4067: raise AttributeError(""'Device' object has no attribute '%s'""
 4068: AttributeError: 'Device' object has no attribute 'lb_ip'
 4069: Test returned in 0:00:00.010581
 4070: Errored reason: 'Device' object has no attribute 'lb_ip'
 4071: 
 *Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-155-SDAFabricAssurance&begin=978451&size=7673&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Pass Log:*

*Testbed details:* NA",2022-09-05T06:19:49.211+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ebc7b30ff9fba196386d3d65246bad864ab64c43,"['Auton', 'Issue', 'Optimized', 'Sanity']",Tran Lam,Closed,Avril Bower
SEEN-515,https://miggbo.atlassian.net/browse/SEEN-515,[Auton] -policyApplicationPolicy  /   Test_TC2_wired_app_policy Policy validation to be update," 

*Reporter Analysis:*  The policy validation needs to be updated, it's not validating the policy correctly for each interface. for example below access-point connected interface has a different policy, but test is not validating it

 

interface TwoGigabitEthernet1/0/1\n 
 description Interface type ACCESS_POINT onboarded by sol script\n
 switchport access vlan 1025\n
 switchport mode access\n
 load-interval 30\n 
 access-session inherit disable interface-template-sticky\n 
 access-session inherit disable autoconf\n 
 no macro auto processing\n
 spanning-tree portfast\n 
 spanning-tree bpduguard enable\n 
 service-policy input prm-MARKING_IN\n
 interface TwoGigabitEthernet1/0/2\n description Interface type

 

code: 

match1_count, match2_count, match3_count = 0,0,0
 for match in match_group:
 interface = ""inter"" + re.findall(""(face\s+\S+\d+/\d+/\d+)"", match)[0]
 match1=re.findall(""(service-policy output DNA-dscp#APIC_QOS_Q_OUT)"", match)
 match2=re.findall(""(service-policy input DNA-MARKING_IN)"", match)
 match3=re.findall(""(service-policy input DNA-APIC_QOS_IN)"", match)
 match1_count += len(match1)
 match2_count += len(match2)
 match3_count += len(match3)
 if not (len(match1) | len(match2) | len(match3)):
 # if it fails, we just print the interface that has neither of the service policy config
 interface_failures.append(interface)
 if len(match_group) <= match1_count+match2_count+match3_count:
 self.log.info(""service-policy configs service-policy output DNA-dscp#APIC_QOS_Q_OUT/service-policy input DNA-MARKING_IN/service-policy input DNA-APIC_QOS_IN is configured successfully on {} each interfaces"".format(dev))
 else:
 faileddev.append(""Failed to push any of the service policy config (DNA-dscp#APIC_QOS_Q_OUT, input DNA-MARKING_IN, DNA-APIC_QOS_IN) on interfaces: {}\n"".format(interface_failures))

 

2nd failure for wireless policy:
 5768: Resource path full url: [https://10.30.0.100/api/v1/task/47f16b86-acd3-4c15-9a35-72a17b09711d]
 5769: \{'endTime': 1662268634451, 'data': 'workflow_id=0;cfs_id=0;rollback_status=not_supported;rollback_taskid=0;failure_task=NA;processcfs_complete=false', 'version': 1662268634451, 'startTime': 1662268634437, 'progress': 'TASK_INTENT', 'errorCode': 'NCSP01001', 'serviceType': 'NCSP', 'failureReason': ""NCSP01001: Error occurred while processing the 'provision' request. Additional info for support: taskId: '47f16b86-acd3-4c15-9a35-72a17b09711d'. Empty 'cfs create, update and delete lists' provided in the request."", 'isError': True, 'instanceTenantId': '6312b5d4d5db1a4237f795de', 'id': '47f16b86-acd3-4c15-9a35-72a17b09711d'}
 5770: Library group ""task"" method ""wait_for_task_complete"" returned in 0:00:00.027414
 5771: Policy deletion failed:NCSP01001: Error occurred while processing the 'provision' request. Additional info for support: taskId: '47f16b86-acd3-4c15-9a35-72a17b09711d'. Empty 'cfs create, update and delete lists' provided in the request.
 5772: Library group ""app_policy"" method ""delete_wireless_policy"" returned in 0:00:00.161323
 5773: Test returned in 0:00:00.252381
 5774: Failed reason: Result :: wireless policy at PSK deletion failed
 5775: The result of section test5_delete_existing_wireless_policy is => FAILED
  
 log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=2433011&size=16573&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
  
 *Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* Sanity all the scripts

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=396862&size=375313&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Pass Log:*

*Testbed details:* NA

*Issue Type :* Fix",2022-09-05T07:12:55.636+0000,"PRs:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5639/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5639/overview]
* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5640/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5640/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5641/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5641/overview]","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'Sanity']",ThangQuoc Tran,Resolved,Avril Bower
SEEN-516,https://miggbo.atlassian.net/browse/SEEN-516,[Auton] - Test_TC161_Brownfield_Workflow_Aireos - Script Should Skip the TC If Its Not Applicable,"Hi Andrew,

        In our sanity testing we observed that the ""TC161_Brownfield_Workflow_Aireos"" is executing and failing even though WLC is not present on the Testbed where we executed this TC. So the testcase should check device role and if no WLC is found then this test should be skipped.

 *Error Snip:*

!image-2022-09-06-16-37-52-372.png!

 

*Branch Name:*  private/Groot-ms/sanity_api_auto

*Script file / Usecase:* solution_test_sanityecamb_lan.py / Test_TC161_Brownfield_Workflow_Aireos

*Source Team:*  Sanity

*Issue Seen first time or day0 issue:* Seen 1'st time

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=18846111&size=4363&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep05_01:43:16.015757.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:* NA

*Testbed details:* NA

 

Thanks,

Vijayakumar G.",2022-09-06T11:14:52.196+0000,"As code has been updated since then, have requested tester to see if issue is still coming Changing logic to something more simple, and not reliant on testcase name.

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4145/overview]

Ghost

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4146/overview]

Groot

 

 

 

 ","['Auton', 'Ghost', 'Groot', 'Issue']",Andrew Chen,Resolved,Avril Bower
SEEN-517,https://miggbo.atlassian.net/browse/SEEN-517,[Auton] enhance retry log for Test_TC189_generate_Port_Reclaim_report test2_generate_Port_Reclaim_JSON_report,"The logging needs to be better to show the retries.

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=48673695&size=14955&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep04_09:41:17.559194.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

!Screen Shot 2022-09-06 at 1.01.47 PM.png|width=964,height=235!

Please use better ""Exception"" construct to avoid this kind of logging.",2022-09-06T20:07:58.994+0000,"PRs:

* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5472/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5472/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5470/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5470/overview]
* Groot: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5468/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5468/overview] [~accountid:63f50bd34c355259db9ccc4d], below three PRs have been approved and merged to respective branches.
Feel free to close this Auton.","['Auton', 'Ghost', 'Groot', 'Halleck', 'Issue']",ThangQuoc Tran,Closed,Avril Bower
SEEN-518,https://miggbo.atlassian.net/browse/SEEN-518,"[Auton] Test_TC15_clear_device_onboarding_virtual_segment_mapping  /   test1_clear_device_onboarding_virtual_segment_mapping , Test failed but result is pass"," 

*Reporter Analysis:* For one of the VN WiredVNFBLayer2, segment removal failed, but the test result is passed. Test must be fail if any of the segment removal fails

*Description:  WiredVNFBLayer2*
5455:  api_switch_call called:
5456:  \{'params': {'limit': 25, 'order': 'DESC', 'sortBy': 'lastUpdateTime', 'source': 'EXTERNAL', 'type': 'DEFAULT'}}
5457:  Resource path full url: [https://172.35.16.152/api/v1/scheduled-job]
5458:  Task with des Segment Removal for WiredVNFBLayer2-Global/USA/SAN_JOSE_US_SJ_Fabric1 on time: 1662592609.1544988 found
5459:  Task done with expected status COMPLETED found
5460:  The Schedduled Job failed: with reason \{'id': 'c9db8ae6-9b05-448a-94cd-9548b3f99d6e', 'triggeredJobTaskId': '294f061e-510c-4417-b347-853b40f1a279', 'triggeredTime': 1662592614073, 'status': 'FAILED', 'failureReason': ""NCSP11001: User intent validation failed while processing the 'modify' request. Additional info for support: taskId: '294f061e-510c-4417-b347-853b40f1a279'."", 'triggeredJobId': 'c9db8ae6-9b05-448a-94cd-9548b3f99d6e'}
5461:  Library group ""schedule-job"" method ""check_status_of_externalScheduled_jobs_with_des"" returned in 0:00:20.735029
 

 

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:*  testcases/mega_topo/dnac_cleanup_script.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:* Seen first time 

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1223975&size=602533&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep07_16:06:41.709223.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Pass Log: NA*

*Testbed details:* NA !image-2022-09-07-18-12-48-915.png!",2022-09-08T01:04:12.939+0000,[2a3db976192|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/2a3db9761924c054106cbfd43aeb9ac9195acc2d],"['Auton', 'Ghost', 'Issue', 'Sanity']",Raji Mukkamala,Closed,Avril Bower
SEEN-519,https://miggbo.atlassian.net/browse/SEEN-519,[Auton][IBSTE] : Clear pass feature code has to be commited to IBSTE script,"Hi 

 

Clear pass feature code has to commited to IBSTE scripts accross guardian and groot branch",2022-09-08T04:03:04.571+0000,"Clear pass need another external AAA server device, Aruba, we don't have that. Only1 used in sanity. So no plan to pull it in IBSTE.","['Auton', 'Enhancement', 'Groot', 'Guardian', 'IBSTE']",Pawan Singh,Resolved,Avril Bower
SEEN-520,https://miggbo.atlassian.net/browse/SEEN-520,[Auton] [Groot] Interface onboarding config preview fails with error in modify request,"*Uber ISO Version tested :* 

Promoted Groot RC4 Uber ISO - *2.1.560.70508,* *Non-FIPS, PUBSUB enabled*
 Promoted Groot RC6 Uber ISO - *2.1.560.70.517, Non-FIPS, PUBSUB enabled*

*Script Name:*
 solution_test_3sites_sjc_nyc_sf_mdnac_dr.py 

*Testbed :* MSTB1
  

*Description :* 

We see interface onboarding for config preview has failed with below errors. However, interface onboarding operation goes through fine. Looks like it has something to do with race condition during multiple parallel operations being executed at that point. 

*Error during Groot RC4 execution:*

_NCSP11051: Error occurred while processing the 'modify' request. A different version of the same user intent already exists in the database. Additional info for support: taskId: '9ee401a9-139f-46e0-bd68-59b3f29bad78'. Name: 'SJ-FE-9500'. Incoming resourceVersion: '12'. resourceVersion in the database: '15'._ 

*Error during Groot RC6 execution:*
 _NCSP11051: Error occurred while processing the 'modify' request. A different version of the same user intent already exists in the database. Additional info for support: taskId: '9efcfde4-7681-4bac-86de-c4b22550c0b9'. Name: 'SJ-FE-9500'. Incoming resourceVersion: '21'. resourceVersion in the database: '24'._

*Logs on Groot RC4:*

1) [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug10_08:45:26.375651.zip&atstype=ATS] -> TC36 to TC41

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=979061&size=356846&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug10_20:59:41.194573.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] -> Refer TC42

*Logs on Groot RC6:* 

*1)* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep07_05:50:25.755594.zip&atstype=ATS] -> TC29 to TC35

2) [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep07_07:28:28.421836.zip&atstype=ATS] -> TC36 to TC41

3) [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1925713&size=1971287&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep07_09:40:20.578121.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] -> Refer TC43

 

 ",2022-09-09T08:47:46.342+0000,"The issue in task response is: 
6376:  Task done with expected status COMPLETED found
6377:  The Schedduled Job failed: with reason {'id': '652683f7-87ae-4b26-a827-7129a97d5f50', 'triggeredJobTaskId': '1ad8a894-1ac7-43dd-8e0a-ab26439d3391', 'triggeredTime': 1660191328344, 'status': 'FAILED', 'failureReason': '*Failure reason is too large to be displayed. Please refer to log file for module: PROVISION*', 'triggeredJobId': '652683f7-87ae-4b26-a827-7129a97d5f50'}
6378:  Library group ""schedule-job"" method ""check_status_of_externalScheduled_jobs_with_des"" returned in 0:00:20.368945


===========
Raise a defect for it. The script can not handle it unless a proper error response is returned in the task API. If it is related to the version mismatch, The task response should indicate it is due to a version mismatch. The version mismatch is already handled in the script. 

This could be any DNA issue, we can not simply ignore it as it is a DNAC issue to send a proper error response so it can be handled. 

==========

 Product issue, for script will only handle version uptick all other error to be raised with SPF team.","['Auton', 'Groot', 'Issue', 'MSTB1', 'Multisite']",SANDEEP SHIVARAMAREDDY,Closed,Avril Bower
SEEN-521,https://miggbo.atlassian.net/browse/SEEN-521,[Auton] Test_TC4_DNAC_RBAC_create_users_roles/test4_verify_RBAC_users- Need enhancement for verifying RBAC users,"*Reporter Analysis:* 

The sub-testcase of Test_TC4_DNAC_RBAC_create_users_roles/test4_verify_RBAC_users - need enhancement for verifying RBAC users. In current script, it is only counting if the number of users are more than or equal to 0. 
Instead the script should verify the api response and check if created users are listed or not. 

 

*Description:  Code Snippet from services/dnaserv/lib/api_groups/idendities_user/group.py*

@library_wrapper
def verify_users(self, count = 0):

 if(self.services.dnatype == ""maglev""):
  response = self.services.base.system.call_api(method=""GET"", resource_path=""/v1/identitymgmt/users"")
  response = \{'response':response['response']['users']}
 else:
  response = self.services.api_switch_call(method=""GET"", resource_path=""/v1/user"")

if*(len(response['response'])) >= count:*
 return PASS
 return FAIL

 

*Branch Name:  private/Groot-ms/sanity_api_auto,private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* 

[Test_TC4_DNAC_RBAC_create_users_roles|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2496302&size=82448&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug15_04:20:22.448173.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] / [test4_verify_RBAC_users  |https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=295086&size=940&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep08_01:35:55.516651.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0*

*Fail Log:N/A*

*Pass Log:[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2576673&size=1911&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug15_04:20:22.448173.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]* 

*Testbed details:* NA",2022-09-09T13:20:35.823+0000,"To do:
 # Get the RBAC users from DNAC 
 # Validate the user list from DNAC with input file users. With latest code, users are verified from DNAC  def verify_users(self, count=0):
        [http://self.log.info|http://self.log.info|smart-link] (f""Total count of additional users: {len(self.services.input_data['USERS_LIST'])}"")
        if count == 0:
            count = len(self.services.input_data['USERS_LIST'])

{noformat}    response = self._get_users()
    return len(response['response']) >= count{noformat}

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/services/dnaserv/lib/api_groups/rbac/group.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#381|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/services/dnaserv/lib/api_groups/rbac/group.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#381]","['Auton', 'Enhancement', 'Ghost', 'Groot', 'Guardian', 'Halleck', 'Sanity', 'hulk']",Raji Mukkamala,Resolved,Avril Bower
SEEN-522,https://miggbo.atlassian.net/browse/SEEN-522,[Auton] Test_TC5_DNAC_Server_Settings_NTP_DHCP_AAA_DNS_settings/ test1_subtest1_verify_adding_credentials  Correcting Typo," *Reporter Analysis:* In below testcase, the verification log should be for HTTP READ 

Test_TC5_DNAC_Server_Settings_NTP_DHCP_AAA_DNS_settings / [test1_subtest1_verify_adding_credentials|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=296940&size=996&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep08_01:35:55.516651.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

 

 

*Description:  The error from log or more info* 

self.log.warn(""HTTP_WRITECredentialsConfigured:\{0}"".format(taskStatus['failureReason']))

else:

self.log.info(""HTTP_WRITECredentialsConfigured"")

 

*Branch Name:  private/Groot-ms/sanity_api_auto,* *private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* testcases/forty_eight_hour/solution_test_sanityecamb_lan.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0*

*Fail Log:NA*

*Pass Log:[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2579506&size=40700&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_auto_job.2022Aug15_04:20:22.448173.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]* 

*Testbed details:* NA",2022-09-09T13:43:29.428+0000,"The fix is committed to Ghost: [7af8c264dd0|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/7af8c264dd09c1f7f1e6e321d882424a62a1ea8a]

Fix for Groot: [6bac56d9a02|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6bac56d9a02d580317428f4f5dfd16a54b2d3731] 

Fix for Guardian: [3f3d73a8048|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3f3d73a8048df5dedd4972328997d34e7d5710c4]

 ","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Ashwini R Jadhav,Closed,Avril Bower
SEEN-524,https://miggbo.atlassian.net/browse/SEEN-524,[Autons][Optimized code]test2_update_vlan_membership usecase is missing,"The following use case is not added in Groot and Ghost, Please add in the usecase for this.

 

test2_update_vlan_membership",2022-09-12T23:05:06.786+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3653/overview] -groot

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3654/overview] -ghost PR Merged by Raju","['Auton', 'Ghost', 'Groot', 'Issue', 'Optimized']",Andrew Chen,Closed,Avril Bower
SEEN-525,https://miggbo.atlassian.net/browse/SEEN-525,[Auton] [Optimized ] Testcases handling in case of test not integrated/hardware limitation," 

*Reporter Analysis:*  Testcase handling based on usecase

 

1) We need to handle the below test case to SKIP or have a different handler. As discussed with Tran,  we want to stop the test if anything fails in the SDAExtnodeOnboarding usecase, Also this test is failing if AEN is not enabled(setup doesn't have AEN, ISE version is lesser, device version is not supported). 

 

Log where test is failed due to AEN not being enabled: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep12_17:40:20.260724.zip&atstype=ATS]

 

 

2) Rep ring passes, Need to be skipped when we don't have the hardware. (Fix is needed from Guardian)

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=26841961&size=183683&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep05_01:43:16.015757.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

 3) TC [Test_TC160_Brownfield_Workflow_Aireos |https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=919173&size=160909&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep13_13:38:11.903389.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Test should be skipped for the Airos or ewlc based on the setup  (Fix is needed from Guardian)

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=18841008&size=155688&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep05_01:43:16.015757.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

 

*Description:*  

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:* 

*Fail Log:*

*Pass Log:*

*Testbed details:* NA",2022-09-13T20:42:41.808+0000,"161:
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/b1d164796e9b1b4573ff0398006e4de9330f90cc

AEN:
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/16c3b0a338412bb5ecebd2880a08dcad4a818a32

 See comments. fixed on Groot/Ghost.","['Auton', 'Ghost', 'Groot', 'Issue', 'Sanity']",Pawan Singh,Resolved,Avril Bower
SEEN-527,https://miggbo.atlassian.net/browse/SEEN-527,[Auton] - [Test_TC5_remove_all_ext_node_from_fabric] [test2_clear_extended_device_all_interface] - TC5 Giving FALSE PASS in Cleanup Script,"*Reporter Analysis:* The Below mentioned sub TC should fail because ISE was not active. In the log it is printing failure but it is giving FALSE PASS.

[est_TC5_remove_all_ext_node_from_fabric|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=308427&size=133622&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep12_07:53:28.297615.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[test2_clear_extended_device_all_interface|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=317643&size=23723&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep12_07:53:28.297615.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Description:*  

*!image-2022-09-14-15-30-42-744.png!*

*Branch Name:*  private/Groot-ms/sanity_api_auto

*Script file/Usecase:* dnac_cleanup_script.py / [Test_TC5_remove_all_ext_node_from_fabric|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=308427&size=133622&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep12_07:53:28.297615.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:*  Sanity

*Issue Seen first time or day0 issue:* First time

*Fail Log:* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=317643&size=23723&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep12_07:53:28.297615.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:* NA

*Testbed details:* NA

 ",2022-09-14T10:02:50.460+0000,"This should be a product/setup issue. I see the commit is in:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/services/dnaserv/lib/api_groups/wired_interface_onboarding/group.py?until=ed4953a15fa0edff19e321fc3f170f57c01eb0d1]

  Found One fix was needed on esxivm_branch:

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6a1c4f709503caf8a5bc3b64fa84564b84bd375e]

 

 ","['Auton', 'Ghost', 'Groot', 'Product_Issue', 'Sanity']",Nethra Thorali Suthagar,Resolved,Avril Bower
SEEN-528,https://miggbo.atlassian.net/browse/SEEN-528,[Auton]  :TC167_wireless_posture_url_filter," 

*Reporter Analysis:* Please sync/commit the TC  of the below script & Optimize sanity,

solution_test_sanityecamb_lan.py
 solution_test_sanityecamb_cert_lan.py
 solution_test_sanityecamb_cert.py

LAN-A, FQDN, FQDN + LAN

*Description:  N/A*

*Branch Name:  private/Groot-ms/sanity_api_auto*

                         *private/Ghost-ms/sanity_api_auto*
                          *private/frey -ms/sanity_api_auto*
                                                  

 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:N/A*

*Pass Log:N/A*

*Testbed details:* NA",2022-09-14T10:10:59.920+0000,,"['Auton', 'Frey', 'Ghost', 'Groot', 'Issue', 'Sanity', 'optimized']",Raji Mukkamala,Closed,Avril Bower
SEEN-529,https://miggbo.atlassian.net/browse/SEEN-529,[Auton]  :TC168_wired_AEN_posture," 

*Reporter Analysis:* Please sync/commit the TC  of the below script and Optimize sanity,

solution_test_sanityecamb_lan.py
 solution_test_sanityecamb_cert_lan.py
 solution_test_sanityecamb_cert.py

LAN-A, FQDN, FQDN + LAN

*Description:  N/A*

*Branch Name:  private/Groot-ms/sanity_api_auto*

                         *private/Ghost-ms/sanity_api_auto*
                          *private/frey -ms/sanity_api_auto*
                                                  

 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:N/A*

*Pass Log:N/A*

*Testbed details:* NA",2022-09-14T10:53:56.099+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/2c733ff3e6a8045f7ba422b90259c916811476d3]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d41c393c6023df871e5f2244da0b0bd8781e8d5f]

// [~620b8357878c2f00729881c8] Please sync the commits to all sanity branches

 ","['Auton', 'Frey', 'Ghost', 'Groot', 'Issue', 'Optimized', 'Sanity']",Raji Mukkamala,Closed,Avril Bower
SEEN-530,https://miggbo.atlassian.net/browse/SEEN-530,[Auton]Groot:  Test_TC186_Check_Network_Security_Trust_Settings/test3_verify_changed_network_security_trust_settings," 

*Reporter Analysis:* Method, we are checking version1 == self.version+1 right
                                 each time the revocation check is enabled or disabled, it usually increments by 1
                                 in this failed case version changed from 23 to 28,



*Description:*  *Code Snippet:***

@aetest.test
 @TestWrapper()
 def test3_verify_changed_network_security_trust_settings(self, dnac_handle):
 instance_uuid1, revocation_check_enabled1, version1 = \
 dnac_handle.get_security_and_trust_instanceuuid_and_revocationcheckenabled()
 if instance_uuid1 != self.instance_uuid and revocation_check_enabled1 != self.revocation_check_enabled and \
 version1 == self.version+1:
 +*Snip from fail Log:*+

{'instanceType': 'timezone', 'instanceUuid': '238e882f-cd55-4fbd-890a-b459c6e4a298', 'namespace': 'global', 'type': 'timezone.setting', 'key': 'timezone.site', 'version': 13, 'value': ['America/Los_Angeles'], 'groupUuid': '-1', 'inheritedGroupUuid': '', 'inheritedGroupName': ''}

,

{'instanceType': 'securityandtrust', 'instanceUuid': 'ea53f546-0ec6-4219-8f44-193b5cc71db7', 'namespace': 'global', 'type': 'securityandtrust.setting', 'key': '**securityandtrust.revocationcheck', 'version': 28**, 'value': [\\{'revocationCheckEnabled': True}

], 'groupUuid': '-1', 'inheritedGroupUuid': '', 'inheritedGroupName': ''},

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*+Fail Log:+*
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep09_09:09:41.362471.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]

*+Pass Log:+*
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fsaimanga-sjc%2Fsol-auto%2Fpy3610%2Fusers%2Fsaimanga%2Farchive%2F22-06%2Fsanity_TB1.2022Jun13_06:57:29.031394.zip&reqseq=&ats=%2Fws%2Fsaimanga-sjc%2Fsol-auto%2Fpy3610&submitter=saimanga&from=trade&view=all&atstype=PYATS]

*Testbed details:* NA
*Note* : Please double commit in Optimized code ",2022-09-14T11:45:09.145+0000,"Thank you [~620b8357878c2f00729881c8] for the great reporting for the issue.  TC is  passed Ghost, hence moving  to close state
Ghost Pass log:
https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct06_23:59:27.503695.zip&atstype=ATS","['Auton', 'Ghost', 'Groot', 'Issue', 'Optimized', 'Sanity']",Moe Saeed,Closed,Avril Bower
SEEN-532,https://miggbo.atlassian.net/browse/SEEN-532,[Auton] [Groot] [Ghost] - Test_TC0_dnac_initial_cleanup/test1dnac_initial_cleanup,"*Uber ISO Version tested :* Ghost Uber ISO - *2.1.610.70412, FIPS*

*Script Name :* solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB2

*Branch :*  private/Ghost-ms/api-auto\

*Failed Log :* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=445652&size=704307&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fauto_MS_job.2022Sep13_08:19:32.576131.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Earlier Passed Log :* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=444249&size=613266&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fauto_MS_job.2022Aug24_03:13:21.762356.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Description :*  DNAC initial cleanup is getting failed for empty fresh installed Ghost FIPS cluster

Error showing in log


Unable to retrieve attribute id for device SJC-T-9500
Unable to retrieve attribute id for device SFO-FIAB-9400
Unable to retrieve attribute id for device FW-9800-11
Unable to retrieve attribute id for device FW-9800-12
Unable to retrieve attribute id for device NYC-FB-ASR
Unable to retrieve attribute id for device NYC-CP-9300
Unable to retrieve attribute id for device NYC-FE-9400
Unable to retrieve attribute id for device NYC-FE-9300
Unable to retrieve attribute id for device NYC-FB-ISR
Unable to retrieve attribute id for device SJC-FE-9300-1
Unable to retrieve attribute id for device SJC-FE-9300-2
Unable to retrieve attribute id for device NYC-FE-9400
Unable to retrieve attribute id for device NYC-FE-9300
Unable to retrieve attribute id for device SJC-IM-9300-1
Unable to retrieve attribute id for device SJC-IM-9300-2
App SDAVC is not found
custom_ssid Design not found in Advanced_SSID_Configuration capability, probably cleaned setup.
guest_ssid Design not found in Advanced_SSID_Configuration capability, probably cleaned setup.
dhcp_enabled Design not found in Advanced_SSID_Configuration capability, probably cleaned setup.
custom_ipv6 Design not found in Global_IPv6_Configuration capability, probably cleaned setup.
custom_multicast Design not found in Multicast_Configuration capability, probably cleaned setup.
custom_cleanair Design not found in CleanAir_Configuration capability, probably cleaned setup.
Failed reason: Clearing of DNAC failed!",2022-09-14T13:15:22.449+0000,"Same issue is also observed during Groot RC7 execution on MSTB1 (DR+MDNAC testbed)

*Uber ISO Version tested :* 
 Promoted Groot RC7 Uber ISO - *2.1.560.70.523, Non-FIPS, PUBSUB enabled, CSRF flag enabled*

*Script Name:*  solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py

*Testbed :* MSTB1

*Description :* 

DNAC cleanup on Active, Recovery and Reader is failing on freshly installed clusters even though the cleanup workflow has gone fine. Looks like false failures.

*Failed logs:* 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep20_05:30:28.116673.zip&atstype=ATS] -> Refer TC0.3

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep20_05:45:54.274909.zip&atstype=ATS] -> Refer TC0.4

 


   Same issue is also observed during Ghost execution on AWS-MS (DR+MDNAC testbed)

*Uber ISO Version tested :* 
 Promoted Ghost Uber ISO - 2.1.610.70412*, Non-FIPS, PUBSUB enabled, CSRF flag enabled*

*Script Name:*  solution_test_3sites_sjc_nyc_sf.py

*Testbed :* AWS-MS

*Description :* 

DNAC cleanup  is failing on freshly installed clusters even though the cleanup workflow has gone fine. Looks like false failures.

*Failed logs:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=759398&size=713519&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fauto_MS_job.2022Sep21_04:16:56.833493.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]–Refer TC0.3

  Commit: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f9dfd26f354de81b132a54cb2863c1f29b5d9278 Issue was fixed and got pass log in Groot Pre RC 2.1.563.70111
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=491253&size=761225&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_MS_job.2022Oct10_05:03:11.058228.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Issue is still observed on MSTB1 (DR testbed) during Groot Patch1 RC2 testing.

Automation code base used - private/Groot-ms/api-auto

Failed logs: 
1) Reader cluster node cleanup Failure - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fsr_mb_multi_sites_mdnac.2022Nov08_01:45:16.161737.zip&atstype=ATS] -> Refer TC0.5

2) Main node cluster cleanup Failure - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fsr_mb_multi_sites_mdnac.2022Nov08_02:44:56.051920.zip&atstype=ATS] -> Refer TC0.6 Hey [~63f50bf5e8216251ae4d59cf] ,

Could you double check this, you commit a fix earlier regarding ssid scheduling.

thank you,

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f9dfd26f354de81b132a54cb2863c1f29b5d9278] [~63f50bf5e8216251ae4d59cf] / [~63f50bfce8216251ae4d59d5], what is the status on this ticket's need? Failure is not w.r.t Model config, it returns True when model config is not configured","['Auton', 'Ghost', 'Groot', 'Issue', 'MSTB1', 'MSTB2', 'Multisite']",Raji Mukkamala,Closed,Avril Bower
SEEN-535,https://miggbo.atlassian.net/browse/SEEN-535,[Auton] [Groot] [Ghost] - Need for change in configuration check for CBAR/NBAR config on devices.,"*Uber ISO Version tested :* 

Promoted Groot RC4 Uber ISO - *2.1.560.70508,* *Non-FIPS, PUBSUB enabled*
 Promoted Groot RC6 Uber ISO - *2.1.560.70.517, Non-FIPS, PUBSUB enabled*

*Script Name:*
 solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py, solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB1, MSTB2
  

*Description :* 

We observed configuration verification check on CBAR/NBAR enabled devices failed with config missing for ""ip nbar http-services"" not pushed on to devices.

On trying to check further with CBAR/NBAR Team (gbenavne@cisco.com), it was confirmed by them that there was recent changes done from Groot onwards releases +17.9.1 Polaris and higher version. 

In order to avoid performance issues ""ip nbar http-services"" was config push was replaced with ""platform wdavc serviceability"" config push.

So, for Groot and higher DNAC version + 17.9.1 and higher Polaris version we need the config cli check for ""ip nbar http-services"" to be replaced with ""platform wdavc serviceability"" config check.

*Example:*

NY-FE-9400#show run | in http-services
NY-FE-9400#show run | in platform wdavc
platform wdavc serviceability 
NY-FE-9400#

*Failed Log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2790502&size=900986&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep11_09:11:44.766398.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 ",2022-09-15T07:30:41.804+0000,"Wrongly updated info intended for another Jira ticket. Hence reopening the issue. https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/142371ccfe8a76b969e57bc5db0dfcbae4bf5cd3 Issue has been resolved. We see its working fine now.

Pass log on Groot Patch1 Pre-RC Uber - 2.1.563.70111 : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2714892&size=369403&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fsr_mb_multi_sites_mdnac.2022Oct25_08:53:40.012591.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Ghost', 'Groot', 'Issue', 'MSTB1', 'Multisite']",Tran Lam,Closed,Avril Bower
SEEN-536,https://miggbo.atlassian.net/browse/SEEN-536,[Auton] [Guardian] Generating DHCP server configs on Fusion fails after L2 Only Vlan commit,"*Uber ISO Version tested :* 

Promoted Guardian Patch2 RC1 Uber ISO - *2.1.515.70102,* *Non-FIPS, PUBSUB enabled, CSRF enabled*

*Script Name:*
 solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py,  solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB1
  

*Description :* 

 

After making changes as per below commit  - [7b3358d1533|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/7b3358d1533999177dbdf9601a9ebad730ba9a66]  to Solution input files corresponding to Multisite Guardian Code, we observed issues at Generating DHCP server configs on Fusion device - *TC3 and TC39*. These TCs are failing to generate the configs properly. Due to this Extended nodes, Aps and Sensors were not getting onboarded to the DNAC.

 

After removing the changes and re-executing, we see its passing fine. Could you please have a look into this and help to fix it? 
 
 *Failed logs after adding changes:*
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug26_07:05:55.363570.zip&atstype=ATS] -> Refer TC3.2 and TC3.3

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug28_07:54:25.754403.zip&atstype=ATS] -> Refer TC39.1 and TC39.2

 

*Pass logs after removing the changes:* 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug28_22:44:06.462401.zip&atstype=ATS] -> Refer TC39.1. Got passed after removing changes on solution json input file for Active DR cum Author node cluster.

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug28_23:39:48.768648.zip&atstype=ATS] -> Refer TC39.2. Got passed after removing changes on solution json input file for Reader node cluster.

 

In order to address this issue fix was again added - 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9edae302d61a897e2fe237b57d83c478246a2edc]

 This Auton is reported to have a track of this issue.",2022-09-15T07:52:35.600+0000,"Fix was added - 
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9edae302d61a897e2fe237b57d83c478246a2edc Will be verifing on upcoming Guardian/Ghost/Groot Regression testing and update our observations. Verified during Ghost testing on MSTB2. Hence closing the ticket.","['Auton', 'Guardian', 'Issue', 'MSTB1', 'MSTB2', 'Multisite']",Tran Lam,Closed,Avril Bower
SEEN-537,https://miggbo.atlassian.net/browse/SEEN-537,[Auton]:Ghost MS2 TB2: Test_TC28_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision/test2_verify_configuration_on_devices_fabric1,"*Uber ISO Version tested :* Ghost Uber ISO - *2.1.610.70412, FIPS*

*Script Name :* solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB2

*Branch :*  private/Ghost-ms/api-auto

*Failed Log :* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4227226&size=4273083&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fauto_MS_job.2022Sep14_08:07:34.878343.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Previous Passed Log :* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2413752&size=917912&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fauto_MS_job.2022Aug24_07:29:39.135051.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Description :*  We are oberseving that script is getting failed because of unable to find the snmp configs pushed in device. But when we check the devices snmp configs were actually pushed. But script is unable to get it and failing


ERROR Following line snmp-server host 204.192.1.114 version [23c]+ \S+ of expected config not present on device:
19249:  SJC-FB-9500
19250:  \{'result': True, 'output': 'show run | s snmp-server host\nsnmp-server host 204.192.1.144 version 3 priv xxxxxxxx \nsnmp-server host 8.8.8.8 version 3 priv xxxxxxxx \nSJC-FB-9500#'}
 
*Screenshot of configs present in device:* 

!image-2022-09-15-21-05-50-778.png|width=523,height=154!

 ",2022-09-15T15:37:03.249+0000,This was due to some incorrect configuration sent to maglev config wizard during cluster installation. It was later resolved after updating the correct IP address. Hence closing the ticket.,"['Auton', 'Ghost', 'Issue', 'MSTB2', 'Multisite']",Unassigned,Closed,Avril Bower
SEEN-542,https://miggbo.atlassian.net/browse/SEEN-542,[Auton] : Guardian - Test_TC18_DNAC_verify_creating_wireless_guest_portal  /   test1_subtest1_verify_addition_of_wireless_guest_policy_and_portal,"*Guardian Uber ISO* : 2.1.512.72132

*Script* : ** sdwan_ibste_multi_site_script.py

*Impacted Test Cases* : Test_TC18_DNAC_verify_creating_wireless_guest_portal / test1_subtest1_verify_addition_of_wireless_guest_policy_and_portal

 

*Full Log* : [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_sdwan.2022Aug03_23:44:05.765193.zip&atstype=ATS]

 

*Failure Log* : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2728035&size=60743&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_sdwan.2022Aug03_23:44:05.765193.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Failure Snip* :

Error Code: 406 URL:[https://10.195.247.212/api/v1/commonsetting/wlan/-1] Data:{'timeout': 30, 'data': b'[{""instanceType"": ""wlan"", ""namespace"": ""wlan"", ""type"": ""wlan.setting"", ""key"": ""wlan.info.GUEST"", ""value"": [

{""ssid"": ""GUEST"", ""profileName"": """", ""wlanType"": ""Guest"", ""authType"": ""open"", ""l3AuthType"": ""open"", ""webPassthrough"": false, ""sleepingClientEnable"": false, ""sleepingClientTimeout"": 720, ""authServer"": """", ""authSecServer"": """", ""redirectUrl"": """", ""peerIp"": """", ""isEnabled"": true, ""isEmailReqd"": false, ""isFabric"": true, ""fabricId"": null, ""isFastLaneEnabled"": false, ""isMacFilteringEnabled"": false, ""trafficType"": ""data"", ""scalableGroupTag"": """", ""passphrase"": """", ""portalType"": """", ""portalName"": """", ""redirectUrlType"": """", ""externalAuthIpAddress"": """", ""isBroadcastSSID"": true, ""sessionTimeOutEnable"": true, ""sessionTimeOut"": 1800, ""clientExclusionEnable"": true, ""clientExclusionTimeout"": 180, ""basicServiceSetMaxIdleEnable"": true, ""basicServiceSetClientIdleTimeout"": 300, ""directedMulticastServiceEnable"": true, ""neighborListEnable"": true, ""managementFrameProtectionClientprotection"": ""Optional"", ""fastTransitionOverTheDistributedSystemEnable"": false, ""fastTransition"": ""Disable"", ""authServers"": [""82.2.2.59""]}

], ""groupUuid"": ""-1""}]'} Headers:{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2MmMzMGE0ZjFlYWI3MjQzOGVkOTBhN2UiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjYyYzMwYTRlMWVhYjcyNDM4ZWQ5MGE3ZCJdLCJ0ZW5hbnRJZCI6IjYyYzMwYTRlMWVhYjcyNDM4ZWQ5MGE3YiIsImV4cCI6MTY1OTU5OTUyMSwiaWF0IjoxNjU5NTk1OTIxLCJqdGkiOiI4ZTZmOWZkNy1mNjMwLTQwM2UtYThmZC1jMjM4ZDBmMjhmODYiLCJ1c2VybmFtZSI6ImFkbWluIn0.edqs7df_KA16v0Owb6z3301HU_WQZHtEF6UPYMFFQtTBX8CY9ErG85Tv3ZvzxOP_GRZqaqWdTyh6Tdd0Z1RlpbB0kOYqNq56PEpFWkmcQzPHmJtq8tfiAIjXHz4RHFXJ9dmJzL5aUZol2LROT4dBEWcHi9JRsPGIwNMCLPeHwL_BTn178x6miBkI7oSnpIX1q6ExKs8F2c4JXZaJWK1-6g7MJwM-BVexr18n0xW9uwGDgV4H3jF_ObcNXly1YUo86S0A4Acz7QVDDGjB2bqfqelegAWB6omnNGiL1-1d-_B8lQ1M90HK645rGQBmkKhadbUQUrY-592zgB40MkRtsQ;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:{""response"":

{""errorCode"":""NCND00001"",""message"":""NCND00001: The Common Settings request has validation errors"",""detail"":""[\""NCND03054: AAA AuthServers are not applicable for an Open/Open-Secured SSID.\""]""}

,""version"":""1.0""}",2022-09-16T00:15:56.547+0000,"Original issue mentioned above is fixed as per PR for SEEN-421.

But still script gave way to other error as seen in debugging.

 

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2644696&size=1298509&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_sdwan.2022Sep21_17:51:40.466662.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3695/overview","['Auton', 'Groot', 'Guardian', 'Issue']",Andrew Chen,Resolved,Avril Bower
SEEN-543,https://miggbo.atlassian.net/browse/SEEN-543,[Auton] [Groot] - Cleanup for Transit Metadata for ISE to address issue in Shared Transit feature enablement,"*Uber ISO Version tested :* 

Promoted Groot RC5 Uber ISO - *2.1.560.70513**,* *Non-FIPS, PUBSUB enabled, CSRF enabled*

*Script Name:*
 solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py

*Testbed :* MSTB1
  

*Description :* 

During ongoing Solution Regression testing on *Groot RC5 (Uber - 2.1.560.70513)* on Multisite TB1 testbed (DR+MDNAC), we again run into Shared Transit SDA feature enablement issue. We see Transit SDA added on Active DR cum Author node cluster (10.195.243.109) is showing as *Shared Transit* and even on Reader node cluster (10.195.243.182) the Transit SDA added is showing as *Shared Transit*. When this issue is hit, we will not have any lisp sessions on the Transit device, and this will in turn affect onboarding of Extended nodes & APs.

*Failed log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug18_03:46:52.530330.zip&atstype=ATS] -> Please Refer TC29, where Transit SDA addition on Author node fails with error - Connectivity Domain configuration for reason:Provisioning failed due to invalid parameter. SDA Transit with the same name already exists

 

*Pass logs for ISE cleanup, ISE integration and mdnac role assignment:*
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug17_22:50:43.606751.zip&atstype=ATS] -> Please Refer TC0 and TC3

 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug18_00:07:32.323821.zip&atstype=ATS] -> Please Refer TC6, TC7 and TC8

 

This is same issue behavior we had observed during Groot RC1 testing where we had reported defect - [CSCwc53587|https://cdetsng.cisco.com/summary/#/defect/CSCwc53587], which eventually got moved to Auton and further tracked via Automation JIRA tickets - SEEN-361 & SEEN-367. Both of these JIRA tickets were addressed, and We did test using the Groot Automation code with these changes during Groot RC4 testing and we had not observed this issue there.

 

Looks like the fix given did not completely resolve the problem. 

 

In order to address this issue, fix was again added - 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ff0822522992ead5fcaedd4b7dae07dc7a633a05]

 This Auton is reported to have a track of this issue.",2022-09-16T06:07:08.891+0000,"Cleanup Metadata for ISE was added by below fix - 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ff0822522992ead5fcaedd4b7dae07dc7a633a05] Issue is no more observed. Tested during Groot RC6 (2.1.560.70517) Solution Regression testing.","['Auton', 'Groot', 'Guardian', 'Issue', 'MSTB1', 'Multisite']",Tran Lam,Closed,Avril Bower
SEEN-544,https://miggbo.atlassian.net/browse/SEEN-544,[Auton] [Groot] [Guardian] - Apply ANC Policy for concurrent mac fails,"*Uber ISO Version tested :* 

Promoted Groot RC1 Uber ISO - *2.1.560.70463,* *Non-FIPS, PUBSUB enabled, CSRF enabled*

*Script Name:*
 solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py

*Testbed :* MSTB1
  

*Description :* 

This is regarding few TC failures we are observing while integrating Guardian feature - ANC Policy ([https://wiki.cisco.com/display/EDPEIXOT/ANC+Policy+with+connectivity+control]) on MSTB1 (DR+MDNAC) testbed.

 

*Reference Log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug08_05:00:59.875142.zip&atstype=ATS] -> Please refer TC102 and TC103

 

We see that apply ANC Policy is failing with session as None for Second wired client. But when we go and check manually from DNAC, we are able to see the sessions available. Also apply of ANC policy is working fine manually from DNAC GUI. Also verify ANC Quarantine, remove ANC policy & remove ANC Quarantine works fine when tested manually. Looks like there is some issue from API calls.

 

The API content had changed on Groot. Fix was later given  - 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/864cf95009a23fd3e74c99ae2b81f46d94b9ea3f]

 

This Auton is reported to have a track of this issue.",2022-09-16T06:38:04.722+0000,"The API content had changed on Groot. Fix was later given  - 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/864cf95009a23fd3e74c99ae2b81f46d94b9ea3f] Issue is no more observed after the fix was added.

*Pass log on Groot RC1* : [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug09_04:26:06.176572.zip&atstype=ATS]

  Hi Tran,

Same issue pattern is now observed during Guardian testing as well. Hence reopening the ticket. Looks like we need to have fix on Guardian (private/Guardian-ms/api-auto) as well. 



*Uber ISO Version tested :* 

*Promoted Guardian Patch2 RC2 Uber ISO - 2.1.515.70134,* *Non-FIPS, PUBSUB enabled, CSRF enabled*

*Script Name:*
solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py

*Testbed :* MSTB1
 

*Failed Log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep28_01:05:49.413891.zip&atstype=ATS] -> Please refer TC102 Cherry Picked to Guardian.","['Auton', 'Groot', 'Guardian', 'Issue', 'MSTB1', 'Multisite']",Tran Lam,Resolved,Avril Bower
SEEN-545,https://miggbo.atlassian.net/browse/SEEN-545,[Auton] [Guardian] [Groot] - Need for Script enhancements for ANC Policy Guardian Feature,"*Uber ISO Version tested :* 

Promoted Groot RC6 Uber ISO - *2.1.560.70517,* *Non-FIPS, PUBSUB enabled, CSRF enabled*

*Script Name:*
 solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py

*Testbed :* MSTB1
  

*Description :* 

Need for script enhancements for the ANC Policy Guardian feature based on the current behavior.

 

*Reference Pass log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug09_04:26:06.176572.zip&atstype=ATS] -> Refer TC102 and TC103

 
 *  In the sub TCs - [test10_verify_ANC_Quarantine_applied_for_concurrent_mac|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2005433&size=65429&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug09_04:26:06.176572.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] & [test12_verify_Remove_ANC_Quarantine_for_concurrent_mac|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2122730&size=89603&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fsr_mb_multi_sites_mdnac.2022Aug09_04:26:06.176572.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS], before verification of the cli on Edge device for ANC policy, Can we have cli - *_clear access-session interface <Interface_name>_* executed on the Edge device so that the next ANC policy verification would show up right configs at that point? Else we would have some stale entries in *“show authentication sessions method Dot1x inter Gig 1/0/7 details”* cli if something dot1x cases were executed earlier. We observed this while integrating this feature.

 
 *  Under TC103, since the clients mac address are being updated to original value, Can we have the Trustscore for the concurrent mac entry and concurrent mac alert to be reset at the end of this TC?  Please refer the red box highlighted for *Reset Trust Score* in the left and *Reset* option in the bottom right, in below snapshot.

  !image-2022-09-16-12-51-20-076.png!

 

 ",2022-09-16T07:21:58.871+0000,"Real scenario will be same if a someone tried to change/fake their MAC, there will be old and new entries in 'show authen session int xxx detail'. We don't need to clear it since we want to keep the scenario as it should be.

We want to keep the Trustscore as it should be in this usecase.

Reset trustscore will be tested in diff usecase. Closing the issue as per update by Tran. ","['Auton', 'Groot', 'Guardian', 'Issue', 'MSTB1', 'Multisite']",Tran Lam,Closed,Avril Bower
SEEN-602,https://miggbo.atlassian.net/browse/SEEN-602,[Auton] [Groot] - Verify neighbor topology fails with missing Fabric role parameters,"*Uber ISO Version tested :* 
 Promoted Groot RC6 Uber ISO - *2.1.560.70.517, Non-FIPS, PUBSUB enabled*

*Script Name:* solution_assurance_test.py

*Testbed :* MSTB1, MSTB2
  

*Description :* 

 

Verify neighbor topology fails with Fabric role mismatch between expected and Actual output. We suspect this could be due to modification introduced on Groot by Assurance team under device 360 page for Border devices. But we are not sure. Need confirmation.



*Error Snip:*

10231:  Verification Failed \{'NY-CP-9300': {'NY-CP-9300': ""Fabric Device Role Mismatch Expected ['BORDER', 'DEFAULT', 'MAP-SERVER'] GOT ['BORDER', 'MAP-SERVER']""}}

 

*Failed log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites.2022Sep09_07:37:38.637115.zip&atstype=ATS]* -> Refer Task-2,TC5",2022-09-16T10:07:17.129+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/181b6fc20c97d5ed58c6a1da494fe05bc3479355 Hey Tran,

Thanks for adding the fix. We will be verifying for this during our next Groot testing and update our observations. Issue is no more observed during Groot Patch1 Pre-RC Uber ISO - 2.1.563.70111  testing:

Reference log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fsr_mb_multi_sites.2022Oct26_00:17:54.277338.zip&atstype=ATS] -> Please refer TC5","['Auton', 'Groot', 'Issue', 'MSTB1', 'MSTB2', 'Multisite']",Tran Lam,Closed,Avril Bower
SEEN-603,https://miggbo.atlassian.net/browse/SEEN-603,[Auton]:Ghost:Test_TC37_DNAC_TSIM_static_onboarding_verifications/test1_tsim_static_onboarding_verifications/test1_onboard_all_sensors_on_all_sites,"*<This cluster is upgraded from Guardian P1 RC4 (2334) <> Groot RC6(2340)<>Ghost(2350)>*

**Reporter Analysis:**

The script didn't added TSIM interface details in FIAB devices eventhough interface was UP

*Impacted Testcases:*

*Test_TC46_DNAC_all_aps_verification_in_wlc /test1_verify_ap_joined_in_wlc*
 *Test_TC48_TSIM_verify_tsim_clients_on_wlc /test1_verify_tsim_clients*
 *Test_TC49_WIRELESS_VERIFY_ACCESS_TUNNEL_ON_EDGE_ROUTER/test1_verify_access_tunnel_on_edges*

*Snip from fail Log:*

TB4-DM-eCA-BORDER#
 18909: Traceback (most recent call last):
 18910: File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Upgrade-Verify-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 18911: result = testfunc(func_self, **kwargs)
 18912: File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Upgrade-Verify-common-Multi-job/testcases/upgrade/after_upgrade_verify.py"", line 1613, in test1_onboard_all_sensors_on_all_sites
 18913: dnac_handle.dnaconfig.sut_and_noshut_port(dev['name'], client['intf'].name)
 18914: File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Upgrade-Verify-common-Multi-job/services/commonlibs/sftopology.py"", line 5004, in sut_and_noshut_port
 18915: result &= self.send_device_cmd_no_shut_port(dev, interface)
 18916: TypeError: unsupported operand type(s) for &=: 'bool' and 'str'
 18918: Errored reason: unsupported operand type(s) for &=: 'bool' and 'str'
  

*Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* *after_upgrade_verify.py*

*Source Team:  Upgarde Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8737067&size=2283647&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F09%2F15%2F00%2F17%2Fenv_auto_job.2022Sep15_00:17:35.339832.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS*]

 

*Testbed details:* *NA*",2022-09-16T11:18:53.289+0000,"PR: [Pull Request #3637: Separated ap link flap issue verification to another test case. - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3637/overview] Verified in Latest Upgrade 

Guardian P1RC4#2.1.514.72142<>Ghost#2.1.610.70530
Pass Log:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2533703&size=481224&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F14%2F21%2F34%2Fenv_auto_job.2022Nov14_21:34:55.357427.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Ghost', 'Issue', 'Sanity', 'Upgrade']",Moe Saeed,Closed,Avril Bower
SEEN-605,https://miggbo.atlassian.net/browse/SEEN-605,[Auton][Groot] [Guardian] - Approve of SGT request fails with string operand error - ITSM ticket Guardian feature,"*Uber ISO Version tested :* 
 Promoted Groot RC6 Uber ISO - *2.1.560.70.517, Non-FIPS, PUBSUB enabled*

*Script Name:*  solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py

*Testbed :* MSTB1
  

*Description :* 

This is related to ITSM ticket generation Guardian feature. We see approve of SGT request and subsequent related TCs fails with string operand error. It looks to be some script side issue while mentioned event parameter.

*Failed log:* 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep13_05:20:17.746909.zip&atstype=ATS] -> Refer TC224

 

*Error Snip:*
11098:  self.log.info(""Status of change request: {}"".format(self.get_integration_event_status(id, event_name=event_name)))
11099:  File ""/auto/dna-sol/ws/sr-mb1_Groot/services/dnaserv/lib/decorators.py"", line 32, in wrapper
11100:  result = method(*args, **kwargs)
11101:  File ""/auto/dna-sol/ws/sr-mb1_Groot/services/dnaserv/lib/api_groups/itsm/group.py"", line 153, in {color:#de350b}get_integration_event_status{color}
{color:#de350b}11102:  if id in event[""ITSMLink""]:{color}
{color:#de350b}11103:  TypeError: 'in <string>' requires string as left operand, not NoneType{color}
 ",2022-09-16T12:53:05.907+0000,"Hey Andrew,

Issue is also observed during Guardian testing as well.



*Uber ISO Version tested :* 
 Promoted Guardian Patch2 RC2 Uber ISO - *2.1.515.70134, Non-FIPS, PUBSUB enabled*

*Script Name:*  solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py

*Testbed :* MSTB1
  

*Failed log:* 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep29_03:58:07.231904.zip&atstype=ATS] -> Refer TC224

 

*Error Snip:*
11881:  File ""/auto/dna-sol/ws/sr-mb1_Guardian/services/dnaserv/lib/api_groups/itsm/group.py"", line 193, in approve_latest_event
11882:  return self.approve_change_request(latest_id, event_name=event_name)
11883:  File ""/auto/dna-sol/ws/sr-mb1_Guardian/services/dnaserv/lib/decorators.py"", line 32, in wrapper
11884:  result = method(*args, **kwargs)
11885:  File ""/auto/dna-sol/ws/sr-mb1_Guardian/services/dnaserv/lib/api_groups/itsm/group.py"", line 205, in approve_change_request
11886:  self.log.info(""Status of change request: {}"".format(self.get_integration_event_status(id, event_name=event_name)))
{color:#de350b}11887:  File ""/auto/dna-sol/ws/sr-mb1_Guardian/services/dnaserv/lib/decorators.py"", line 32, in wrapper{color}
{color:#de350b}11888:  result = method(*args, **kwargs){color}
{color:#de350b}11889:  File ""/auto/dna-sol/ws/sr-mb1_Guardian/services/dnaserv/lib/api_groups/itsm/group.py"", line 153, in get_integration_event_status{color}
{color:#de350b}11890:  if id in event[""ITSMLink""]:{color}
{color:#de350b}11891:  TypeError: 'in <string>' requires string as left operand, not NoneType{color}  

Issue observed also on AWS-MS TB while testing Ghost-2.1.610.70432 .


*Uber ISO Version tested :*
 
Promoted Groot RC6 Uber ISO - *2.1.610.70432, Non-FIPS, PUBSUB enabled*

*Script Name:*  solution_test_3sites_sjc_nyc_sf.py

*Testbed :* AWS-MSTB
 

*Description :* 

This is related to ITSM ticket generation Guardian feature. We see approve of SGT request and subsequent related TCs fails with string operand error. It looks to be some script side issue while mentioned event parameter.

*Failed log:* 

https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-09/auto_MS_job.2022Sep29_01:43:50.131823.zip&atstype=ATS -> Refer TC165

 

*Error Snip:*
{color:#de350b}10193:  return self.approve_change_request(latest_id, event_name=event_name){color}
{color:#de350b}10194:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/decorators.py"", line 32, in wrapper{color}
{color:#de350b}10195:  result = method(*args, **kwargs){color}
{color:#de350b}10196:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/api_groups/itsm/group.py"", line 205, in approve_change_request{color}
{color:#de350b}10197:  self.log.info(""Status of change request: {}"".format(self.get_integration_event_status(id, event_name=event_name))){color}
{color:#de350b}10198:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/decorators.py"", line 32, in wrapper{color}
{color:#de350b}10199:  result = method(*args, **kwargs){color}
{color:#de350b}10200:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/api_groups/itsm/group.py"", line 153, in get_integration_event_status{color}
{color:#de350b}10201:  if id in event[""ITSMLink""]:{color}
{color:#de350b}10202:  TypeError: 'in <string>' requires string as left operand, not NoneType{color}
{color:#de350b}10203:  The result of section test3_approve_SGT_request is => ERRORED{color} Resolved wrong issue, this one should still be in progress

  Issue is still observed during Groot Patch1 Testing. 

Failed log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fsr_mb_multi_sites_mdnac.2022Oct27_04:31:17.629633.zip&atstype=ATS] -> Refer TC224 [~63f50bcece6f37e5ed93c87e]: Did you get a chance to look into this issue?

Regards
Sandeep S Hi Sandeep, does not look like a script issue, seems to be connectivity issue to either servicenow or midserver. The itsm event is getting created but its not getting an itsm id or itsm link. I see the tb is not up yet, when it is up please let me know and i will do some manual testing (if you want we can do together to show you). Have restarted midserveribste, the midserver being used by most testbeds. Seems to be fixed now, so please try again. [https://wiki.cisco.com/display/EDPEIXOT/ITSM+Ticket+Generation+Usecase]

 

I have added steps to restart the midserver in the above wiki in the ""debugging tips"" section, please refer next time this error is seen. If seeing the issue, please create a new midserver as per the wiki. [~63f50bcece6f37e5ed93c87e] / [~62ab7a399cd13c0068b18fe0]:
 Same Issue is still observed even after restarting the midserver and also when tried creating new midserver. 

Could you please have a look into this once?



Please refer below Team space for more details:
 webexteams://im?space=bfcf9d60-903e-11ed-84cc-4b71e6df604f [~63f50bcece6f37e5ed93c87e]: Thanks for look into the issue further on sharing the setup in problem state.

As discussed over the team space - webexteams://im?space=bfcf9d60-903e-11ed-84cc-4b71e6df604f

The current issue was due to incorrect Host IP address being applied under System->Settings->Integration settings for Integration Callback URLs.

Incorrect IP - 10.195.243.110 (which is DR Witness IP) was being input from Automation script instead of Active DR Cluster IP (10.195.243.123 currently, it can be 10.195.243.109. It depends on which one is Active DR during the execution).

*Failed log:* https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fsr_mb_multi_sites_mdnac.2023Jan18_23:03:06.256350.zip&atstype=ATS -> Refer TC224

After adding 10.195.243.123 under System->Settings->Integration settings we started seeing ITSM id and ITSM link getting generated as DNAC stated to interact with Service now instance. So we need to make sure Cluster IP address has to be input under System->Settings->Integration settings. 

During exception IP is getting updated from witness local.json file - configs/sr_mb/sr_mb1-10.195.243.110-local.json which is incorrect. Rather it has to be always Active DR node cluster IP depending on whichever is Active during the script execution. So we need script changes to be added accordingly. [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4601/overview] Guardian

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4603/overview]

Groot

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4602/overview]

Ghost Issue is no more observed. The code changes has fixed the issue.

*Uber ISO version tested* - Promoted Guardian Patch4 Pre-RC2 Interim Build : 2.1.518.72292

*Pass log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fsr_mb_multi_sites_mdnac.2023Feb06_23:25:56.345473.zip&atstype=ATS] (Refer TC224)","['AWS_MSTB', 'Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'MSTB1', 'MSTB2', 'Multisite']",Andrew Chen,Closed,Avril Bower
SEEN-613,https://miggbo.atlassian.net/browse/SEEN-613,[Auton]   TC67_DNAC_CriticalVLAN_onboarding_ixia_scale  /   test66_subtest2_dot1x_auth_ixia  sessions need to be verified on all the ixia connected devices," 

*Reporter Analysis:*  [TC67_DNAC_CriticalVLAN_onboarding_ixia_scale|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8528888&size=33254&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep16_06:36:24.747864.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test66_subtest2_dot1x_auth_ixia  verify the dot1x sessions only for the edge device, this test must verify the ixia session on all the devices before checking overall sessions on fusion.

*Description:   following code to be fixed*

if (dnac_handle.dnaconfig.verifyDot1x(domain=""UNKNOWN"", sessioncount=10)):
 logger.info(""Dot1x Authentication Sessions Present on Devices"")
else:
 dnac_handle.dnaconfig.noshut_fusion_ise_interface()
 logger.error(""Dot1x Authentication Sessions Present on Devices are not as expected"")
 # self.failed(""RESULT: Dot1x Authentication Sessions Present on Devices are not as expected"")

 ** 

*Branch Name:  private/Groot-ms/sanity_api_auto (all branches)*

*Script file/Usecase:*  All sanity scripts and  trafficdot1x

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0 issues*

*Fail Log:* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8529458&size=30814&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep16_06:36:24.747864.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:*

*Testbed details:* NA",2022-09-18T20:28:03.578+0000,"[~accountid:5e1415780242870e996f0b2f]  [~accountid:61efa8c457b25b006877eda3]  Could you please confirm if the issue is still observed Hi [~accountid:63f50bf5e8216251ae4d59cf] 
Yes we are seeing issue still IXIA testcases are passing except this particular one

Thanks,
ANusha [~accountid:61efa8c457b25b006877eda3] Please share the latest log Latest Failed log from Ghost P2:
[test66_subtest2_dot1x_auth_ixia|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19819688&size=31364&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul25_02:18:10.478149.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Hi [~accountid:63f50bf5e8216251ae4d59cf] 
Any PR we have for the issue
Can you please share if any PR is there","['Auton', 'Ghost', 'Groot', 'Guardian', 'Hulk', 'Issue', 'Sanity', 'hulk-vm-sanity', 'shockwave']",Raji Mukkamala,Resolved,Avril Bower
SEEN-614,https://miggbo.atlassian.net/browse/SEEN-614,"Unable to Provision ECA & WLC Device - Showing ""Duplicate or Conflicting Designs"" under Model Configuration Wizard"," 

Hi Raji,

    When we are trying to provision the ECA device manually, it is showing ""Duplicate or Conflicting Designs"" under model configuration wizard.

This warning is showing for below Design names only,

     1. custom_ssid

     2. dhcp_enabled

Due to this we are unable to provision the device manually, via script provision TC got passed.

 

DNAC Release Used : Shockwave P3 RC3 #2.1.390.72158

Branch Used : private/Shockwave-ms/sanity_api_auto 

 

Attached the supporting screen shots.

 

!image-2022-09-19-16-23-32-149.png!

 

!image-2022-09-19-16-23-44-361.png!

 

!image-2022-09-19-16-33-20-659.png!",2022-09-19T10:53:50.399+0000,[c68f36aca45|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c68f36aca450c96b97a4b211161cf57c9b6e0ce2] [~63f53512263233e653a96a29] Sync the commit with all solution_test_input files being used Working fine in latest executions.,"['Auton', 'Issue', 'Provision', 'Sanity', 'Shockwave']",Raji Mukkamala,Closed,Avril Bower
SEEN-617,https://miggbo.atlassian.net/browse/SEEN-617,[Ghost] - Compliance type EOX and Image showing as failed though the status showing as not applicable,"Issue: Compliance type EOX and Image showing as failed as part of device compliance check, though the status showing as not applicable. Please see below o/p. Can this be modified. Please confirm.

'complianceType': 'IMAGE', 'status': 'NOT_APPLICABLE', 

'complianceType': 'EOX', 'status': 'NOT_APPLICABLE', 

 

Below is the complete o/p:

170940:  Non Compliant Devices: [\{'dev_name': 'sda-9k-102-FiaB', 'non_compliant_areas': [{'deviceUuid': '39d1f333-6e3d-4a14-bad5-d7d3c1c853e2', 'complianceType': 'IMAGE', 'status': 'NOT_APPLICABLE', 'state': 'SUCCESS', 'lastSyncTime': 1663614955736, 'lastUpdateTime': 1663637822666, 'sourceInfoList': [], 'message': 'Golden image is not available', 'additionalDataURL': '/api/v2/device-image/device?id=39d1f333-6e3d-4a14-bad5-d7d3c1c853e2', 'ackStatus': 'UNACKNOWLEDGED'}, \{'deviceUuid': '39d1f333-6e3d-4a14-bad5-d7d3c1c853e2', 'complianceType': 'NETWORK_PROFILE', 'status': 'NON_COMPLIANT', 'state': 'SUCCESS', 'lastSyncTime': 1663636371903, 'lastUpdateTime': 1663637866991, 'sourceInfoList': [{'name': 'APJoinProfile', 'nameWithBusinessKey': 'APJoinProfile-default-ap-profile', 'sourceEnum': 'NETWORK_PROFILE', 'type': 'wlan', 'appName': 'Wireless', 'count': 1, 'ackStatus': 'UNACKNOWLEDGED', 'businessKey': {'resourceName': 'APJoinProfile', 'businessKeyAttributes': {'owningEntityId': '406406_406406', 'apProfileName': 'default-ap-profile'}, 'otherAttributes': \{'name': 'rfs', 'cfsAttributes': {'displayName': 'AP Join Profile', 'appName': 'Wireless', 'description': 'This is description of AP Join Profile', 'source': 'NETWORK_PROFILE', 'type': 'wlan'}}}}], 'ackStatus': 'UNACKNOWLEDGED'}, \{'deviceUuid': '39d1f333-6e3d-4a14-bad5-d7d3c1c853e2', 'complianceType': 'EOX', 'status': 'NOT_APPLICABLE', 'state': 'SUCCESS', 'lastSyncTime': 1663614918494, 'lastUpdateTime': 1663637822829, 'sourceInfoList': [], 'message': 'EOX status not available.', 'ackStatus': 'UNACKNOWLEDGED'}]}, 'sda-9k-103-sda-transit', 'sda-9k-108-NF', 'sda-wlc-2']

 

Script and TC : solution_test_sanityecamb_lan.py and  [Test_TC107_Compliance_verification|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=43237476&size=5219448&archive=%2Fhome%2Fsda-pyats%2Fusers%2Fapic%2Farchive%2F22-09%2FSanity_TB1.2022Sep19_11:24:15.814239.zip&ats=%2Fhome%2Fsda-pyats&submitter=ksinigam&atstype=PYATS]  /   test10_verify_overall_compliance_status_for_devices

Branch : private/Ghost-ms/api-auto

Failed Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=48290209&size=166545&archive=%2Fhome%2Fsda-pyats%2Fusers%2Fapic%2Farchive%2F22-09%2FSanity_TB1.2022Sep19_11:24:15.814239.zip&ats=%2Fhome%2Fsda-pyats&submitter=ksinigam&from=trade&view=all&atstype=pyATS]

 

 ",2022-09-20T09:42:20.008+0000,"Seeing this issue during Solution Sanity Regression as well. Please commit fix on below script as well

Script: dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

Testcase: Test_TC107_Compliance_verification / test10_verify_overall_compliance_status_for_devices

Please commit fix in all applicable branches PR raised in below branches

 

Groot :

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3710/overview]

 

Guardian :

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3711/overview]

 

Ghost :

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3712/overview]

 

Shockwave

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3713/overview]","['Auton', 'Ghost', 'Issue']",Faiz Habibbhai Babuna,Resolved,Avril Bower
SEEN-618,https://miggbo.atlassian.net/browse/SEEN-618,[Auton] [Groot] - Need for script enhancement for trustscore_and_alert_customization Groot Feature,"*Uber ISO Version tested :* 

Promoted Groot RC6 Uber ISO - *2.1.560.70517,* *Non-FIPS, PUBSUB enabled, CSRF enabled*

*Script Name:*
 solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py

*Testbed :* MSTB1

 

*Reference Pass log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep12_07:02:08.545600.zip&atstype=ATS] -> Refer TC230

 

*Description :* 

Need for script enhancement for the DC features related to trustscore_and_alert_customization Groot feature. This TC involves Concurrent mac address update - [https://wiki.cisco.com/display/EDPEIXOT/DCS+-+Concurrent+MAC] , where we change MAC of a Windows VM to be same MAC as another Windows VM.  

After doing the this concurrent mac changes and once the testing is completed we have to revert back the mac address of  respective Windows VM as before. Currently its not implemented after TC230. 

We already have such TC implemented part to cleanup concurrent mac configs and revert back to earlier, under ANC Policy Guardian Feature TCs

*Reference log:* 
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep12_04:06:57.695414.zip&atstype=ATS] -> Please refer TC102 and TC103, where TC103 is used to cleanup concurrent configs. 

We can have this TC103 implementation added as part of cleanup of concurrent configs after TC230.

 

 

 ",2022-09-20T11:07:22.703+0000,PR: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4600/overview,"['Auton', 'Groot', 'Issue', 'MSTB1', 'MSTB2', 'Multisite']",Moe Saeed,Resolved,Avril Bower
SEEN-619,https://miggbo.atlassian.net/browse/SEEN-619,Guardian - [Auton]  Test_TC118_vnid_override/ test3_override_vnid_BLD_SF," 

*Reporter Analysis:* San Francisco has no site on the Sanity testbed.We have only San Jose to New York site,Need to skip this test

*Description:*  

*Branch Name:  private/Groot-ms/sanity_api_auto*
                           *Private/Guardian-ms/sanity_api_auto* 

*Script file/Usecase:*

1.Solution_test_sanityecamb_lan.py

2.Solution_test_sanityecamb.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=88853567&size=554607&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep18_16:11:21.732010.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:*

*Testbed details:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9902044&size=5817&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul14_12:46:52.407317.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2022-09-21T13:20:13.211+0000,"Skip for ""test3_override_vnid_BLD_SF"" as confirmed by Omkar Sharad Wagh (Regression Team)

PR-Guardian: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4524/overview

PR-Groot: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4525/overview

PR-Ghost: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4526/overview","['Auton', 'Groot', 'Guardian', 'Issue', 'Sanity']",NhanHuu Nguyen,Resolved,Avril Bower
SEEN-620,https://miggbo.atlassian.net/browse/SEEN-620,[Auton] Test_TC153_apply_custom_profile_issue_on_site_level custom profile is not removed after the test," 

*Reporter Analysis:*  There are two issues 1) the custom profile is not removed after the test

   2) We need to check if this can affect other parallel use cases as discussed           

*Description:*  

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:*  testcases/sanityusecases/assuranceHealthMetrics/assurance_health_metric.py and TC153

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

*Pass Log:*

*Testbed details:* NA",2022-09-21T18:52:51.321+0000,"updating the severity since it's blocking the use case Hi Raju, 
What is the issue?

Can you share log? [~63f50bcf4e86f362d39acde5], can you add custom_profile deletion ? [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct19_11:22:34.182811.zip&atstype=ATS]

 

  !image-2022-10-20-00-09-35-471.png! https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4041/overview Merged to Groot.
Can you PR to Ghost too? please check it [~62d2fe9f8afb5805e5d5af49]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4396/overview] i raised another PR for you to review. please check it

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4427/overview","['Auton', 'Ghost', 'Groot', 'Issue', 'Sanity', 'optimized']",QuangVinh Nguyen,Resolved,Avril Bower
SEEN-622,https://miggbo.atlassian.net/browse/SEEN-622,[Auton]  [Optimized code] Meta json handling in optimized code," 

*Reporter Analysis:* 

We need to handle the following code in the optimized code to generate the meta.json, Currently, this code is in common_Cleanup, and it generate the file end of the run.

 

*class* common_cleanup(aetest.CommonCleanup):
     @aetest.subsection
     *def* collect_testbed_metadata(self, dnac_handle):
         dnac_handle.collect_testbed_metadata()

 

*Description:  The error from log or more info* 

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:*  optimized code

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

*Pass Log:* https://earms-trade.cisco.com/tradeui/logs/fileviewer?logfile=meta.json&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep18_16:11:21.732010.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=pyATS&from=trade&view=all

*Testbed details:* NA",2022-09-22T01:28:18.445+0000,"A generic Meta collection was added to job util.

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d12f822f61b6629a7e5f12e0d77e49e545ca72ef#job/jobutils.py]

+

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3d699147b6b73b67b006389e0a9003a800d9c667]

 

Pulled on guardian and groot.

 ","['Auton', 'Issue', 'Sanity', 'optimized']",Pawan Singh,Resolved,Avril Bower
SEEN-624,https://miggbo.atlassian.net/browse/SEEN-624,[Auton] Groot: Test_TC3_aaa_per_ssid /test1_add_new_ssid /test2_edit_existing_ssid,"*Reporter Analysis:* 

a. This Test_TC3_aaa_per_ssid performs following steps:   

-Create SSID ""test_AAA"" with AAA selected - passed
 -Add SSID to all wireless profiles available - passed
 -Provision to resync then verify changes to WLCs - Failed 

while provisioning the device with role=WLC or ECA device this testcase failed

b. test2_edit_existing_ssid 
 -Edit an exisiting SSID ""SSIDDot1XIndia"" to select AAA associated - passed
 -Provision to resync then verify changes to WLCs - Failed

 while provisioning the device with role=WLC or ECA device this testcase failed

 

*Description:* Error snip from log
 FAILED TO VERIFY WIRELESS SSID AAA
 5228: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Optimized-Deployment_and_Sanity-Job/services/dnaserv/lib/api_groups/device_config_validation/group.py"", line 1442, in verify_aaa_per_ssid
 5229: if device_role.find(""WLC"") == -1 or ""ECA"" not in device_role.role:
 5230: 'str' object has no attribute 'role
 *Branch Name:*  private/Groot-ms/sanity_api_auto
                          private/Ghost-ms/sanity_api_auto



*Script file/Usecase:* [dnac-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse?at=refs%2Fheads%2Fprivate%2FGroot-ms%2Fsanity_api_auto]/testcases/sanityusecases/cmxConfigsAndValidations

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*N/A

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep19_18:53:14.285038.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS] ** 

*Pass Log:*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep03_16:13:09.813448.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS] 

*Testbed details:* Sanity Testbed 7

[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] ",2022-09-22T06:06:25.050+0000,"TC Failed On DMZ -TB in Ghost -AWS
double commit as well Ghost Branch 
*Failed Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-cmx_config_push_and_device_validations.py-156-cmxConfigsAndValidations&begin=1378457&size=23632&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep15_08:55:47.244498.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS Seeing same issue in Sanity Sanity REG Script also for Groot RC7
 *Script Used:*solution_test_sanityecamb.py
 *Failed Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=62512858&size=64726&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep27_01:57:34.109871.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Required PR has been merged to Groot and Ghost branches:
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4150/overview This ticket can be closed as it resolved the issue for esxivm_branch.
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=39181730&size=823118&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov14_20:41:34.186749.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

Marking is as ""Closed"". Still we are seeing this issue in Shockwave P4. Please double confirm the fix is available in shockwave branch also.

Branch Used: private/Shockwave-ms/sanity_api_auto

Main Branch: private/Shockwave-ms/api-auto

 

Fail Log: https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=50778506&size=277005&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F17%2F22%2F43%2Fenv_auto_job.2022Nov17_22:43:41.354688.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse?at=refs%2Fheads%2FSEEN-624-Shockwave] 

 

Missing method in shockwave added, need testbed to test. Hi [~63f50bcece6f37e5ed93c87e],

Tested in Shockwave cluster after merging the changes .
Tc got passed
Passed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=459213&size=1986369&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F02%2F07%2F20%2F24%2Fenv_auto_job.2023Feb07_20:24:48.172026.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

Thanks,
Anusha","['AWS_Sanity', 'Auton', 'Ghost', 'Groot', 'Issue', 'Optimized', 'Sanity', 'Shockwave']",Andrew Chen,Closed,Avril Bower
SEEN-625,https://miggbo.atlassian.net/browse/SEEN-625,[Auton]Groot:Test_TC5_DNAC_dot1x_onboarding_ixia_scale /test3_subtest2_dot1x_auth_ixia_ipv4  test failed with key Error 'CLIENTS_VN_MAP',"*Reporter Analysis:* In this testcase, First IXIA will be connecting to IXIA ports and then testscript is looking for the key 'CLIENTS_VN_MAP' in config file. Key is missing from config file.

 

*Description:  The error from log* 
 File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Groot/Groot-Optimized-Deployment_and_Sanity-Job/testcases/sanityusecases/trafficdot1x/traffic_unicst_multicast_dot1x_cp.py"", line 439, in test3_subtest2_dot1x_auth_ixia_ipv4
 36346: for ixiauser in dnac_handle.dnaconfig.testbed.custom[dnac_handle.dnaconfig.sid][""CLIENTS_VN_MAP""]:
 36347: KeyError: 'CLIENTS_VN_MAP'
 *Branch Name:*  private/Groot-ms/sanity_api_auto

*Script file/Usecase:*  [dnac-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse?at=refs%2Fheads%2Fprivate%2FGroot-ms%2Fsanity_api_auto]/testcases/sanityusecases/trafficdot1x/traffic_unicst_multicast_dot1x_cp.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:* N/A

*Fail Log*:[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x&begin=10157929&size=63802&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep19_12:02:55.553779.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyAT|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x&begin=10157929&size=63802&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep19_12:02:55.553779.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:* N/A

*Testbed details:* 

Sanity Testbed 7

[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] ",2022-09-22T09:54:31.780+0000,"Seem like new attributes needed for fabric json: CLIENTS_VN_MAP.

Ref: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb6/solution_sanityeca_lan_SanityTB6.json?at=refs%2Fheads%2Fprivate%2FGroot-ms%2Fapi-auto#1165]

 
{code:java}
// code placeholder
""CLIENTS_VN_MAP"":[
{ 
 ""vn_name"": ""WiredVNFBLayer2"",
 ""site"" : ""BLD23"",
 ""username"": ""soltestgp1_1"",
 ""password"": ""Lablab123"",
 ""segment"" :""""
},
{ 
 ""vn_name"": ""WiredVNFBLayer2"",
 ""site"" : ""BLDNYC"",
 ""username"": ""soltestnyc_1"",
 ""password"": ""Lablab123"",
 ""segment"" :""""
 },
{ 
 ""vn_name"": ""WiredVNFBLayer2"",
 ""site"" : ""BLD_SF"",
 ""username"": ""soltestsf1_1"",
 ""password"": ""Lablab123"",
 ""segment"" :""""
 }],{code} Please add CLIENTS_VN_MAP to your fabric json.","['Auton', 'Groot', 'Issue']",Tran Lam,Closed,Avril Bower
SEEN-626,https://miggbo.atlassian.net/browse/SEEN-626,Ghost-AWS - [Auton]   Task-testbed_cdp_links_discovery.py-12-testbedTopologyGenerate /Test_TC1_DNAC_testbed_topology_discovery_through_cdp  /  generate_cdp_data," 

*Reporter Analysis:* Test case failed on TB2-DMZ-Fusion for CDP Entries:
*Description:*  217:  KeyError: 'Cisco-VM-'

*Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* env_optimized_auto_job.py/[testbedTopologyGenerate|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-testbed_cdp_links_discovery.py-12-testbedTopologyGenerate&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep14_01:02:10.438791.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-testbed_cdp_links_discovery.py-12-testbedTopologyGenerate&begin=4829&size=34059&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep14_01:02:10.438791.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log: On-Prem* 
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-testbed_cdp_links_discovery.py-12-testbedTopologyGenerate&begin=4829&size=79956&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsanity_TB3_cert.2022Jul26_21:50:43.798353.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS

*Testbed details:* NA",2022-09-22T10:06:12.878+0000,"Below PR has been raised for the changes required:
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3781/overview [~620b8357878c2f00729881c8] , the fix has been merged to private/Groot-ms/api-auto branch.

Please check and confirm on the closure of this ticket. the fix is also available in private/Ghost-ms/api-auto branch. TC is passed on AWS-Ghost, ","['AWS_Sanity', 'Auton', 'DMZ', 'Ghost', 'Groot', 'Issue', 'Optimized', 'Sanity']",Amardeep Kumar,Closed,Avril Bower
SEEN-627,https://miggbo.atlassian.net/browse/SEEN-627,Ghost -AWS [Auton] Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding  /   Test_TC2_DNAC_TSIM_static_onboarding_verifications  /   test3_onboard_all_sensors_on_all_sites," *Reporter Analysis:Type  Error :* unsupported operand 

*Description:*  

def sut_and_noshut_port(self,dev, interface):
 result = self.send_device_cmd_shut_port(dev, interface)
 time.sleep(1)
 result &= self.send_device_cmd_no_shut_port(dev, interface)
 logger.info(result)
 return result

*Branch Name:  private/Ghost-ms/sanity_api_auto*
                         *private/Groot-ms/sanity_api_auto***

*Script file/Usecase:* env_optimized_auto_job.py/[SDAwiredHostOnboarding|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep14_01:02:10.438791.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding&begin=2280729&size=54956&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep14_01:02:10.438791.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log: On-Prem*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding&begin=2298391&size=98676&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-08%2Fenv_optimized_auto_job.2022Aug30_19:49:25.486750.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Testbed details:* NA",2022-09-22T10:48:15.359+0000,It should already be fixed.,"['AWS_Sanity', 'Auton', 'Ghost', 'Groot', 'Issue', 'Optimized', 'Sanity']",Tran Lam,Closed,Avril Bower
SEEN-628,https://miggbo.atlassian.net/browse/SEEN-628,Ghost- [Auton][Optimized]Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x  /   Test_TC5_DNAC_dot1x_onboarding_ixia_scale  /   test3_subtest2_dot1x_auth_ixia_ipv4," 

*Reporter Analysis:* Tc case failed for Key Error

*Description:*  
 for ixiauser in dnac_handle.dnaconfig.testbed.custom[dnac_handle.dnaconfig.sid][""CLIENTS_VN_MAP""]:
 KeyError: 'CLIENTS_VN_MAP'
 *Branch Name:  private/Groot-ms/sanity_api_auto*

                         *                         **private/Ghost-ms/sanity_api_auto***

*Script file/Usecase:* env_optimized_auto_job.py/trafficdot1x

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x&begin=10419207&size=64403&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep14_14:01:16.776498.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:*

*Testbed details:* NA",2022-09-22T13:09:33.710+0000,"Seem like new attributes needed for fabric json: CLIENTS_VN_MAP.

Ref: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb6/solution_sanityeca_lan_SanityTB6.json?at=refs%2Fheads%2Fprivate%2FGroot-ms%2Fapi-auto#1165]

 
{code:java}
// code placeholder
""CLIENTS_VN_MAP"":[
{ 
 ""vn_name"": ""WiredVNFBLayer2"",
 ""site"" : ""BLD23"",
 ""username"": ""soltestgp1_1"",
 ""password"": ""Lablab123"",
 ""segment"" :""""
},
{ 
 ""vn_name"": ""WiredVNFBLayer2"",
 ""site"" : ""BLDNYC"",
 ""username"": ""soltestnyc_1"",
 ""password"": ""Lablab123"",
 ""segment"" :""""
 },
{ 
 ""vn_name"": ""WiredVNFBLayer2"",
 ""site"" : ""BLD_SF"",
 ""username"": ""soltestsf1_1"",
 ""password"": ""Lablab123"",
 ""segment"" :""""
 }],
{code} Please add CLIENTS_VN_MAP to your fabric json.","['AWS_Sanity', 'Auton', 'Ghost', 'Groot', 'Issue', 'Optimized', 'Sanity']",Tran Lam,Resolved,Avril Bower
SEEN-629,https://miggbo.atlassian.net/browse/SEEN-629,Ghost - [Auton]  Task-ext_node_link_failover_ip_addr_change.py-151-SDAExtendedNodelinkfailoverIpChange  /   Test_TC2_verify_assurance_data_after_extnode_ip_change  /   test1_verify_assurance_health_nw_health_update_inv_data_ext_nodes," 

*Reporter Analysis:* Issue is seen in the optimized run, There is no TC2, Please help to update the test case to work dynamically.

*Description:*  AttributeError: 'Device' object has no attribute 'lb_ip'

*Branch Name:  private/Groot-ms/sanity_api_auto*
                          **                          *private/Ghost-ms/sanity_api_auto***

*Script file/Usecase:* env_optimized_auto_job.py/[SDAExtendedNodelinkfailoverIpChange|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ext_node_link_failover_ip_addr_change.py-151-SDAExtendedNodelinkfailoverIpChange&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep15_08:55:47.244498.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ext_node_link_failover_ip_addr_change.py-151-SDAExtendedNodelinkfailoverIpChange&begin=311640&size=26662&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep15_08:55:47.244498.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:*

*Testbed details:* NA",2022-09-22T13:43:58.193+0000,,"['AWS_Sanity', 'Auton', 'Ghost', 'Groot', 'Issue', 'Optimized', 'Sanity']",Nethra Thorali Suthagar,Resolved,Avril Bower
SEEN-630,https://miggbo.atlassian.net/browse/SEEN-630,[Auton] Groot/Ghost : Test_TC142_Add_interace_description  /   test1_add_interface_description,"*Description:  The error from log or more info* 

Description and interface NOT found in summary Interface TwoGigabitEthernet1/0/35 (Interface description: LAN Interface type USER_DEVICE onboarded by sol script) is flapping on network device ""TB7-SJ-EDGE.cisco.com"" within time 35.666666666666664 in minutes 

 

*Branch Name:*  private/Groot-ms/sanity_api_auto

*Script file/Usecase:* Not added in optimized code

[Test_TC142_Add_interace_description|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=83435917&size=985202&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7%2Fusers%2Fadmin%2Farchive%2F22-03%2Fenv_auto_job.2022Mar29_03:29:36.559586.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal_cel7&submitter=admin&atstype=PYATS]  /   test1_add_interface_description

solution_test_sanityecamb_lan.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:* N/A

*Fail Log:*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9498664&size=455137&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep20_00:16:08.928228.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  **  

*Pass Log:*

*Testbed details:* 

Sanity Testbed 7

[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] ",2022-09-22T14:16:36.017+0000,"here is the PR

 

Groot:

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3720/overview]

 

Ghost:

 

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3721/overview

 

 ","['AWS_Sanity', 'Auton', 'Ghost', 'Groot', 'Hulk', 'Issue']",Avril Bower,Resolved,Avril Bower
SEEN-631,https://miggbo.atlassian.net/browse/SEEN-631,Ghost [Auton]   Test_TC4_DNAC_verify_anchorvn_withIXIA_clients_and_traffic_noauth  /   verify_anchorvn_withIXIA  - Test fails in optimized code," 

Reporter Analysis: There is a script issue, it looks like the variable is not set. We have already pulled the latest code, but the issue persists

Description: 
 28661: RESULT: IXIA Successfuly connected to IXIA ports
 28662: Traceback (most recent call last):
 28663: File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/DMZ_Ghost_Optimized-Deployment_and_Sanity-Job/testcases/sanityusecases/SDAFabricAnchorvn/anchor_vn_validations.py"", line 483, in verify_anchorvn_withIXIA
 28664: count=IXIA_STATIC_ENDPOINT)
 28665: NameError: name 'IXIA_STATIC_ENDPOINT' is not defined
 28666: Exception while checking IXIA traffic for AnchorVN WiredVNStatic2: name 'IXIA_STATIC_ENDPOINT' is not defined
 28667: Verification for some Anchoredvn ['WiredVNStatic1', 'WiredVNStatic2'] with IXIA clients and traffic failed
 28668: Test returned in 0:37:22.650785
 28669: Failed reason: Verification for some Anchoredvn ['WiredVNStatic1', 'WiredVNStatic2'] with IXIA clients and traffic failed

 
 *Branch Name:* private/Ghost-ms/sanity_api_auto
                          private/Groot-ms/sanity_api_auto

*Script file/usecase:*  SDAFabricAnchorvn

*Source Team:*  Sanity

Issue Seen first time or day0 issue: day0

*Fail Log:* 
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-anchor_vn_validations.py-161-SDAFabricAnchorvn&begin=5587816&size=3880520&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep15_08:55:47.244498.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Pass Log:

Testbed details: NA",2022-09-22T14:50:32.821+0000,"It was fixed in Groot. Synced Groot -> Ghost. Hi Tran,

Issue  seen  Latest GrootP1RC1 Run :

*+Branch Name+:* Groot-ms/sanity_api_auto

+*Failed Log:*+
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-anchor_vn_validations.py-161-SDAFabricAnchorvn&begin=4245141&size=1746573&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct20_19:35:46.282266.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS This is a different issue that could be bug or ixia issue. Please check and debug on your ixia to see why it got exception to start traffic. ","['AWS_Sanity', 'Auton', 'Ghost', 'Groot', 'Issue', 'Optimized', 'Sanity']",Tran Lam,Resolved,Avril Bower
SEEN-633,https://miggbo.atlassian.net/browse/SEEN-633,[Auton] IBSTE based script is required for Feature I'd: Twj882606r,IBSTE based script is required for Feature I'd: Twj882606r as the overall execution time is not feasible for Sanity Regression.,2022-09-23T04:50:03.164+0000,"[~accountid:5f3c6ae932360700388f7b4b]  / [~accountid:62d2fe9f8afb5805e5d5af49]  are we going to have this feature automated for IBSTE? This is just one event, verifying the test in sanity is enough.","['Auton', 'Groot']",Unassigned,Closed,Avril Bower
SEEN-634,https://miggbo.atlassian.net/browse/SEEN-634,"[Auton]Groot: TC143_system_health_test/test2_add_cimc_info, Encountered unhandled error in Internal API Call","*Description:  The error from log or more info* 
 Encountered unhandled error in Internal API Call
 29913: Flagging result as FAIL!
 29914: Reason: <class 'requests.exceptions.ReadTimeout'>
 29915: Kwargs:
 29916: {'data':

{'cimcaddress': '10.30.0.99', 29917: 'dnacaddress': '10.30.0.100', 29918: 'key': '39556977-2896-49e5-b3ce-1917aa9afca1', 29919: 'password': 'Lablab123', 29920: 'username': 'admin'}

,
 29921: 'timeout': 30}
  

*Branch Name:*  private/Groot-ms/sanity_api_auto

*Script file/Usecase:* solution_test_sanityecamb_lan.py / TC143_system_health_test / test2_add_cimc_info

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:* N/A

*Fail Log:*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9959291&size=29826&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep20_00:16:08.928228.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ** 

*Pass Log:*

*Testbed details:* 

Sanity Testbed 7

[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] ",2022-09-23T07:55:08.977+0000,"Issue is also seen in:

*Branch Name:*  private/Groot-ms/sanity_api_auto
*Script:*solution_test_sanityecamb.py

TC143_system_health_test /test2_add_cimc_info

*Source Team:  Sanity*
*Failed log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=70545956&size=30155&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep27_01:57:34.109871.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS This turned out to be a bug for which [https://cdetsng.cisco.com/webui/#view=CSCwd15910] has been raised.

Once the bug gets fixed, test-case execution should go through. attached GIT Branch info is not correct. This has been resolved in Ghost 70471 - for subtest case: test2_add_cimc_info
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7194506&size=3242&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct07_03:07:36.489100.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ","['AWS_Sanity', 'Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-636,https://miggbo.atlassian.net/browse/SEEN-636,[Auton]  swim_updgrade_os_image  (TC31) serviceability," 

*Reporter Analysis:*  'show install summary' cli output is very important during swim test failures. Please add this cli before and after the upgrade for the device, pls use the command runner.

*Description:  The error from log or more info* 

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:*  TC31 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0issue*

*Fail Log: na*

*Pass Log: na*

*Testbed details:* NA",2022-09-24T19:08:15.378+0000,"Required PR has been raised: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3750/overview] PR has been merged to Groot branch, hence marking this ticket as ""Resolved"".

[~70121:27365d3e-893a-4523-9fca-b89a7e8d0d4c], pls. check the latest execution and let us know if this ticket can be closed. PR got merged into Release branch.","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity', 'shockwave']",Amardeep Kumar,Closed,Avril Bower
SEEN-637,https://miggbo.atlassian.net/browse/SEEN-637,[Auton]  underlay config generation   Test_TC1_devices_initial_cleanup  /   test1_underlay_config_generation," 

*Reporter Analysis:*  

The following config is not correct for ASR, it's generating the vlan 51, that cannot be configured on router.

 

TB8-SJ-BORDER-CP-ASR#conf t
 TB8-SJ-BORDER-CP-ASR#conf terminal 
 Enter configuration commands, one per line. End with CNTL/Z.
 TB8-SJ-BORDER-CP-ASR(config)#vlan 51
 vlan 51

 
 1993: vlan 51
 1994: default interface 26
 1995: interface 26.51
 1996: encapsulation dot1Q 51
 1997: ip address 204.1.1.25 255.255.255.252
 1998: ip route 0.0.0.0 0.0.0.0 204.1.1.26
  
  
 yaml input
 TB8-SJ-BORDER-CP-ASR:
 <<: *BASE-DEVICE
 type: 'IOS-ASR'
 role: ""BORDERNODE,INERNAL""
 custom:
 serialNumber: TTM24470764
 connections:
 <<: *BASE-CONN
 a:
 protocol: telnet
 ip: 10.22.45.70
 port: 2004
  
 1940: 
 1941: Config for deviec: TB8-SJ-BORDER-CP-ASR ==
 1942: 
 1943: user cisco privilege 15 password Cisco#123
 1944: enable password Cisco#123
 1945: no logging console
 1946: no logging monitor
 1947: 
 1948: aaa new-model
 1949: aaa authentication login default local
 1950: aaa authorization exec default local
 1951: aaa session-id common
 1952: ip scp server enable
 1953: 
 1954: ip multicast-routing distributed
 1955: ip pim ssm default
 1956: ip igmp snooping
 1957: 
 1958: ip igmp snooping
 1959: router isis 1
 1960: no net 49.0001.1111.1111
 1961: net 49.0001.1111.1111.0015.00
 1962: is-type level-2-only
 1963: metric-style wide
 1964: log-adjacency-changes
 1965: bfd all-interfaces
 1966: exit
 1967: 
 1968: interface Loopback0
 1969: ip address 204.1.2.3 255.255.255.255
 1970: ip pim sparse-mode
 1971: ip router isis 1
 1972: 
 1973: ip pim register-source Loopback0
 1974: ip pim rp-address 204.1.2.36
 1975: 
 1976: default interface GigabitEthernet0/0/1
 1977: interface GigabitEthernet0/0/1
 1978: no shut
 1979: mtu 9100
 1980: 
 1981: ip address 204.1.1.13 255.255.255.252
 1982: no ip proxy-arp
 1983: ip router isis 1
 1984: 
 1985: no ip redirects
 1986: load-interval 30
 1987: ip pim sparse-mode
 1988: bfd interval 750 min_rx 750 multiplier 3
 1989: no bfd echo
 1990: exit
 1991: router isis 1
 1992: default-information originate
 1993: vlan 51
 1994: default interface 26
 1995: interface 26.51
 1996: encapsulation dot1Q 51
 1997: ip address 204.1.1.25 255.255.255.252
 1998: ip route 0.0.0.0 0.0.0.0 204.1.1.26
 1999: 
  
  
 *Description:  The error from log or more info* 

*Branch Name:  private/Guardian-ms/sanity_api_auto*

*Script file/Usecase:*  TC1

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0issue*

*Fail Log:*  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=267783&size=95715&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal%2Fusers%2Frajsaran%2Farchive%2F22-09%2Fsanity_TB8.2022Sep24_10:55:27.194552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=rajsaran&from=trade&view=all&atstype=pyATS]

*Pass Log: na*

*Testbed details:* NA

 

Correct config:

 

interface GigabitEthernet0/0/0.51
 encapsulation dot1Q 51
 ip address 204.1.1.25 255.255.255.252
!",2022-09-24T20:02:38.662+0000,"Underlay config:

 

TB8-SJ-BORDER-CP-ASR#more flash:underlay-config.conf
!
! Last configuration change at 03:02:14 UTC Wed Sep 28 2022 by wlcaccess
!
version 17.9
service timestamps debug datetime msec
service timestamps log datetime msec
service call-home
platform qfp utilization monitor load 80
platform punt-keepalive disable-kernel-core
!
hostname TB8-SJ-BORDER-CP-ASR
!
boot-start-marker
boot system bootflash:/packages.conf
boot-end-marker
!
!
vrf definition Mgmt-intf
 !
 address-family ipv4
 exit-address-family
 !
 address-family ipv6
 exit-address-family
!
logging buffered 100000000
no logging console
no logging monitor
aaa new-model
!
!
aaa authentication login default local
aaa authorization exec default local 
!
!
aaa session-id common
!
!
!
!
!
!
!
ip domain name cisco.com
ip multicast-routing distributed
!
!
!
login on-success log
!
!
!
!
!
!
!
subscriber templating
! 
! 
! 
! 
!
!
multilink bundle-name authenticated
!
!
!
!
!
!
!
!
crypto pki trustpoint TP-self-signed-711861603
 enrollment selfsigned
 subject-name cn=IOS-Self-Signed-Certificate-711861603
 revocation-check none
 rsakeypair TP-self-signed-711861603
!
!
crypto pki certificate chain TP-self-signed-711861603
 certificate self-signed 05
 3082032E 30820216 A0030201 02020105 300D0609 2A864886 F70D0101 05050030 
 30312E30 2C060355 04031325 494F532D 53656C66 2D536967 6E65642D 43657274 
 69666963 6174652D 37313138 36313630 33301E17 0D323230 39323830 33303134 
 375A170D 33323039 32373033 30313437 5A303031 2E302C06 03550403 1325494F 
 532D5365 6C662D53 69676E65 642D4365 72746966 69636174 652D3731 31383631 
 36303330 82012230 0D06092A 864886F7 0D010101 05000382 010F0030 82010A02 
 82010100 C1D6242C 9DA2CCF3 65A55FC2 46924369 973B0992 EDBDD259 70F57F0F 
 AEB517DC B775E7EF 3B338F6F 6B2CCD10 C36B4A3F 01501523 A0F80F46 3FFD4790 
 A62ABCC7 47179F14 65AD767D 1B85CBA4 4ED97C03 4BB1BBCB 7083816E BF22C76A 
 5BFAF96A 5E5221C7 71506D81 85BA9C6F 04E38662 993565E5 C3F83532 9363AD2F 
 301F9AA3 59AF3041 0D38A1B6 F8352E48 0C16098B 31F80AC3 75103574 82D977FF 
 646173DB D8AC8F05 5D61F836 BDB80AB3 10858491 4570078D 45614B27 45A9076F 
 68796444 BE54742C 6B5A1DF6 CC442F8D 060E9BE0 4CDDF1F6 246CD456 1959259E 
 AB090981 BDE85217 8D5E0AD2 7D951AFF B922C4EE 383549A4 C264FF44 BEAF6EE6 
 56F9B6ED 02030100 01A35330 51300F06 03551D13 0101FF04 05300301 01FF301F 
 0603551D 23041830 168014AD B34FED17 CD69DC46 A8DE6E67 BB1ADA66 2A5EDE30 
 1D060355 1D0E0416 0414ADB3 4FED17CD 69DC46A8 DE6E67BB 1ADA662A 5EDE300D 
 06092A86 4886F70D 01010505 00038201 0100AF9B C3B994B6 84BC20DE 83768A04 
 08D455BA D2F24D8E CF6D85F1 2F8AC43C 4A3ED156 40B7CA60 190E3C81 BEF8BA75 
 CBEFEB6C C8B2FFCC E069A462 A6EB6BE8 34C7CB58 21601F63 13BA9CFF C5AC0211 
 BC8FE95E AEF39C9D F4BAA5D5 A07B1FE4 3AC59EE1 50F6C758 08ECB08C 7ACF4358 
 4A99B7B8 803974A8 A5478DE6 DB31590B 5D0FE563 39F4AED2 10F68F29 C99212D1 
 4F57B0D8 F6DCD357 0373F21F 21730A36 3ACFA18D 9822F87A 5AF5C3D2 ED717181 
 A18A853F 1B351049 B534BDB4 F3E4AB7E F72E8437 31C0FB15 4A738E38 12CC306D 
 FD370EB0 66B32F45 8EFE0145 C5CB47BC 7194C59F 8944DB26 BDFB51D1 30A8BCD6 
 7FFFAB91 F27D2EC5 F2FC689B 4441396A 7C96
 quit
!
!
license udi pid ASR1001-HX sn JAE25031V0P
license boot level adventerprise
license smart transport off
archive
 log config
 logging enable
 logging size 1000
 notify syslog contenttype plaintext
memory free low-watermark processor 676980
!
!
spanning-tree extend system-id
diagnostic bootup level minimal
!
!
enable password Cisco#123
!
username cisco privilege 15 password 0 Cisco#123
username wlcaccess privilege 15 password 0 Lablab#123
!
redundancy
 mode none
!
!
!
!
!
cdp run
!
!
! 
!
!
!
!
!
!
!
!
!
!
!
! 
! 
!
!
interface Loopback0
 ip address 204.1.2.3 255.255.255.255
 ip router isis 1
 ip pim sparse-mode
!
interface GigabitEthernet0/0/0
 mtu 9100
 no ip address
 negotiation auto
 cdp enable
!
interface GigabitEthernet0/0/0.51
 encapsulation dot1Q 51
 ip address 204.1.1.25 255.255.255.252
!
interface GigabitEthernet0/0/1
 mtu 9100
 ip address 204.1.1.13 255.255.255.252
 no ip redirects
 no ip proxy-arp
 ip router isis 1
 ip pim sparse-mode
 load-interval 30
 negotiation auto
 bfd interval 750 min_rx 750 multiplier 3
 no bfd echo
!
interface GigabitEthernet0/0/2
 no ip address
 negotiation auto
 cdp enable
!
interface GigabitEthernet0/0/3
 no ip address
 negotiation auto
 cdp enable
!
interface GigabitEthernet0/0/4
 no ip address
 negotiation auto
 cdp enable
!
interface GigabitEthernet0/0/5
 no ip address
 negotiation auto
 cdp enable
!
interface GigabitEthernet0/0/6
 no ip address
 negotiation auto
 cdp enable
!
interface GigabitEthernet0/0/7
 no ip address
 negotiation auto
 cdp enable
!
interface TenGigabitEthernet0/1/0
 no ip address
 no negotiation auto
 cdp enable
!
interface TenGigabitEthernet0/1/1
 no ip address
 no negotiation auto
 cdp enable
!
interface TenGigabitEthernet0/1/2
 no ip address
 no negotiation auto
 cdp enable
!
interface TenGigabitEthernet0/1/3
 no ip address
 no negotiation auto
 cdp enable
!
interface TenGigabitEthernet0/1/4
 no ip address
 no negotiation auto
 cdp enable
!
interface TenGigabitEthernet0/1/5
 no ip address
 no negotiation auto
 cdp enable
!
interface TenGigabitEthernet0/1/6
 no ip address
 no negotiation auto
 cdp enable
!
interface TenGigabitEthernet0/1/7
 no ip address
 no negotiation auto
 cdp enable
!
interface GigabitEthernet0
 vrf forwarding Mgmt-intf
 no ip address
 shutdown
 negotiation auto
 cdp enable
!
router isis 1
 net 49.0001.1111.1111.0015.00
 is-type level-2-only
 metric-style wide
 log-adjacency-changes
 default-information originate
 bfd all-interfaces
!
ip http server
ip http authentication local
ip http secure-server
ip forward-protocol nd
!
ip tftp source-interface GigabitEthernet0
ip pim rp-address 204.1.2.36
ip pim register-source Loopback0
ip pim ssm default
ip route 0.0.0.0 0.0.0.0 204.1.1.26
ip scp server enable
!
!
!
snmp-server group default v3 priv 
!
!
!
!
!
control-plane
!
!
!
!
!
!
line con 0
 exec-timeout 0 0
 stopbits 1
line aux 0
line vty 0 4
 transport input ssh
!
call-home
 ! If contact email address in call-home is configured as sch-smart-licensing@cisco.com
 ! the email address configured in Cisco Smart License Portal will be used as contact email address to send SCH notifications.
 contact-email-addr sch-smart-licensing@cisco.com
 profile ""CiscoTAC-1""
 active
 destination transport-method http
!
!
!
!
!
!
netconf-yang
end

TB8-SJ-BORDER-CP-ASR# PR raised with the required changes:
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3960/overview The PR got merged to Groot branch and cherry-picked to Ghost and Guardian branches. Marking it as ""Done"".","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity', 'shockwave']",Amardeep Kumar,Closed,Avril Bower
SEEN-638,https://miggbo.atlassian.net/browse/SEEN-638,[Auton][Groot]: Test_TC10_DNAC_Verify_adding_range_discovery_ssh_global_credentials,"*Release: Groot*
 *ISO: 2.1.560.70065*

Script Name:  solution_test_sanityecamb.py

Testcases Impacted: 
||[Test_TC11_DNAC_Verify_adding_range_discovery_ssh_global_credentials|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=719308&size=98833&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F22-09%2Fsanity_TB14.2022Sep25_00:30:10.973907.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fauto%2Fdna-sol%2Fws%2Fphannguy8%2Fdnac-auto%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb.py&lineno=1066&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F22-09%2Fsanity_TB14.2022Sep25_00:30:10.973907.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&from=trade&view=all]|Errored|
||[test1_verify_adding_range_discovery_ssh_global_credentials|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=720060&size=82040&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F22-09%2Fsanity_TB14.2022Sep25_00:30:10.973907.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS]||

 

Failure log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=720060&size=82040&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F22-09%2Fsanity_TB14.2022Sep25_00:30:10.973907.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] 

 

Snip from failure:
{code:bash}
4042:  Discovery successfully created, number of devices found in discovery: 1.
4043:  Library group ""discovery"" method ""configure_verify_discovery_range"" returned in 0:01:06.576170
4044:  Traceback (most recent call last):
4045:    File ""/auto/dna-sol/ws/phannguy8/dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 1087, in test1_verify_adding_range_discovery_ssh_global_credentials
4046:      self.failed(""Result: Device Discovery is failed"", goto=['common_cleanup'])
4047:    File ""/auto/dna-sol/pyats-ws/pyats-phannguy-seen-53/lib/python3.8/site-packages/pyats/aetest/base.py"", line 545, in failed
4048:      raise signals.AEtestFailedSignal(reason, goto, from_exception, data)
4049:  pyats.aetest.signals.AEtestFailedSignal: ('Result: Device Discovery is failed', ['common_cleanup'], None, None)
4050: 
4051:  During handling of the above exception, another exception occurred:
4052: 
4053:  Traceback (most recent call last):
4054:    File ""/auto/dna-sol/ws/phannguy8/dnac-auto/services/commonlibs/test_wrapper.py"", line 301, in wrapper
4055:      result = testfunc(func_self, **kwargs)
4056:    File ""/auto/dna-sol/ws/phannguy8/dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 1104, in test1_verify_adding_range_discovery_ssh_global_credentials
4057:      if self.lldp_devices:
4058:  AttributeError: 'Test_TC11_DNAC_Verify_adding_range_discovery_ssh_g' object has no attribute 'lldp_devices'
4059:  Test returned in 0:01:07.257376
4060:  Errored reason: 'Test_TC11_DNAC_Verify_adding_range_discovery_ssh_g' object has no attribute 'lldp_devices'
4061: 
4062:  Exception:
4063:  Traceback (most recent call last):
4064:    File ""/auto/dna-sol/ws/phannguy8/dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 1087, in test1_verify_adding_range_discovery_ssh_global_credentials
4065:      self.failed(""Result: Device Discovery is failed"", goto=['common_cleanup'])
4066:    File ""/auto/dna-sol/pyats-ws/pyats-phannguy-seen-53/lib/python3.8/site-packages/pyats/aetest/base.py"", line 545, in failed
4067:      raise signals.AEtestFailedSignal(reason, goto, from_exception, data)
4068:  pyats.aetest.signals.AEtestFailedSignal: ('Result: Device Discovery is failed', ['common_cleanup'], None, None)
4069: 
4070:  During handling of the above exception, another exception occurred:
4071: 
4072:  Traceback (most recent call last):
4073:    File ""/auto/dna-sol/ws/phannguy8/dnac-auto/services/commonlibs/test_wrapper.py"", line 301, in wrapper
4074:      result = testfunc(func_self, **kwargs)
4075:    File ""/auto/dna-sol/ws/phannguy8/dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 1104, in test1_verify_adding_range_discovery_ssh_global_credentials
4076:      if self.lldp_devices:
4077:  AttributeError: 'Test_TC11_DNAC_Verify_adding_range_discovery_ssh_g' object has no attribute 'lldp_devices'
4078:  The result of section test1_verify_adding_range_discovery_ssh_global_credentials is => ERRORED
{code}",2022-09-25T21:52:27.801+0000,"When there are more than one set of devices (such as having a set of WLC), there will be more than one Discovery jobs.

Before the final job has been done, the number of discovered devices (numDev) is less than device_list, and the result is set to FAIL

!image-2022-09-25-15-02-56-430.png|width=882,height=274!

After all jobs have been performed, the condition numDev >= len(device_list) is True but the result is not updated to True, resulting a failed test.

Also, there is a flaw in test1_verify_adding_range_discovery_ssh_global_credentials that self.lldp_devices is referenced in the finally: before assignment","['Auton', 'Groot', 'Issue']",Phan Nguyen,Closed,Avril Bower
SEEN-639,https://miggbo.atlassian.net/browse/SEEN-639,[Auton]Test_TC5_install_system_packages/test1_install_system_packages,"*Reporter Analysis:* Timeout issue while running this TC due to which packages  installation failing and other TC got blocked

*Description:*  
 1772: DNAC not yet ready!
 1773: Waiting for DNAC to come back after restore from backup or from reload......
 1774: 
 1775: Trying to reconnect to DNAC
 1776: 
 1777: 
 1778: Setting up maglev-based DNAC
 1779: 
 1780: 
 1781: 
 1782: Setting up maglev-based DNAC apicem_client: username admin, Password Maglev123
 1783: Connecting to the Apic-em northbound API client.
 1784: Resource path full url: [https://10.195.227.31/api/system/v1/identitymgmt/login]
 1785: Traceback (most recent call last):
 1786: File ""/users/rvonmize/pyats-inst/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py"", line 472, in wrap_socket
 1787: cnx.do_handshake()
 1788: File ""/users/rvonmize/pyats-inst/lib/python3.7/site-packages/OpenSSL/SSL.py"", line 1716, in do_handshake
 1789: self._raise_ssl_error(self._ssl, result)
 1790: File ""/users/rvonmize/pyats-inst/lib/python3.7/site-packages/OpenSSL/SSL.py"", line 1431, in _raise_ssl_error
 1791: raise WantReadError()
 1792: OpenSSL.SSL.WantReadError
 6293: Encountered unhandled error in Internal API Call
 6294: Flagging result as FAIL!
 6295: Reason: <class 'requests.exceptions.ReadTimeout'>
 6296: Kwargs:
 6297: {}
 6459: Encountered unhandled error in Internal API Call
 6460: Flagging result as FAIL!
 6461: Reason: <class 'requests.exceptions.ReadTimeout'>

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* restore_frombackup_upgrade_script.py

*Source Team:  Upgrade Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=208526&size=4328919&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F19%2F22%2F25%2Fauto_upgrade_job.2022Aug19_22:25:11.019737.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:*N/A

*Testbed details:* NA",2022-09-26T09:12:31.355+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/services/dnaserv/lib/api_groups/upgrade/group.py?until=33b785f0892d4daace76438cc69dc5b02e38bdd5]

New way of release upgrade needed handling.","['Auton', 'Guardian', 'Issue', 'Upgrade']",Nethra Thorali Suthagar,Resolved,Avril Bower
SEEN-640,https://miggbo.atlassian.net/browse/SEEN-640,[Auton]Ghost:  Test_TC5_TSIM_verify_tsim_clients_on_wlc/test1_verify_tsim_clients failed with NameError: name 'time' is not defined,"*Reporter Analysis:* This testcase verifies if the TSIM clients have joined TB6-DM-eCA-BORDER. while running command 'show wireless client summary' multiple times on device using command runner. The command runner throws error as 'This device is already under process by command runner in another session,try with other device'. 

As seeing this script is trying to sleep for 30s, where the script fails with ""NameError: name 'time' is not defined"".

*Description:  The error from log or more info* 
 Resource path full url: [https://10.195.227.92/api/v1/task/07b9db50-d2e0-4640-8747-8b512b5c078d/tree]
 5205: {'response': [{'startTime': 1664092088530, 'endTime': 1664092108785, 'version': 1664092108785, 'progress': '

{""fileId"":""3d5979e8-b6cb-4b9b-a36f-dabcf577ef84""}

', 'lastUpdate': 1664092108785, 'rootId': '07b9db50-d2e0-4640-8747-8b512b5c078d', 'username': 'provision', 'serviceType': 'Command Runner Service', 'isError': False, 'instanceTenantId': '632f762c67650065978116ad', 'id': '07b9db50-d2e0-4640-8747-8b512b5c078d'}, \{'startTime': 1664092108539, 'endTime': 1664092108780, 'version': 1664092108539, 'progress': 'Could not execute command on the device', 'parentId': '07b9db50-d2e0-4640-8747-8b512b5c078d', 'rootId': '07b9db50-d2e0-4640-8747-8b512b5c078d', 'failureReason': 'This device is already under process by command runner in another session,try with other device', 'serviceType': 'Command Runner Service', 'isError': True, 'instanceTenantId': '632f762c67650065978116ad', 'id': '2d09ade5-c79d-4522-a39f-9d007a5e38b3'}], 'version': '1.0'}
 5206: TB6-DM-eCA-BORDER
 5207: 
 5208: 
 5209: api_switch_call called:
 5210: {}
 5211: Resource path full url: [https://10.195.227.92/api/v1/file/3d5979e8-b6cb-4b9b-a36f-dabcf577ef84]
 5212: [
 \{'deviceUuid': 'fdee48c1-f8e7-4c6e-a9c3-1bdfd67a0fb3', 'commandResponses': {'SUCCESS': {}, 'FAILURE':

{'show wireless client summary ': 'This device is already under process by command runner in another session,try with other device'}

, 'BLACKLISTED': {}}}]
 5213: Traceback (most recent call last):
 5214: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost_Optimized-Deployment_and_Sanity-Job/services/dnaserv/lib/api_groups/command_runner/group.py"", line 86, in execute_command_on_device
 5215: time.sleep(30)
 5216: NameError: name 'time' is not defined
 5217: Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:20.475489
 5218: No output received from device
 5219: \{'result': False, 'output': 'Command Failed'}
 5220: Library group ""cli_check"" method ""verify_client_count"" returned in 0:00:20.476315
 5221: Library group ""cli_check"" method ""verify_client_count"" returned in 0:00:46.898965
 *Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* [FEWAccessPointAndCLients|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep25_00:14:20.367278.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/ Test_TC5_TSIM_verify_tsim_clients_on_wlc/test1_verify_tsim_clients

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*  N/A

*Fail Log:*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1593854&size=65344&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep25_00:14:20.367278.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Pass Log:* pass log from Groot RC7 2.1.560.70523

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=2435851&size=181058&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep19_12:02:55.553779.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-6] ",2022-09-27T09:14:28.629+0000,"Added Import stmt
[31697fc79a9|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/31697fc79a9729fe71590d96346bc5f0a7d2bcc6] This has been fixed in Ghost 70471 run : 
The name error 'time' has been resolved
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=2349669&size=60508&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct07_00:25:49.937028.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ","['Auton', 'Ghost', 'Issue', 'Optimized']",Nethra Thorali Suthagar,Closed,Avril Bower
SEEN-641,https://miggbo.atlassian.net/browse/SEEN-641,[Auton]: Test_TC4_configure_catalog_server  /   test1_configure_catalog_server,"*Reporter Analysis:* TC got false passed, didn't even set catalog server of AWS


*Expected Behaviour:*TC should fail if it is not setting AWS Catalog Server

*Description:*  
 **
 865: Sending line: `magctl service unsetenv catalogserver PARENT_CATALOG_SERVER_TOKEN`
 866: Response received:
 867: ERROR: No Environment value with name PARENT_CATALOG_SERVER_TOKEN exist
 868: Send complete
 869: Sending line: `magctl service unsetenv catalogserver PARENT_CATALOG_SERVER_REPOSITORY`
 870: Response received:
 871: deployment.extensions/catalogserver patched
 872: Send complete
 873: Sending line: `magctl service restart catalogserver`
 874: Response received:
 875: ERROR: No container matches found
 876: Send complete
 877: Sending line: `magctl service setenv catalogserver PARENT_CATALOG_SERVER [https://staging.tesseractcloud.com:443`|https://staging.tesseractcloud.com:443`/]
 878: Response received:
 879: deployment.extensions/catalogserver patched
 880: Send complete
 881: Sending line: `magctl service setenv catalogserver PARENT_CATALOG_SERVER_REPOSITORY cisco-dnac-groot`
 882: Response received:
 883: deployment.extensions/catalogserver patched
 884: Send complete
 885: Sending line: `magctl service setenv catalogserver PARENT_CATALOG_SERVER_OVERRIDE true`
 886: Response received:
 887: deployment.extensions/catalogserver patched
 888: Send complete
 889: Sending line: `magctl service unsetenv catalogserver CATALOG_SRV_INSECURE `
 890: Response received:
 891: ERROR: No Environment value with name CATALOG_SRV_INSECURE exist
 892: Send complete
 893: Sending line: `magctl service setenv catalogserver CATALOG_SRV_INSECURE true`
 894: Response received:
 895: deployment.extensions/catalogserver patched
 896: Send complete
 897: Sending line: `maglev catalog settings validate`
 898: Response received:
 899: Validating catalog server settings...
 900: Successfully scheduled job to validate catalog settings.
 901: Send complete
 902: Sending line: `magctl service unsetenv catalogserver MAGLEV_HTTPS_PROXY`
 903: Response received:
 904: ERROR: No Environment value with name MAGLEV_HTTPS_PROXY exist
 *Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* restore_frombackup_upgrade_script.py

*Source Team: Upgrade Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:NA*

*False Pass Log:*
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=184967&size=23248&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F19%2F22%2F25%2Fauto_upgrade_job.2022Aug19_22:25:11.019737.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:* NA",2022-09-27T10:52:31.722+0000,,"['Auton', 'Groot', 'Issue', 'Upgrade']",Anusha John,Closed,Avril Bower
SEEN-642,https://miggbo.atlassian.net/browse/SEEN-642,[Auton] Shockwave : Test_TC68_configure_policy /test2_aduit_log_verify," 

*Reporter Analysis:* 

Audit log from the Failed log :
 'description': 'Received Scalable Group Provisioning/ Application registry/ Application registry request for custom_class_UDP_1_10.'

Expected Audit log description :
 'description': 'Received Scalable Group Provisioning/ Application registry request for UDP_31.'

*Description:* description': 'Received Scalable Group Provisioning/ Application registry/ Application registry request for custom_class_UDP_1_10',

*Branch Name:  private/Shockwave-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=39763697&size=78558&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F09%2F25%2F14%2F29%2Fenv_auto_job.2022Sep25_14:29:16.368901.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:*

*Testbed details:* NA",2022-09-27T14:34:04.217+0000,Same as https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-397 https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/5a21ed58828cca47ffd5a2606565e64ddb9d1599 Verified the fix,"['Auton', 'Issue', 'Sanity', 'shockwave']",Nethra Thorali Suthagar,Closed,Avril Bower
SEEN-644,https://miggbo.atlassian.net/browse/SEEN-644,"[Auton] :Groot -verify admin should be able to create and run ""Port Reclaim"" report to manage capacity for the switches ,Need to  wiki  & WebEx recording"," 

*Reporter Analysis:[Twj882640r|https://tims.cisco.com/warp.cmd?ent=Twj882640r]* 
 Need to wiki & Webex recording, Verification/Screenshot for Feature integration. as well add testcases in optimized sanity code.


 
 

 

*Description:  N/A*

*Branch Name:  private/Groot-ms/sanity_api_auto*

                         *private/Ghost-ms/sanity_api_auto*
                          
                                                  

 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:N/A*

*Pass Log:N/A*

*Testbed details:* NA",2022-09-27T18:26:54.101+0000,"wiki : [https://wiki.cisco.com/display/EDPEIXOT/Port+Reclaim+Report]
Pass log:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2330564&size=53860&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct15_09:11:11.924290.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS","['AWS_Sanity', 'Auton', 'Groot', 'Issue', 'Optimized', 'Sanity']",Rishika Bangalore Chandrashekar,Closed,Avril Bower
SEEN-645,https://miggbo.atlassian.net/browse/SEEN-645,[Auton]  Groot:DNAC RLAN Workflow feature-Need to WebEx Recording & include in optimized code," *Reporter Analysis:* Please provide a feature WebEx recording 

*Description:  The error from log or more info* 

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

*Pass Log:*

*Testbed details:* NA

*NOTE:Please commit this code in optimized sanity*",2022-09-27T18:52:07.617+0000,"*NOTE:Please commit this code in optimized sanity* here is the feature video 

 

[Webex Enterprise Site - Recorded Meeting Password|https://cisco.webex.com/recordingservice/sites/cisco/recording/playback/dbc8738e31e7103bbe7100505681412a]

 

HvMBm9KC Please raise a new Jira for adding the code into optimised one","['AWS_Sanity', 'Auton', 'Groot', 'Issue', 'Optimized', 'Sanity']",Avril Bower,Closed,Avril Bower
SEEN-646,https://miggbo.atlassian.net/browse/SEEN-646,[Auton]  BAPI usecase," 

*Reporter Analysis:*  Just creating the ticket to track the issue in the script

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-bapi_validations.py-161-BAPI&begin=115165&size=142003&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep26_14:22:48.364411.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Description:  The error from log or more info* 

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

*Pass Log:*

*Testbed details:* NA",2022-09-27T21:32:38.392+0000,Closing this ticket as the report does not have much detail and the log is also not accessible.,"['Auton', 'Issue', 'Sanity', 'optimized']",Raju Saran,Closed,Avril Bower
SEEN-647,https://miggbo.atlassian.net/browse/SEEN-647,"[Auton] EXT_API workaround for CSCwc60501: ""Rate Limit exceeded"" Error even Execution-status Success"," 

*Reporter Analysis:* Venkat, Can you please add the workaround in groot branch for the defect CSCwc60501

 

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:*  

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:* Seen first time 

*Fail Log:* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4013028&size=121158&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep27_01:28:17.309271.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

 

*Pass Log: NA*

*Testbed details:* NA !image-2022-09-07-18-12-48-915.png!",2022-09-27T23:55:42.870+0000,Workaround added in scripts to handle both SSID and Profile rate limits.,"['Auton', 'Ext-API', 'Ghost', 'Groot', 'Issue', 'Sanity']",Pawan Singh,Resolved,Avril Bower
SEEN-648,https://miggbo.atlassian.net/browse/SEEN-648,[Auton] [Groot] -  500 server error on accessing api/assurance/v2/analytics from Global level events page,"*Uber ISO Version tested :* 
 Promoted Groot RC6 Uber ISO - *2.1.560.70.517, Non-FIPS, PUBSUB enabled*

Promoted Groot RC7 Uber ISO - *2.1.560.70.523, Non-FIPS, PUBSUB enabled*

*Script Name:* solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py, solution_test_3sites_sjc_nyc_sf.py, solution_test_sanityecamb_lan.py

*Testbed :* MSTB1, MSTB2, Sanity
  

*Description :* 

We see observing 500 server error on accessing *https://<cluster_ip>/api/assurance/v2/analytics* POST call in the Global level events verification TC. This corresponds to Assurance->Dashboard->Issues & Events page in the DNAC GUI. 

We had reported defect for this -  [https://cdetsng.cisco.com/summary/#/defect/CSCwd04233] 

As per discussion with Saikrishna from DE Team, there has been a recent code change on Groot and we need to pass deviceFamily along with associated values in POST call under Assurance->Dashboard->Issues & Events TC check.

Example:

{""key"":""deviceFamily"",""value"":[""Routers"",""Switches and Hubs"",""Wireless Controller""],""operator"":""in""} \{""key"":""deviceFamily"",""value"":[""Wired Client""],""operator"":""in""} \{""key"":""deviceFamily"",""value"":[""Wireless Client""],""operator"":""in""}

!09_26_31.jpg!

*Failed log:*

*1)* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1995593&size=23998&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fsr_mb_multi_sites_mdnac.2022Sep13_05:20:17.746909.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] -> Refer TC214

*2)* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19462401&size=24153&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep20_03:18:59.717984.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] -> Refer TC180",2022-09-28T10:41:22.344+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3997/overview]

[Auton] [Groot] - 500 server error on accessing api/assurance/v2/analytics from Global level events page.

  [~62d2fec15d6f5fd2c3db8f9f], please verify and close if fixed. @[~62d2fe9f8afb5805e5d5af49]: Regression testbeds are currently on Guardian P4 / Ghost RC1. Will try to check and update the observations if issue on these.

With respect to Groot, once we get Groot Patch1 RC3 to test, we will verify [~62d2fe9f8afb5805e5d5af49]/ [~63f50bcafb3ac4003fa2c6dd]: We are observed same this issue on Ghost as well on during Ghost RC1 testing. So we need to have fix also on Ghost.

Failed log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8781497&size=22901&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fsr_mb_multi_sites_mdnac.2022Dec02_03:24:20.960946.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] -> Refer TC214 Please refer latest comments with respect to Ghost. [Auton] [Ghost] - 500 server error on accessing api/assurance/v2/analytics from the Global level events page.

PR: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4336/overview [~62d2fe9f8afb5805e5d5af49] / [~63f50bcafb3ac4003fa2c6dd]: We do not see the issue during Guardian Patch4 testing. So the fix is not required on Guardian code base.

Pass log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1419627&size=378193&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fauto_MS_job.2022Dec05_04:21:44.555395.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] -> Refer TC155 Merged to Groot, Ghost.","['AWS_MSTB', 'Auton', 'Ghost', 'Groot', 'Integration', 'Issue', 'MSTB1', 'MSTB2', 'Multisite', 'Sanity']",NhanHuu Nguyen,Resolved,Avril Bower
SEEN-650,https://miggbo.atlassian.net/browse/SEEN-650,"[Auton]: Need to add ""ap dtls-version DTLS_1_0"" CLI for ECA device in Shockwave.","Hi Nethra,

    Need to add CLI ""ap dtls-version DTLS_1_0"" on the ECA device when it has 17.9.1 image in Shockwave branch code. The TSIM APs are not joining to the ECA device when we are upgrading from Shockwave to Guardian.

 

Thanks,

Vijayakumar G.",2022-09-29T06:50:58.050+0000,"[https://wiki.cisco.com/display/EDPEIXOT/Guidelines+-+TSIM+Issues+debugging]

 

Several things to be added. Fixed in Ghost, Halleck, Groot and Shockwave.

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f635293e3558eec863f769adb4287e6d1f7c05d8|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f635293e3558eec863f769adb4287e6d1f7c05d8]","['Auton', 'Issue', 'Sanity', 'Shockwave', 'Upgrade']",Pawan Singh,Resolved,Avril Bower
SEEN-651,https://miggbo.atlassian.net/browse/SEEN-651,[Auton]  Airos wlc and ewlc to be added in Meta.json," 

*Reporter Analysis:* 

We need to enhance the meta.json for dashboard data

-WLC/ewlc sku and version to be added

- Latest API to get the uber version from the dna-c

 

 

*class* common_cleanup(aetest.CommonCleanup):
     @aetest.subsection
     *def* collect_testbed_metadata(self, dnac_handle):
         dnac_handle.collect_testbed_metadata()

 

*Description:  The error from log or more info* 

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:*  optimized code and legacy code

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*https://earms-trade.cisco.com/tradeui/logs/fileviewer?logfile=meta.json&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep20_00:16:08.928228.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=pyATS&from=trade&view=all

*Testbed details:* NA",2022-09-30T01:04:31.137+0000,"from your logs: (eWLC SKUs are already in the report).

41512:  C9800-40-K9, C9800-40-K9 :: 17.9.20220918:160808 :: Cisco Catalyst 9800 Series Wireless Controllers

 

Also script collects all SKU available in DNAC irrespective of type. it even includes APs.

 

 ","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'Sanity', 'shockwave']",Pawan Singh,Resolved,Avril Bower
SEEN-652,https://miggbo.atlassian.net/browse/SEEN-652,"[Auton]  TC1 cleanup uses the config replace method, Need to use copy underlay to startup"," 

*Reporter Analysis:*  we are planning to use TC1 for production run, but the cleanup test is just replacing thing config file, but we need to copy the underlay config file in startup and reload the device.

 

*Description:  The error from log or more info* 

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: Day0 issue*

*Fail Log:* https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep29_21:42:11.793941.zip&atstype=ATS

*Pass Log:*

*Testbed details:* NA",2022-09-30T07:01:10.861+0000,"Raju, Nethra: To use copy cleanup_config to start and reload. you need to have this flag  enabled in your fabric input file. This is to enforce that is it done on purpose.

 
wr_erase_reload=true
 
The code part is good and working.","['Auton', 'Day0', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity', 'shockwave']",Pawan Singh,Closed,Avril Bower
SEEN-654,https://miggbo.atlassian.net/browse/SEEN-654,"[Auton] Add the testcases in optimized sanity code _TC186,190,191,193","Please add the below mentioned testcases in optimized sanity code. 

 
 * Test_TC186_netconf_tdl_notification_for_swim
 * Test_TC190_bgp_down_issue
 * Test_TC191_enable_AP_parameters_recurrence_using_workflow
 * Test_TC193_default_route_verification

 

Please double commit this in Guardian,Groot & Ghost branches",2022-09-30T10:08:03.545+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3938/overview]

add optimized TC193, not mapping yet [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3937/overview]

add optimized TC191, not mapping yet [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3936/overview]

add optimized TC190, not mapping yet [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3935/overview]

add optimized TC186, not mapping yet","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'sanity']",QuangVinh Nguyen,Closed,Avril Bower
SEEN-655,https://miggbo.atlassian.net/browse/SEEN-655,"[Auton] Add the testcases in optimized sanity code _TC152,160,171","Please add the below mentioned testcases in optimized sanity code. 

Sanity_script_file: solution_test_sanityecamb_lan.py

 
 * TC152_verify_AI_anaylitic_data
 * Test_TC160_Brownfield_Workflow_Aireos
 * Test_TC171_syslog_server_event_notification

 
 Please double commit this in Guardian,Groot & Ghost branches",2022-09-30T10:12:57.438+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3930/overview]

add optimized TC171, not mapping yet [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3929/overview]

add optimized TC160, not mapping yet [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3928/overview]

add optimized TC152, not mapping yet 152---kairos","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'sanity']",QuangVinh Nguyen,Closed,Avril Bower
SEEN-656,https://miggbo.atlassian.net/browse/SEEN-656,"[Auton] Add the testcases in optimized sanity code _TC139,142,146,147","Please add the below mentioned testcases in optimized sanity code. 

Sanity_script_file: solution_test_sanityecamb_lan.py
 * Test_TC139_Enhance_RCA_AAA_Issue
 * Test_TC142_Add_interace_description
 * Test_TC146_Random_mac_enable
 * Test_TC147_Radius_profile_enable

 
Please double commit this in Guardian,Groot & Ghost branches",2022-09-30T10:18:26.482+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3905/overview]

Add optimized code for TC139, not mapping yet. [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3906/overview]

Add optimized code for TC142, not mapping yet. [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3907/overview]

Add optimized code for TC146, not mapping yet. [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3908/overview]

Add optimized code for TC147, not mapping yet. 139—15

142–17

146–--19

147--19","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'Sanity']",NhanHuu Nguyen,Closed,Avril Bower
SEEN-657,https://miggbo.atlassian.net/browse/SEEN-657,"[Auton] Add the testcases in optimized sanity code _TC132,135,137,170","Please add the below mentioned testcases in optimized sanity code. 

Sanity_script_file: solution_test_sanityecamb_lan.py
 * Test_TC132_verify_border_edge_kpi
 * Test_TC135_security_advsiory
 * Test_TC137_ip_pool_block_listing
 * Test_TC170_overlapping_pools_negative_operations

 
 Please double commit this in Guardian,Groot & Ghost branches",2022-09-30T10:22:52.060+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3927/overview]

add optimized TC170, not mapping yet [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3926/overview]

add optimized TC137, not mapping yet [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3925/overview]

add optimized TC135, not mapping yet [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3924/overview]

add optimized TC, not map132ping yet 132----17

135—18

137—18

170--?","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'Sanity']",QuangVinh Nguyen,Closed,Avril Bower
SEEN-658,https://miggbo.atlassian.net/browse/SEEN-658,"[Auton] Add the testcases in optimized sanity code _TC121,125,126,128,129","Please add the below mentioned testcases in optimized sanity code. 

Sanity_script_file: solution_test_sanityecamb_lan.py
 * Test_TC121_verify_custom_vlan_ids
 * Test_TC125_verify_site_provision_readiness_design_view
 * Test_TC126_verify_inventory_insights
 * Test_TC128_verify_global_devices_count
 * Test_TC129_Disconnect_Delete_Reason

 
 Please double commit this in Guardian,Groot & Ghost branches",2022-09-30T10:25:14.862+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3923/overview]

add optimized TC129, not mapping yet [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3922/overview]

add optimized TC128, not mapping yet [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3921/overview]

add optimized TC126, not mapping yet [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3919/overview]

add optimized TC121, not mapping yet

(merged) [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3920/overview]

add optimized TC125, not mapping yet 121—mapping 19

125—mapping 19

126—mapping19

128–mapping 14

129---17 125 already exists","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'Sanity']",QuangVinh Nguyen,Closed,Avril Bower
SEEN-659,https://miggbo.atlassian.net/browse/SEEN-659,"[Auton] Add the testcases in optimized sanity code _TC138,143,172,173","Please add the below mentioned testcases in optimized sanity code. 

Sanity_script_file: solution_test_sanityecamb_lan.py
 * Test_TC138_system_health_assurance_checks
 * TC143_system_health_test
 * TC172_verify_fabric_assurance_metrics
 * TC173_verify_sdatransit_fabric_assurance_metrics

 
 Please double commit this in Guardian,Groot & Ghost branches",2022-09-30T10:26:57.473+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3916/overview]

add optimized TC138, not mapping yet [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3918/commits]

add optimized TC172 and TC173,not mapping yet [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3917/overview]

add optimized TC143, not mapping yet 143 ----new 21

172,173---13","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'Sanity']",QuangVinh Nguyen,Closed,Avril Bower
SEEN-660,https://miggbo.atlassian.net/browse/SEEN-660,"[Auton] Add the testcases in optimized sanity code _TC94,119,187,188","Please add the below mentioned testcases in optimized sanity code. 

Sanity_script_file: solution_test_sanityecamb_lan.py
 * Test_TC94_generate_link_flap_issues
 * Test_TC119_ap_attributes
 * Test_TC187_generate_device_link_AP_flap_issues
 * Test_TC188_generate_Port_Reclaim_report

 
 Please double commit this in Guardian, Groot & Ghost branches",2022-09-30T10:30:45.234+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3889/diff]

Add optimized code for TC94, not mapping yet. [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3890/overview]

Add optimized code for TC119, not mapping yet. [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3903/overview]

Add optimized code for TC187, not mapping yet. [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3904/overview]

Add optimized code for TC188, not mapping yet. tC94 ---mapping 18

TC119—mapping18 (required console)","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'Sanity']",NhanHuu Nguyen,Closed,Avril Bower
SEEN-661,https://miggbo.atlassian.net/browse/SEEN-661,"[Auton] Add the testcases in optimized sanity code _TC85,102,103,148","Please add the below mentioned testcases in optimized sanity code. 

Sanity_script_file: solution_test_sanityecamb_lan.py
 * Test_TC85_generate_exceutive_summary_report
 * Test_TC102_DNAC_External_Authentication_Radius
 * Test_TC103_DNAC_External_Authentication
 * Test_TC148_ap_report_generation

 
 Please double commit this in Guardian, Groot & Ghost branches",2022-09-30T10:32:19.984+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3886/overview]

add optimized code for TC85 [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3887/overview]

add optimized code for TC102 and TC 103

i see TC102 and TC103 have common ""External Authentication"", i think they do the relate jobs so i put it into one folder

  [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3888/overview]

add optimized code for TC148 TC85: sanity mapping 15 Merged TC85, TC102 TC102—sanity mapping new 20

148---17","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'Sanity']",QuangVinh Nguyen,Closed,Avril Bower
SEEN-662,https://miggbo.atlassian.net/browse/SEEN-662,[Auton][IBSTE] : Extra log is required while onboarding extended nodes/AP/Tsim/Sensors,"Hi Team,

 

Currently after making extended nodes ready for PNP reload command is given. After reload command is given we want extra logs that gets generated on both the devices i.e., Edgenode and extended_node, so that it would be easy for debugging incase device onboarding fails.

 

Example :

SN-FOC2416L4X3#
157569:  +++ SN-FOC2416L4X3 with via 'a': executing command 'delete /force /recursive nvram:*.cer' +++
delete /force /recursive nvram:*.cer SN-FOC2416L4X3#
157572:  +++ SN-FOC2416L4X3 with via 'a': receive +++
wr erase Erasing the nvram filesystem will remove all configuration files! Continue? [confirm]
157574:  Timeout occurred : Timeout value : 5
157575:  Target: SN-FOC2416L4X3
157576:  Command sent: wr erase
157577:  Pattern: 'Erasing the nvram filesystem will remove all configuration files! Continue? [confirm]'
157578:  Got: 'wr erase\r\nErasing the nvram filesystem will remove all configuration files! Continue? [confirm]'
157579:  +++ SN-FOC2416L4X3 with via 'a': receive +++
y[OK] Erase of nvram: complete SN-FOC2416L4X3# SN-FOC2416L4X3#reload Proceed with reload? [confirm]
157582:  Timeout occurred : Timeout value : 5
157583:  Target: SN-FOC2416L4X3
157584:  Command sent: reload
157585:  Pattern: 'Proceed with reload? [confirm]'
157586:  Got: 'wr erase\r\nErasing the nvram filesystem will remove all configuration files! Continue? [confirm]y[OK]\r\nErase of nvram: complete\r\nSN-FOC2416L4X3#\r\nSN-FOC2416L4X3#reload\r\nProceed with reload? [confirm]'
 
We are expecting next part of logs so that it would be easy for us to track the issue",2022-09-30T14:35:44.479+0000,"As per PNP ask, the device should not be touched or logged in during pnp process or it will break pnp workflow. That's it can not be done. 

In case pnp process does not work, the device will be in the same state, you can always take logs from the device console.

 

 ","['Auton', 'Groot', 'Guardian', 'IBSTE', 'Issue']",Pawan Singh,Resolved,Avril Bower
SEEN-663,https://miggbo.atlassian.net/browse/SEEN-663,[Auton]  toolsDiscovery usecsae is not working for nonlanusecase suite," 

*Reporter Analysis:* toolsDiscovery usecsae is not working for nonlanusecase suite, currently it's doing the discovery only for the non-lan

 
deftest1_verify_adding_range_discovery_ssh_global_credentials(self, dnac_handle):
dev_list = []
dnac_handle.reconnect_clients()
fordeviceindnac_handle.dnaconfig.devlist:
if (dnac_handle.dnaconfig.testbed.devices[device].pnprole.find(""SEED"") != -1 \
ordnac_handle.dnaconfig.testbed.devices[device].pnprole.find(""PEER"") != -1 \
ordnac_handle.dnaconfig.testbed.devices[device].role.find(""WLC"") != -1):
logger.info(""Seed devivce found: {}"".format(device))
dev_list.append(device)

logger.info(""Action: Add a new DNAC Discovery, Verify discovery added successfuly and reaches to complete"")
try:
ifdnac_handle.configure_verify_discovery_range(""DNAC-Discovery_lan_auto"", device_list=dev_list,
discoveryiptype=""mgmt"", addresstype=""random""):
logger.info(
""Result: Device Discovery addition is successful, and state is complete, all devices discovered."")
else:
self.failed(""Result: Device Discovery is failed"")
exceptAssertionError:
self.failed(""Result: Device Discovery is failed"", goto=['common_cleanup'])
 

*Description:  The error from log or more info* 

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep29_23:00:17.010597.zip&atstype=ATS]

 

*Pass Log:*

*Testbed details:* NA",2022-09-30T20:11:20.091+0000,[~5f3c6ae932360700388f7b4b] fixed the issue,"['Auton', 'Issue', 'Sanity', 'optimized']",Nethra Thorali Suthagar,Resolved,Avril Bower
SEEN-667,https://miggbo.atlassian.net/browse/SEEN-667,"[Auton] Add the testcases in optimized sanity code _TC64,TC68","Please add the below mentioned testcases in optimized sanity code. 

Sanity_script_file: solution_test_sanityecamb_lan.py
 * Test_TC64_DEV_STRESS_mcast_traffic_convergence_test_primary_border_reload
 * Test_TC68_configure_policy

Please double commit this in Guardian, Groot & Ghost branches",2022-10-03T06:48:02.490+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3875/overview]

 

[SEEN-667] add optimized TC68 and mapping","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'Sanity']",QuangVinh Nguyen,Closed,Avril Bower
SEEN-668,https://miggbo.atlassian.net/browse/SEEN-668,[Auton]Need Script Modification on Device Compliance Check in DNA-C for Ghost Release,"After upgrade to Ghost,  To check the device compliance on DNA-C compliance page, we need to click the ""Fix all Configuration Compliance issues"" banner to fix compliance issue in DNA-C Ghost release.

Attached mail thread of DNA-C compliance team suggestion.

 

 

!image-2022-10-03-12-48-32-565.png!",2022-10-03T07:22:44.296+0000,Issue no longer seen,"['Auton', 'Compliance', 'DNAC', 'Ghost', 'Issue', 'Sanity', 'Upgrade']",Raji Mukkamala,Closed,Avril Bower
SEEN-669,https://miggbo.atlassian.net/browse/SEEN-669,[Auton][Guardian] Talos cloud authenticate TC failing due to ElementClickInterceptedException,"*Uber ISO Version tested :* 
 Promoted Guardian Patch2 RC2 Uber ISO - *2.1.515.70134, Non-FIPS, PUBSUB enabled*

 

*Script Name:* solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py

*Testbed :* MSTB1
  

*Description :* 

Even after the recent Talos commit changes - [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3652/overview] , we are still observing issue in integrating Talos feature. We see cloud authenticate TC is failing due to ElementClickInterceptedException. Looks to be Xpath issue.

 

*Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fsr_mb_multi_sites_mdnac.2022Oct02_23:30:06.284838.zip&atstype=ATS] -> Refer TC225",2022-10-04T12:27:38.692+0000,PR: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4159/overview,"['Auton', 'Guardian', 'Issue', 'MSTB1', 'Multisite']",Moe Saeed,Resolved,Avril Bower
SEEN-670,https://miggbo.atlassian.net/browse/SEEN-670,Auton:[Groot]:Test_TC4_configure_catalog_server/test1_configure_catalog_server,"*Reporter Analysis:* After setting CATALOG_SERVER_REPOSITORY via script, script is taking latest Groot  package  instead of the  Groot release need to be selected

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* restore_frombackup_upgrade_script.py

*Source Team:  Upgrade Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:NA*

*Pass Log:*
https://earms-trade.cisco.com/tradeui/logs/details?ats=%2Fusers%2Frvonmize%2Fpyats-inst&client=web&host=st-ucs-sanity.cisco.com&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_upgrade_job.2022Oct04_01:41:53.898691.zip

*Testbed details:* NA

*Affected TC's:*

[Test_TC5_install_system_packages|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=185627&size=2654003&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F04%2F01%2F41%2Fauto_upgrade_job.2022Oct04_01:41:53.898691.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

[Test_TC7_upgrade_all_available_packages|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2839630&size=124430&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F04%2F01%2F41%2Fauto_upgrade_job.2022Oct04_01:41:53.898691.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]",2022-10-04T15:19:15.351+0000,"The release needs to be provided from Jenkins Job:

[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-AUTO/job/upgradejob2/configure]

 

""catalog_server"": {
 ""server"":""https://staging.tesseractcloud.com:443"",
 ""repo"":""cisco-dnac-new"",
 ""release_number"": ""2.3.5.0"",  <==============Your destination release need to be provided in catalog_server inputs.
 ""proxy_server"":""http://proxy-wsa.esl.cisco.com"",
 ""proxy_port"":""80"",
 ""token"":"""",
 ""ignore"":true
 }

====

 ","['Auton', 'Groot', 'Issue', 'Upgrade']",Pawan Singh,Resolved,Avril Bower
SEEN-682,https://miggbo.atlassian.net/browse/SEEN-682,[Ghost] - TSIM client verification is getting failed,"Script and the TC :

solution_test_sanityecamb.py

>> Test_TC56_TSIM_verify_tsim_clients_on_wlc / test1_verify_tsim_clients
 >> Test_TC59_DEV_STRESS_verify_edge_reload_ap_clients_stability / test1_verify_edge_reload_ap_clients_stability

Branch: private/Ghost-ms/api-auto

Issue:

We are noticing an issue with the TSIM client verification in the Ghost from long time, initially we were under impression that there is some timing issue due to which there is a delay for the TSIM clients to join the WLC. Please find the below info in detail.

1.We see TC56 and TC59 getting failed in the Ghost and later after the completion of the regression run when we see the WLC the 200 clients were joined, so we were under impression there was some delay for the TSIM clients to join WLC.

2. The issue was constantly seen in the Ghost runs so we tried to add sleep (5 minutes, 10, 15, 20, 25) before TSIM client verification but it did not help.

3. So we thoroughly checked and find the TSIM client joining the WLC after execution of TC (Test_TC60_Verify_DHCP_server_change_on_segments / test4_verify_provision_the_devices_fabric1), so just for testing purpose we modified the script and provided the TC56 and TC59 after TC60 and see the clients joining.

Failed log link:

https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fsda-pyats%2Fusers%2Fapic%2Farchive%2F22-09%2FSanity_TB4.2022Sep29_04:36:46.505170.zip&atstype=ATS


Log link( After interchanging the TC):

https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fsda-pyats%2Fusers%2Fapic%2Farchive%2F22-10%2FSanity_TB4.2022Oct05_06:10:27.719824.zip&atstype=ATS

 

Can you please take a look and help us to understand if there is any behavioral change in the Ghost for the TSIM clients to join, as we don't see any issue on the same testbed for Groot run.

Please let me know if you need any further info on this.

Note: On ECA device (sda-9k-118-FiaB) we are not seeing this issue.",2022-10-06T12:00:38.652+0000,"Seeing this issue during Solution Sanity Regression with Ghost.

Script Used: dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

  Are you still hitting the same issue ? It could have been a product issue. If you are not observing it now. Close this issue. Being tracked by product bug: 

CSCwe21378

 

Nothing changed from script side.  Will reopen if issue seen on CSCwe21378 fix merged build once available","['Auton', 'Ghost', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-683,https://miggbo.atlassian.net/browse/SEEN-683,[Auton][IBSTE] : Test_TC87_Verify_border_priority  /   test1_verify_border_priority_config_before_update,"I’ve started integrating Border priority feature on IBSTE, but it’s failing with below error

 

[\{'deviceUuid': 'ce422fbd-31fa-42ee-9719-b0dc59542fbb', 'commandResponses': {'SUCCESS': {'show ip cef vrf ANCHORVN2 204.192.4.30': 'show ip cef vrf ANCHORVN2 204.192.4.30\n0.0.0.0/0\n attached to LISP0.4104\nSJC-FE-9300-1#'}, 'FAILURE': {}, 'BLACKLISTED': {}}}]

21709: Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:02.148534

21710:

21711: Got: show ip cef vrf ANCHORVN2 204.192.4.30

21712: 0.0.0.0/0

21713: attached to LISP0.4104

21714: SJC-FE-9300-1#

21715:

21716: ERROR Following line nexthop 204.101.192.4 LISP of output not present on device unexpectedly:

21717: SJC-FE-9300-1

21718:

21719: ERROR Following line nexthop 204.101.192.3 LISP of output not present on device unexpectedly:

 

Script is expecting “nexthop 204.101.192.X LISP” output on device, but since CLI is not present on device testcase is failing.

 

When checked on DNAC UI, Border priority was added successfully on Border device

Failed Log : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4815690&size=783045&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr%2Fusers%2Fadmin%2Farchive%2F22-10%2Fsr_ibste.2022Oct05_22:22:25.653611.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats20-sr&submitter=admin&from=trade&view=all&atstype=pyATS",2022-10-06T17:23:15.604+0000,"Hi
We are seeing the same issue while integrating Border priority Tc in AWS-Multisite TB.

DNAC-ISO Version :2.1.610.70412(Ghost)

Script Name :  solution_test_3sites_sjc_nyc_sf.py

Branch details: private/Ghost-ms/sanity_api_auto  --(dnac-auto/configs/sr_cl_ms)

Testcases Impacted : Test_TC152_Verify_border_priority Test_TC153_Verify_border_priority_with_fabric_update
Fail Log:
https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-09/auto_MS_job.2022Sep29_06:24:13.122227.zip&atstype=ATS -->TC 152,153 As per discussion with [~62d2fe9f8afb5805e5d5af49], testbed in issue state is required to debug the failure.

[~63f50bddc1685a24e1314c87] / [~63f50bf0e8216251ae4d59ca], pls. reproduce the issue and share the testbed is issue state. Hi Neelima,

 

Your failures are different than Divakar's. It failed for WiredVNFBLayer2 while Divakar was failed for Anchor VN.

I checked your testbed. It seem like a bug.

There were missing PETR Locators for 0.0.0.0/0 in map-cache on Edge for vrf WiredVNFBLayer2 hence it showed ‘attached to LISP0.4105’ instead of nexthop.

When removing a command relating to multicast on Edge, it worked fine. So seem like issue related to multicast.

 

However your cluster was too old version 2.3.5.0-70412 (a build back in June) that had many known issues that should be fixed in latest version. Why did you use such old version?

Can you try with latest Ghost version and share the testbed if you still see the same issue?

 

If we still see the same behavior with latest version again, we can try those before file defect:
 - remove multicast config -> see if it work

 - Try with 17.9.1 instead of 17.9.2 to see if its 17.9.2 issue. Divakar, 
Can you share the testbed again once its up?  Hi Tran,

 

By tomorrow (10/27) EOD testbed will be shared.

 

Regards,

Divakar Sure Tran,I will test border priority on Ghost-2.1.610.70492 and check. @[~63f50bddc1685a24e1314c87]: We have reported below defect on Groot Patch1 for the issue you have observed

*CSCwd44751 - [SR]Functional - PETRs missing for 0.0.0.0 in map-cache for ipv4 lisp instance*

May be this needs to be fixed for the usecase to work as earlier. Monitor on this defect further to see when the issue would get fixed.

So lets not track this issue (observed on Multisite testbeds) in the current Jira ticket. 

  If see the issue again, please reopen the defect *CSCwd44751 .*","['Auton', 'Guardian', 'IBSTE', 'Issue']",Tran Lam,Resolved,Avril Bower
SEEN-686,https://miggbo.atlassian.net/browse/SEEN-686,[Auton]Test_TC184_accept_cisco_cx_cloud_eula_for_scanned_devices /test2_verifying_eox_status_for_scanned_devices discrepancy in response of api call,"*Description:*  The test failed while verifying eox status on DNAC because of discrepancy in the parameter list of API [https://10.195.227.92/api/v1/eox-status/summary] response. 

Expected parameters from response:

hardwareCount
softwareCount
moduleCount
totalCount
failedCount


Returned parameters from response: 

hardwareCount
softwareCount
moduleCount
totalCount

*Branch Name:*  private/Groot-ms/sanity_api_auto

*Script file/Usecase:*   [Test_TC184_accept_cisco_cx_cloud_eula_for_scanned_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1159394&size=99521&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct04_01:09:34.829014.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] / test2_verifying_eox_status_for_scanned_devices 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:* N/A

*Fail Log:* Failed in Ghost 70463

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1171109&size=87621&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct04_01:09:34.829014.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ** 

*Pass Log:* Passed in Groot 2.1.560.70517

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=34404233&size=137593&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_auto_job.2022Sep05_01:43:16.015757.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Testbed details:* Sanity Testbed 6

[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-6|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7]
h4. Smart Checklist",2022-10-07T10:23:14.601+0000,"here is the PR

 

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3865/diff#services/dnaserv/lib/api_groups/inventory/group.py [~ajadhav2]/[~rakdomma]  is this fix needed for Groot as well ? [~70121:27365d3e-893a-4523-9fca-b89a7e8d0d4c] not required for Groot. These changes required from Ghost Merged to Groot and Ghost.","['Auton', 'Ghost', 'Groot', 'Issue']",Avril Bower,Closed,Avril Bower
SEEN-699,https://miggbo.atlassian.net/browse/SEEN-699,Ghost  [Auton] : Task-multicast_enable_in_fabric.py-112-SDAmulticast  /   Test_TC5_DNAC_edit_border_attributes  /   test3_ntp_sync_check_on_fusion_after_bgp_config,"*Reporter Analysis:* before start execution Need to clear the line

*Description:  The error from log or more info* 

*Branch Name:  private/Groot-ms/sanity_api_auto*
                           *private /Ghost-ms/sanity_api_auto*

*Script file/Usecase:* SDAmulticast 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-112-SDAmulticast&begin=12193174&size=29858&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct10_16:47:27.603131.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:*

*Testbed details:* NA",2022-10-11T10:48:20.777+0000,"Groot - [6c288bf48de|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6c288bf48de94e5f570dbb4a822d0225717daa58]

Ghost - [8ebd2c26dd9|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/8ebd2c26dd957a5c275947b6d26b6df5a3a7ed69] Validated in Ghost

Pass log :

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-111-SDAmulticast&begin=428611107&size=10103&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct17_08:42:46.768277.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS","['AWS_Sanity', 'Auton', 'Ghost', 'Groot', 'Issue', 'Optimized', 'sanity']",Nethra Thorali Suthagar,Closed,Avril Bower
SEEN-700,https://miggbo.atlassian.net/browse/SEEN-700,[Ghost]-Getting key error while authenticating cloud on dnac while testing Talos Feature,"Ghost Version :2.1.610.70412

Script Name :  solution_test_3sites_sjc_nyc_sf.py

Branch details: private/Ghost-ms/sanity_api_auto

Testcases Impacted : [Test_TC166_cloud_Talos_anomaly_detection|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1685065&size=152566&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_MS_job.2022Oct11_03:56:25.025942.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Fail Log:
--->Refer TC 166

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_MS_job.2022Oct11_03:56:25.025942.zip&atstype=ATS |https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_MS_job.2022Oct11_03:56:25.025942.zip&atstype=ATS]

While testing Talos Feature on AWS-Multisite TB,we are seeing the below error;

Error Snippet:
10081:  Cloud API handler called:
10082:  {}
10083:  GET - /papi/users
10084:  Encountered unhandled error in cloud API Handle
10085:  Flagging result as FAIL!
10086:  Reason: <class 'KeyError'>
10087:  Kwargs:
10088:  {}
10089:  Traceback (most recent call last):
10090:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
10091:  result = testfunc(func_self, **kwargs)
10092:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py"", line 16778, in test2_authenticate_cloud_on_dnac
10093:  otp = dnac_handle.register_dnac_on_cloud_talos_and_get_otp()
10094:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/api_groups/dnac_cloud/group.py"", line 189, in register_dnac_on_cloud_talos_and_get_otp
10095:  result &= self.create_talos_region(new_region=region)
10096:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/api_groups/dnac_cloud/group.py"", line 76, in create_talos_region
10097:  user_id = self.get_user_id()
10098:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/api_groups/dnac_cloud/group.py"", line 45, in get_user_id
10099:  response = self.api_cloud_handle(method=""GET"", resource_path=url)
10100:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/api_groups/dnac_cloud/group.py"", line 21, in api_cloud_handle
10101:  self.api_call_response = self.services.dnac_cloud.call_api(method, resource_path, **kwargs)
10102:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/dnaservices.py"", line 391, in dnac_cloud
10103:  chrome_driver=cloud[""driver""],
10104:  KeyError: 'driver'
10105:  Test returned in 0:00:00.004149
10106:  Errored reason: driver
10107: 
10108:  Exception:
10109:  Traceback (most recent call last):
10110:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
10111:  result = testfunc(func_self, **kwargs)
10112:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py"", line 16778, in test2_authenticate_cloud_on_dnac
10113:  otp = dnac_handle.register_dnac_on_cloud_talos_and_get_otp()
10114:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/api_groups/dnac_cloud/group.py"", line 189, in register_dnac_on_cloud_talos_and_get_otp
10115:  result &= self.create_talos_region(new_region=region)
10116:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/api_groups/dnac_cloud/group.py"", line 76, in create_talos_region
10117:  user_id = self.get_user_id()
10118:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/api_groups/dnac_cloud/group.py"", line 45, in get_user_id
10119:  response = self.api_cloud_handle(method=""GET"", resource_path=url)
10120:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/api_groups/dnac_cloud/group.py"", line 21, in api_cloud_handle
{color:#de350b}10121:  self.api_call_response = self.services.dnac_cloud.call_api(method, resource_path, **kwargs){color}
{color:#de350b}10122:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/dnaservices.py"", line 391, in dnac_cloud{color}
{color:#de350b}10123:  chrome_driver=cloud[""driver""],{color}
{color:#de350b}10124:  KeyError: 'driver'{color}
{color:#de350b}10125:  The result of section test2_authenticate_cloud_on_dnac is => ERRORED{color}
 ",2022-10-11T13:30:00.164+0000,"Hi Moe,
Those details are already mentioned actually,but still seeing the issue.

Yaml File:
 !image-2022-10-27-21-04-03-605.png!

 

Local json:

!image-2022-10-27-21-05-22-006.png!

!image-2022-10-27-21-05-46-644.png!

 

 

 

 

  [https://wiki.cisco.com/display/EDPEIXOT/Talos+Feature] Hi [~63f50bfce8216251ae4d59d5],

As discussed over webex Team space - webexteams://im?space=73157080-560a-11ed-9ce3-0b88e3da78a1, once the fixed is added for this, we will verify and update on observations as to how it goes.

Regards
Sandeep S

 

  @[~63f50bfce8216251ae4d59d5]: Tried executing the Talos TC on Ghost-540 build using *talos_fixes* branch ([https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4159/overview]).

We are no more observing driver key error. But we are observing different error now where 
we see verification of Talos block list setp is failing due to dnac connection login error, but the maglev cli credentials that are being used are correct ones. Due to this, we see *Failed to configure Talos blocked list on the maglev cluster!! error.*
So we are going to close this Jira ticket and open a new ticket for current issue. Please add the *talos_fixes* branch commit ([https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4159/overview]) to Guardian as well as on Ghost. Fixed partially. Other issue will be tracked using a new Jira ticket. 

Please refer latest comment updates for more details.

 

Regards
Sandeep S","['AWS_MSTB', 'Auton', 'Ghost', 'Groot', 'Guardian', 'Integration', 'Issue', 'MSTB1', 'Multisite']",Moe Saeed,Closed,Avril Bower
SEEN-707,https://miggbo.atlassian.net/browse/SEEN-707,[Auton] Shockwave:Test_TC3_generate_dhcp_server_config_on_fusion/est2_ise_cleanup," 

*Reporter Analysis:* 

AttributeError: 'DnaServices' object has no attribute 'get_cluster_version'

*Description:*  

022-10-12T23:55:14: %SERVICES-ERROR: cluster_version = self.services.get_cluster_version() 2022-10-12T23:55:14: %SERVICES-ERROR: File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Shockwave-sanity-common-Multi-job/services/dnaserv/dnaservices.py"", line 222, in __getattr__ 2022-10-12T23:55:14: %SERVICES-ERROR: raise AttributeError(err_msg) 2022-10-12T23:55:14: %SERVICES-ERROR: AttributeError: 'DnaServices' object has no attribute 'get_cluster_version'

*Branch Name:  private/Shockwave-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

*Pass Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2347716&size=11126&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F08%2F06%2F06%2F23%2Fenv_auto_job.2022Aug06_06:23:45.283041.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS

*Testbed details:* NA",2022-10-13T08:49:47.560+0000,Shockwave - [8e45d507bd1|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/8e45d507bd1a598f36b20b22e7a961d19eb353eb#services/dnaserv/dnaservices.py],"['Auton', 'Issue', 'Sanity', 'Shockwave']",Nethra Thorali Suthagar,Resolved,Avril Bower
SEEN-710,https://miggbo.atlassian.net/browse/SEEN-710,[Auton][IBSTE] : : Task-itsm_validations.py-142-itsmValidations  /   Test_TC2_ITSM_ticket_generation_test,"Hi Team,

 

After code was added on IBSTE optimized script. Testcase is failing with below error.
Caught an exception while executing section test1_enable_configure_ITSM_bundle:
1461:  ValueError: Missing parameters \{'dnac_handle'} in scope of '<bound method Test_TC2_ITSM_ticket_generation_test.test1_enable_configure_ITSM_bundle of <class 'Test_TC2_ITSM_ticket_generation_test' uid='Test_TC2_ITSM_ticket_generation_test'>>'
1462:  The result of section test1_enable_configure_ITSM_bundle is => ERRORED
 
Currently script is expecting dnac_handle value to be called in the script. 
Required lib file for dnac_handle needs to be updated in script
 
Script location : dnac-auto/testcases/ibsteusecases/itsmValidations/itsm_validations.py
 
Tried running main script and it works fine
 
Failed Log : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-142-itsmValidations&begin=403274&size=20026&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Foptimized_ibste_job.2022Oct11_08:45:11.607681.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS",2022-10-14T08:35:41.810+0000,"PR with required changes have been raised: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3985/overview]

  execution went well with the changes, hence marking this ticket as ""Done"".

 * [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Foptimized_ibste_job.2022Nov04_07:22:12.462520.zip&atstype=ATS]
 changes have been pushed to Groot and Ghost branch only.","['Auton', 'Groot', 'IBSTE', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-712,https://miggbo.atlassian.net/browse/SEEN-712,[Auton]Ghost:Test_TC1_SWIM_image_upload_and_golden_tag/test2_construct_device_image_urls_from_image_dir_avail_files,"*Reporter Analysis:* This testcase should handle the scenario when the image directory ""/www/swim/sanity_image_regr/tb3"" is not having any images listed then skip this TC.

*Description:  The error from log* 
No Image path constructed for path:sanity_image_regr/tb3 on server output .:
73:  total 0
*Branch Name:*  private/Ghost-ms/sanity_api_auto

*Script file/Usecase:*  [dnac-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse?at=refs%2Fheads%2Fprivate%2FGroot-ms%2Fsanity_api_auto]/testcases/sanityusecases/[SwimImageUploadGoldenTag|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-swim_image_upload_mark_golden.py-73-SwimImageUploadGoldenTag&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct06_19:00:32.109699.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]/swim_image_upload_mark_golden.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:* N/A

*Fail Log*:[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-swim_image_upload_mark_golden.py-73-SwimImageUploadGoldenTag&begin=4895&size=11098&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct06_19:00:32.109699.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Pass Log:* N/A

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-3] ",2022-10-14T09:36:16.928+0000,"Testcase failed in latest Groot FCS 70523 execution: 
Please commit the changes in Groot Optimized code 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-swim_image_upload_mark_golden.py-73-SwimImageUploadGoldenTag&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct28_20:14:13.613547.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  Assign Back to you, nothing to fix here.
Please set you your Swim Server before using it and add it to your input file. The intention for creating this Jira is, when we don't want to upgrade any device images then if the swim directory is empty then this test should be skipped but instead its failing","['Auton', 'Ghost', 'Groot', 'Issue']",Ashwini R Jadhav,Resolved,Avril Bower
SEEN-713,https://miggbo.atlassian.net/browse/SEEN-713,[Auton][MS]: Need to add script in Multisite for CS SAP: Deploy a fabric site with SDA handoff and introduce delay between Transit control-plane and site border - Guradian-Solution,"Hi Rakesh,

We are trying to integrate the SAP Delay Feature in Multisite testbeds also. As of now, code was committed to sanity scripts. Can you please double-commit the code in Multisite Script?

Feature :  CS SAP: Deploy a fabric site with SDA handoff and introduce delay between
Transit control-plane and site border - Guardian-Solution
TIMS ID : Twj876617r


 
 ",2022-10-14T10:59:24.035+0000,"raised PR in guardian and groot and ghost

 

ghost:

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3900/overview]

 

Groot:

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3899/overview]

 

Guardian:

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3898/overview] ","['Auton', 'Guardian', 'Issue', 'MS', 'MSTB1', 'MSTB2', 'Multisite']",Avril Bower,Closed,Avril Bower
SEEN-714,https://miggbo.atlassian.net/browse/SEEN-714,[Auton][MS] : Syslog server workflow validation,"Hi Rakesh,

we have tried integrating Syslog feature in Multisite. We are hitting the issue CSCwc68907.
Seems the workflow is going fine when we try manually. But through script, it was failing. Unable to see wired client flap events in Syslog server. Can you please take a look and update the script modifications.

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_MS_job.2022Oct03_23:53:53.327657.zip&atstype=ATS]

 

 ",2022-10-14T11:10:24.719+0000,"Verified the issue in latest Ghost looks like event logs are not coming for the device and wired client. Informed to DE and waiting for his response Ghost:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3949/overview]

 Groot:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3950/overview]

 

Guardian:

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3951/overview PR got approved.
Script integration completed and moved to production in Groot P1 RC2","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'MSTB1', 'MSTB2', 'Multisite', 'Sanity']",Avril Bower,Closed,Avril Bower
SEEN-715,https://miggbo.atlassian.net/browse/SEEN-715,[Auton]  Cleanup script removes the devices in Test_TC18_unprovision_devices," 

*Reporter Analysis:* Devices are deleted during unprovision testcase, is it the correct flow, Can we do the un-provision separately and remove the devices part of next test ?

 

The device was deleted before the unprovision happened successfully, so we can't collect the logs and share the cluster with DE for debugging the un-provision failures 

 

Also should we use here the CFS API or not, does CFS api supposed to delete the device or it's a defect ?

res=self.services.api_switch_call(method=""DELETE"",

resource_path=""/v2/data/customer-facing-service/DeviceInfo/\{0}"".format(dev),params=params)

 
 : Removing Device:c42e0fa2-9a54-45a9-adc2-1dcaeb1e4106 Unable to push configuration to device 204.192.4.2
 4248: Removing Device:{'response':

{'taskId': '9e562681-df7e-4ae9-8fe5-c88f83b63fb4', 'url': '/api/v1/task/9e562681-df7e-4ae9-8fe5-c88f83b63fb4'}

, 'version': '1.0'} Unable to push configuration to device 204.192.4.2
 4263: Failed reason: Result: Failed in unprovision_devices
  
  
 *Description:  The error from log or more info* 

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* dnac_cleanup_script.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=924253&size=105817&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct14_10:40:32.114736.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]*

 

*Pass Log:* 

*Testbed details:* NA",2022-10-14T19:17:12.178+0000,"!image-2022-10-14-12-21-16-279.png! ||[Test_TC18_unprovision_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=923689&size=106539&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct14_10:40:32.114736.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FExpress-Sanity%2FGroot%2FGroot-Legacy-Deployment_and_Express_Sanity%2Ftestcases%2Fmega_topo%2Fdnac_cleanup_script.py&lineno=438&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct14_10:40:32.114736.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Failed|00:00:57|Test_TC18_unprovision_devices| |
||[Test_TC19_remove_devices_from_inventory|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1030228&size=871&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct14_10:40:32.114736.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FExpress-Sanity%2FGroot%2FGroot-Legacy-Deployment_and_Express_Sanity%2Ftestcases%2Fmega_topo%2Fdnac_cleanup_script.py&lineno=450&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct14_10:40:32.114736.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Blocked|00:00:00|Test_TC19_remove_devices_from_inventory| UNPROVISION IS a way of deletion for the devices. There is no just un-provision support in DNAC. Devices that are provisioned are deleted through un-provision.  A device that is not yet provisioned is deleted through inventory delete. that's why there are 2 cases. 

  Actually, we will break further execution if un-provision fails so you can debug. We are already blocking further execution:
else:
self.failed(""Result: Failed in unprovision_devices"", goto=['common_cleanup'])
self.passed(""unprovisioned_devices."")
||[Test_TC18_unprovision_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=923689&size=106539&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct14_10:40:32.114736.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FExpress-Sanity%2FGroot%2FGroot-Legacy-Deployment_and_Express_Sanity%2Ftestcases%2Fmega_topo%2Fdnac_cleanup_script.py&lineno=438&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct14_10:40:32.114736.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Failed|00:00:57|Test_TC18_unprovision_devices| |
||[Test_TC19_remove_devices_from_inventory|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1030228&size=871&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct14_10:40:32.114736.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FExpress-Sanity%2FGroot%2FGroot-Legacy-Deployment_and_Express_Sanity%2Ftestcases%2Fmega_topo%2Fdnac_cleanup_script.py&lineno=450&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct14_10:40:32.114736.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Blocked|00:00:00|Test_TC19_remove_devices_from_inventory|","['Auton', 'Issue', 'Sanity']",Pawan Singh,Resolved,Avril Bower
SEEN-716,https://miggbo.atlassian.net/browse/SEEN-716,[Auton]:Groot:Test_TC37_DNAC_TSIM_static_onboarding_verifications  /   test1_onboard_all_sensors_on_all_sites,"*Reporter Analysis:* After removing configs in sensor connected interface in TC36(
Test_TC36_ext_node_ipphone_onboarding_verifications ) TC37 is not re-adding that interface configs back and TC is false passing

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* after_upgrade_verify.py

*Source Team:  Upgrade Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*NA

*Pass Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=26888071&size=42119&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F14%2F21%2F24%2Fenv_auto_job.2022Oct14_21:24:19.857491.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS

*Testbed details:* NA",2022-10-15T10:29:48.199+0000,"Required PRs for below releases have been merged:
 # Shockwave - https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3942/overview
 # Guardian - https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3943/overview
 # Groot - https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3910/overview
 # Ghost - [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3944/overview]

[~61efa8c457b25b006877eda3], pls. validate the fix and provide your comment for the closure.","['Auton', 'Groot', 'Issue', 'Upgrade']",Nethra Thorali Suthagar,Resolved,Avril Bower
SEEN-731,https://miggbo.atlassian.net/browse/SEEN-731,[Auton]  Test_TC177_cloud_Talos_anomaly_detection," 

*Reporter Analysis:* Aws TB we are facing connectivity issues to the cloud.
 DE suggested using the below URL: [https://|https://staging-dna.cisco.com/] [dna.cisco.com|https://dna.cisco.com/],
 *magctl service setenv  registration CLOUD_URL [https://www.ciscoconnectdna.com|https://www.ciscoconnectdna.com/]*
 We need to make script changes accordingly.
 *Description:  The error from log or more info* 

*Branch Name:  private/Groot-ms/sanity_api_auto*
                           *private/Ghost-ms/sanity_api_auto*
                           *private/Guardian-ms/sanity_api_auto*
                                                     

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-10/env_auto_job.2022Oct11_23:23:40.181601.zip&atstype=ATS]

*Pass Log:* no any Pass log

*Testbed details:* NA",2022-10-17T08:18:06.254+0000,Commit: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4159/overview,"['AWS_Sanity', 'Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity']",Moe Saeed,Resolved,Avril Bower
SEEN-732,https://miggbo.atlassian.net/browse/SEEN-732,[Auton]  Test_TC170_overlapping_pools_negative_operations," 

*Reporter Analysis:* The method is repeated in 2 files
get_local_ip_for_vn, library is duplicated, which causes the test case to Failed .need to remove the duplicate method.
->services/dnaserv/lib/api_groups/assurance/group.py
->services/dnaserv/lib/api_groups/wired_interface_onboarding/group.py
 *Description:*  
 10/14/2022 9:56 PM • 
 2825: Encountered duplicate method name ""get_local_ip_for_vn"" in group ""assurance"", seen earlier in group ""wired_interface_onboarding""!

*Branch Name:  private/Groot-ms/sanity_api_auto*
                           *private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=447015&size=69941&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct14_08:34:37.074160.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:* NA

*Testbed details:* NA",2022-10-17T08:57:06.468+0000,"Already merged  Hi [~63f50bfce8216251ae4d59d5] ,

We ran test cases on DMZ TB  with the latest pull code. TC failed, but the execution took approx ~2  hours to finish. Could you please check and confirm.Also, Add the test cases in optimized sanity code

*Branch Name:*  private/Ghost-ms/sanity_api_auto

*Ghost-AWS-2.1.610.70454*
 Failed Log:
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-10/env_auto_job.2022Oct18_09:10:44.102639.zip&atstype=ATS]
||[Test_TC170_overlapping_pools_negative_operations|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1357066&size=5223917&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct18_09:10:44.102639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FDMZ-SANITY%2FGhost%2FDMZ-Ghost-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=12240&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct18_09:10:44.102639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Errored|01:52:47|
||[test1_configure_dhcp_address_helper_on_VNs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1357630&size=87951&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct18_09:10:44.102639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FDMZ-SANITY%2FGhost%2FDMZ-Ghost-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=12248&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct18_09:10:44.102639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Errored|00:00:03|
||[test3_verify_dhcp_devices_after_adding_template|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1947966&size=204875&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct18_09:10:44.102639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FDMZ-SANITY%2FGhost%2FDMZ-Ghost-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=12275&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct18_09:10:44.102639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Failed|00:03:08|
||[test6_validate_wired_clients_traffic_after_onboarding|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3635631&size=54166&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct18_09:10:44.102639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FDMZ-SANITY%2FGhost%2FDMZ-Ghost-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=12352&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct18_09:10:44.102639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Failed|00:33:34|
||[test11_configure_dhcp_address_helper_on_VNs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4701555&size=82480&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct18_09:10:44.102639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FDMZ-SANITY%2FGhost%2FDMZ-Ghost-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=12439&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct18_09:10:44.102639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Errored|00:00:04|
||[test13_verify_dhcp_devices_after_adding_template|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5212719&size=199214&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct18_09:10:44.102639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FDMZ-SANITY%2FGhost%2FDMZ-Ghost-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=12465&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct18_09:10:44.102639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Failed|00:03:09|
||[test15_validate_wired_clients_traffic_after_onboarding|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6387367&size=53807&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct18_09:10:44.102639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fdata%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FDMZ-SANITY%2FGhost%2FDMZ-Ghost-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=12528&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct18_09:10:44.102639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&from=trade&view=all]|Failed|00:33:34|

Regards,
 Omkar Hi Moe,

Please Print the failures or responses, if in case of failure. We did not understand, what are expecting, what is getting the output, and where searching In the below failures message. We can't identify what is the problem.
 
+Failed message:+
 
301845:  Ip could not be extracted from the VN1 handoff
301846:  Library group ""wired_interface_onboarding"" method ""validate_clients_connection"" returned in 0:00:00.368833


+Branch:+ private/Guardian-ms/sanity_api_auto 
+
TB 6 Failed Log:+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=61538873&size=44787&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov24_04:31:56.753978.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Hey [~620b8357878c2f00729881c8] , 

I just added more logs, looks like the cluster you used has more than one transits and empty handoff. Please fetch if any latest changes. Hi [~63f50bfce8216251ae4d59d5] 
we have one transit with both IP and SDA transit This feature is working as expected in sanity, and I also added more logs on groot and ghost. Please rerun it and share the logs you have with me. For the transit, I added a handle if empty handoff in transit.  Please verify it if it is working as expected.","['AWS_Sanity', 'Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'Sanity']",Moe Saeed,Resolved,Avril Bower
SEEN-733,https://miggbo.atlassian.net/browse/SEEN-733,[Auton] Groot: Test_TC193_default_route_verification /test1_generate_default_route_issue_on_border," *Reporter Analysis:* Sub TC case failed for ,KeyError: 'localIpAddress'

*Description:*  
@library_wrapper()
 def generate_default_route_issue(self, vn_list: list = []) -> bool:
 """"""This lib is to remove default route from the VNs on fusion""""""
 result = PASS
 vn_list = vn_list or self.services.input_data[""default_route""]
 vn_ip4_list = {}
 vn_ip6_list = {}
 for vn in vn_list:
 vn_info = self.services.get_local_ip_for_vn(vn)
 vn_ip4_list[vn] = vn_info[""localIpAddress""] 
 vn_ip6_list[vn] = vn_info[""localIpv6Address""]

 

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5768477&size=10468&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct15_09:11:11.924290.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:N/A*

*Testbed details:* NA",2022-10-17T09:44:19.864+0000,"Recent Groot runs have not seen the issue, so Jira is moving to a close state.
Pass Log:
https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-10/env_auto_job.2022Oct20_03:09:05.978353.zip&atstype=ATS","['Auton', 'Groot', 'Issue', 'Sanity']",Omkar Sharad Wagh,Closed,Avril Bower
SEEN-734,https://miggbo.atlassian.net/browse/SEEN-734,CLONE - [Auton] Groot: Test_TC193_default_route_verification /test2_verify_default_route_issue_on_device (took ~5 hours execution )," *Reporter Analysis:*  Jira SEEN-733  in First sub tc failed  KeyError: 'localIpAddress',then 2nd sub took  5 hours for  complete execution then  failed with key error

*Description:*  

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*   

[test2_verify_default_route_issue_on_device  |https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5778945&size=42894392&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct15_09:11:11.924290.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]**
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5768477&size=10468&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct15_09:11:11.924290.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

]
|[Test_TC193_default_route_verification|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5767907&size=42905430&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct15_09:11:11.924290.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|Errored|05:08:01|
  |
|[test1_generate_default_route_issue_on_border|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5768477&size=10468&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct15_09:11:11.924290.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|Errored|00:00:00| |
|[test2_verify_default_route_issue_on_device|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5778945&size=42894392&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct15_09:11:11.924290.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|Errored|05:08:01| |

[|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5768477&size=10468&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct15_09:11:11.924290.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:N/A*

*Testbed details:* NA",2022-10-17T09:55:52.926+0000,"Observed same issue during Solution Sanity Regression

Script Name: dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

Branch: private/Ghost-ms/api-auto

Impacted Testcase: TC195 in below shared taas log - test2 takes 5hrs 5 mins during failure scenario - default route verification feature

Also please pls fix if any regexp issue exists w.r.t device release output change

Taas Log: [https://ngdevx.cisco.com/services/taas/results/ec1c131a-965b-4ebe-8e3d-4d13583f9a22] resolved:

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4125/diff#services/dnaserv/lib/api_groups/cli_check/group.py","['Auton', 'Groot', 'Issue', 'Sanity']",Moe Saeed,Closed,Avril Bower
SEEN-770,https://miggbo.atlassian.net/browse/SEEN-770,[Auton] Add the testcases in optimized sanity code _Test_TC185_Check_Network_Security_Trust_Settings,"Please add the below mentioned testcases in optimized sanity code. 

 
 * Test_TC185_Check_Network_Security_Trust_Settings
 * test1_check_existing_network_security_trust_settings
 * test2_change_network_security_trust_settings
 * test3_verify_changed_network_security_trust_settings
 * test4_check_existing_cli_on_device
 * test5_provision_the_device
 * test6_check_cli_after_provision

 

Please double commit this in ,Groot & Ghost branches",2022-10-18T15:53:11.442+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3965/overview]

Add optimized code for TC185, not mapping yet.","['Auton', 'Ghost', 'Groot', 'Issue', 'Optimized', 'sanity']",NhanHuu Nguyen,Resolved,Avril Bower
SEEN-772,https://miggbo.atlassian.net/browse/SEEN-772,Test_TC167_generate_Worst_Interferers_report -Email not getting generated for AWS specific testbed,"*Uber ISO Version tested :* 2.1.610.70412-Ghost

*Script Name :* solution_test_3sites_sjc_nyc_sf.py

*Testbed :* AWS MSTB

*Testcases Impacted :*  [Test_TC167_generate_Worst_Interferers_report|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1692625&size=359039&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_MS_job.2022Oct17_02:07:45.730129.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 *Branch Used*:private/Ghost-ms/sanity_api_auto

*Fail Log :*

https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-10/auto_MS_job.2022Oct17_02:07:45.730129.zip&atstype=ATS

*Description :* 

I am trying to execute Worst Interferes feature in AWS Tb and after the latest fix also we are seeing mail not getting generated to the mentioned User Email.

Is this AWS limitation or any bug?
When I am trying to ping outbound.cisco.com from my cluster,I am seeing the below error.

[Mon Oct 17 10:40:31 UTC] maglev@169.254.6.66 (maglev-master-169-254-6-66) ~
$ ping outbound.cisco.com
ping: outbound.cisco.com: Temporary failure in name resolution
[Mon Oct 17 10:40:37 UTC] maglev@169.254.6.66 (maglev-master-169-254-6-66) ~",2022-10-19T04:36:21.572+0000,"How is aws DNS set up? Is it using the same DNS server as other testbeds or something different? Spoke with [~accountid:63f50be9e76fc61320f4eab3] and decided that email verification not required for DMZ testbed, since the testbed would not be able to contact the [outbound.cisco.com|http://outbound.cisco.com] server. Since the testcase itself is not checking for email, don’t think there is any action required on script side either. ","['AWS_MSTB', 'Auton', 'Issue', 'Multisite']",Andrew Chen,Closed,Avril Bower
SEEN-773,https://miggbo.atlassian.net/browse/SEEN-773,[Auton][IBSTE] : SWIM Customer Tag feature is not part of IBSTE script,"Hi Team,

 

Swim Customer tag feature code needs to be commited to IBSTE script.  Code needs to be commited to optimized code

 

Thanks,

Divakar",2022-10-19T12:36:53.091+0000,"Hi Moe,
Swim Customer Tag Feature was not added to both Optimized and Normal Script.  Can you please double commit the code for  IBSTE Script also. 
Also, If this Feature is not part of IBSTE Script ? Can you please let me know. We will close the Jira and Feature for IBSTE? Needs more testing, need IBSTE testbed

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5342/diff#services/dnaserv/ibsteservices.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5342/diff#services/dnaserv/ibsteservices.py] [~accountid:712020:96c61f39-1766-4002-9406-bfc1c806f040] / [~accountid:63f50bfde8216251ae4d59d8] , pls. sync-up with [~accountid:63f50bfce8216251ae4d59d5] on this ticket’s requirements and get it concluded. [~accountid:63f50bfce8216251ae4d59d5], because of back to back regression and testbed issues testbed was not idle to share so far for script testing.

We’ll provide the set-up once available. Because of back to back regression & Dnac issues  testbed was not idle
We’ll provide the set-up once available. [~accountid:63f50bfce8216251ae4d59d5] Cluster deployment issue  testbed has been shared to Maglev team for  debugging   will provide the setup once  available  This PR has been for a while.
[~accountid:63f50bfde8216251ae4d59d8] , [~accountid:712020:96c61f39-1766-4002-9406-bfc1c806f040] , can you share the ibste testbed so that we can close on this? [~accountid:62d2fe9f8afb5805e5d5af49]   Testbed occupied in back to back regression.  will try to share once it is free. [~accountid:712020:96c61f39-1766-4002-9406-bfc1c806f040] , pls. share the ETA for testbed sharing? Current week regression will be over 26th.  will be sharing it on 27th Sep. Tried executing   from PR and  few TC’s are failed  becoz of script issue.

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_ibste.2023Oct10_21:09:55.471600.zip&reqseq=&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_ibste.2023Oct10_21:09:55.471600.zip&reqseq=&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=PYATS].


Moe fixed the issue need to rerun after the current regression","['Auton', 'Ghost', 'IBSTE', 'Improvement']",Moe Saeed,Pending Code Review,Avril Bower
SEEN-774,https://miggbo.atlassian.net/browse/SEEN-774,"[Auton][MS] :  Device provision on vEWLC fails with ""NCSP11001: User intent validation failed""","*Uber ISO Version tested :* Groot Uber ISO - *2.1.563.70111, FIPS*

*Script Name :* solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB2

*Branch :*  private/Groot-ms/api-auto


*Description :*  During Groot Solution Testing, We have seen that VEWLC Device provision is getting failed because of NCSP11001: User intent validation failed while processing the 'provision' request. Additional info for support: taskId: '4334219d-5e46-4c7e-a857-f5a94136fdc1.

SSID Schduler which is passing in payload for the device provision is not created in the testbed . From TC18.2 ssid scheduler created is ssid-scheduler-weekly
but for device provision paylod is sending with"" wireless.wlanSchedulerName "", which is not created on DNAC

*Failed log* : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=813013&size=2278184&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_MS_job.2022Oct10_22:07:46.627924.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Reference bug ID* : CSCwd26446

!Screenshot3 (1).PNG!

 ",2022-10-20T07:47:45.929+0000,"[~62ab7a399cd13c0068b18fe0] /[~63f50bfde8216251ae4d59d8]. Could you share the testbed in same state, I don't see the same in automation multisiteTB even though  deviceWlanSchedulerInfo: [](empty).

Automation testbed log https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1317084&size=199646&archive=%2Fws%2Frmukkama-sjc%2Fpyats%2Fusers%2Frmukkama%2Farchive%2F22-11%2Fthree_sites.2022Nov01_22:33:50.536710.zip&ats=%2Fws%2Frmukkama-sjc%2Fpyats&submitter=rmukkama&from=trade&view=all&atstype=pyATS HI [~63f50bf5e8216251ae4d59cf],

We have seen this device provision failure in our current regression with Ghost also. 
Failed Log :[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5269382&size=971626&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fauto_MS_job.2022Nov02_01:20:02.441329.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


Currently, Execution is going on for regression. Once after that we can share the setup which is in issue state.

Regards,
Sathwick. [~63f50bfde8216251ae4d59d8], pls. reproduce the issue and share the update by EOD. Hi [~rmukkama]/[~amardkum],

Issue was reproduced in testbed. You can use it for debugging.
Cluster details and device details were shared over Webex team space created for this issue.

Regards,

Sathwick. Hi Raji,

As per the discussion, I will update""SSID"":""SSID_Scheduler"" to ""SSID"":""SSIDScheduler"" in my solution input file, and please let me know let you know once you commit the fix for verification of ssid configs.

 

Thanks & Regards,
Sathwick [~63f50bfde8216251ae4d59d8] Added fix [f33895ee622|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f33895ee622ce58c5ace823560154bb1588fa186] [f33895ee622|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f33895ee622ce58c5ace823560154bb1588fa186] [~63f50bfde8216251ae4d59d8], please verify the ticket and close if fixed. Issue as fixed after the changes done.
Pass log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=516845&size=43475&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fauto_MS_job.2022Nov17_21:05:44.424493.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 ","['Auton', 'Groot', 'Issue', 'MSTB2', 'Multisite']",Raji Mukkamala,Closed,Avril Bower
SEEN-775,https://miggbo.atlassian.net/browse/SEEN-775,"[Auton][MS] : Unable to validate Telemetry connection status. Because of cli ""show telemetry connection all "" not supported","*Uber ISO Version tested :* Groot Uber ISO - *2.1.563.70111, FIPS*

*Script Name :* solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB2

*Branch :*  private/Groot-ms/api-auto

*Description :* In MST2 we are trying to integrate NETCONF TDL Notification for SWIM Feature. Script is getting falied becasue of Telemetry connection status is not active on devices. Actually when we can check manually Telementry connection status was enabled and in active. 
Script is trying to check ""show telemetry connection all"" from the device command runner. But since device is having 17.6.5 image, this command is not supported. We need to check like ""show telemetry internal connection"".

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2207709&size=393099&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_MS_job.2022Oct17_03:32:54.398383.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Checked manually on devices for Telementry deatils : 

SFO-FIAB-9400#sh telemetry internal connection 
Telemetry connections

Index Peer Address Port VRF Source Address State
----- -------------------------- ----- --- -------------------------- ----------
 879 204.192.1.114 25103 0 204.1.2.7 Active

SFO-FIAB-9400#

Can you please fix this issue giving proper command to check for Telementry connection status based of image version. ?

Regards,
Sathwick.",2022-10-20T09:01:43.392+0000,"PR Raised with the required change: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4025/overview Hi Amardeep,

Can you please commit the changes to the Ghost branch also. As of now we are having Ghost build in our cluster and i tried to integrate this feature.
 We still see the same, script is trying to verify with the command ""show telemetry connection all""

Failed Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1863335&size=332929&archive=sr_mb2_three_sites_FIPS.2022Nov07_19:57:29.380270.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Regards,
 Sathwick. Done with cherry-picking of the change required in Ghost branch.

Tested the same via Jenkins:
 [https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/SR-CL-MS/job/MSTB2_Multijob_Solution_Regression_Git/177/]

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1559356&size=1187031&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fauto_MS_job.2022Nov08_09:07:34.282951.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Please include TC174 into the regular testing cycle.

 

Marking this Ticket as ""Done"".","['Auton', 'Groot', 'Issue', 'MSTB2', 'Multisite']",Amardeep Kumar,Closed,Avril Bower
SEEN-776,https://miggbo.atlassian.net/browse/SEEN-776,[Auton] MS2 TB2:  Unable to validate power supply issue feature because of CLI trying to verify is not supported,"*Uber ISO Version tested :* Groot Uber ISO - *2.1.563.70111, FIPS*

*Script Name :* solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB2

*Branch :*  private/Groot-ms/api-auto

*Description :*  In MST2 we are trying to integrate [Inter Device Link Flap Issue and Power Supply Issue|https://wiki.cisco.com/display/EDPEIXOT/Inter+Device+Link+Flap+Issue+and+Power+Supply+Issue] Feature. Script is getting failed because of non supported CLI is passing to the device to power off the power supply.
script is trying to verify cli ""power supply 1 slot A off"", which is not supported. 

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1645011&size=328084&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_MS_job.2022Oct20_00:10:11.990478.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Failed Reason : 
11689:  raise SubCommandFailure(""Command execution failed"", err) from err
11690:  unicon.core.errors.SubCommandFailure: ('Command execution failed', SubCommandFailure('sub_command failure, patterns matched in the output:', ['^%\\s*[Ii]nvalid (command|input)'], 'service result', ""power supply 1 slot A off\r\npower supply 1 slot A off\r\n ^\r\n% Invalid input detected at '^' marker.\r\n\r\nNYC-FE-9400#""))
11691:  Power supply failure issue is not found

Could you please fix this with proper cli to pass from script?


Regards, 
Sathwick.
 ",2022-10-20T10:45:37.981+0000,"[~63f50bfde8216251ae4d59d8], the CLI in use is specific to 9300 device and the failure that you observed is with 9400.

[~62d2ffcd3d382dfc9c61359b], pls. upgrade the usecase to consider 9400 model as well. [~63f50bfde8216251ae4d59d8], Based on the discussion with [~62d2ffcd3d382dfc9c61359b], 9400 or 9600 do not have cli command to turn off the power supply.

Hence, the testcase needs to be limited to 9300 model only.

Required change has been merged to Groot branch:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/3969/overview]

 

Please re-test and confirm on the closure of this ticket.. Retested the use-case with the fix where it skipped non-C9300 devices and continued with C9300.

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1558496&size=431537&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_MS_job.2022Oct24_10:09:22.147602.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Marking this Ticket as ""Closed"". Hi Amardeep,

Thanks for the Fix, we have validated it in Groot Pre RC 2.1.563.70111. Please find the log 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-10/auto_MS_job.2022Oct24_21:39:39.303514.zip&atstype=ATS]



Regards,
Sathwick","['Auton', 'Groot', 'Issue', 'MSTB2', 'Multisite']",Amardeep Kumar,Closed,Avril Bower
SEEN-777,https://miggbo.atlassian.net/browse/SEEN-777,[Auton]MSTB2 : Test_TC30_SWIM_UPGRADE_ECA_DEVICE : Failing with Attribute Error,"*Uber ISO Version tested :* Groot Uber ISO - *2.1.563.70111, FIPS*

*Script Name :* solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB2

*Branch :*  private/Groot-ms/api-auto

*Description :* In MSTB2, we are trying to integrate SWIM CCO Feature. But script is getting failed because of failing for connecting cco server from the library function if not self.imageserver.is_connected():

*Failed Log :* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_MS_job.2022Oct20_03:34:24.788200.zip&atstype=ATS]

*Error :* 
9729:  Errored reason: 'SFTopology' object has no attribute 'imageserver'
9730: 
9731:  Exception:
9732:  Traceback (most recent call last):
9733:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/MSTB2_Multijob_Solution_Regression_Git/services/commonlibs/test_wrapper.py"", line 301, in wrapper
9734:  result = testfunc(func_self, **kwargs)
9735:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/MSTB2_Multijob_Solution_Regression_Git/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py"", line 1457, in test10_assign_tag
9736:  if dnac_handle.dnaconfig.check_customer_tag_images():
9737:  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/MSTB2_Multijob_Solution_Regression_Git/services/commonlibs/sftopology.py"", line 258, in check_customer_tag_images
9738:  if not self.imageserver.is_connected():
9739:  AttributeError: 'SFTopology' object has no attribute 'imageserver'
Could you please help this to fix this issue as priority

Regards,
Sathwick",2022-10-20T12:50:04.886+0000,"Please refer to the custom tag feature in guardian for integration:

https://wiki.cisco.com/display/EDPEIXOT/SWIM+Customer+Tag This TC Contains 2 Features, Need to work on SWIM CCO first and then need to trigger SWIM Customer tag.","['Auton', 'Groot', 'Guardian', 'Issue', 'MS', 'MSTB2', 'Multisite']",Sathwick Reddy Polamreddy,Closed,Avril Bower
SEEN-778,https://miggbo.atlassian.net/browse/SEEN-778,[Auton]  Test_TC1_SWIM_image_upload_and_golden_tag  /   test4_verify_upload_os_image_mark_golden  failed with KeyError: 'image_dir'," *Reporter Analysis:* The script from the optimized code has failed with key error 'image_dir', where as in the yaml file we have the key image_dir: ""/www/swim"". This failed on TB3, yaml file is SanityTB3.yaml

*Description:  The error from log or more info* 
 Exception:
 1770: Traceback (most recent call last):
 1771: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 1772: result = testfunc(func_self, **kwargs)
 1773: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/testcases/sanityusecases/SwimImageUploadGoldenTag/swim_image_upload_mark_golden.py"", line 133, in *test4_verify_upload_os_image_mark_golden*
 1774: if (dnac_handle.swim_image_upload_assign_and_mark_golden(key=""image_dir"", site=""Global"")):
 1775: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 1776: result = method(*args, **kwargs)
 1777: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/swim/group.py"", line 94, in swim_image_upload_assign_and_mark_golden
 1778: *result = self.swim_image_upload_url(imageurl=imageurl,key=key)*
 1779: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 1780: result = method(*args, **kwargs)
 1781: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/swim/group.py"", line 18, in swim_image_upload_url
 1782: *images = self.services.dnaconfig.testbed.custom[key]*
 *1783: KeyError: 'image_dir'*
 *Branch Name:*  private/Ghost-ms/sanity_api_auto

*Script file/Usecase:* [SwimImageUploadGoldenTag|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-swim_image_upload_mark_golden.py-73-SwimImageUploadGoldenTag&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct06_19:00:32.109699.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] /[swim_image_upload_mark_golden.py|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-swim_image_upload_mark_golden.py-73-SwimImageUploadGoldenTag&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct06_19:00:32.109699.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-swim_image_upload_mark_golden.py-73-SwimImageUploadGoldenTag&begin=17925&size=415620&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct06_19:00:32.109699.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Pass Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-swim_image_upload_mark_golden.py-73-SwimImageUploadGoldenTag&begin=26394&size=7078767&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-09%2Fenv_optimized_auto_job.2022Sep30_09:54:55.680394.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-3] ",2022-10-21T12:01:57.883+0000,"Testcase failed in latest Groot FCS 70523 execution: 
Please commit the changes in Groot Optimized code 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-swim_image_upload_mark_golden.py-73-SwimImageUploadGoldenTag&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct28_20:14:13.613547.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ","['Auton', 'Ghost', 'Groot', 'Issue', 'Sanity', 'optimized']",Ashwini R Jadhav,Closed,Avril Bower
SEEN-779,https://miggbo.atlassian.net/browse/SEEN-779,[Auton] Test_TC1_DNAC_EXT_NODE_interface_config_verifications/test5_AEN_onboarding_version_Validation - Testcase failed with version validation for different versions," *Reporter Analysis:* The script verifies if the version of the extended node is >= 17.1.1 for those nodes whose roles are EXTNODE. But on TB3 we have EN nodes with role as EXTNODE but there device model are different. 

In yaml i found ENs as -*SN-JAD23450NLQ with version (17.6.20220707:154304 )* ,*SN-FOC2327V1GH with version (15.2(7.2.13)E )* and both are with role EXTNODE 

As the EN SN-FOC2327V1GH is not similar to polaris version or >17.1.1 Hence TC failed 

+*Device series & platform:*+

1.*SN-FOC2327V1GH* :: Cisco Catalyst Digital Building Series Switches & CDB-8P

2.*SN-JAD23450NLQ* :: Cisco Catalyst 9200 Series Switches & C9200-24PB

Please handle the device model and there versions respectively

 

*Description:  The error from log or more info* 
 ------------------------------------------------------------
 2846: ACTION: Verify Edge node and ext device node, both should be >= 17.1.1
 2847: ------------------------------------------------------------
 2848: Test returned in 0:00:27.303208
 2849: Failed reason:
 2850: ------------------------------------------------------------
 2851: Result: EXT Nodes with EXTNODE device role are not present on testbed, or NO eligible AEN ext nodes to be onboarded.!
  
 *Branch Name:*  private/Ghost-ms/sanity_api_auto

*Script file/Usecase:* [SDAExtnodeOnboarding|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-122-SDAExtnodeOnboarding&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct06_19:00:32.109699.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] /[extNode_onboarding.p|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-122-SDAExtnodeOnboarding&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct06_19:00:32.109699.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]y /[Test_TC1_DNAC_EXT_NODE_interface_config_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-122-SDAExtnodeOnboarding&begin=3900&size=3117650&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct06_19:00:32.109699.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-122-SDAExtnodeOnboarding&begin=3063091&size=20417&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct06_19:00:32.109699.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Pass Log:* 

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-3] ",2022-10-21T12:36:13.622+0000,Not an issue,"['Auton', 'Ghost', 'Issue', 'Sanity', 'optimized']",Ashwini R Jadhav,Closed,Avril Bower
SEEN-780,https://miggbo.atlassian.net/browse/SEEN-780,[Auton]Test_TC138_system_health_assurance_checks/ test1_verify_devices_categorized_health_no_health- testcase failed with keyerror while finding devices with role AP," *Reporter Analysis:* While trying to access devices with role as 'AP', testcase has failed with key error: 'AP502f'.

 On device TB3-DM-NF-Switch#
 23048: Found following CDP Entries:
 23049: [

{'localintf': 'GigabitEthernet1/0/24', 23050: 'name': 'TB3-DM-Fusion', 23051: 'remoteIntf': 'TenGigabitEthernet1/0/19'}

,
 23052:

{'localintf': 'TenGigabitEthernet1/1/2', 23053: 'name': 'SN-JAD23450NLQ', 23054: 'remoteIntf': 'TenGigabitEthernet1/1/2'}

,
 23055:

{'localintf': 'TenGigabitEthernet1/1/1', 23056: 'name': 'SN-JAD23450NLQ', 23057: 'remoteIntf': 'TenGigabitEthernet1/1/1'}

,
 23058:

{'localintf': 'GigabitEthernet1/0/7', 23059: *'name': 'AP502f',* *23060: 'remoteIntf': 'GigabitEthernet0'}

,*
 *23061:

{'localintf': 'GigabitEthernet1/0/48',* 23062: 'name': 'AP2436', 23063: 'remoteIntf': 'GigabitEthernet0'}

,
 23064:

{'localintf': 'GigabitEthernet1/0/37', 23065: 'name': 'AP3C41', 23066: 'remoteIntf': 'GigabitEthernet0'}

,
 23067:

{'localintf': 'GigabitEthernet1/0/13', 23068: 'name': 'TB3-DM-TSIM', 23069: 'remoteIntf': 'GigabitEthernet0/0/2'}

,
 23070:

{'localintf': 'GigabitEthernet1/0/14', 23071: 'name': 'TB3-DM-TSIM', 23072: 'remoteIntf': 'GigabitEthernet0/0/4'}

,
 23073:

{'localintf': 'GigabitEthernet0/0', 23074: 'name': 'MGMT_NW_123', 23075: 'remoteIntf': 'GigabitEthernet1/0/32'}

,
 23076:

{'localintf': 'GigabitEthernet1/0/25', 23077: 'name': 'AP7488', 23078: 'remoteIntf': 'GigabitEthernet0'}

,
 23079:

{'localintf': 'GigabitEthernet1/0/6', 23080: 'name': 'TB3-DM-Transit', 23081: 'remoteIntf': 'GigabitEthernet1/0/4'}

]
  
 In Yaml file *configs/sanity_tb3/SanityTB3_legacy.yaml*:   we do have the AP AP502f.a80f.2c80 with role= AP.

*Description:  The error from log or more info* 
 Traceback (most recent call last):
 23083: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 23084: result = testfunc(func_self, **kwargs)
 23085: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 9607, in test1_verify_devices_categorized_health_no_health
 23086: if dnac_handle.verify_devices_categorized_health_no_health():
 23087: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 6557, in verify_devices_categorized_health_no_health
 23088: if self.services.dnaconfig.testbed.devices[ap].role == ""AP"":
 23089: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/topology/bases.py"", line 100, in __getitem__
 23090: return super().__getitem__(key)
 23091: *KeyError: 'AP502f'*
 23092: Test returned in 0:00:09.510709
 23093: Errored reason: AP502f
  

*Branch Name:*  private/Ghost-ms/sanity_api_auto

                          private/Groot-ms/sanity_api_auto

                           private/Guradian-ms/sanity_api_auto

*Script file/Usecase:* From LAN script:  solution_test_sanityecamb_lan.py 
  [Test_TC138_system_health_assurance_checks|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6988651&size=121180&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct07_03:07:36.489100.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_devices_categorized_health_no_health

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6989221&size=21692&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct07_03:07:36.489100.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Pass Log:* 

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-3] ",2022-10-21T13:28:29.266+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4421/overview]

 

PR rasied. Please check  PR Approved, Raise PR for other branches. 

  Hi [~63f50bcf4e86f362d39acde5],

Can you please commit the changes to Guardian Branch also
Hitting same issue in Guardian Branch.

*Branch Used:*
private/Guardian-ms/sanity_api_auto

*Script file/Usecase:* 
solution_test_sanityecamb_lan.py , solution_test_sanityecamb.py

*Source Team:  Sanity*



*Failed Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=102452025&size=15732&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_auto_job.2023Mar06_03:23:27.437639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

 

  Hi [~accountid:63f50bcf4e86f362d39acde5] 

Closing the Jira as recently TC is passing on Guardian Branch:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=104653810&size=129733&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May09_06:24:08.609168.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=104653810&size=129733&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May09_06:24:08.609168.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Thanks,
Anusha John Hi [~accountid:63f50bcf4e86f362d39acde5]  
Can you please raise a PR for Halleck & Hulk branches  PR-link:

* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5974/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5974/overview]
* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5975/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5975/overview] PR for Halleck and Hulk have been approved and merged. [~accountid:63f50bcf4e86f362d39acde5] 
 Seeing the same issue in Ghost branch
*Branch: private/Ghost-ms/sanity_api_auto*

*Reporter Analysis:*
AP which is throwing error : is up and reachable on dnac

I have cherry picked commit of halleck to ghost branch and retried but hitting same error:
*Commit id:*
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c3f4b283e0f273b30e843f21119aa429855def26|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c3f4b283e0f273b30e843f21119aa429855def26]


*Failed Log Before Commit:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_360_nw_health.py-133-assuranceHealth360&begin=847402&size=26501&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_14:28:16.880913.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_360_nw_health.py-133-assuranceHealth360&begin=847402&size=26501&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_14:28:16.880913.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Error snip Before commit:*

{noformat}2782: 
   File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/topology/bases.py"", line 101, in __getitem__{noformat}

{noformat}2783: 
     return super().__getitem__(key){noformat}

{noformat}2784: 
 KeyError: 'APCC9C.3EEF.C390'{noformat}





*Failed log after pulling commit from halleck:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2412021&size=11422&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul18_03:57:06.444689.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2412021&size=11422&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul18_03:57:06.444689.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


*error snip from rerun log:*
 no devices with role AP in switch cdpdata

{noformat}10922: 
 Test returned in 0:00:00.599947{noformat}

{noformat}10923: 
 Failed reason: Check failed{noformat}





Can you please raise a separate PR for Ghost branch PR has been raised: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6487/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6487/overview] [~accountid:63a2a522082abdd71bb36e09] mentioned PR for Ghost branch has been approved and merged.

Please re-execute the TC and confirm if issue has been addressed. If not, pls. reopen this Auton.","['Auton', 'Ghost', 'Guardian', 'Hulk', 'Issue', 'Sanity', 'assurance', 'optimized']",QuangVinh Nguyen,Resolved,Avril Bower
SEEN-781,https://miggbo.atlassian.net/browse/SEEN-781,[Auton]Test_TC94_generate_link_flap_issues/test1_generate_link_flap_issues- TC failed because of device role being 'Border',"*Reporter Analysis:* 

 This testcase is expecting the device role to be 'ACCESS' where as the ECA border device role is 'Border Router'. 
 Error details :""Port actions are only supported on devices in Access role"". This testcase when re-run after changing the ECA device role from Border router to Access then the TC is passed. Please make the changes based on the feature requirement.

*Description:  The error from log or more info* 

Message:{""response"":

{""errorCode"":""Unexpected error"",""message"":""Unexpected error"",""detail"":""Port actions are only supported on devices in Access role""}

,""version"":""1.0""}

Reason: 500 Server Error: for url: [https://10.195.227.48/api/v1/interface/9722d234-be09-48fa-8264-c214822bb0bc?deploymentMode=Deploy]

*Branch Name:*  private/Ghost-ms/sanity_api_auto

*Script file/Usecase:* From LAN script:  solution_test_sanityecamb_lan.py 
  [Test_TC94_generate_link_flap_issues|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1630223&size=57271&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct07_03:07:36.489100.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_generate_link_flap_issues 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1630787&size=56542&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct07_03:07:36.489100.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Pass Log:* 

script passed when we change the ECA device role to ACCESS

[Test_TC94_generate_link_flap_issues|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=662116&size=41530&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb20_00:04:27.856902.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-3] ",2022-10-21T16:12:14.335+0000,"The summary and description are not related to each other?

  I missed adding the details
Please find the summary and description  # PR Ghost-ms/api_auto link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5443/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5443/overview]
# PR Ghost-ms/sanity_api_auto link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5444/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5444/overview]
# Test Case:  {{TC_generate_link_flap_issues}}
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb_lan.py
# Trade log link- Ghost-ms/api_auto: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-04/sanity_TB3.2023Apr26_00:54:50.100651.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-04/sanity_TB3.2023Apr26_00:54:50.100651.zip&atstype=ATS]
# Trade log link- Ghost-ms/sanity_api_auto: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-04/sanity_TB3.2023Apr26_02:56:49.752163.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-04/sanity_TB3.2023Apr26_02:56:49.752163.zip&atstype=ATS] Hi [~accountid:5e1415780242870e996f0b2f] , Could you help me review this PR Auton? Hi [~accountid:63f50bcafb3ac4003fa2c6dd] 
Executed the TC Generate link flap issues on TB8 - using branch Ghost-ms/sanity_api_auto after merging the PR on Ghost-ms/sanity_api_auto

Please find the pass log: [Test_TC94_generate_link_flap_issues|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=762276&size=99685&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May16_00:27:26.417829.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  [~accountid:63f50bcafb3ac4003fa2c6dd] based on the confirmation from [~accountid:5e1415780242870e996f0b2f] , I have approved and merged the PR for {{private/Ghost-ms/api-auto}} branch as well.

You may need to raise another PR for {{private/Halleck-ms/api-auto}} branch as well.

After that we can conclude on this Auton ticket. # PR {{private/Halleck-ms/api-auto}} link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5682/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5682/overview]
# Test Case:  {{TC_generate_link_flap_issues}}
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb_lan.py
# Trade log link- {{private/Halleck-ms/api-auto}}: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-05/sanity_TB1.2023May16_20:14:10.968545.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-05/sanity_TB1.2023May16_20:14:10.968545.zip&atstype=ATS]","['Auton', 'Ghost', 'Issue', 'Sanity', 'assurance', 'optimized']",NhanHuu Nguyen,Closed,Avril Bower
SEEN-782,https://miggbo.atlassian.net/browse/SEEN-782,Groot - [Auton]TC143_system_health_test/test2_add_cimc_info," 

*Reporter Analysis:* Manullya is able to add CIMC INFO in the cluster. but TC continues failing in Groot for adding details  :
33232:  Encountered unhandled error in Internal API Call.
Please refer .gif file 

*Description:*
33229:  api_switch_call called:
33230:  \{'data': {'key': '31533e73-4354-4df6-bc17-19478d046622', 'dnacaddress': '10.195.227.92', 'cimcaddress': '10.195.227.91', 'password': 'Lablab123', 'username': 'admin'}, 'timeout': 30}
33231:  Resource path full url: [https://10.195.227.92/api/v1/diagnostics/cimc/details/31533e73-4354-4df6-bc17-19478d046622]
33232:  Encountered unhandled error in Internal API Call
33233:  Flagging result as FAIL!
33234:  Reason: <class 'requests.exceptions.ReadTimeout'>
33235:  Kwargs:
33236:  {'data': {'cimcaddress': '10.195.227.91',
33237:  'dnacaddress': '10.195.227.92',
33238:  'key': '31533e73-4354-4df6-bc17-19478d046622',
33239:  'password': 'Lablab123',
33240:  'username': 'admin'},
33241:  'timeout': 30}
33242:  Traceback (most recent call last):
33243:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 449, in _make_request
33244:  six.raise_from(e, None)
33245:  File ""<string>"", line 3, in raise_from
33246:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 444, in _make_request
33247:  httplib_response = conn.getresponse()
33248:  File ""/auto/pysw/cel8x/python64/3.8.2/lib/python3.8/http/client.py"", line 1322, in getresponse
33249:  response.begin()
33250:  File ""/auto/pysw/cel8x/python64/3.8.2/lib/python3.8/http/client.py"", line 303, in begin
33251:  version, status, reason = self._read_status()
33252:  File ""/auto/pysw/cel8x/python64/3.8.2/lib/python3.8/http/client.py"", line 264, in _read_status
33253:  line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
33254:  File ""/auto/pysw/cel8x/python64/3.8.2/lib/python3.8/socket.py"", line 669, in readinto
33255:  return self._sock.recv_into(b)
33256:  File ""/auto/pysw/cel8x/python64/3.8.2/lib/python3.8/ssl.py"", line 1241, in recv_into
33257:  return self.read(nbytes, buffer)
33258:  File ""/auto/pysw/cel8x/python64/3.8.2/lib/python3.8/ssl.py"", line 1099, in read
33259:  return self._sslobj.read(len, buffer)
33260:  socket.timeout: The read operation timed out
33261: 
33262:  During handling of the above exception, another exception occurred:
*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10316394&size=170826&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct19_18:08:12.427976.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:N/a*

*Testbed details:* NA",2022-10-21T21:03:02.781+0000,Increased timeout value https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/625d039ed2f5461915ff61987a70b84f311b1e4d,"['Auton', 'Groot', 'Issue', 'Sanity']",Raji Mukkamala,Resolved,Avril Bower
SEEN-784,https://miggbo.atlassian.net/browse/SEEN-784,"[Auton]  Multiple sensor onboarding tests in Sanity script, needs cleanup"," 

*Reporter Analysis:*  I see we have the sensor onboarding test twice, first the sensor onboarding happens in TC43.2 and there is another sub test in TC45.3.

TC45.3 fails, this sub test can be removed or is there any use of it ?

 

*Description:*  
102169:  api_switch_call called:
102170:  \{'method': 'GET', 'resource_path': '/v2/data/customer-facing-service/ConnectivityDomain'}
102172:  Found required Authprofile : \{'id': 'f452d137-0206-4cc1-94b4-b4cff6d5cf55', 'instanceId': 29469484, 'displayName': '18123b3b[No Authentication,19b5e672-0ea5-4fea-a1c2-7278ba939b99]', 'instanceCreatedOn': 1665648709344, 'instanceUpdatedOn': 1665648709344, 'instanceVersion': 1, 'name': 'No Authentication', 'profileUuid': '19b5e672-0ea5-4fea-a1c2-7278ba939b99', 'profileVersion': 1, 'type': 'WIRED_NOAUTH', 'wakeOnLan': False, 'webauthBypassMap': []}
102173:  Library group ""connectivity_domain"" method ""get_authentication_profile_id_from_cd"" returned in 0:00:00.151847
102174: 
102175:  authProfile_id= 35332ddc-6891-4e46-86c1-faf69e538695
102176: 
102177:  Getting segment info for params:\{'name': 'SENSORPool_sub-INFRA_VN', 'namespace': '19b5e672-0ea5-4fea-a1c2-7278ba939b99'}
102178: 
102179: 
102180:  api_switch_call called:
102181:  \{'method': 'GET', 'resource_path': '/v2/data/customer-facing-service/Segment', 'params': {'name': 'SENSORPool_sub-INFRA_VN', 'namespace': '19b5e672-0ea5-4fea-a1c2-7278ba939b99'}}
102184:  \{'response': [], 'version': '1.0'}
102185:  Library group ""onboarding"" method ""get_segment_info"" returned in 0:00:00.037706
102186:  The segment section is empty
102187:  Library group ""wired_interface_onboarding"" method ""onboard_device_on_interface"" returned in 0:00:00.537315
102188:  Result:FAILED static onboarding failed forSENSOR:\{'name': 'AP70F3.5A7A.13C8', 'type': 'sensor', 'intf': <Interface TwoGigabitEthernet1/0/24 at 0x7f0befc6b908>}
102189:  +++ TB7-SJ-EDGE: config +++
*config term*

 

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0 issue, in all sanity scripts and optimized code*

*Fail Log:* https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&reqseq=&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=PYATS

*Pass Log:*

*Testbed details:* NA

 
||[Test_TC43_DNAC_EXT_NODE_interface_config_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27334083&size=3714964&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F12%2F23%2F35%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fhome%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FShockwave%2FShockwave-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=2627&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F12%2F23%2F35%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&from=trade&view=all]|Passed|00:33:56|Test_TC43_DNAC_EXT_NODE_interface_config_verifications| |
||[test1_dnac_ext_node_onboarding_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27334253&size=46677&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F12%2F23%2F35%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fhome%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FShockwave%2FShockwave-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=2628&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F12%2F23%2F35%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&from=trade&view=all]|Passed|00:00:01|test1_dnac_ext_node_onboarding_verifications| |
||[test2_dnac_sensor_onboarding_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27380930&size=327805&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F12%2F23%2F35%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]||

 

 
||[Test_TC45_DNAC_TSIM_static_onboarding_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=31241897&size=3906015&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F12%2F23%2F35%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fhome%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FShockwave%2FShockwave-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=2966&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F12%2F23%2F35%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&from=trade&view=all]|Failed|01:22:44|Test_TC45_DNAC_TSIM_static_onboarding_verifications| |
||[test1_tsim_static_onboarding_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=31242064&size=755472&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F12%2F23%2F35%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fhome%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FShockwave%2FShockwave-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=2967&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F12%2F23%2F35%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&from=trade&view=all]|Passed|00:08:07|test1_tsim_static_onboarding_verifications| |
||[test2_onboard_all_wired_clients|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=31997536&size=464178&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F12%2F23%2F35%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fhome%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FShockwave%2FShockwave-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=3011&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F12%2F23%2F35%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&from=trade&view=all]|Passed|00:04:27|test2_onboard_all_wired_clients| |
||[test3_onboard_all_sensors_on_all_sites|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=32461714&size=70369&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F12%2F23%2F35%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]|[!https://earms-trade.cisco.com/assets/images/testscripticon.svg|height=20!|https://earms-trade.cisco.com/tradeui/logs/scriptfile?scriptfile=%2Fhome%2Fjenkins%2Fws_sjc2_ds%2Fworkspace%2FSOL-SAN%2FShockwave%2FShockwave-sanity-common-Multi-job%2Ftestcases%2Fforty_eight_hour%2Fsolution_test_sanityecamb_lan.py&lineno=3040&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F12%2F23%2F35%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&from=trade&view=all]|Passed|00:00:25|test3_onboard_all_sensors_on_all_sites|",2022-10-21T22:56:49.583+0000,"It is known issue with the unicon version you are using for your run.

 

[https://earms-trade.cisco.com/tradeui/logs/fileviewer?logfile=env.txt&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct12_23:35:23.180856.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&atstype=pyATS&from=trade&view=all]

 

ats 19.6 /users/rvonmize/pyats-inst/lib/python3.7/site-packages ats.aereport 19.6 /users/rvonmize/pyats-inst/lib/python3.7/site-packages

unicon 19.7.4 /users/rvonmize/pyats-inst/lib/python3.7/site-packages

 

You can setup your own env within just 5 minutes, by running ./setup.sh  from the checkout directory of the solution codebase.  It will setup new env with latest unicon/pyats and etc. 

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/README.md]

 

 

 

 

 

 ","['Auton', 'Issue', 'Sanity']",Pawan Singh,Resolved,Avril Bower
SEEN-795,https://miggbo.atlassian.net/browse/SEEN-795,[Auton] [Groot] - Verify neighbor topology fails with missing Fabric role - 'INTERNET' for Border devices,"*Uber ISO Version tested :* 
 Promoted Groot Patch1 Pre-RC Uber ISO - *2.1.563.70111, Non-FIPS, PUBSUB enabled*

*Script Name:* solution_assurance_test.py

*Testbed :* MSTB1, MSTB2
  

*Description :* 

Verify neighbor topology fails with Fabric role mismatch between expected and Actual output for Border devices on SJ site. We suspect this could be due to modification introduced on Groot by Assurance team under device 360 page for Border devices. But we are not sure. Need confirmation.

*Error Snip:*

10733:  Verification Failed \{'SJ-FB-9500': {'SJ-FB-9500': ""Fabric Device Role Mismatch Expected ['MAP-SERVER', 'BORDER', 'INTERNET'] GOT ['MAP-SERVER', 'BORDER']""}, 'SJ-FB-9600': \{'SJ-FB-9600': ""Fabric Device Role Mismatch Expected ['MAP-SERVER', 'BORDER', 'INTERNET'] GOT ['MAP-SERVER', 'BORDER']""}}

 

*Failed log:*  [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fsr_mb_multi_sites.2022Oct26_00:17:54.277338.zip&atstype=ATS] ** -> Refer TC5

 

 ",2022-10-26T11:29:01.056+0000,"Same issue is also observed during Groot execution on MSTB2

*Uber ISO Version tested :* 
Promoted Groot Patch1 Pre-RC Uber ISO - *2.1.563.70111, FIPS*

*Script Name:* solution_assurance_test.py

*Testbed :* MSTB2
**

*Description:* Verify neighbor topology fails with Fabric role mismatch between expected and Actual output for Border devices on SJ site.

*Error Snip :* 
12989:  SJC-FB-9600 Device is Not Having Correct Fabric Role
12990:  Verification Failed \{'SJC-FB-9600': {'SJC-FB-9600': ""Fabric Device Role Mismatch Expected ['MAP-SERVER', 'BORDER', 'INTERNET'] GOT ['MAP-SERVER', 'BORDER']""}}
*Failed Log :* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1840928&size=3104613&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_MS_job.2022Oct16_20:24:40.793261.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS Please refer team space - webexteams://im?space=f0abdfa0-3e8d-11ed-9d33-1f5cf35ca26a for more details from Assurance Team PR-Guardian: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4308/overview

 

PR-Groot: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4303/overview]

 

PR-Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4305/overview] Added Guardian and Ghost labels as per update from Kartheek (Assurance Team)

Please refer Webex Team space - webexteams://im?space=f0abdfa0-3e8d-11ed-9d33-1f5cf35ca26a for more details.","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'MSTB1', 'MSTB2', 'Multisite', 'assurance', 'sanity']",NhanHuu Nguyen,Resolved,Avril Bower
SEEN-796,https://miggbo.atlassian.net/browse/SEEN-796,[Auton] config per neighbor should be pushed at once,"Currently the [BGP Down Issue|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/services/dnaserv/lib/api_groups/assurance/group.py#8874] related use-case is dumping bgp neighbor config per neighbor which can be done at one shot as well.

Enhancing the use-case code to do the same and save some execution time.",2022-10-26T19:10:43.697+0000,"Required PR got merged into Groot branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4013/overview]

Marking this ticket as ""Done"".","['Auton', 'Ghost', 'Groot', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-797,https://miggbo.atlassian.net/browse/SEEN-797,[Auton]  upload_trusted_certifcate_to_ISE to be added in LAN and CERT_LAN scripts," 

*Reporter Analysis:* 

dnac_handle.upload_trusted_certifcate_to_ISE

code need to be added in LAN and cert_LAN scripts

*Description:  The error from log or more info* 

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

*Pass Log:*

*Testbed details:* NA",2022-10-26T21:27:54.038+0000,"Guardian - [fff42727dbb|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/fff42727dbb96fcc56db016ea64704e1663d1e69]

Groot - [735ac6a6588|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/735ac6a65880b452b5a83e25029fc9214ca8a8b2]

Ghost - [d950004f975|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d950004f975142357ea78b304738338cfd09b3bf]","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity']",Nethra Thorali Suthagar,Resolved,Avril Bower
SEEN-800,https://miggbo.atlassian.net/browse/SEEN-800,Guardian =TC35_enable_fabric_wireless_eca,"*Sub TC failed*:
test3_disable_fabric_wireless

*Error snip shot* :
 api_switch_call called:
73802:  \{'params': {'sortBy': 'startTime', 'order': 'DESC', 'limit': 25, 'intent': 'CONFIG_PREVIEW'}}
73803:  Resource path full url: [https://10.106.133.152/api/v1/actionable-task]
73805:  Library group ""orchestration_engine"" method ""list_actionable_tasks"" returned in 0:00:00.033985
73806: 
73807:  Current Tasks Description:Scheduling task for connectivity domain update for fabric Global/USA/SAN-FRANCISCO_US_SJ_Fabric1 at time 1666223834.597631, expecting Scheduling task for connectivity domain update for fabric Global/USA/SAN-FRANCISCO_US_SJ_Fabric1 at time 1666223834.597631
73808:  Task Found :\{'activityId': '43d0d421-4a2a-429b-8dbe-b4c05f2fe554', 'description': 'Scheduling task for connectivity domain update for fabric Global/USA/SAN-FRANCISCO_US_SJ_Fabric1 at time 1666223834.597631', 'intent': 'CONFIG_PREVIEW', 'type': 'ACTIONABLE', 'module': 'PROVISION', 'status': 'COMPLETED', 'startTime': 1666223834648, 'lastUpdateTime': 1666223834648, 'actionComplete': 'NO', 'username': 'admin'} for description:Scheduling task for connectivity domain update for fabric Global/USA/SAN-FRANCISCO_US_SJ_Fabric1 at time 1666223834.597631
73809:  Library group ""orchestration_engine"" method ""wait_for_top_actionable_task_states"" returned in 0:00:20.160873
73810: 
73811: 
73812:  api_switch_call called:
73813:  {}
73814:  Resource path full url: [https://10.106.133.152/api/v1/orchestration-engine/activities/43d0d421-4a2a-429b-8dbe-b4c05f2fe554/devices]
73815:  Error Code: 404 URL:https://10.106.133.152/api/v1/orchestration-engine/activities/43d0d421-4a2a-429b-8dbe-b4c05f2fe554/devices Data:\{'timeout': 30} Headers:\{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2MzRmYWRhMzFlMTYyZDZhYTA3OWRjMDQiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjYzNGZhZDlkMWUxNjJkNmFhMDc5ZGMwMyJdLCJ0ZW5hbnRJZCI6IjYzNGZhZDliMWUxNjJkNmFhMDc5ZGMwMSIsImV4cCI6MTY2NjIyNzA4NiwiaWF0IjoxNjY2MjIzNDg2LCJqdGkiOiI3MWNjZjA3MC1jNTIwLTRhNmYtOTk2Zi1lNzI3ZTJjMTQ1YzQiLCJ1c2VybmFtZSI6ImFkbWluIn0.Oe-PJxe8TV0dwV8tTq3ziDt53rPJpOa_Km-gNW6vwc_aUy07cgHNhbi5kqQj42VzLAKqLZ0jr2qUrxl5tjtA8TduRmM-63pqqEXPIgZg-41EogRGIDpTvXtlEZkUiXD4DaBeydLzLyQPBs0HSKT_I0dmqmVW9NOf7tXoz7oQzOYffKFiCIym6wE7tBLOWHBrqXsUIEqzQ5RWMrTZFkqEPExopZPxQL9oiyq88fd4nqu6Jqe_wjTEI92uyTqvZtO0OpocI0IW4CQy8370g4dG-esd1YTXXJaqCqF-k03mfeIghRRe8nIDqEf3mvIUI3UO8yVKVjpYXCl_Lo9dWmAIsg;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:
73816:  Traceback (most recent call last):
73817:  File ""/ws/yiyamper-bgl/dnac_soln_reg/Guardian/dnac-auto/services/dnaserv/client_manager.py"", line 295, in call_api
73818:  response.raise_for_status()
73819:  File ""/ws/yiyamper-bgl/dnac_soln_reg/pyats/lib/python3.8/site-packages/requests/models.py"", line 953, in raise_for_status
73820:  raise HTTPError(http_error_msg, response=self)
73821:  requests.exceptions.HTTPError: 404 Client Error: Not Found for url: [https://10.106.133.152/api/v1/orchestration-engine/activities/43d0d421-4a2a-429b-8dbe-b4c05f2fe554/devices]
73822:  Encountered unhandled HTTPError in Internal API Call
73823:  Flagging result as FAIL!
73824:  Reason: 404 Client Error: Not Found for url: [https://10.106.133.152/api/v1/orchestration-engine/activities/43d0d421-4a2a-429b-8dbe-b4c05f2fe554/devices]
73825:  Kwargs:
73826:  {}
73827:  Error Caught While Querying the Internal API
73828:  Caught HTTPError; invalid or empty activity ID
73829:  No response for activity ID: ""43d0d421-4a2a-429b-8dbe-b4c05f2fe554""
73830:  Library group ""orchestration_engine"" method ""parse_activity_device_config_previews"" returned in 0:00:00.055913
73831:  Failed to receive config preview
73832: 
*Analysis*:
This is consistently seeing on Guardian branch . Not seen on Ghost 
Previously running solution regression for multisite but recently converted to Solution sanity.
Hitting for 9400 model AIO/FIAB device on NY site. for 9300 AIO/FIAB model working fine on San Francisco site

*Trade logs*:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19002985&size=965271&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F22-10%2Fsanity.2022Oct20_03:16:27.354992.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

*Passed Ghost log*:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19518238&size=1134206&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F22-10%2Fsanitycombine.2022Oct11_12:24:23.903247.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]



Hitting this issue in Guardian .

*Branch used*:

bgl/Guardian-ms/api-auto

Test suite:
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=refs%2Fheads%2Fbgl%2FGuardian-ms%2Fapi-auto",2022-10-27T08:08:45.410+0000,,"['Auton', 'Guardian', 'Issue']",Raji Mukkamala,Closed,Avril Bower
SEEN-803,https://miggbo.atlassian.net/browse/SEEN-803,TC118_vnid_override,"*Sub TC failed*:
test5_verify_wlans_VNIDs_mapping

*Error snip shot* :
320202:  Resource path full url: [https://10.106.133.152/api/v1/file/cdbe8e1d-7dfa-41a5-99ff-4c0be885636f]
320205:  [\{'deviceUuid': '832a2f73-4883-45a9-a50b-ed915f85b720', 'commandResponses': {'SUCCESS': {'show wireless fabric vnid mapping | i 8188': 'show wireless fabric vnid mapping | i 8188\n WClients_sf-WirelessVNFB 8188 0 0.0.0.0 default-control-plane \nTB4-DM1-9KB1#'}, 'FAILURE': {}, 'BLACKLISTED': {}}}]
320206:  Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:02.290822
320207:  j value CiscoSensorProvisioning
320208:  j value CWA_GUEST_SSID_AAAmega1
320209:  Clearpass server not found in fabric json file hence skipping verifying the details for wlan : CWA_GUEST_SSID_AAAmega1
320210:  Failed ON [\{'TB4-DM1-9KB1': 'SSID Single5KBandmega1 Not Found'}]
320211:  Test returned in 0:01:47.142025
320212:  Failed reason: Failed to verify Sensor SSIDs
320213:  The result of section test5_verify_wlans_VNIDs_mapping is => FAILED

Is this SSID applicable for San Francisco site ?

*FYI*:

Previously running solution regression for multisite but recently converted to Solution sanity.
Hitting this issue on Guardian & Ghost branch as well

TradeLogs:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=118669233&size=660430&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F22-10%2Fsanity.2022Oct20_03:16:27.354992.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]



Hitting this issue in Guardian & Ghost branches.

Branch used:

bgl/Ghost-ms/api-auto

Test suite:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Ghost-ms/api-auto]",2022-10-27T14:16:37.239+0000,"Hi Andrew ,
Did you get a chance to look into this issue? PRs:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5693/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5693/overview]
* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5694/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5694/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5695/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5695/overview]","['Auton', 'Issue']",ThangQuoc Tran,Resolved,Avril Bower
SEEN-804,https://miggbo.atlassian.net/browse/SEEN-804,TC121_verify_custom_vlan_ids,"*Sub TC failed*:
test1_verify_custom_vlanid_on_sites

*Error snip shot* :
Cust vlan id is not found on border nodes.


328677:  VLAN ID was not found in config on device: TB4-DM1-B95K
328678:  \{'IPPOOLNAME': 'overlapping_sub', 'trafficType': 'DATA', 'isCommonPool': 'false', 'isFloodAndLearn': 'true', 'isDefaultEnterprise': 'false', 'isExtendedProvisioning': 'false', 'isSelectiveFloodingEnabled': 'true', 'isApProvisioning': 'false', 'isCriticalPool': 'false', 'wlan': [], 'vlanId': '1087'}
328679:  VLAN ID was not found in config on device: TB4-DM1-B95K
328680:  \{'IPPOOLNAME': 'overlapping_sub', 'trafficType': 'DATA', 'isCommonPool': 'false', 'isFloodAndLearn': 'true', 'isDefaultEnterprise': 'false', 'isExtendedProvisioning': 'false', 'isSelectiveFloodingEnabled': 'true', 'isApProvisioning': 'false', 'isCriticalPool': 'false', 'wlan': [], 'vlanId': '1087'}


329471:  VLAN ID was not found in config on device: TB4-DM1-9600B1
329472:  \{'IPPOOLNAME': 'Fabric_VN-sub', 'trafficType': 'DATA', 'isExtendedProvisioning': 'false', 'isCommonPool': 'false', 'isFloodAndLearn': 'true', 'isDefaultEnterprise': 'false', 'isCriticalPool': 'false', 'isApProvisioning': 'false', 'vlanId': '9', 'wlan': []}
329473:  VLAN ID was not found in config on device: TB4-DM1-9600B1
329474:  \{'IPPOOLNAME': 'Fabric_VN-sub', 'trafficType': 'DATA', 'isExtendedProvisioning': 'false', 'isCommonPool': 'false', 'isFloodAndLearn': 'true', 'isDefaultEnterprise': 'false', 'isCriticalPool': 'false', 'isApProvisioning': 'false', 'vlanId': '9', 'wlan': []}

*FYI*:

Previously running solution regression for multisite but recently converted to Solution sanity.
 Hitting this issue on Guardian & Ghost branch as well

*TradeLogs*:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=118669233&size=660430&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F22-10%2Fsanity.2022Oct20_03:16:27.354992.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]



Hitting this issue in Guardian & Ghost branches.

Branch used:

bgl/Ghost-ms/api-auto

Test suite:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Ghost-ms/api-auto]",2022-10-27T18:36:03.293+0000,"Hi Andrew ,
Did you get a chance to look into this issue? Hi [~accountid:63f50bcece6f37e5ed93c87e]  ,
The same TC’s failed on Ghost Optimized  Run : 
Failed  Log:   [Test_TC121_verify_custom_vlan_ids|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_custom_vlan_id.py-191-verifyCustomVlanID&begin=3900&size=140821&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr28_06:52:09.069773.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
*Branch Name:*  private/Ghost-ms/sanity_api_auto

*Testbed details:*[*https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7*|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7]
could you please check . Hi 
Same issue is also observed during Halleck exctuon on Sanity Tb7 Cluster 2.1.660.70351
*Uber ISO Version tested :* 2.1.660.70351
*Script Name:* Optimized code 

*Testbed :*TB7
*Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_custom_vlan_id.py-191-verifyCustomVlanID&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May05_22:22:14.013507.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_custom_vlan_id.py-191-verifyCustomVlanID&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May05_22:22:14.013507.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
*Error Snip :* 

{noformat}8: 
 Cannot track test: tracking auth info must be set in order to transfer test tracking data{noformat}

{noformat}74: 
 VLAN ID was not found in config on device: TB7-SJ-eCA-BORDER-CP{noformat}

{noformat}75: 
 {'IPPOOLNAME': 'overlapping_sub', 'trafficType': 'DATA', 'isCommonPool': 'false', 'isFloodAndLearn': 'true', 'isDefaultEnterprise': 'false', 'isExtendedProvisioning': 'false', 'isSelectiveFloodingEnabled': 'true', 'isApProvisioning': 'false', 'isCriticalPool': 'false', 'wlan': [], 'vlanId': '1087'}{noformat}

{noformat}76: 
 VLAN ID was not found in config on device: TB7-SJ-eCA-BORDER-CP{noformat}

{noformat}77: 
 {'IPPOOLNAME': 'overlapping_sub', 'trafficType': 'DATA', 'isCommonPool': 'false', 'isFloodAndLearn': 'true', 'isDefaultEnterprise': 'false', 'isExtendedProvisioning': 'false', 'isSelectiveFloodingEnabled': 'true', 'isApProvisioning': 'false', 'isCriticalPool': 'false', 'wlan': [], 'vlanId': '1087'}{noformat}

{noformat}120: 
 VLAN ID was not found in config on device: TB7-SJ-eCA-BORDER-CP{noformat}

{noformat}121: 
 {'IPPOOLNAME': 'overlapping_sub', 'trafficType': 'DATA', 'isCommonPool': 'false', 'isFloodAndLearn': 'true', 'isDefaultEnterprise': 'false', 'isExtendedProvisioning': 'false', 'isSelectiveFloodingEnabled': 'true', 'isApProvisioning': 'false', 'isCriticalPool': 'false', 'wlan': [], 'vlanId': '1087'}{noformat}

{noformat}122: 
 VLAN ID was not found in config on device: TB7-SJ-eCA-BORDER-CP{noformat}

{noformat}123: 
 {'IPPOOLNAME': 'overlapping_sub', 'trafficType': 'DATA', 'isCommonPool': 'false', 'isFloodAndLearn': 'true', 'isDefaultEnterprise': 'false', 'isExtendedProvisioning': 'false', 'isSelectiveFloodingEnabled': 'true', 'isApProvisioning': 'false', 'isCriticalPool': 'false', 'wlan': [], 'vlanId': '1087'}{noformat}

{noformat}166: 
 VLAN ID was not found in config on device: TB7-SJ-eCA-BORDER-CP{noformat}

{noformat}167: 
 {'IPPOOLNAME': 'overlapping_sub', 'trafficType': 'DATA', 'isCommonPool': 'false', 'isFloodAndLearn': 'true', 'isDefaultEnterprise': 'false', 'isExtendedProvisioning': 'false', 'isSelectiveFloodingEnabled': 'true', 'isApProvisioning': 'false', 'isCriticalPool': 'false', 'wlan': [], 'vlanId': '1087'}{noformat} Hi,
On Sanity TB7, it failed because there are missing some Vlan ID configurations on TB7-SJ-eCA-BORDER-CP

From my failed log executed on Tb7:

{noformat}1389:  show vlan id 1087
1390:  VLAN id 1087 not found in current VLAN database
1391:  TB7-SJ-eCA-BORDER-CP#{noformat}

{noformat}1438:  show vlan id 1088
1439:  VLAN id 1088 not found in current VLAN database
1440:  TB7-SJ-eCA-BORDER-CP#{noformat}

{noformat}1493:  show vlan id 2
1494:  VLAN id 2 not found in current VLAN database
1495:  TB7-SJ-eCA-BORDER-CP#{noformat}

{noformat}1541:  show vlan id 9
1542:  VLAN id 9 not found in current VLAN database
1543:  TB7-SJ-eCA-BORDER-CP#{noformat}

{noformat}1548:  Failed to verify custom VLAN ID: ['1087', '1088', '2', '9'] on dev TB7-SJ-eCA-BORDER-CP{noformat}

I was run this tc on Sanity TB1 and it works fine, passed log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-thangqtr/users/thangqtr/archive/23-05/sanity_TB1.2023May17_01:09:13.934075.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-thangqtr/users/thangqtr/archive/23-05/sanity_TB1.2023May17_01:09:13.934075.zip&atstype=ATS]

[~accountid:620b8357878c2f00729881c8], Please help check it on the ECA device and add the missing vlan config on Sanity TB7 PRs:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5693/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5693/overview]
* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5694/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5694/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5695/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5695/overview] Hi [~accountid:63f50bd34c355259db9ccc4d] ,

Build: Hulk 2370-70389
the same  issue is seen latest Hulk run : 
Failed log:
[Test_TC121_verify_custom_vlan_ids|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_custom_vlan_id.py-192-verifyCustomVlanID&begin=3900&size=111296&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul02_06:11:33.461321.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

could you please check 

!image-20230703-071106.png|width=1637,height=440! Hi [~accountid:620b8357878c2f00729881c8],

From your failed log, it failed because the Vlan Id was not found in the current VLAN database on {{TB7-SJ-eCA-BORDER-CP}}

{noformat}456:  Output of 'show vlan id 9' executed on TB7-SJ-eCA-BORDER-CP:
457:  show vlan id 9
458:  VLAN id 9 not found in current VLAN database{noformat}

Just checked on the device, it’s missing vlan configurations, and the scripts failed as expected.

!image-20230705-031248.png|width=437,height=168!

I have run this test case on Sanity TB1 and it works fine, so there's no need to revise the code.

Pass log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-thangqtr/users/thangqtr/archive/23-07/sanity_TB1.2023Jul04_19:24:55.454941.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-thangqtr/users/thangqtr/archive/23-07/sanity_TB1.2023Jul04_19:24:55.454941.zip&atstype=ATS]

Please recheck on your device. Remove border for the role list , should be checked only on Edges devices unless the val is a L2Handoff. Updated in the following PRs:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6790/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6790/overview]
* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6791/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6791/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6792/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6792/overview]","['AWS-Santiy', 'Auton', 'Ghost', 'Halleck', 'Hulk', 'Issue', 'Optimized', 'Sanity']",ThangQuoc Tran,Resolved,Avril Bower
SEEN-805,https://miggbo.atlassian.net/browse/SEEN-805,[Auton]:Guardian: Test_TC104_wifi_readniness /test1_wifi_readniness,"*Reporter Analysis:* Test is failing for wifi6 and TC should give clear and proper error message

*ISO:*2.1.515.70134

*Description:*  
 **
 6568: Wifi data is as not expected!! RESPONSE: {'type': 'wifi6Readiness', 'records': [

{'name': 'legacy', 'count': '0'}

, \{'name': '11n', 'count': '0'}, \{'name': '11ac', 'count': '43'}, \{'name': '11ax', 'count': '4'}, \{'name': '11ax6', 'count': '1'}, \{'name': 'protocol unknown', 'count': '0'}, \{'name': 'enabled', 'count': '4'}, \{'name': 'disabled', 'count': '0'}, \{'name': '6E-enabled', 'count': '1'}, \{'name': '6E-disabled', 'count': '0'}]},
 6569: Wi-Fi 6 APs: 4, Non Wi-Fi 6 APs: 43
 6571: Failed reason: AP Wifi 6 Readiness is not as Expected!!
  

*Branch Name:*  private/Guardian-ms/sanity_api_auto

*Script file/Usecase:* solution_test_sanityecamb_lan.py

 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1330116&size=100747&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct25_04:52:19.225660.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS**]

 

*Pass Log:*

*Testbed details:* NA",2022-10-28T13:44:40.403+0000,"[Groot:|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f91ffb5472f94cec9061ee6b73e430c2fac74914]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f91ffb5472f94cec9061ee6b73e430c2fac74914]

 

Ghost:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/284f1473b24514bac2c554a5ce84e047c8d861da]

Hallack/Hulk will get it by forward merge next week.

 

INFO:api-group-assurance:---------------------------------------------------------------------------

ERROR:api-group-assurance:Expected number of wifi AP are 2 but received number from dashboard is:1, mismatching with inventory APs

ERROR:api-group-assurance:Wifi received from dashboard!! RESPONSE: \{'type': 'wifi6Readiness', 'records': [{'name': 'legacy', 'count': '0'}, \{'name': '11n', 'count': '0'}, \{'name': '11ac', 'count': '22'}, \{'name': '11ax', 'count': '1'}, \{'name': '11ax6', 'count': '1'}, \{'name': 'protocol unknown', 'count': '1'}, \{'name': 'enabled', 'count': '1'}, \{'name': 'disabled', 'count': '0'}, \{'name': '6E-enabled', 'count': '1'}, \{'name': '6E-disabled', 'count': '0'}]},

Wi-Fi 6 APs: 1, Non Wi-Fi 6 APs: 22

ERROR:api-group-assurance:Wifi6 capable APs are 2 : ap names: ['AP2C57.4184.2BF4', 'APCC9C.3EF1.04F0'], 

ERROR:api-group-assurance:Wifi 6 Readiness is not as Expected!!

INFO:api-group-assurance:---------------------------------------------------------------------------

False

 ","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity', 'assurance']",Pawan Singh,Resolved,Avril Bower
SEEN-806,https://miggbo.atlassian.net/browse/SEEN-806,TC132_verify_border_edge_kpi,"*Sub TC failed*:
 test1_verify_border_edge_kpi

*Error snip shot* :
 299593: Calculating If the required result found in the Response!!
 299594: Result Generated is ['1667107200000']
 299595: Fabric Control Plane Scores or ISE are not as Expected!! on TB4-DM1-4SLOT-9400 \{'modificationtime': '1667107200000', 'overallScore': '8', 'tcpConnScore': '-1', 'time': '2022-10-30T05:20:00.000+0000', 'enIseConnScore': '10'}

*Analysis*:

In Newyork site, 9400 device is not having a role as border node .

its not a timing issue . hitting continuously on Guardian and Ghost branches

Set up is Previously running solution regression for multisite but recently converted to Solution sanity.

*Trade logs*:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=120989671&size=164090&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F22-10%2Fsanitycombine.2022Oct29_23:13:39.011595.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

 

Hitting this issue in Guardian & Ghost branches.

Branch used:

bgl/Ghost-ms/api-auto

Test suite:
 [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Ghost-ms/api-auto]",2022-10-30T18:18:45.046+0000,"Hi Moe
Did you get a chance to look into this issue? [~accountid:60796a73b5dffc006f64258a], Please check why the score is low on some devices on 360 page or share the cluster in the issue state. Is it a product issue or is it something else? 

I will but this Ticket on Blocked until you provide me with more details.  Will go ahead close this ticket. Please check why the border got low score from 360 page.","['Auton', 'Issue', 'assurance']",Moe Saeed,Closed,Avril Bower
SEEN-810,https://miggbo.atlassian.net/browse/SEEN-810,TC138_system_health_assurance_checks,"*Sub TC failed*:
test1_verify_devices_categorized_health_no_health

*Error snip shot* :
356509:  api_switch_call called:
356510:  \{'params': {'startTime': 1667106880602, 'endTime': 1667110480639}, 'data': \{'filters': {}}}
356511:  Resource path full url: [https://10.106.133.152/api/assurance/v1/network-device]
356512:  Basic, no filtered api returned correctly, continuing...
356513:  Response found, devices are present
356514:  cannot find NF switch for NY site
356515:  Test returned in 0:00:00.420022
356516:  Failed reason: Check failed
356517:  The result of section test1_verify_devices_categorized_health_no_health is => FAILED
*Analysis*:

do I need to change any device role which can acts NF switch

its not a timing issue . hitting continuously on Guardian and Ghost branches

Set up is Previously running solution regression for multisite but recently converted to Solution sanity.

*Trade logs*:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=133171209&size=5345&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F22-10%2Fsanitycombine.2022Oct29_23:13:39.011595.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS

 

Hitting this issue in Guardian & Ghost branches.

Branch used:

bgl/Ghost-ms/api-auto

Test suite:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Ghost-ms/api-auto]",2022-10-30T18:40:03.997+0000,"Hi Andrew ,
Did you get a chance to look into this issue? [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/7a527ecceabf55c41672a742af69225f026bf79c|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/7a527ecceabf55c41672a742af69225f026bf79c]



[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/19bc66b9f16f90a17752f5dfa1905c1584cea13d|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/19bc66b9f16f90a17752f5dfa1905c1584cea13d]

Ghost fix (one for picking any edge switch with ap, one for fixing cdp nei function), also pushed to guardian, groot, halleck","['Auton', 'Issue', 'assurance']",Andrew Chen,Resolved,Avril Bower
SEEN-811,https://miggbo.atlassian.net/browse/SEEN-811,TC139_Enhance_RCA_AAA_Issue,"*Sub TC failed*:
test1_disconnect_clients_to_ssids
|test2_Enhance_RCA_AAA_Issue|

test1_reconnect_clients_to_ssids

*Error snip shot* :
357819:  Library method ""reconnect_clients"" returned in 0:00:00.146652
357820:  Wireless client Found:[\{'name': 'tb4-wireless-client1', 'type': 'not-wireless-client', 'ap': '', 'sw': ''}, \{'name': 'tb4-wireless-client2', 'type': 'not-wireless-client', 'ap': '', 'sw': ''}, \{'name': 'tb4-wireless-client4', 'type': 'not-wireless-client', 'ap': '', 'sw': ''}, \{'name': 'tb4-wireless-client5', 'type': 'not-wireless-client', 'ap': '', 'sw': ''}, \{'name': 'tb4-wireless-client3', 'type': 'wireless-client,static', 'ap': 'AP700F.6A53.F144', 'sw': 'TB4-DM1-9300-36p'}]
357821:  Traceback (most recent call last):
357822:  File ""/ws/yiyamper-bgl/dnac_soln_reg/Ghost/dnac-auto/services/commonlibs/test_wrapper.py"", line 301, in wrapper
357823:  result = testfunc(func_self, **kwargs)
357824:  File ""/ws/yiyamper-bgl/dnac_soln_reg/Ghost/dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 9570, in test1_disconnect_clients_to_ssids
357825:  if not dnac_handle.dnaconfig.auth_failure_event_wireless_clients(ssid=ssid, profile=""dcs_fail"",
357826:  File ""/ws/yiyamper-bgl/dnac_soln_reg/Ghost/dnac-auto/services/commonlibs/sftopology.py"", line 6234, in auth_failure_event_wireless_clients
357827:  user = self.testbed.devices[client['name']].custom['profiles'][profile][""username""]
357828:  KeyError: 'profiles'
357829:  Test returned in 0:00:00.148714
357830:  Errored reason: profiles
*Analysis*:

its not a timing issue . hitting continuously on Guardian and Ghost branches

Set up is Previously running solution regression for multisite but recently converted to Solution sanity.

*Trade logs*:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=133443618&size=42702&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F22-10%2Fsanitycombine.2022Oct29_23:13:39.011595.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS

 

Hitting this issue in Guardian & Ghost branches.

Branch used:

bgl/Ghost-ms/api-auto

Test suite:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Ghost-ms/api-auto]",2022-10-30T18:44:09.878+0000,"Hi Andrew ,
Did you get a chance to look into this issue? https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/configs/config_48hr_test/solution_test_input.json?until=4881ae69eff08dd5c2fb65e8b65be073af16ac17&at=refs%2Fheads%2Fbgl%2FGhost-ms%2Fapi-auto

 

Your solution_inputs.json file was not synced with mainline, was missing several items.

 

I have done it for this branch. but for newer, you should fix it yourself. Take the latest from mainline and just change the address to India address on sites. Resolved by pulling manual merge.","['Auton', 'Issue']",Pawan Singh,Resolved,Avril Bower
SEEN-813,https://miggbo.atlassian.net/browse/SEEN-813,[Auton][MS] : Clear Pass Feature : Unable to configure clear pass ip through script.,"Hi Rakesh, 

We have tried integrating the clear pass feature in MS TB2. We are seeing issues like the script is not taking clear IP from fabric JSON to configure. It was taking a Secondary aaa server ip which we mentioned in fabric JSON.
 In test1_DNAC_verify_eca_fabric_guest_wlan_summary_aaa_config  testcase also, we can see clear pass configs are not pushed and showing like 
 6710: aaa server ip 10.195.247.249 could not find under radius group
 6711: Failed for reason ,['aaa server ip 10.195.247.249 could not find under radius group'] but script is passing instead of failing. 
 Please find the log : 
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_MS_job.2022Oct27_02:16:12.162734.zip&atstype=ATS]

  !613d4f96-5832-470a-a747-d3ae74362439.PNG!
  ",2022-10-31T12:44:36.463+0000,"there is no script issue for configuring the clearpass server ip in dnac. Even in the regression setup also it is passing.

 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/rmukkama-sjc/pyats/users/rakdomma/archive/22-11/three_sites_mdnac.2022Nov07_01:51:22.354456.zip&atstype=ATS]

 

About CWA ssid verification due to previous TCS failing CWA ssid verification also failing nothing to do with script let fix the below Jira first 

 

https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-451 Hi Rakesh,

Thanks for helping on this feature.
As discussed, I will follow up on this fixing the previous test cases (SEEN-451) and then try to verify the clear pass feature.

Regards,
Sathwick ","['Auton', 'Groot', 'Issue', 'MSTB2']",Avril Bower,Closed,Avril Bower
SEEN-814,https://miggbo.atlassian.net/browse/SEEN-814,[Auton]:Groot:  Task-assurance_fabric_assurance.py-155-SDAFabricAssurance / TC12_verify_fabric_360  /   test1_verify_fabric_SD_access," 

*Reporter Analysis:* Tc Failed for KeyError 

*Description: (Snip from Failed log)*
 3927: ======================================================================================================================================================
 3928: Traceback (most recent call last):
 3929: File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/Ghost_Optimized-Sanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 3930: result = testfunc(func_self, **kwargs)
 3931: File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/Ghost_Optimized-Sanity/testcases/sanityusecases/SDAFabricAssurance/assurance_fabric_assurance.py"", line 246, in test1_verify_fabric_SD_access
 3932: if dnac_handle.verify_fabric_SD_access():
 3933: File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/Ghost_Optimized-Sanity/services/dnaserv/lib/api_groups/assurance_fabric/group.py"", line 425, in verify_fabric_SD_access
 3934: device_details[val['name']]['Fabric Infrastructure'] = val[""fabsiteFnodeScore""]
 3935: KeyError: 'fabsiteFnodeScore
 *Branch Name:  private/Ghost-ms/sanity_api_aut0*

*Script file/Usecase:* SDAFabricAssurance

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-155-SDAFabricAssurance&begin=1742696&size=54311&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_optimized_auto_job.2022Oct28_10:37:47.023780.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:*

*Testbed details:* NA",2022-10-31T18:08:09.551+0000,"# PR-Ghost link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4991/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4991/overview]
# PR-Groot link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5011/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5011/overview]
# Test Case:  {{TC_verify_fabric_360}}
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link- Ghost: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB10.2023Mar13_19:38:32.398889.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB10.2023Mar13_19:38:32.398889.zip&atstype=ATS]
# Trade log link- Groot: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB12.2023Mar14_20:43:06.014855.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB12.2023Mar14_20:43:06.014855.zip&atstype=ATS] [~accountid:63f50bcafb3ac4003fa2c6dd] , Complain is about KeyError while processing below code:
{{device_details[val['name']]['Fabric Infrastructure'] = val[""fabsiteFnodeScore""]}}

The change you have made is not even near to that.

Please revisit the code and handle the exception accordingly. Also, pls. make use of Sanity Testbeds other than Sanity TB1 as the execution log does not seem to completly covering all steps.
Do check my comment on [SEEN-177|https://bitbucket-eng-sjc1.cisco.com/bitbucket/plugins/servlet/jira-integration/issues/SEEN-177] for more info. Sure [~accountid:62ab7a399cd13c0068b18fe0], I will update the code accordingly.

Thanks","['AWS_Sanity', 'Auton', 'Ghost', 'Issue', 'Optimized', 'Sanity']",NhanHuu Nguyen,Resolved,Avril Bower
SEEN-815,https://miggbo.atlassian.net/browse/SEEN-815,Test_TC109_DNAC_maps test3 third loop is failing,"*Reporter Analysis:* Under Test_TC109_DNAC_maps, test3 is failing for the third loop - ekahau_without_lat_long

Based on the Warning message, the failure seems to be expected:
{code:java}
Warning messages: [{'message': ""Ekahau Project File doesn't contain latitude-longitude info for building Test Ekahau""}, {'message': ""Building 'Test Ekahau' doesn't exist and cannot be created, archive site hierarchy starting from this building will be ignored""}]{code}
*Branch Name:*  esxivm_branch

*Script file/Usecase:* solution_test_sanityecamb.py / Test_TC109_DNAC_maps / test3_import_Ekahau_file

*Source Team:*  Sanity

*Issue Seen first time or day0 issue:* Not sure

*Fail Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=792517&size=9406&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct31_12:25:48.754267.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:* NA

*Testbed details:* TB2, ESXi VM DNAC - 10.22.45.61",2022-10-31T20:16:24.025+0000,"Third loop can be removed ekahau_without_lat_long

@aetest.loop(ekahau_type=['ekahau_without_lat_long', 'ekahau_with_lat_long', 'ekahau_without_lat_long'],

             expect_failure=[True, False, False])

Change to — @aetest.loop(ekahau_type=['ekahau_without_lat_long', 'ekahau_with_lat_long'],

             expect_failure=[True, False])

 

  [~62d2fe9f8afb5805e5d5af49], looks like the loop part of this use-case is not properly copied into Optimized script in Groot branch.

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py#8821]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/sanityusecases/ssidEditSpecialChar/ssid_edits_special_char.py#132]

 

Is this intentional or need correction?

  [~63f50bf5e8216251ae4d59cf], the test is now failing for the first loop - ekahau_type=ekahau_without_lat_long, expect_failure=True
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=760594&size=9405&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct31_14:06:24.485585.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Please suggest what next should we do.. [~62ab7a399cd13c0068b18fe0] Can you check if the expected_failure is being passed as True (print/log the expect_failure variable) to see why it's not going to else condition below

//else:
 # Failure expected if site doesn't have building already and Ekahau has no latitude, longitude data
 if ekahau_type == 'ekahau_without_lat_long' and expect_failure:
 logger.info(""Ekahau import failed as expected"")
 else:
 self.failed(""Failed to import Ekahau file"") with the latest executions, TC is passing without any error:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=32711406&size=48824&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov14_20:41:34.186749.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

 

Marking this Ticket as ""Closed"".","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'Sanity']",Raji Mukkamala,Closed,Avril Bower
SEEN-816,https://miggbo.atlassian.net/browse/SEEN-816,[Auton] - TC124_verify_SDA_fabric_issue/test2_verify_fabric_dhcp_issue,"*Reporter Analysis:* This TCs should skip If the device image if above 17.6.2

*Description:*  As per the previous logs this sub TC is skipping if the device image is above 17.6.2 but in current execution we had 17.10.01prd4 image which should skip ideally.  **  

*Branch Name:*  private/Guardian-ms/sanity_api_auto

*Script file/Usecase:* testcases/forty_eight_hour/solution_test_sanityecamb.py

*Source Team:*  Sanity

*Issue Seen first time or day0 issue:* First time

*Fail Log:* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=49777547&size=1101655&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct28_11:49:07.744882.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=31399106&size=5067&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct25_04:52:19.225660.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Testbed details:* NA

 ",2022-11-02T12:40:33.442+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/16dc6a1ef022f4455ff6a96a19d930fb0e3e3b23]

 

Fixed on Ghost, Groot, and Guardian, will come to Hallack and Hunlk through weekly forward merge.","['17.10', 'Auton', 'Guardian', 'Issue', 'Sanity']",Pawan Singh,Resolved,Avril Bower
SEEN-817,https://miggbo.atlassian.net/browse/SEEN-817,[Auton] Prime feature  automation is not working in groot," 

*Reporter Analysis:*  Discussed the issue with Andrew, the Groot prime code is not handling the prime login, due to the Groot P1 testing is blocked, Now we are using the latest chrome driver, since the chrome version was updated automtaically 

 

[Fail log : https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-10/env_auto_job.2022Oct21_18:59:29.276164.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/22-10/env_auto_job.2022Oct21_18:59:29.276164.zip&atstype=ATS]

*Description:  The error from log or more info* 

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

*Pass Log: This test was passed in Guardian.*

*Testbed details:* NA",2022-11-02T21:59:29.699+0000,"pass log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=289147&size=555567&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov29_22:24:34.946006.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

There is one check failed, which Andrew has fixed need to try in the next run.","['Auton', 'Groot', 'Issue', 'Sanity']",Andrew Chen,Closed,Avril Bower
SEEN-819,https://miggbo.atlassian.net/browse/SEEN-819,Auton[Ghost]:Test_TC23_WIRELESS_VERIFY_ACCESS_TUNNEL_ON_EDGE_ROUTER/test1_verify_access_tunnel_on_edges,"*Reporter Analysis:* 
 The command *""show platform software access-tunnel switch active r0""* is not working in TB7-NY-FIAB but it is working in TB7-SJ-EDGE.

The command *""show platform software access-tunnel R0""* is working in TB7-NY-FIAB


*Affected TC:*
Test_TC49_WIRELESS_VERIFY_ACCESS_TUNNEL_ON_EDGE_ROUTER/test1_verify_access_tunnel_on_edges

 

*Description:* 
 TB7-NY-FIAB# show platform software access-tunnel switch active r0 show platform software access-tunnel switch active r0 ^ % Invalid input detected at '^' marker. TB7-NY-FIAB#
 34474: Traceback (most recent call last):
 34475: File ""/users/rvonmize/pyats-inst/lib/python3.7/site-packages/unicon/plugins/generic/service_implementation.py"", line 680, in call_service
 34476: self.result = self.get_service_result()
 34477: File ""/users/rvonmize/pyats-inst/lib/python3.7/site-packages/unicon/bases/routers/services.py"", line 177, in get_service_result
 34478: self.match_list)
 34479: unicon.core.errors.SubCommandFailure: ('sub_command failure, patterns matched in the output:', ['^%
s*[Ii]nvalid (command|input)'])
 34480: 
 *Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* after_upgrade_verify.py

*Source Team:  Upgrade Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=25272131&size=36571&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F03%2F05%2F56%2Fenv_auto_job.2022Nov03_05:56:41.061868.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log From TB3 Ghost Upgrade:* 
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13995723&size=33753&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F21%2F01%2F50%2Fenv_auto_job.2022Oct21_01:50:34.513534.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:*
 [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7]",2022-11-03T16:13:42.696+0000,"Fixed on Ghost, Groot, Guardian

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9fcf64a2b492a4552acd88ca2395c6234cf0135d

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9fcf64a2b492a4552acd88ca2395c6234cf0135d

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/47d9894381d5c01e02b0b6f468a8bc37ac514cd7
 https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/7c3835e421b7d2063b250f262f2c99ed06a47657 Passed in Latest Upgrade Combination of GhostRC1#2.1.610.70567<>Halleck#2.1.660.70163++
 
 
 TC23:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=25272131&size=36571&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F03%2F05%2F56%2Fenv_auto_job.2022Nov03_05:56:41.061868.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]
 

TC 49:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=467290&size=123265&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F12%2F15%2F01%2F40%2Fenv_auto_job.2022Dec15_01:40:59.181660.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 

 ","['Auton', 'Ghost', 'Issue', 'Sanity', 'Upgrade']",Pawan Singh,Closed,Avril Bower
SEEN-820,https://miggbo.atlassian.net/browse/SEEN-820,Auton:[Ghost]:Test_TC34_Remove_Edge_from_fabric_and_add_back/test1_remove_an_edge_from_the_fabric,"*Reporter Analysis:* 



*Description:*  
**
17202:  Exception:
17203:  Traceback (most recent call last):
17204:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Upgrade-Verify-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
17205:  result = testfunc(func_self, **kwargs)
17206:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Upgrade-Verify-common-Multi-job/testcases/upgrade/after_upgrade_verify.py"", line 1326, in test1_remove_an_edge_from_the_fabric
17207:  if ""EDGENODE"" in dnac_handle.dnaconfig.testbed.devices[border[""name""]].role:
17208:  NameError: name 'border' is not defined
 

*Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:*
after_upgrade_verify.py
 

*Source Team:  Upgrade Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16167164&size=8324&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F03%2F05%2F56%2Fenv_auto_job.2022Nov03_05:56:41.061868.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log from TB5:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=23673912&size=4054&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F10%2F17%2F02%2F52%2Fenv_auto_job.2022Oct17_02:52:55.717906.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS

*Testbed details:* 
https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7",2022-11-04T09:34:48.165+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3f15ead935754f49de63a7bc68dd80b82d133b72

Fixed on Groot, Ghost, Guardian. https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3f15ead935754f49de63a7bc68dd80b82d133b72

Fixed on Groot, Ghost, Guardian.","['Auton', 'Ghost', 'Issue', 'Sanity', 'Upgrade']",Pawan Singh,Resolved,Avril Bower
SEEN-822,https://miggbo.atlassian.net/browse/SEEN-822,[Auton][IBSTE] : Test_TC8_DNAC_configure_ssm_muticast_and_verify_traffic  /   cleanup_existing_multicast,"Affected testcase:

[Test_TC8_DNAC_configure_ssm_muticast_and_verify_traffic|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27122931&size=1106646&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fsr_ibste.2022Nov03_18:30:20.405183.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   

cleanup_existing_multicast

 

Script : ibste_asm_ssm_exrp_inrp_native_multicast

Branch : private/Ghost-ms/api-auto

Failure :

Script is expecting response as integers or slices not as string

**
92802:  task not found in activity response.
92803:  Deploying device in fabric ERROR !!!
92804:  Traceback (most recent call last):
92805:  File ""/auto/dna-sol/ws/sr-ibste/ghost/services/dnaserv/lib/api_groups/connectivity_domain/group.py"", line 1161, in update_device_info_in_cd
92806:  activity_id = self.services.wait_for_top_actionable_task_states(intent=""CONFIG_PREVIEW"",description=description, count=25)
92807:  File ""/auto/dna-sol/ws/sr-ibste/ghost/services/dnaserv/lib/decorators.py"", line 32, in wrapper
92808:  result = method(*args, **kwargs)
92809:  File ""/auto/dna-sol/ws/sr-ibste/ghost/services/dnaserv/lib/api_groups/orchestration_engine/group.py"", line 175, in wait_for_top_actionable_task_states
92810:  self.log.error(""Response:\n {}"".format(pprint.pformat(response['response'])))
92811:  TypeError: list indices must be integers or slices, not str
92812:  Library group ""connectivity_domain"" method ""update_device_info_in_cd"" returned in 0:10:02.344638
92813:  #################False
92814:  !!!!!!!! Clear Multicast RP failed from fabric site Global/SAN-FRANCISCO_US_SJ_Fabric1 !!!!!!!!
 
Failed Log : https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27123691&size=1104811&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fsr_ibste.2022Nov03_18:30:20.405183.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS",2022-11-04T10:23:34.844+0000,"Required PR: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4107/overview got approved and merged to Groot, Ghost, Guardian and esxivm_branch.

Marking this ticket as ""Done"".","['Auton', 'Ghost', 'IBSTE', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-824,https://miggbo.atlassian.net/browse/SEEN-824,[Auton][Ghost] : Test_TC91_Remove_Border_add_back/test2_re_add_border_to_the_fabric,"*Uber ISO Version tested:* Ghost Uber ISO - *2.1.610.70512, FIPS*

*Script Name:* solution_test_3sites_sjc_nyc_sf.py

*Testbed:* MSTB2

*Branch:*  private/Ghost-ms/api-auto

*Failed Log :* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16843912&size=1087567&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F04%2F00%2F23%2Fsr_mb2_three_sites_FIPS.2022Nov04_00:23:43.398410.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Description :* While executing removing and adding back site border scenario. Border devices are removed from DNAC successfully, But after removal of devices, those were not added back properly. We are seeing issue while adding back the devices. This causes further test cases to also fail. All the extended node devices went unreachable after executing this testcase.


44023:  api_switch_call called:
44024:  \{'params': {'limit': 25, 'order': 'DESC', 'sortBy': 'lastUpdateTime', 'source': 'EXTERNAL', 'type': 'DEFAULT'}}
44025:  Resource path full url: [https://172.23.241.114/api/v1/scheduled-job]
44026:  Task with des Fabric update on Device(s) for False on time: 1667559002.780968 found
44027:  Task done with expected status COMPLETED found
44028:  The Schedduled Job failed: with reason \{'id': '02d0605b-8b68-4a89-8f79-30646cbb6601', 'triggeredJobTaskId': '4f7d77e5-f361-4061-9622-4791849c486a', 'triggeredTime': 1667559002920, 'status': 'FAILED', 'failureReason': ""NCSP11017: Operation failed on '4' devices."", 'triggeredJobId': '02d0605b-8b68-4a89-8f79-30646cbb6601'}
44029:  Library group ""schedule-job"" method ""check_status_of_externalScheduled_jobs_with_des"" returned in 0:05:02.884166
44030:  Library group ""connectivity_domain"" method ""update_device_info_in_cd"" returned in 0:06:31.689529
44031:  #################False
44032: 
44033: 
44034:  api_switch_call called:
44035:  \{'params': {'offset': 1, 'limit': 500}}
44036:  Resource path full url: [https://172.23.241.114/api/v2/data/customer-facing-service/DeviceInfo]
44037: 
44038: 
44039:  api_switch_call called:
44040:  \{'params': {'offset': 501, 'limit': 500}}
44041:  Resource path full url: [https://172.23.241.114/api/v2/data/customer-facing-service/DeviceInfo]
44042:  Test returned in 0:07:05.207021
44043:  Failed reason: Result: Failed to readd border device to fabric!!
44044:  The result of section test2_re_add_border_to_the_fabric is => FAILED",2022-11-07T07:45:05.042+0000,"It could be bug. Please debug and analyze the issue for bug. Close this ticket since submitter didnot have any new udpate for a long time. [~accountid:63f50bfde8216251ae4d59d8] , do we still have this issue? Assigning to Divakar, As he is the owner of MS TB2.
Divakar, Can you please update the comments whether you are still seeing this issue in MSTB2??? Issue is not seen with latest execution.

Passlog : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=35799565&size=1496562&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fauto_MS_job.2023Apr05_03:27:46.999567.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=35799565&size=1496562&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fauto_MS_job.2023Apr05_03:27:46.999567.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Ghost', 'Issue', 'MSTB2', 'Multisite']",Divakar Kumar Yadav,Closed,Avril Bower
SEEN-825,https://miggbo.atlassian.net/browse/SEEN-825,[Auton] - Test_TC9_onboard_device_clear_multicastrp / test1_onboard_device_clear_multicastrp,"*Reporter Analysis:* Clearing multicast rp failed with error ""Encountered unhandled HTTPError in Internal API Call""

*Description:   !image-2022-11-07-14-59-37-732.png!*

*Branch Name:  private/Shockwave-ms/sanity_api_auto*

*Script file/Usecase:* dnac_cleanup_script.py / [Test_TC9_onboard_device_clear_multicastrp|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=383424&size=289490&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F06%2F23%2F25%2Fenv_auto_job.2022Nov06_23:25:07.689950.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:*  Sanity

*Issue Seen first time or day0 issue:* First time

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=383579&size=289162&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F06%2F23%2F25%2Fenv_auto_job.2022Nov06_23:25:07.689950.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:* NA

*Testbed details:* NA

 ",2022-11-07T09:33:17.383+0000,"Expected failure, there is no multicast enabled in the fabric. so failure is expected. if yu are running it on purpose, then ignore the failure. ","['Auton', 'Cleanup', 'Issue', 'Sanity']",Pawan Singh,Resolved,Avril Bower
SEEN-826,https://miggbo.atlassian.net/browse/SEEN-826,Test_TC87_DNAC_Configuring_L3Handoff_and_bgp_on_border_fusion_router_anchoredvn/test1_configure_loopback_bgp_on_border_NY,"*Uber ISO Version tested:* Ghost Uber ISO - *2.1.610.70512, FIPS*

*Script Name:* solution_test_3sites_sjc_nyc_sf.py

*Testbed:* MSTB2

*Branch:*  private/Ghost-ms/api-auto

*Failed Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7145169&size=342524&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F03%2F23%2F11%2Fsr_mb2_three_sites_FIPS.2022Nov03_23:11:02.857381.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

Description : After execution of Anchor VN Testcases, we are seeing issue in configuring loopback bgp on NYC site. It was showing as ""Could not collect bgp l3handoff data"" and Failed Reason is showing in log like ""Result: BGP L3handoff config could not be reterievd!!""
 Could you please have a look and fix the issue.
 31401: This is a border device
 31402: This is a border device
 31403: False
 31404: Could not collect bgp l3handoff data
 31405: Library group ""fabric_wired"" method ""get_bgp_data_for_fusion_config"" returned in 0:00:01.364998
 31406: Test returned in 0:00:01.562770
 31407: Failed reason: Result: BGP L3handoff config could not be reterievd!!
 31408: The result of section test1_configure_loopback_bgp_on_border_NY is => FAILED

*Pass history: ??*
  ",2022-11-07T09:34:30.629+0000,"We are still observing this issue in configuring loopback bgp on NYC site. It was showing as ""Could not collect bgp l3handoff data"" and Failed Reason is showing in log like ""Result: BGP L3handoff config could not be reterievd!!""


*Uber ISO tested -* Ghost Pre RC #2.1.613.70079

*Script Name:* solution_test_3sites_sjc_nyc_sf.py

*Testbed:* MSTB2

*Branch:*  private/Ghost-ms/api-auto

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6980888&size=357167&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fauto_MS_job.2023Feb02_21:44:19.386130.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] For AnchorVN, you will need a connection between NY border and fusion for L3handoff to work in NY.","['Auton', 'Ghost', 'Groot', 'Issue', 'MSTB2', 'Multisite']",Tran Lam,Resolved,Avril Bower
SEEN-827,https://miggbo.atlassian.net/browse/SEEN-827,[Auton] - Test_TC23_delete_all_discoveries / test1_delete_all_discoveries,"*Reporter Analysis:* Discoveries got deleted but TC failed with error ""*Failed in delete_template_projects*""

*Description:   !image-2022-11-07-15-08-51-302.png!*

*Branch Name:*  private/Shockwave-ms/sanity_api_auto

*Script file/Usecase:* dnac_cleanup_script.py / [Test_TC23_delete_all_discoveries|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=156977&size=18406&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F07%2F00%2F25%2Fenv_auto_job.2022Nov07_00:25:51.473370.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:*  Sanity

*Issue Seen first time or day0 issue:* First Time

*Fail Log:* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=157123&size=18096&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F07%2F00%2F25%2Fenv_auto_job.2022Nov07_00:25:51.473370.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:* NA

*Testbed details:* NA",2022-11-07T09:42:18.255+0000,"It is already resolved in latest code. the message is also corrected. Try in latest and reopen if needed with detail of branch, etc.","['Auton', 'Cleanup', 'Issue', 'Sanity']",Pawan Singh,Resolved,Avril Bower
SEEN-828,https://miggbo.atlassian.net/browse/SEEN-828,[Auton] - Script Should Not Continue If Device Discovery Fails,"*Reporter Analysis:* Script Should Not Continue If Device Discovery Fails

*Description:*  We are seeing the script ""solution_test_sanityecamb.py"" is continuing even the device discovery fails. Ideally it should not continue like this.

*Branch Name:  private/Shockwave-ms/sanity_api_auto*

*Script file/Usecase:* solution_test_sanityecamb.py / [Test_TC11_DNAC_Verify_adding_range_discovery_ssh_global_credentials|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1981110&size=105641&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F05%2F18%2F56%2Fenv_auto_job.2022Nov05_18:56:55.230310.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:*  Sanity

*Issue Seen first time or day0 issue:* First time

*Fail Log:* https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov05_18:56:55.230310.zip&reqseq=&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=PYATS

*Pass Log:* NA

*Testbed details:* NA",2022-11-07T09:50:53.845+0000,"Fixed: on Guardian, ghost, groot. 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/56be925a72a7c28c9b5fed2f67820c27247bcb0c#testcases/forty_eight_hour/solution_test_sanityecamb.py]

  https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/56be925a72a7c28c9b5fed2f67820c27247bcb0c#testcases/forty_eight_hour/solution_test_sanityecamb.py","['Auton', 'Issue', 'Sanity']",Pawan Singh,Resolved,Avril Bower
SEEN-829,https://miggbo.atlassian.net/browse/SEEN-829,[Auton] - Test_TC0_dnac_initial_cleanup / cleanup_ise_profile,"*Reporter Analysis:* Getting below error while checking method ""check_CSRF_eligibility""
""404 Client Error: Not Found for url: [https://10.195.227.67/api/api/system/v1/maglev/release/current""|https://10.195.227.67/api/api/system/v1/maglev/release/current]

*Description:   !image-2022-11-07-17-59-52-009.png!*

*Branch Name:*  private/Shockwave-ms/sanity_api_auto

*Script file/Usecase:* solution_test_sanityecamb.py / [Test_TC0_dnac_initial_cleanup|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=139267&size=369289&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F07%2F02%2F14%2Fenv_auto_job.2022Nov07_02:14:47.419365.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS] & [Test_TC3_generate_dhcp_server_config_on_fusion|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=586136&size=960651&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F07%2F02%2F14%2Fenv_auto_job.2022Nov07_02:14:47.419365.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:*  Sanity

*Issue Seen first time or day0 issue:* First time

*Fail Log:* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=188594&size=14732&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F07%2F02%2F14%2Fenv_auto_job.2022Nov07_02:14:47.419365.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log: NA*

*Testbed details:* NA",2022-11-07T12:31:56.206+0000,"/api was repeated in the url, removed it in all branches
Shockwave - [0c352e84ae1|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0c352e84ae1613809dd07b59fd057e7d02145f85]

Guardian - [c9b00f10993|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c9b00f10993f5a1ba35ff30746c6b2fcecadaa74]

Groot - [d95dc5f27da|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d95dc5f27da932f693778d0ec92387f20e6ece12]

Ghost - [32919685a70|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/32919685a70b18c0ec8ba7c162334c7d72d6173b]","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity', 'shockwave']",Nethra Thorali Suthagar,Resolved,Avril Bower
SEEN-832,https://miggbo.atlassian.net/browse/SEEN-832,[AWS-MS]-Inter Device Link Flap Issue and Power Supply Issue,"Ghost Version-2.1.610.70492
 Script name:solution_test_3sites_sjc_nyc_sf.py

Branch:/private/Ghost-ms/sanity_api_auto

Device Image:17.09.02
 Testcases Impacted:[Test_TC175_generate_device_link_AP_flap_issues|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1719043&size=2618800&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fauto_MS_job.2022Nov06_20:55:01.103759.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Failed Log: Refer TC175

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fauto_MS_job.2022Nov06_20:55:01.103759.zip&atstype=ATS]

 

While integrating this feature,we observed few issues.
 1.For checking Auto resolve power failure ,it is picking SJC-FE-9300-1 device(having single power supply) instead of NYC-FE-9300 device(Having dual power supply),so we are seeing failure in  sub TC-[test2_check_power_supply_failure_issue_and_is_auto_resolved .

2.It is taking so much time for syncing devices and running almost 2.5 hours
  
 Please find the TB details.

*+Jumper PCs:+*

Windows (SR-Cloud-Jumper-Win) : 172.23.241.171 (assurance/C1sco123!)

 

DNAC details:172.35.16.155(admin/maglev1@3)

 

*+Testbed wiki+*: [https://wiki.cisco.com/display/EDPEIXOT/DMZ+AWS+S.R.+MS+Testbed]

 

Config Files:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sr_cl_ms?at=private/Ghost-ms/sanity_api_auto]

 ",2022-11-07T16:04:39.914+0000,"Hi Quang Vinh Nguyen,
Could you please let me know ETA to resolve this issue?

Thanks,
Neelima  Hi Quang Vinh Nguyen,
Could you please help resolve this issue?



Thanks,
Neelima [~accountid:63f50bcf4e86f362d39acde5] can we have some update on the progress here.. Hi I have Shared the testbed with Quangand  he collected the logs and return back the setup [~accountid:63f50bcf4e86f362d39acde5] when we will get a fix for this issue [~accountid:62d2fe9f8afb5805e5d5af49]  / [~accountid:63f50bcf4e86f362d39acde5] , can we have some update here? [~accountid:63f50bcf4e86f362d39acde5] Can you please provide update on this ? [~accountid:63f50bcf4e86f362d39acde5] Can we have some update on this? [~accountid:63f50bcf4e86f362d39acde5] Could you please update on this? PR-link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7586/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7586/overview]

Passlog: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-10/sanity_TB23_standalone.2023Oct24_02:56:14.559317.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-10/sanity_TB23_standalone.2023Oct24_02:56:14.559317.zip&atstype=ATS] [~accountid:641058d57222b08f3e7064d0] / [~accountid:63f50bddc1685a24e1314c87] , pls. help with the validation of the provided fix locally by picking *private/quangvin-hulkpatch/SEEN-832* branch.","['AWS_MSTB', 'Auton', 'Ghost', 'Issue', 'Multisite']",QuangVinh Nguyen,Resolved,Avril Bower
SEEN-833,https://miggbo.atlassian.net/browse/SEEN-833,[Auton] - Script Should Not Continue If Extended Node or PEN Node Fails to Onboard,"*Reporter Analysis:* Script Should Not Continue If Extended Node or PEN Node Fails to Onboard

*Description:*  Script Should Not Continue If Extended Node or PEN Node Fails to Onboard

*Branch Name:  private/Shockwave-ms/sanity_api_auto*

*Script file/Usecase:* solution_test_sanityecamb.py / [Test_TC43_DNAC_EXT_NODE_interface_config_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=18059917&size=1243560&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F07%2F07%2F49%2Fenv_auto_job.2022Nov07_07:49:54.614898.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:*  Sanity

*Issue Seen first time or day0 issue:* Day 0 Issue

*Fail Log:* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=18059917&size=1243560&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F07%2F07%2F49%2Fenv_auto_job.2022Nov07_07:49:54.614898.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:* NA

*Testbed details:* NA",2022-11-08T10:30:39.906+0000,"The extended node onboarding failure is not a blocker, we do not stop at it since the whole night execution may go waisted due to one issue. If you want to still break it, you can do it from Jenkin job itself. In Jenkins job parameter, set the parameter BREAK_ON_FAIL=true

It will stop execution on any test failure. ","['Auton', 'Issue', 'Sanity']",Pawan Singh,Resolved,Avril Bower
SEEN-834,https://miggbo.atlassian.net/browse/SEEN-834,[Auton]:Guardian -Test_TC107_Compliance_verification / test9_verify_POE_compliance," 

*Reporter Analysis:* 

250832: ZeroDivisionError: division by zero

*Description:  The error from log or more info* 

*Branch Name:  private/Guardian-ms/sanity_api_auto*

*Script file/Usecase:* test-script:testcases/forty_eight_hour/solution_test_sanityecamb_lan.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=79966955&size=99573&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov04_21:37:59.981081.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:*

*Testbed details:* NA",2022-11-08T12:55:14.940+0000,"[~620b8357878c2f00729881c8], for this issue the problem was actually with the power-supply which was not enough for the device to show data that would get validated under POE Compliance test. That's the reason it was showing 0.0 and caused `ZeroDivisionError` exception.
{code:java}
SN-FDO2027U0J4#show power inline 
Available:0.0(w) Used:0.0(w) Remaining:0.0(w)
Interface Admin Oper Power Device Class Max
 (Watts) 
--------- ------ ---------- ------- ------------------- ----- ----
{code}
 

After fixing the power-supply source, the numbers are showing more than 0.0.
{code:java}
SN-FDO2027U0J4#show power inline 
Available:125.0(w) Used:0.0(w) Remaining:125.0(w)
Interface Admin Oper Power Device Class Max
 (Watts) 
--------- ------ ---------- ------- ------------------- ----- ----
Gi1/9 auto off 0.0 n/a n/a 30.0 
Gi1/10 auto off 0.0 n/a n/a 30.0 
Gi1/11 auto off 0.0 n/a n/a 30.0 
Gi1/12 auto off 0.0 n/a n/a 30.0 
Gi1/13 auto off 0.0 n/a n/a 30.0 
Gi1/14 auto off 0.0 n/a n/a 30.0 
Gi1/15 auto off 0.0 n/a n/a 30.0 
Gi1/16 auto off 0.0 n/a n/a 30.0
{code}
  

Still, I have made some change on the library to make sure next time tester know about it.
PR has been raised for Ghost branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4358/overview] Cherry-picked to Groot and raise below PR for Guardian branch:

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4383/overview","['AWS_Sanity', 'Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity']",Amardeep Kumar,Closed,Avril Bower
SEEN-835,https://miggbo.atlassian.net/browse/SEEN-835,Groot-  Test_TC55_WIRELESS_VERIFY_ACCESS_TUNNEL_ON_EDGE_ROUTER/test1_verify_access_tunnel_on_edges,"Groot - 2.1.563.70150.iso

*Script name:* solution_test_3sites_sjc_nyc_sf.py

Groot- Test_TC55_WIRELESS_VERIFY_ACCESS_TUNNEL_ON_EDGE_ROUTER/test1_verify_access_tunnel_on_edges 

*Fail log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=707820&size=732842&archive=%2Fauto%2FestgSanity-bgl%2Fpyats-sol%2Fusers%2Fkaldodda%2Farchive%2F22-11%2Fphoenix-tb1_multi_sites.2022Nov09_11:52:39.915727.zip&ats=%2Fauto%2FestgSanity-bgl%2Fpyats-sol&submitter=kaldodda&from=trade&view=all&atstype=pyATS]

*Snip from log:*
8424:  Output show platform software fed active ifm interfaces access-tunnel
8425:  Interface IF_ID State
8426:  ----------------------------------------------------------------------
8427:  Ac0 0x00000097 READY
8428:  Ac1 0x00000098 READY
8429:  Ac2 0x00000099 READY
8430:  Ac3 0x0000009a READY
8431:  Ac4 0x0000009b READY
8432:  Ac5 0x0000009c READY
8433:  Ac6 0x0000009d READY
8434:  Ac7 0x0000009e READY
8435:  Ac8 0x0000009f READY
8436:  Ac9 0x000000a0 READY
8437: 
8438:  NY-FE-9400#
8439:  Access tunnels foud:10, expected count:7
8440:  AccessTunnels and fed entries don't match
8441:  AccessTunnel count:10 r0 count:0 fed count:10
8442:  NY-FE-9400 Access tunnels not as expected, Reached Max retry..Failed. show access-tunnel summary
8443: 
8444:  Access Tunnels General Statistics:
8445:  Number of AccessTunnel Data Tunnels = 10
8446: 
8447: 
8448:  Name RLOC IP(Source) AP IP(Destination) VRF ID Source Port Destination Port
8449:  ------ --------------- ------------------ ------ ----------- ----------------
8450:  Ac0 204.1.2.5 204.1.212.47 0 N/A 4789
8451:  Ac1 204.1.2.5 204.1.212.48 0 N/A 4789
8452:  Ac2 204.1.2.5 204.1.212.49 0 N/A 4789
8453:  Ac3 204.1.2.5 204.1.212.40 0 N/A 4789
8454:  Ac4 204.1.2.5 204.1.212.41 0 N/A 4789
8455:  Ac5 204.1.2.5 204.1.212.43 0 N/A 4789
8456:  Ac6 204.1.2.5 204.1.212.42 0 N/A 4789
8457:  Ac7 204.1.2.5 204.1.212.44 0 N/A 4789
8458:  Ac8 204.1.2.5 204.1.212.45 0 N/A 4789
8459:  Ac9 204.1.2.5 204.1.212.46 0 N/A 4789
8460: 
8461: 
8462:  Name IfId Uptime
8463:  ------ ---------- --------------------
8464:  Ac0 0x00000097 0 days, 09:33:07
8465:  Ac1 0x00000098 0 days, 09:33:04
8466:  Ac2 0x00000099 0 days, 09:33:03
8467:  Ac3 0x0000009A 0 days, 09:32:45
8468:  Ac4 0x0000009B 0 days, 09:32:44
8469:  Ac5 0x0000009C 0 days, 09:32:41
8470:  Ac6 0x0000009D 0 days, 09:32:41
8471:  Ac7 0x0000009E 0 days, 09:32:40
8472:  Ac8 0x0000009F 0 days, 09:32:39
8473:  Ac9 0x000000A0 0 days, 09:32:38
8474: 
8475:  NY-FE-9400#",2022-11-09T10:49:51.215+0000,Duplicate [https://miggbo.atlassian.net/browse/SEEN-364|https://miggbo.atlassian.net/browse/SEEN-364|smart-link] ,"['Auton', 'Issue']",QuangVinh Nguyen,Closed,Avril Bower
SEEN-836,https://miggbo.atlassian.net/browse/SEEN-836,[Auton] Test_TC102_DNAC_External_Authentication_Radius / test3_enable_external_auth_radius,"Test_TC102_DNAC_External_Authentication_Radius / test3_enable_external_auth_radius gets into recursion loop in case of Exception happening while trying to disable external authentication.

A breakpoint/check to avoid the recursive loop after a known number of retries in below method would be required.

disable_external_auth() from services/dnaserv/lib/api_groups/rbac/group.py

Execution log: look for execution flow w.r.t. test3_enable_external_auth_radius
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/VM-sanity-common-Multi-job/83/consoleFull]



 ",2022-11-09T21:31:43.121+0000,"Required PR Raised and merged to Ghost branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4135/overview]

 

Same has been cherry-picked to Guardian and Groot branches.

Marking this Ticket as ""Closed"".","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Optimized', 'Sanity']",Raji Mukkamala,Closed,Avril Bower
SEEN-837,https://miggbo.atlassian.net/browse/SEEN-837,[Auton]  Test_TC112_renew_ip_address_client / test1_renew_ip_address_client RPC Response wait time should be restricted,"*Reporter Analysis:* Test_TC112_renew_ip_address_client / test1_renew_ip_address_client had been waiting for response forever from the Client. Based on the analysis, RPC Response wait time should be restricted to max 60 seconds to avoid forever waiting.

Only after killing the RPC process by logging into the Client, helped to continue the execution.

*Branch Name:*  applicable to all branches

*Script file/Usecase:* solution_test_sanityecamb.py / Test_TC112_renew_ip_address_client / test1_renew_ip_address_client 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: N/A*

*Fail Log:* took 3 hours. 18 minutes and still waiting for response

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov09_11:33:25.956234.zip&atstype=ATS]

*Pass Log:*

*Testbed details:* NA",2022-11-09T23:38:44.241+0000,"The RPC related issue is also affecting other use-cases - Overlapping and blocking us to conclude on the Integration side.

[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Guardian/job/Guardian-sanity-common-Multi-job/698/]

Took 4 hours. [~62d2fed26eba7198372366ca], do we have any update on this ticket. As mentioned, this is kind of blocking us with the integration activity as well. For record: [Test_TC159_overlapping_pools_negative_operations |https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1463676&size=5626671&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fauto_MS_job.2022Dec16_15:24:36.365621.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]had similar issue.

  [~62d2fed26eba7198372366ca], This issue is being observed across multiple test-beds.

Please prioritize this one. [~62d2fed26eba7198372366ca], can we have some update on this issue? [~62ab7a399cd13c0068b18fe0], this was not reproducible, and is the issue reproducible? Fix add to handle timeout. it will timeout in 3 minutes. 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/fb3d6f8fb9821f39f11deaea8a9b9c06a924aeec|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/fb3d6f8fb9821f39f11deaea8a9b9c06a924aeec] added on Halleck, which will merge with Hulk. If you need on other branches, cherry-pick on that branch.","['Auton', 'Guardian', 'Issue', 'Sanity']",Pawan Singh,Closed,Avril Bower
SEEN-840,https://miggbo.atlassian.net/browse/SEEN-840,[Auton] : Halleck 2.1.660.70113 Test_TC2_DNAC_provision_all_aps  /   test2_enable_kairos_cs_details  (Failed),"*Reporter Analysis:* 

*Description:* 
 5713: Missing AI_ANALYTICS_CONFIG in cluster json hence couldnot setup AI analytics.
 5714: Library group ""kairos"" method ""setup_AI_analytics"" returned in 0:00:00.000818
 5715: Test returned in 0:00:00.322349
 5716: Failed reason: Result: Failed On Enabling Cloud Connectivity
 5717: The result of section test2_enable_kairos_cs_details is => FAILED
 *Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* [Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_optimized_auto_job.2022Nov08_09:56:31.160029.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   [Test_TC2_DNAC_provision_all_aps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1239797&size=702377&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_optimized_auto_job.2022Nov08_09:56:31.160029.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test2_enable_kairos_cs_details  (Failed)

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1677186&size=8093&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_optimized_auto_job.2022Nov08_09:56:31.160029.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:*

*Testbed details: NA*",2022-11-10T14:14:09.302+0000,"Not script issue.
The log already provided the reason:

'Missing AI_ANALYTICS_CONFIG in cluster json hence couldnot setup AI analytics.'

Please do integration for the AI Cloud. Please check with Raju for steps. [~62d2fe9f8afb5805e5d5af49] due to cost-cutting on the RRM cloud data, they are limiting us to enable the Kairos feature in multiple testbeds, So we are looking if we can skip the test if AI_ANALYTICS_CONFIG is not defined in the input files.

 

Plan is to keep kairos in IBSTE,MS and in sanity two testbed, and two upgrade testbeds for now.

 

Thanks,

Raju ","['Auton', 'Issue', 'Sanity']",Divya Prabhalika,Resolved,Avril Bower
SEEN-841,https://miggbo.atlassian.net/browse/SEEN-841,[Auton] Test_TC55_TSIM_verify_tsim_client_start / test1_verify_tsim_client_start handle exception for prompt,"Test_TC55_TSIM_verify_tsim_client_start / test1_verify_tsim_client_start handle exception for prompt.

Failed log:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=130355274&size=18511&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov07_16:52:14.688764.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Snippet from fail log:
h6. 432666: +++ TB2-DM-TSIM with via 'a': executing command 'sim client start 1 100 10 1' +++

sim client start 1 100 10 1
h6. (Cisco Capwap Simulator) >Client 1 to 100 started
h6. {color:#de350b}432672: Traceback (most recent call last):
 432673: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 725, in call_service
 432674: dialog_match = dialog.process(
 432675: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialogs.py"", line 476, in process
 432676: return dp.process()
 432677: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialog_processor.py"", line 326, in process
 432678: self.timeout_handler()
 432679: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialog_processor.py"", line 273, in timeout_handler
 432680: raise TimeoutError('Prompt timeout occured, please check the hostname\n \
 432681: unicon.core.errors.TimeoutError: Prompt timeout occured, please check the hostname
 432682: hostname: TB2-DM-TSIM
 432683: timeout value: 10
 432684: last_command: 'sim client start 1 100 10 1\r'
 432685: pattern: ['^.*\\[confirm\\]*^^*s*^^[y/n^^].*$', '*^*.*\\[confirm((y/n))?].*$', '^.^*\\[yes[/,][Nn][Oo]]s?:?s*$', '.*?%\\w+(**-S+)?-**d+---**-w+.*$', '(.*?)Press any key to continue', '(.*?)Are you sure.
*([yY]/[nN]**)**s*$', '(.?)Press Enter to continue.*', '^.^*-s?[Mm]ore--s?--.*$', '(.*?)((TB2-DM-TSIM|Cisco Capwap Simulator))s*>s*$', '^.*?User:*^^*s*$', '^(.*?)**((TB2-DM-TSIM|Cisco Capwap Simulator)**)**s*show>**s*$', '^(.*?)*^^*((TB2-DM-TSIM|Cisco Capwap Simulator)*^^*)*^^*s*config>**^*^*s^*$', '(.*?)**((TB2-DM-TSIM|Cisco Capwap Simulator)**)**s*debug>**s*$', '^(.*?)*^^*((TB2-DM-TSIM|Cisco Capwap Simulator)*^^*)*^^*s*test>**^*^*s^*$', '(.*?)**((TB2-DM-TSIM|Cisco Capwap Simulator)**)**s*transfer>**s*$', '^(.*?)*^^*((TB2-DM-TSIM|Cisco Capwap Simulator)*^^*)*^^*s*license>**^*^*s^*$', '(.*?)**((TB2-DM-TSIM|Cisco Capwap Simulator)**)**s*reset>**s*$', '^(.*?)**((TB2-DM-TSIM|Cisco Capwap Simulator)**)**s*save>**s*$', 'bash.*#**s*$']
 432686: buffer:'AP 1 to 40 started on switch 1 with module 0\r\nsim client start 1 100 10 1\r\r\n\r\r\n\r\r\n(Cisco Capwap Simulator) >Client 1 to 100 started\r\n'
 432687: {color}",2022-11-10T22:55:44.953+0000,"Pushed required change to address the problem: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/63e29825504103807b856c15c7e52f4da36fe7b4 changes pushed to ""esxivm_branch"" ","['AWS_Sanity', 'Auton', 'ESXi', 'Ghost', 'Groot', 'Issue', 'Optimized', 'Sanity']",Amardeep Kumar,Closed,Avril Bower
SEEN-842,https://miggbo.atlassian.net/browse/SEEN-842,[Auton]  WLC controller prompt need to gracefully handle  Test_TC136_enable_ICMP_ping_check_AP_reachability," 

*Reporter Analysis:* The wlc controller prompt is not handled gracefully, due to that test is failing

*Description:*  

 

*(Cisco Controller) > (Cisco Controller) >*
 60712: Traceback (most recent call last):
 60713: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 733, in call_service
 60714: self.result = self.get_service_result()
 60715: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 240, in get_service_result
 60716: raise SubCommandFailure(
 60717: unicon.core.errors.SubCommandFailure: ('sub_command failure, patterns matched in the output:', ['^[Ww]arning'], 'service result', ""config interface ap-manager management disable\r\r\n\r\r\n\r\r\nWarning! You have no AP manager on this port.\r\r\nThe controller behavior will be unpredictable.\r\r\nDisabling the AP-manager interface will reboot\r\r\nAP's connected on this interface.\r\r\nAre you sure you want to continue? (y/n) "")
 60718: 
 60719: The above exception was the direct cause of the following exception:
 60720: 
 60721: Traceback (most recent call last):
 60722: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/dnaserv/lib/api_groups/Compilance/group.py"", line 1741, in enable_ICMP_verify_AP_reachability
 60723: out=self.services.dnaconfig.testbed.devices[dev].execute(cmd,reply=prompt,prompt_recovery=True)
 60724: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 264, in __call__
 60725: self.call_service(*args, **kwargs)
 60726: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 738, in call_service
 60727: raise SubCommandFailure(""Command execution failed"", err) from err
 60728: unicon.core.errors.SubCommandFailure: ('Command execution failed', SubCommandFailure('sub_command failure, patterns matched in the output:', ['^[Ww]arning'], 'service result', ""config interface ap-manager management disable\r\r\n\r\r\n\r\r\nWarning! You have no AP manager on this port.\r\r\nThe controller behavior will be unpredictable.\r\r\nDisabling the AP-manager interface will reboot\r\r\nAP's connected on this interface.\r\r\nAre you sure you want to continue? (y/n) ""))
 *Branch Name:  private/Guardian-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*  https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19714744&size=112592&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov10_09:13:57.586307.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:*

*Testbed details:* NA",2022-11-11T01:09:02.446+0000,"PRs:

* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5051/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5051/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5040/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5040/overview]
* Groot: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5057/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5057/overview]
* Guardian: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5065/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5065/overview]","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity']",ThangQuoc Tran,Resolved,Avril Bower
SEEN-845,https://miggbo.atlassian.net/browse/SEEN-845,Groot - Test_TC4_wireless_policy_PSK/  test6_verify_policy_config,"Groot - 2.1.563.70150.iso

*Script name:* solution_test_apppolicy.py

Groot - Test_TC4_wireless_policy_PSK/ test6_verify_policy_config

*Fail log:*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-3&begin=2483466&size=60577&archive=%2Fauto%2FestgSanity-bgl%2Fpyats-sol%2Fusers%2Fkaldodda%2Farchive%2F22-11%2Fphoenix-tb1_multi_sites.2022Nov10_23:04:14.155003.zip&ats=%2Fauto%2FestgSanity-bgl%2Fpyats-sol&submitter=kaldodda&from=trade&view=all&atstype=pyATS]

*Snip from log:*
 8904: FW-5520-1
 8905: 
 8906: 
 8907: api_switch_call called:
 8908: {}
 8909: Resource path full url: [https://10.106.17.105/api/v1/file/11d0d88f-6424-4706-a464-96134ccd3869]
 8910: [{'deviceUuid': '52385ade-1822-4d16-bdd5-29830de7471d', 'commandResponses': {'SUCCESS':

{'show flexconnect avc profile summary': 'show flexconnect avc profile summary\n\n Profile-Name Number of Rules status\n ============ =============== ========\n\n(FW-5520-1) >'}

, 'FAILURE': {}, 'BLACKLISTED': {}}}]
 8911: Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:02.196886
 8912: Traceback (most recent call last):
 8913: File ""/ws/kaldodda-bgl/sol-reg/groot/dnac-auto/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 8914: result = testfunc(func_self, **kwargs)
 8915: File ""/ws/kaldodda-bgl/sol-reg/groot/dnac-auto/testcases/mega_topo/solution_test_apppolicy.py"", line 325, in test6_verify_policy_config
 8916: if (dnac_handle.verify_wireless_device_config(res_val, types=""PSK"")):
 8917: File ""/ws/kaldodda-bgl/sol-reg/groot/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 8918: result = method(*args, **kwargs)
 8919: File ""/ws/kaldodda-bgl/sol-reg/groot/dnac-auto/services/dnaserv/lib/api_groups/app_policy/group.py"", line 694, in verify_wireless_device_config
 8920: if value.group(1) == inp and value.group(2) == ""Applied"":
 8921: AttributeError: 'NoneType' object has no attribute 'group'
 8922: Test returned in 0:00:11.083620
 8923: Errored reason: 'NoneType' object has no attribute 'group'
 8924: 
 8925: Exception:
 8926: Traceback (most recent call last):
 8927: File ""/ws/kaldodda-bgl/sol-reg/groot/dnac-auto/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 8928: result = testfunc(func_self, **kwargs)
 8929: File ""/ws/kaldodda-bgl/sol-reg/groot/dnac-auto/testcases/mega_topo/solution_test_apppolicy.py"", line 325, in test6_verify_policy_config
 8930: if (dnac_handle.verify_wireless_device_config(res_val, types=""PSK"")):
 8931: File ""/ws/kaldodda-bgl/sol-reg/groot/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 8932: result = method(*args, **kwargs)
 8933: File ""/ws/kaldodda-bgl/sol-reg/groot/dnac-auto/services/dnaserv/lib/api_groups/app_policy/group.py"", line 694, in verify_wireless_device_config
 8934: if value.group(1) == inp and value.group(2) == ""Applied"":
 8935: AttributeError: 'NoneType' object has no attribute 'group'",2022-11-14T10:43:57.055+0000,"Please assign to respective engineer if its script issue, else provide your comments. [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5089/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5089/overview]","['Auton', 'Issue']",Moe Saeed,Resolved,Avril Bower
SEEN-846,https://miggbo.atlassian.net/browse/SEEN-846,[16ssid limit] Removing SSID Scheduler from Script After Verification,"*Issue*: SSID overloading  that results to testcase failing to some use cases that need to use the same wireless profile.

*Fix*: Need to remove as many SSIDs as possible including the SSID Scheduler after done using them",2022-11-15T22:49:05.839+0000,"PR:
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4611/overview","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity']",Moe Saeed,Resolved,Avril Bower
SEEN-847,https://miggbo.atlassian.net/browse/SEEN-847,[Auton]MSTB2 : Account CCO Changes,"*Uber ISO Version tested :* Groot Patch1 RC2 Uber ISO - *2.1.563.70154, FIPS*

*Script Name :* solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB2 

*Branch :*  private/Groot-ms/api-auto

*Description :* In MSTB2, we are trying to integrate SWIM CCO Feature. But script is getting failed while getting the image from CCO because of not having enough authorization to download from CCO after adding Cisco ID also.

*Failed Log :* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1508663&size=466841&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fauto_MS_job.2022Oct25_03:22:19.536679.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Error  :* 

11569:  \{'response': {'username': 'spolamre', 'encrypted_password': '$5$WG3U49mZw5/beUmwiQnTZfcvQfv5BXp6ZuMfrlhtKPX44vnMnS6BOpY8c5yAZZrz5cDiHgixFiHSLYccrstEw7gJRqFTPNd/', 'eula': False}, 'version': '1.5.1'}
11570:  You do not have enough authorization to download from CCO, Use your CCO account: ex. person@cisco.com!
11571:  Library group ""swim"" method ""is_userId_in_CCO_valid"" returned in 0:00:00.039026
11572:  Test returned in 0:00:06.005486
11573:  Failed reason:
11574:  ********************************************************************************
11575:  USE your CCO username/ID in DNAC to processed with test case for authorization!
11576:  ********************************************************************************
 
Regards,
Sathwick",2022-11-16T15:58:54.196+0000,The fix merged!!,"['Auton', 'Groot', 'Issue', 'MS', 'MSTB2', 'Multisite']",Moe Saeed,Resolved,Avril Bower
SEEN-848,https://miggbo.atlassian.net/browse/SEEN-848,[Auton]-Ghost- Test_TC160_Brownfield_Workflow_Aireos/test3_add_BF_ssid_and_provision," 

*Reporter Analysis:* Testcase failed for local variable 'whole_ssid' referenced before assignment

*Description:*  

ssid_entry = {
 ""key"": ""wireless.ssid"",
 *""value"": whole_ssid,*
 ""attribs"": [
 {
 ""key"": ""wireless.fabric"",
 ""value"": fabric
 },

*Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* solution_test_sanityecamb_lan.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*  
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1151614&size=30192&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov17_07:50:50.467936.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:*

*Testbed details:* NA",2022-11-17T18:13:54.065+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c1435b5937cfbf2efca74c3921b29cf915a7234f https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c1435b5937cfbf2efca74c3921b29cf915a7234f TC Failed for ""BF ssid added and associated with profile failure""
 Failed log:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1113937&size=136617&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov17_22:43:49.223965.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] From script side, no issue besides that the ssid didn't appear on the device after provisioning. Can you either:
 # Manually try adding an ssid and provisioning wlc to see if it gets added.
 # Provide setup for testing and let me know?

Note: I see in the logs:

Updating HBL license statistics file  Done.

Right before checking the wlans, which usually means a provision just happened, so I think the device was successfully provisioned. And in the payload for the successful network profile update I see the wlan added, so need to do some more testing. Could be a product bug Hi [~63f50bcece6f37e5ed93c87e],

Guardian P4RC2 -2.1.518.72310
 Tc Failed on WLC (9800)  controller & Airos controller  ,  could you please check ..

*+Reporter Analysis:+*
 6501: TypeError: associate_ssid_with_nw_profile() got an unexpected keyword argument 'ssid_configs'

+*Branch Name:*+  private/Guardian-ms/sanity_api_auto

+*Script file/Usecase:*+ solution_test_sanityecamb_lan.py

+*Failed Log: WLC (9800)  controller Failed Log*+
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=999547&size=823421&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb19_21:08:56.882122.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
 +*Failed Log*+*:*+*AireOS WLC**controller Failed Log:*
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4339331&size=14961&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb11_08:53:36.627752.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS+]

 

+*Testbed details:*+ [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4892/overview]

 

For above comment its fixed. Please raise a product bug next time 1. ssid is added successfully, 2. provisioning succeeds 3. wlans still dont show up in device.


Issue with ""ssid_configs"" params brought up in comments of this auton are a different issue than the original, and PR has been raised.","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity']",Andrew Chen,Resolved,Avril Bower
SEEN-849,https://miggbo.atlassian.net/browse/SEEN-849,[Auton] Test_TC144_ISE_PAN_failover / test2_syncup_secondary_ISE_node require update in terms of XPATH for ISE 3.1,"Test_TC144_ISE_PAN_failover / test2_syncup_secondary_ISE_node require update in terms of XPATH for ISE 3.1 P4 Instances w.r.t. below listed elements:
# toast_container
# last_toast_title_element
# last_toast_message_element
# table_loading_indicator

Execution log:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=852170&size=126040&archive=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats%2Fusers%2Famardkum%2Farchive%2F22-11%2Fenv_auto_job.2022Nov17_19:57:11.164564.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats&submitter=amardkum&from=trade&view=all&atstype=pyATS",2022-11-18T06:45:03.578+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/224ea23d3b1413d79a4e48af8d266e4ce59ec719 [~63f50bcece6f37e5ed93c87e], I'll provide the Sanity TB access after setting 3 ISE in the testbed. Issue seems to be that failover isnt working. Need 3 node ise testbed to test, could be a product bug, or misconfiguration on ISE pan failover settings. [~accountid:63f50bcece6f37e5ed93c87e] , did you get chance to check on this using your testbed with 3 ISE set-up?

Can we have some ETA for this?","['Auton', 'Halleck', 'Issue', 'Sanity']",Andrew Chen,In Progress,Avril Bower
SEEN-850,https://miggbo.atlassian.net/browse/SEEN-850,[Auton] : CSCwd41078 Halleck Express Sanity Test_TC18_unprovision_devices Failed,"*Reporter Analysis:* 

*Description:* 
Test_TC18_unprovision_devices has Failed

 *IMAGE BUILD VERSION* : Halleck 2.1.660.70096

Error: 13054: Removing Device:8656bc97-5c6c-4bc5-8229-b6b255e3786c Unable to push configuration to device 204.192.4.2. Device response - application error: data-missing


 *Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* Task-1 dnac_cleanup_script Test_TC18_unprovision_devices 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*  [TRADe v2 | Logs: env_auto_job (cisco.com)|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4528637&size=93961&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-10%2Fenv_auto_job.2022Oct25_18:35:42.420534.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*CDET :* [CSCwd41078|https://cdetsng.cisco.com/webui/#view=CSCwd41078]

*Eng Notes :* DE Analysis added as part of the CDET Eng Notes : issue is with the script

 DeviceUUID - e4baeaa4-23f5-4c97-b08c-db9aabca1b4c DeviceName - TB13-eWLC

Root cause: --------------- - DELETE request is wrongly sent for id ""64a8e7c4-01f2-4702-bad2-9489c9ff846b"" which is a cfsId (attaching the RfsWorkflowInfo from NP log below) instead of using a deviceUUID 'e4baeaa4-23f5-4c97-b08c-db9aabca1b4c' and even after GET call for id ""/apic-em-inventory-manager-service/network-device?id=64a8e7c4-01f2-4702-bad2-9489c9ff846b"" has returned error saying ""Unable to find MEI with instanceUuid 64a8e7c4-01f2-4702-bad2-9489c9ff846b"" (attached the request details from inventory service logs below).

- Hence ResourceNotFoundException is thrown during deletion. Test_TC18_unprovision_devices Logs - to get taskId '58fd214d-e085-44c7-87ab-7e6a7cda7f1c':

 

Please refer to the above CDET for more detailed analysis on the issue

*Pass Log:*

 

*Testbed details: NA*

 ",2022-11-18T09:32:32.598+0000,"Hi [~62d2fe9f8afb5805e5d5af49],

Could you please take a look at this Auton : https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-850

We have been seeing this issue frequently in the past builds in Sanity for the CLEAN_UP Script.

Ref log: [TRADe v2 | Logs: env_auto_job (cisco.com)|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4942546&size=100569&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fenv_auto_job.2022Dec20_11:54:29.142642.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Ref CDET: [CSCwd41078 - Halleck Express Sanity Test_TC18_unprovision_devices Failed (cisco.com)|https://cdetsng.cisco.com/summary/#/defect/CSCwd41078] [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5067/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5067/overview]","['Auton', 'Issue', 'Sanity']",Moe Saeed,Resolved,Avril Bower
SEEN-852,https://miggbo.atlassian.net/browse/SEEN-852,[Auton]  Test_TC3_generate_dhcp_server_config_on_fusion/test4_ise_cleanup_guest," 

*Reporter Analysis:* Please add the below-mentioned sub testcase in optimized sanity code.

*Description:*  

Test_TC3_generate_dhcp_server_config_on_fusion
   -test4_ise_cleanup_guest

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* [ISECleanupGoldenConfig|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_cleanup_profile_nw_device.py-21-ISECleanupGoldenConfig&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_optimized_auto_job.2022Nov07_04:26:52.181499.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

*Pass Log:*

*Testbed details:* NA",2022-11-20T12:17:58.590+0000,,"['Auton', 'Ghost', 'Groot', 'Issue', 'Sanity', 'optimized']",Raji Mukkamala,Resolved,Avril Bower
SEEN-855,https://miggbo.atlassian.net/browse/SEEN-855,[Auton] Test_TC53_DNAC_provision_all_aps / test3_verfies_syslog_events_generated_wlc handle KeyError,"Handle KeyError with Test_TC53_DNAC_provision_all_aps / test3_verfies_syslog_events_generated_wlc

Fail log:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9034382&size=16611&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov14_17:18:27.961250.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
{code:java}
33531:  Resource path full url: https://10.22.45.61/api/assurance/v1/events/deviceEventsView
33534:  Following Events {'version': '1.0', 'totalCount': None} occured with device IP 204.1.2.1
33535:  Traceback (most recent call last):
33536:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
33537:      result = testfunc(func_self, **kwargs)
33538:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 4419, in test3_verfies_syslog_events_generated_wlc
33539:      if dnac_handle.verify_syslog_events_wlcs():
33540:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 7714, in verify_syslog_events_wlcs
33541:      for event in  wlc_events['response']:
33542:  KeyError: 'response'33543:  Test returned in 0:00:00.59960133544:  Errored reason: response
{code}",2022-11-22T00:19:19.240+0000,"Required changes have been pushed to below Branches:
 # esxivm_branch
 # ghost
 # groot

Execution log after change:
 [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=507285&size=13876&archive=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats%2Fusers%2Famardkum%2Farchive%2F22-11%2Fenv_auto_job.2022Nov21_16:04:45.464484.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats&submitter=amardkum&from=trade&view=all&atstype=pyATS]
{code:java}
2664:  Resource path full url: https://10.22.45.61/api/assurance/v1/events/deviceEventsView
2667:  Following Events {'version': '1.0', 'totalCount': 0} occured with device IP 204.1.2.1
2668:  Device Event Viewer does not have 'response' dictionary in the response received.
2669:  Test returned in 0:00:01.395794
2670:  Failed reason: Verification of WLCs Events Failed
{code} Marking this ticket as ""Done"".","['Auton', 'ESXi', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-856,https://miggbo.atlassian.net/browse/SEEN-856,[Auton]  Test_TC58_verify_assurance_health_nw_health_border_edge_wlc_ext_node_and_ipv6_migration/test8_verify_all_edge_devices_poe_details handle KeyError,"Handle KeyError with  [Test_TC58_verify_assurance_health_nw_health_border_edge_wlc_ext_node_and_ipv6_migration|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=23913538&size=169401&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov19_01:37:46.044475.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test8_verify_all_edge_devices_poe_details

Fail log:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24068139&size=11322&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov19_01:37:46.044475.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS
{code:java}
105178:  Resource path full url: https://10.22.45.61/api/assurance/v1/network-device/55c00ffa-ca66-4be8-bd4e-4aae2ecf25d6/info-top
105181:  {'healthScore': None, 'deviceModel': None, 'ipAddress': None, 'location': None, 'softwareVersion': None, 'role': None, 'stackStatus': None, 'haStatus': None, 'upTime': None}
105182:  Traceback (most recent call last):105183:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
105184:      result = testfunc(func_self, **kwargs)
105185:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 4596, in test8_verify_all_edge_devices_poe_details
105186:      if not dnac_handle.verify_all_edge_devices_poe_details():
105187:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/lib/api_groups/poe_assurance/group.py"", line 64, in verify_all_edge_devices_poe_details
105188:      if self.read_device_info_top_device_ready_for_poe(dev_id):105189:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/lib/api_groups/poe_assurance/group.py"", line 26, in read_device_info_top_device_ready_for_poe
105190:      if response[""deviceModel""].find(""C9300"") != -1 or response[""deviceModel""].find(""C940"") != -1:
105191:  AttributeError: 'NoneType' object has no attribute 'find'105192:  Test returned in 0:00:00.733613
105193:  Errored reason: 'NoneType' object has no attribute 'find'
{code}",2022-11-22T01:28:10.174+0000,"Required changes have been merged to Halleck, ESXi, hallack_esxivm_branch, Ghost.

Raised PR for Groot: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4844/overview Cherry-picked Groot commit to Guardian branch. Marking it as ""Done"".","['Auton', 'ESXi', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-857,https://miggbo.atlassian.net/browse/SEEN-857,Auton:Shockwave:Test_TC100_aca_test/test12_update_ise_authorization_profile,"*Reporter Analysis:* 

FAILED TO UPDATE AUTHORIZATION PROFILE

 
*DESCRIPTION*
87084:  ###################################################
87085:  #!!!FAILED TO UPDATE AUTHORIZATION PROFILE profile_guest_sf in ISE. ERROR local variable 'id' referenced before assignment----#
87086:  ###################################################
87087:  Traceback (most recent call last):
87088:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Shockwave/Shockwave-sanity-common-Multi-job/services/iseserv/iseapi.py"", line 901, in update_sg_for_authprofile
87089:  ""id"": id,
87090:  UnboundLocalError: local variable 'id' referenced before assignment
*Branch Name:  private/Shockwave-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=25418895&size=5959&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F17%2F22%2F43%2Fenv_auto_job.2022Nov17_22:43:41.354688.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS

 

*Testbed details:* NA",2022-11-22T10:19:50.611+0000,"Hey [~63f50bf84c355259db9ccc59],

did you check if the profile ```profile_guest_sf``` was there on you got this issue? If it was already deleted or not there, then this is expected. It looks like it is already deleted. I added enhancement to handle the error. But we need to know if the profile was there or not. 

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4523/diff#services/iseserv/iseapi.py Hi [~63f50bfce8216251ae4d59d5] 

After creating ""profile_guest_sf"" in ISE GUI manually , the TC was passed 

Passed Log:

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=471129&size=40083&archive=env_auto_job.2023Jan12_23:49:24.519156.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS 

 

Thanks ,

Tulasi Reddy","['Auton', 'Issue', 'Sanity', 'Shockwave']",Moe Saeed,Resolved,Avril Bower
SEEN-858,https://miggbo.atlassian.net/browse/SEEN-858,[Auton]-Guardian -Test_TC91_wired_wired_path_trace/test1_wired_wired_path_trace," 

*Reporter Analysis:* Source IP & Destination IP were found to be using the same IP address in the path trace test case. This test case needs to be enhanced.

*Description:*  3855:  Calculating the the Path between Source IP 204.1.100.170 and Destination IP 204.1.96.170

*Branch Name:  private/Guardian-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

 ** *Guardian Pass* *Log:*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=729448&size=28017&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov22_08:56:06.448674.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Ghost Pass Log:* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=712915&size=34533&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov22_08:01:54.606111.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Testbed details:* NA",2022-11-22T17:16:31.081+0000,"# PR Guardian-ms/api_auto link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5524/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5524/overview]
# Test Case:  {{TC_wired_wired_path_trace}}
# Sanity Testbed 1
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link- Guardian-ms/api_auto: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-nhanhngu%2Fusers%2Fnhanhngu%2Farchive%2F23-05%2Fsanity_TB1.2023May05_00:46:54.905785.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-nhanhngu%2Fusers%2Fnhanhngu%2Farchive%2F23-05%2Fsanity_TB1.2023May05_00:46:54.905785.zip&atstype=ATS] Hi [~accountid:620b8357878c2f00729881c8], Could you help me review this PR Auton?","['Auton', 'Guardian', 'Issue', 'Sanity']",NhanHuu Nguyen,Resolved,Avril Bower
SEEN-860,https://miggbo.atlassian.net/browse/SEEN-860,Guardian : AUTOMATION SCRIPT CHANGE REQUIRED  (DNAC tag provision in 2.3.3.7),"Hi Tran 

Received an update from DEs & Mangers that We require  automation changes for provision of tags. attached the mail conversation.

As discussed with Pawan, Suggested to to raise a JIRA.
 Please have a look into it.
 [^Re AUTOMATION SCRIPT CHANGE REQUIRED  (DNAC tag provision in 2.3.3.7).msg]",2022-11-23T06:20:28.418+0000,No changes needed in Guardian. Please share the setup in the issue state when it is available. Will close this issue for now.,"['Auton', 'Issue']",Moe Saeed,Closed,Avril Bower
SEEN-861,https://miggbo.atlassian.net/browse/SEEN-861,[Auton] Test_TC3_DNAC_verify_dnac_ise_integration_is_authenticated/test2_verify_aca_migrate: Need to check if the api call is doing retry internally,"*Reporter Analysis:* In regular/optimized code, [Test_TC3_DNAC_verify_dnac_ise_integration_is_authenticated|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_integration_verification_aca_sync.py-32-ISEIntegration&begin=364590&size=40314&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_optimized_auto_job.2022Nov09_16:25:14.343820.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test2_verify_aca_migrate, script failed because of script where it hit the url twice at the same time. This behavior of api needs to be checked internally.

In the below description, we see that there are two entries for same ISE entry: *ipAddress': '10.22.45.101,* where there are two states, one is ACTIVE and other IN-PROGRESS. 
 DE suspects there are 2 threads access this url at the very same time. 

Please check the flow and inputs from DE in the bug below.

URL */v1/aaa*

Referenced bug: [CSCwd55733|https://cdetsng.cisco.com/webui/#view=CSCwd55733] 

*Description:  The error from log or more info* 
 Resource path full url: [https://10.195.227.31/api/v1/aaa]
 1213: *Method GET*
 1214: * URL /v1/aaa*
 1215: Data {}
 1216: Response {'response': [{*'ipAddress': '10.22.45.101'*, 'sharedSecret': '', 'protocol': 'RADIUS', 'role': 'primary', 'port': 49, 'authenticationPort': 1812, 'accountingPort': 1813, 'retries': 1, 'timeoutSeconds': 10, 'isIseEnabled': True, 'instanceUuid': 'ad6f3dce-451f-4994-a1af-932fbf34609c', 'rbacUuid': '8b9bbf6c-add4-4eca-8afb-d4cef666a3e3', *'state': 'ACTIVE'*, 'ciscoIseDtos': [\\{'subscriberName': 'pxgrid_client_1668041454', 'description': '', 'password': '', 'userName': 'admin', 'fqdn': 'SSTB8-ISE.cisco.com', 'ipAddress': '10.22.45.101', 'trustState': 'TRUSTED', 'instanceUuid': '2eb58ba3-cf4d-5908-3db2-a48ac896b7ac', 'sshkey': None, 'type': 'ISE', 'failureReason': None, 'role': 'PXGRID'}, \\{'subscriberName': 'pxgrid_client_1668041454', 'description': '', 'password': '', 'userName': 'admin', 'fqdn': 'SSTB8-ISE.cisco.com', 'ipAddress': '10.22.45.101', 'trustState': 'TRUSTED', 'instanceUuid': 'b24fbd6c-899c-4f7e-aa54-e0c84144a3c9', 'sshkey': '', 'type': 'ISE', 'failureReason': None, 'role': 'PRIMARY'}], 'externalCiscoIseIpAddrDtos': [], 'encryptionScheme': '', 'messageKey': '', 'encryptionKey': '', 'useDnacCertForPxgrid': False, 'iseEnabled': True, 'pxgridEnabled': True}, {*'ipAddress': '10.22.45.101',* 'sharedSecret': '', 'protocol': 'RADIUS', 'role': 'primary', 'port': 49, 'authenticationPort': 1812, 'accountingPort': 1813, 'retries': 1, 'timeoutSeconds': 10, 'isIseEnabled': True, 'instanceUuid': 'afa1bb15-8ae0-443f-99c5-94c6e603002a', *'state': 'INPROGRESS',* 'ciscoIseDtos': [\\{'subscriberName': 'pxgrid_client_1668041452', 'description': '', 'password': '', 'userName': 'admin', 'fqdn': 'SSTB8-ISE.cisco.com', 'ipAddress': '10.22.45.101', 'trustState': 'INIT', 'instanceUuid': 'f3fc3e0a-dd1b-449d-803e-90bbd3035ff8', 'sshkey': '', 'type': 'ISE', 'failureReason': None, 'role': 'PRIMARY'}], 'externalCiscoIseIpAddrDtos': [], 'encryptionScheme': '', 'messageKey': '', 'encryptionKey': '', 'useDnacCertForPxgrid': False, 'iseEnabled': True, 'pxgridEnabled': True}], 'version': '1.0'}
  

*Branch Name:*  private/Ghost-ms/sanity_api_auto

*Script file/Usecase:* testcases/forty_eight_hour/solution_test_sanityecamb_lan.py, 

Test_TC3_DNAC_verify_dnac_ise_integration_is_authenticated/test2_verify_aca_migrate

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_integration_verification_aca_sync.py-32-ISEIntegration&begin=378020&size=26640&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_optimized_auto_job.2022Nov09_16:25:14.343820.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ** 

 

*Testbed details:* [https://wiki.cisco.com/pages/viewpage.action?pageId=1240274195] ",2022-11-23T08:21:56.806+0000,"This is old test since 2 years ago and there is no change in the area. It seems working fine in all other testbed.

This should be bug or some specific steps that created that scenario.

Please share the testbed if you see the issue again.","['Auton', 'Ghost', 'Guardian', 'Issue']",Tran Lam,Resolved,Avril Bower
SEEN-862,https://miggbo.atlassian.net/browse/SEEN-862,Auton:Guardian:Upgrade:Test_TC26_DNAC_dot1x_onboarding_ixia_scale/test3_subtest2_dot1x_auth_ixia_ipv4/test4_subtest2_ipv6_dot1x_auth_ixia,"*Reporter Analysis:* 

Unsupported cli used to check dot1x sessions 

Cli used : show authentication sessions method dot1x

Cli need to use : show access-session method dot1x

 

*Description:*  

 
54478:  Expected Number of dot1x sessions not found, wait for some more time
54479:  +++ SN-FOC2311T18E with via 'a': executing command 'show authentication sessions method dot1x' +++
*show authentication sessions method dot1x show authentication sessions method dot1x  ^ % Invalid input detected at '^' marker. SN-FOC2311T18E#*
54496:  Exception occured ('Command execution failed', SubCommandFailure('sub_command failure, patterns matched in the output:', ['% Invalid input detected at'], 'service result', ""show authentication sessions method dot1x\r\nshow authentication sessions method dot1x\r\n ^\r\n% Invalid input detected at '^' marker.\r\n\r\nSN-FOC2311T18E#""))
 
*Branch Name:  private/Guardian-ms/sanity_api_auto*

*Script file/Usecase:* 

/testcases/upgrade/after_upgrade_verify.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13630630&size=1894817&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov22_22:20:27.744274.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Testbed details:* NA",2022-11-23T11:43:23.291+0000,"Both CLI get tried, if first one does not work, second one is tried, as the CLI is different on different platforms. 

{{SN-FOC2311T18E# }}

{noformat}54771: 
 +++ SN-FOC2311T18E with via 'a': configure +++{noformat}

{{config term Enter configuration commands, one per line. End with CNTL/Z. SN-FOC2311T18E(config)#no logging console SN-FOC2311T18E(config)#line console 0 SN-FOC2311T18E(config-line)#exec-timeout 0 SN-FOC2311T18E(config-line)#end SN-FOC2311T18E# }}

{noformat}54797: 
 +++ SN-FOC2311T18E with via 'a': executing command 'sh access-session method dot1x' +++{noformat}

{{sh access-session method dot1x No sessions match supplied criteria. SN-FOC2311T18E# }}

{noformat}54807: 
 Sessions expected:10 in state Auth{noformat}

{noformat}54808: 
 Expected Number of dot1x sessions not found, Marking Failed, returning Failed{noformat}

{noformat}54809: 
 +++ SN-FCW2307G03S logfile /home/admin/.pyats/runinfo/env_auto_job.2022Nov22_22:20:27.744274/SN-FCW2307G03S-cli-1669190971.log +++{noformat}

{noformat}54810: 
 +++ Unicon plugin iosxe +++{noformat}

{{Trying 172.19.186.119... }}

{noformat}54812: 
 +++ connection to spawn: telnet 172.19.186.119 2059, id: 140372811704592 +++{noformat}

{noformat}54813: 
 connection to SN-FCW2307G03S{noformat}

{{Connected to 172.19.186.119.}}

Here the sessions are not. there. Raise a product bug. Hi [~accountid:5f3c6ae932360700388f7b4b] ,

Recently we got a pass log in Ghost P2
Cluster is upgraded from Ghost P1 to Ghost P2
Pass log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1020222&size=2158314&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun15_20:56:13.662639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1020222&size=2158314&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun15_20:56:13.662639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Upgrade']",Raji Mukkamala,Cancelled,Avril Bower
SEEN-863,https://miggbo.atlassian.net/browse/SEEN-863,[Auton] -Ghost- Test_TC160_Brownfield_Workflow_Aireos /test22_remove_brownfield_network_profile_from_sites," 

*Reporter Analysis:* Unable to delete  network profile-brownfield

*Description:*  
 Message:{""response"":

{""errorCode"":""NCND00050"",""message"":""NCND00050: An internal error occurred while processing the request"",""detail"":""Request method 'DELETE' not supported""}

,""version"":""1.0""}
 4611: Traceback (most recent call last):
 4612: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 295, in call_api
 4613: response.raise_for_status()
 4614: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status
 4615: raise HTTPError(http_error_msg, response=self)
 4616: requests.exceptions.HTTPError: 500 Server Error: Server Error for url: [https://10.195.227.48/api/v1/siteprofile//site/c9223172-496e-4fd3-be91-bfcd3e8931c0
 ]
 4617: Encountered unhandled HTTPError in Internal API Call
 4618: Flagging result as FAIL!
 4619: Reason: 500 Server Error: Server Error for url: [https://10.195.227.48/api/v1/siteprofile//site/c9223172-496e-4fd3-be91-bfcd3e8931c0]
 4620: Kwargs:
 4621: {}
 [https://10.195.227.48/api/v1/siteprofile//site/c9223172-496e-4fd3-be91-bfcd3e8931c0]
 *Branch Name:  private/Ghost -ms/sanity_api_auto*

*Script file/Usecase:* solution_test_sanityecamb_lan.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=910664&size=26821&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov21_16:30:25.125783.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:* 
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4430542&size=20431&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fenv_auto_job.2022Jul08_16:37:37.598979.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-3
 ]

The cluster is available till 24th Nov  

[ |https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-3]",2022-11-23T16:46:07.113+0000,"This should be some incorrect rerun error. The network profile is already deleted at the time of this testcase, so it cannot be removed from the site. Rerun issue, this case should not have been rerun at this point, expected to fail.","['Auton', 'Ghost', 'Groot', 'Issue', 'Optimized', 'Sanity']",Andrew Chen,Resolved,Avril Bower
SEEN-865,https://miggbo.atlassian.net/browse/SEEN-865,[Auton]: Shockwave - Test_TC150_Client_AP_360 / test2_verify_AP360_missing_KPI,"We have 4800 AP in our sanity TB which is connected to AireOS WLC. When we are testing Radio 2 of the AP not showing any KPI values.

Raised bug for that and DE confirms that the Radio 2 of the 4800 AP will not show any KPI values as it will be in Monitor mode which is expected behavior. 

So in our script side we need to handle the same. Script should not check the Radio 2 KPI values if the AP model is 4800. 

*Bug id:* [https://cdetsng.cisco.com/summary/#/defect/CSCwd65356]

*Branch Used:* private/Shockwave-ms/sanity_api_auto

*Main Branch:* private/Shockwave-ms/api-auto

*Script file:* testcases/forty_eight_hour/solution_test_sanityecamb.py

*Failed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=50778506&size=277005&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F11%2F17%2F22%2F43%2Fenv_auto_job.2022Nov17_22:43:41.354688.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

 ",2022-11-24T06:16:11.610+0000,"[~63f53512263233e653a96a29], required version check has been added as part of below PR for Ghost branch:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4356/overview] Shockwave branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4402/overview]

Guardian branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4403/overview]

Groot branch: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4404/overview Required fix has been merged to Ghost, Groot, Guardian and Shockwave branches.

Marking this ticket as ""Done"". Recently Passed Log after pulling code

[test1_AP_CDP_neighbor_info|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=20257040&size=238007&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F03%2F01%2F01%2F33%2Fenv_auto_job.2023Mar01_01:33:10.838037.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity', 'Shockwave']",Amardeep Kumar,Closed,Avril Bower
SEEN-866,https://miggbo.atlassian.net/browse/SEEN-866,[Auton][IBSTE] : SWIM CCO feature is not part of IBSTE main script,"Hi Team,

 

SWIM CCO feature related testcase is not part of IBSTE script.

Script : On Guardian branch : ibste_main.py

On Groot and Ghost : Optimized script",2022-11-25T10:40:28.216+0000,Script On Guardian branch is already added! I am not sure why say it is not integrated. PR for optimized folders: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4544/overview,"['Auton', 'Guardian', 'IBSTE', 'Issue']",Moe Saeed,Resolved,Avril Bower
SEEN-867,https://miggbo.atlassian.net/browse/SEEN-867,Auton: Upgrade Cleanup: Test_TC10_cleanup_anchorvn/test1_cleanup_anchorvn,"*Reporter Analysis:* 
 Script should delete and re-create the ACL's 

 

*Description:*  
 FAILED to remove segments for AnchoredVN WiredVNStatic2-Global/USA/SAN_JOSE_US_SJ_Fabric1 in Ultilize site Global/USA/SAN_JOSE_US_SJ_Fabric1 for reason:Unable to push configuration to device 204.1.2.1. Device response - inconsistent value: Device refused one or more commands
 8592: Failed to remove Anchored VNs ['WiredVNStatic2-Global/USA/SAN_JOSE_US_SJ_Fabric1'] from ultilize site Global/USA/SAN_JOSE_US_SJ_Fabric1 . Error:Unable to push configuration to device 204.1.2.1. Device response - inconsistent value: Device refused one or more commands
 10216: FAILED to remove segments for AnchoredVN WiredVNStatic1-Global/USA/SAN_JOSE_US_SJ_Fabric1 in site Global/USA/SAN_JOSE_US_SJ_Fabric1 for reason:Internal Error while provisioning
 10666: Failed reason: Result: Failed in cleanup_anchorvn
 *Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* dnac_cleanup_script.py

*Source Team:   Upgrade Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1550280&size=2083261&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov28_00:40:01.263955.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:* NA",2022-11-28T10:11:32.344+0000,"Seem like its a bug. Please file defect if you saw the issue again. It is clearly a bug, the device is rejecting commands from DNAC.
 
8589:  Library group ""task"" method ""wait_for_task_complete"" returned in 0:01:09.239834
8590:  \{'data': 'workflow_id=4ecb5272-cf89-4e2f-b1ae-b6f6d66208f5;cfs_id=ca03da67-e7c6-4e28-a1c5-a96e005327f7;rollback_status=not_supported;rollback_taskid=0;failure_task=Determination of network intent deployment status:b8afc6c3-e825-4d36-bb8f-642f87860af9;processcfs_complete=true', 'progress': 'TASK_MODIFY_PUT', 'version': 1669626023482, 'startTime': 1669625955907, 'endTime': 1669626023482, 'errorCode': 'NCNP51100', 'serviceType': 'NCSP', 'lastUpdate': 1669626023462, 'isError': True, 'failureReason': 'Unable to push configuration to device 204.1.2.1. Device response - inconsistent value: Device refused one or more commands', 'instanceTenantId': '637e6ac7f340ad3383426463', 'id': '147409e7-e6ba-4faa-b06f-4557fe2f13df'}
8591:  #################################################
8592:  Failed to remove Anchored VNs ['WiredVNStatic2-Global/USA/SAN_JOSE_US_SJ_Fabric1'] from ultilize site Global/USA/SAN_JOSE_US_SJ_Fabric1 . Error:Unable to push configuration to device 204.1.2.1. Device response - inconsistent value: Device refused one or more commands
8593:  #################################################
8594:  Library group ""connectivity_domain"" method ""remove_remote_anchored_vn_from_site"" returned in 0:02:38.842428","['Auton', 'Ghost', 'Issue', 'Upgarde']",Tran Lam,Resolved,Avril Bower
SEEN-868,https://miggbo.atlassian.net/browse/SEEN-868,Auton: Upgrade Cleanup:Test_TC19_remove_devices_from_inventory  /   test1_remove_devices,"*Reporter Analysis:* 

Script is checking the task id which is already in progress

*Description:*  

7802: api_switch_call called:
7803: {}
7804: Resource path full url: https://10.195.227.80/api/v1/task/f84fd0b2-ae9f-43e8-9977-56b23def9e43
7805: \{'progress': 'Delete device request for 0d426ffe-6069-4393-a477-379c39ab6e1b is already in progress', 'version': 1669628189802, 'startTime': 1669628189776, 'endTime': 1669628189801, 'errorCode': 'Network device deletion failed', 'serviceType': 'Inventory service', 'isError': True, 'failureReason': 'Delete device request for 0d426ffe-6069-4393-a477-379c39ab6e1b is already in progress', 'instanceTenantId': '637e6ac7f340ad3383426463', 'id': 'f84fd0b2-ae9f-43e8-9977-56b23def9e43'}
7806: Library group ""task"" method ""wait_for_task_complete"" returned in 0:00:00.028403
7807: \{'progress': 'Delete device request for 0d426ffe-6069-4393-a477-379c39ab6e1b is already in progress', 'version': 1669628189802, 'startTime': 1669628189776, 'endTime': 1669628189801, 'errorCode': 'Network device deletion failed', 'serviceType': 'Inventory service', 'isError': True, 'failureReason': 'Delete device request for 0d426ffe-6069-4393-a477-379c39ab6e1b is already in progress', 'instanceTenantId': '637e6ac7f340ad3383426463', 'id': 'f84fd0b2-ae9f-43e8-9977-56b23def9e43'}
7808: Removing Device \{'response': {'taskId': 'f84fd0b2-ae9f-43e8-9977-56b23def9e43', 'url': '/api/v1/task/f84fd0b2-ae9f-43e8-9977-56b23def9e43'}, 'version': '1.0'}: Delete device request for 0d426ffe-6069-4393-a477-379c39ab6e1b is already in progress
7809: Starting Task wait for task:\{'response': {'taskId': 'dcb7e464-bf5e-41bf-9b72-60287ef53c0c', 'url': '/api/v1/task/dcb7e464-bf5e-41bf-9b72-60287ef53c0c'}, 'version': '1.0'}

*Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* 

dnac_cleanup_script.py

*Source Team:  Upgrade Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2459448&size=150267&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov28_01:12:57.283424.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Testbed details:* NA",2022-11-28T10:19:57.071+0000,"[~accountid:63f50bf84c355259db9ccc59] / [~accountid:61efa8c457b25b006877eda3], do we still have this issue?  Hi [~accountid:62ab7a399cd13c0068b18fe0] ,
Recently we  tested  on Ghost p1RC6 build  again TC19 was failing its not removing devices from inventory 
Will test on next testing and will update

Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=683350&size=33883&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun07_22:18:45.958224.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=683350&size=33883&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun07_22:18:45.958224.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Yes [~accountid:62ab7a399cd13c0068b18fe0]  Issue is seen on Latest Hulk RC3:
4143: 
 Removing Device {'response': {'taskId': 'e6aa174d-cd66-4c07-846d-3c5204c571b7', 'url': '/api/v1/task/e6aa174d-cd66-4c07-846d-3c5204c571b7'}, 'version': '1.0'}: Delete device request for 204.192.4.2 is already in progress

{noformat}4366: 
 Failed reason: Result: Failed in remove_devices_from_inventory{noformat}

!Screenshot (475).png|width=1920,height=1080!



Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=907101&size=151705&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug05_00:34:33.878227.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=907101&size=151705&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug05_00:34:33.878227.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


When I can’t see any devices in inventory just after the testcase failure

Attaching screenshot , inventory was empty. [~accountid:63f50bf84c355259db9ccc59] Did you check inventory after tc18/19 to say devices werent deleted? Or just going off of logs? For example in Anushas logs it gets failed but in dnac devices are deleted. Just want to make sure its the same issue, not 2 seperate issues. Hi [~accountid:63f50bcece6f37e5ed93c87e] ,
I have checked the inventory after TC19 , device deletion is in progress , its taking time to delete the devices. Hi [~accountid:63f50bcece6f37e5ed93c87e] 
I attached screenshot 477 
[~accountid:63f50bf84c355259db9ccc59]  she is saying this issue which i added screenshot after 1 min if i refresh device might have for deleted



Recently i observed issue while cleaning Ghost P2 RC3
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-08/env_auto_job.2023Aug23_02:08:39.696708.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-08/env_auto_job.2023Aug23_02:08:39.696708.zip&atstype=ATS]


Thanks,

 Anusha John
 [Pull Request #6747: Task wait for tree to finish - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6747/overview]



Fixed in this PR, please give a try and let me know [~accountid:63f50bcece6f37e5ed93c87e] I have observed that in *Guardian*  and *Hulk* Branch  Facing the Same  Issue 
Can you Please raise  Pr  for *private/Guardian-ms/sanity_api_auto*  &  *private/Hulk-ms/sanity_api_auto*

Failed Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5302838&size=328629&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep21_22:37:53.868959.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5302838&size=328629&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep21_22:37:53.868959.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Raised for guardian main line, please pull from latest","['Auton', 'Ghost', 'Guardian', 'Hulk', 'Issue', 'Upgrade']",Andrew Chen,Resolved,Avril Bower
SEEN-870,https://miggbo.atlassian.net/browse/SEEN-870,[Auton] Ghost-new-feature: Test_TC199_config_preview_treeview_cliconfig_enabled /test2_schedule_config_preview_wireless_device  Errored reason: 'bool' object is not subscriptable,"*Reporter Analysis:* The sub-testcases [test2_schedule_config_preview_wireless_device|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1945844&size=6754&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov24_02:32:14.254335.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] & [test3_verify_treeview_cliconfig_enabled|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1952598&size=4254&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov24_02:32:14.254335.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] have errored due to 'bool' object is not subscriptable. The proc dnac_handle.schedule_config_preview(self.dev_name) is expected ro return device details, rather it is returning boolean value, Hence scripts are failing

 

*Description:  The error from log or more info* 
[test2_schedule_config_preview_wireless_device|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1945844&size=6754&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov24_02:32:14.254335.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] : Errored due to script Issue 


8483: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 14317, in test2_schedule_config_preview_wireless_device 
8484: if self.dev_details[0] and self.dev_details[1]: 
8485: TypeError: 'bool' object is not subscriptable 
8486: Test returned in 0:00:00.051694 
8487: Errored reason: 'bool' object is not subscriptable 
 
def test2_schedule_config_preview_wireless_device(self, dnac_handle): 
    '''schedule config preview for wireless device''' 
    logger.info(""config preview schedule"") 

    **  *self.dev_details = dnac_handle.schedule_config_preview(*self*.dev_name)- This proc is expecting device details, rather it is getting boolean value Hence script is failing here*

    *if self.dev_details[0] and self.dev_details[1]: -**- Hence script is failing here*


 

[test3_verify_treeview_cliconfig_enabled|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1952598&size=4254&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov24_02:32:14.254335.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] : Errored due to script Issue 
File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 14327, in test3_verify_treeview_cliconfig_enabled 
8507: if dnac_handle.verify_treeview_cliconfig_enabled(self.dev_details[0],self.dev_details[1]): 
8508: TypeError: 'bool' object is not subscriptable 
8509: Test returned in 0:00:00.001554 
8510: Errored reason: 'bool' object is not subscriptable 
 
def test3_verify_treeview_cliconfig_enabled(self, dnac_handle): 
    '''verify config preview has the treeview and cliconfig enabled''' 
    logger.info(""verifying config preview has the treeview and cliconfig enabled"") 

    *if dnac_handle.verify_treeview_cliconfig_enabled(self.dev_details[0],self.dev_details[1]):-- This proc is expecting device details, rather it is getting boolean value Hence script is failing here*

*Branch Name:*  Ghost Handoff Branch : *ghost-handoff-111822*

*Script file/Usecase :* testcases/forty_eight_hour/solution_test_sanityecamb_lan.py, 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0 issue*

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1942978&size=14056&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov24_02:32:14.254335.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS |https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1942978&size=14056&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov24_02:32:14.254335.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-3] ",2022-11-29T05:31:24.646+0000,"Same issue is also observed on MSTB2.



*Uber ISO Version tested :* 
Promoted Ghost Uber - *2.1.610.70551* *FIPS*

*Script Name:* solution_test_3sites_sjc_nyc_sf.py

*Testbed :*MSTB2

 

*Failed logs:*

[https://earms-trade.cisco.com/tradeui/logs/details?ats=%2Fusers%2Frvonmize%2Fpyats-inst&client=web&host=st-ucs-sanity.cisco.com&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fsr_mb2_three_sites_FIPS.2022Nov28_07:33:15.850003.zip] -> Refer TC183 [~63f50bcf4e86f362d39acde5] please update here if a fix pushed. 
Thank you! [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4382/overview]

the issue is fixed. Please review it [~62d2fe9f8afb5805e5d5af49] [~62d2fec15d6f5fd2c3db8f9f], From log, it couldnot get TB7-eWLC device info. Please debug and check in the testbed why, TB7-eWLC wasnot found in Fabric?

 
8468:  dev details are [\{'name': 'TB7-eWLC', 'site': 'Global/USA/New York/BLDNYC', 'role': 'EWLC', 'managed_ap_site': 'Global/USA/New York/BLDNYC/FLOOR1'}]
8469:  device TB7-TSIM site is Global/USA/New York/BLDNYC
8470: 
8471: 
8472:  api_switch_call called:
8473:  \{'params': {'name': 'TB7-eWLC.cisco.com'}}
8474:  Resource path full url: [https://10.30.0.100/api/v2/data/customer-facing-service/DeviceInfo]
8475:  device_response []
8476:  Could not find the device details Executed on TB8 Halleck Version 2.3.6.0-8010354 : Test has passed with Ghost sanity branch using lan script  testcases/forty_eight_hour/solution_test_sanityecamb_lan.py, 
Pass log: 
[Test_TC210_config_preview_treeview_cliconfig_enabled|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1258670&size=46429&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb21_20:07:07.678231.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  This auton has been fix in other commit","['Auton', 'Ghost', 'Integration', 'Issue', 'MSTB2', 'Multisite', 'Sanity']",QuangVinh Nguyen,Closed,Avril Bower
SEEN-871,https://miggbo.atlassian.net/browse/SEEN-871,[Auton]  Enhancement for the router/ASR configuration in LAN-A script," 

*Reporter Analysis:*  Team, we have added an ASR router in Sanity, ASR router doesn't support LAN-A, So we need to add the workaround in the script to discover and configure the ASR underlay after TC22,23.

*Description:  The error from log or more info* 

*Script file/Usecase:*  LAN 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: New request* 

*Fail Log: NA*

*Pass Log: NA*

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8]

 ",2022-11-29T19:21:23.571+0000,Run Brownfield Sanity on ASR based testbed.,"['Auton', 'Ghost', 'Groot', 'Issue', 'Sanity', 'shockwave']",Andrew Chen,Resolved,Avril Bower
SEEN-872,https://miggbo.atlassian.net/browse/SEEN-872,[Auton] GHOST: Task-inventory_checks_brownfield.py-61-provisionInventoryBrownfield/Test_TC3_DNAC_Device_Inventory_verifications_configure_roles_on_devices/test1_verifications_configure_roles_on_devices," 

*Reporter Analysis:*  While configuring the roles on the devices, there is an attribute (lb_ip) missing.

*Description:  The error from log or more info* 

*Branch Name:  private/Ghost-ms/sanity_api_auto*
                          **                          *private/Ghost-ms/sanity_api_auto***
                          **                          

*Script file/Usecase:* provisionInventoryBrownfield

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-inventory_checks_brownfield.py-61-provisionInventoryBrownfield&begin=62351&size=29182&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_optimized_auto_job.2022Nov29_11:08:45.364487.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS*]

*Pass Log: NA*

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8]",2022-11-30T11:01:46.541+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a99ea6140319c44b58c14c46d95a99aad23162d5,"['Auton', 'Ghost', 'Issue', 'Optimized', 'Sanity']",Pawan Singh,Resolved,Avril Bower
SEEN-873,https://miggbo.atlassian.net/browse/SEEN-873,[Auton]  BAPI ext_add_devices_to_site using internal API," 

*Reporter Analysis:*  ext_add_devices_to_site this function is using the internal API instead of BAPI, please update the testcsae for BAPI



89.68/dna/system/api/v1/assign-device-to-site/d373d25a-b35a-45ee-923e-d9774a77016a/device

 

*Description:  The error from log or more info* 

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8779597&size=27254&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov22_15:27:32.767644.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:*

*Testbed details:* NA",2022-11-30T22:05:33.855+0000,PR Raised,"['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Sanity']",Raju Saran,Closed,Avril Bower
SEEN-874,https://miggbo.atlassian.net/browse/SEEN-874,Auton:Guardian: Sanity: Test_TC28_DNAC_Device_Provisioning  /   test5_verify_provision_the_devices_fabric1,"*Reporter Analysis:* 

Provision the fabric devices in fabric1.

*Description:*  
File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 295, in call_api
11008:  response.raise_for_status()
11009:  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status
11010:  raise HTTPError(http_error_msg, response=self)
11011:  Failed to create custom Model Configs 500 Server Error: for url: [https://10.30.0.100/api/v2/named-capability/design?validateSchema=False]
11013:  Failed reason: Failed:to create custom model configs
*Branch Name:  private/Guardian-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2258635&size=67697&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_auto_job.2022Nov30_12:15:29.278693.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

 

*Testbed details:* NA",2022-12-01T05:03:50.469+0000,"[~accountid:63f50bf84c355259db9ccc59] Do you still see this issue now? If yes, can you share some new failed log? If no, please close this ticket. Hi [~accountid:62d2fe9f8afb5805e5d5af49] ,
Now a day we are not seeing this issue ,TC was passing & I will close the ticket
Latest pass log:
[test5_verify_provision_the_devices_fabric1|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9709857&size=326520&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May31_04:25:33.197866.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Guardian', 'Issue', 'sanity']",Raji Mukkamala,Closed,Avril Bower
SEEN-884,https://miggbo.atlassian.net/browse/SEEN-884,"TC36 Enable Multicast failing, need the API payload change to the script being used","Hi Team,
 
I am facing an issue where TC36 Sanity run is failing because of the below recent API change 
[https://wiki.cisco.com/display/APICEMUCI/Ghost+Multicast+enhancements#GhostMulticastenhancements-RESTAPIchanges.]
 
New fields isV4default and isV6Default is not set 
[TRADe v2 | Logs: sst_sanity (cisco.com)|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10603890&size=502904&archive=%2Fhome%2Fdnac%2F.pyenv%2Fversions%2F3.6.4%2Fenvs%2FENV-PYATS%2Fusers%2Fdnac%2Farchive%2F22-11%2Fsst_sanity.2022Nov29_17:14:48.375807.zip&ats=%2Fhome%2Fdnac%2F.pyenv%2Fversions%2F3.6.4%2Fenvs%2FENV-PYATS&submitter=dnac&from=trade&view=all&atstype=pyATS]
Recently our sanity test run for TC36 started failing with Start: Nov 29, 2022 06:47 PMValidating user intent for Fabric SAN JOSE
NCSO20659: Invalid ASM Group Range. Rendezvous Point null must have at least one ASM Group Range configured if Rendezvous Point is not default.End: Nov 29, 2022 06:47 PM
 
Script the test is using: dnac-auto/testcases/forty_eight_hour/solution_test_sanityeca.py
Latest code sync from : private/Ghost-ms/api-auto
 
Kindly let me know if we need to make any changes or raise a ticket for the change needed.
 
Thanks
Jaya",2022-12-02T16:43:37.372+0000,"Hi Jaya, 

The issue should be fixed. 

If you still see the issue, please update your latest trade log and other required information based on https://wiki.cisco.com/display/EDPEIXOT/Raising+Jira+Tickets+for+script+issues","['Auton', 'Change_Request']",Tran Lam,Resolved,Avril Bower
SEEN-886,https://miggbo.atlassian.net/browse/SEEN-886,[Auton][Guardian][Groot][Ghost] - Talos block verfication fails with DNAC connection error,"*Uber ISO Version tested :* 
 Promoted Ghost Uber - *2.1.610.70540, Non-FIPS, PUBSUB enabled*

*Script Name:* solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py, solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB1, MSTB3/AWS_MSTB
  

*Description :* 

Tried executing the Talos TC on Ghost-540 build using *talos_fixes* branch ([https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4159/overview])

We see verification of Talos block list step is failing due to dnac connection login error, but the maglev cli credentials that are being used are correct ones. Due to this, we see *Failed to configure Talos blocked list on the maglev cluster!!* error

*Error Snip:*
11275:  Session closed.
11276:  ('10.195.243.123', 'maglev', 'Maglev123')
11277:  Encountered error on login. Check login details or try again. Error details:
11278:  Could not establish connection to host
11279:  Could not establish connection to host
11280:  Test returned in 0:00:00.266104
11281:  Failed reason: Failed to configure Talos blocked list on the maglev cluster!!
11282:  The result of section test4_verify_talos_block_list_from_cluster is => FAILED
 

*Failed log:*  [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fsr_mb_multi_sites_mdnac.2022Nov23_21:29:22.902179.zip&atstype=ATS] -> Refer TC225",2022-12-05T11:57:30.727+0000,"A fix for this issue has been added to talos_fixes, this fix is for groot/ghost. I am not sure if this will work on Guardian since the flow has changed since Guardain. The fix is to use the regular shell instead of the bash shell on DNAC cluster. Moe, Can you add PR link to the ticket? https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4592 Hi [~63f50bfce8216251ae4d59d5],

We have tested the above fix changes for *private/Groot-ms/api-auto* branch and its working fine.

*Reference log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fsr_mb_multi_sites_mdnac.2023Mar01_04:10:51.559993.zip&atstype=ATS] -> TC225.4

 Could you please commit these changes to Guardian, Ghost, Halleck and Hulk branches as well?

  Regards
Sandeep S","['AWS_MSTB', 'Auton', 'Ghost', 'Groot', 'Guardian', 'Integration', 'Issue', 'MSTB1', 'MSTB3', 'Multisite']",Moe Saeed,Resolved,Avril Bower
SEEN-887,https://miggbo.atlassian.net/browse/SEEN-887,[Auton] - Script changes need on Multicast TCs to handle failure scenarios,"*Uber ISO Version tested :* 
 Promoted Ghost Uber - *2.1.610.70551,* *2.1.610.70567, FIPS,* *Non-FIPS, PUBSUB enabled*

*Script Name:* solution_test_3sites_multicast.py

*Testbed :* MSTB1, MSTB2, MSTB3/AWS_MSTB
  

*Description :* 

In case of TC failures in each of the TC under multicast, the script only is blocking the execution on its sub TCs but not blocking subsequent TCs. Ideally if configure multicast / clearing multicast fails, corresponding configs would not be pushed and also traffic validation fails. So need to enhance script to make other related TCs as blocked if dependent TC Fails. Also this would save the run time execution, as the in case of failures, the script keeps running in loop trying to validate expected outputs.

 

 

*Failed logs:*
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fsr_mb_multi_sites.2022Dec04_01:51:18.793577.zip&atstype=ATS] -> Failed log on MSTB1

[https://earms-trade.cisco.com/tradeui/logs/details?ats=%2Fusers%2Frvonmize%2Fpyats-inst&client=web&host=st-ucs-sanity.cisco.com&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fsr_mb2_three_sites_FIPS.2022Nov27_00:19:58.011034.zip] -> Failed log on MSTB2",2022-12-05T14:59:34.502+0000,"Raised PR for the required change:
Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4537/overview]

[Groot: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4541/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4537/overview]

Guardian: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4542/overview]  Required changes have also been updated into esxivm_branch:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/70d28a02ca98dd0970cacd8d2760870a4d97d8b0]

Halleck branch got it from forward merge. Marking this ticket as ""Done"".","['AWS_MSTB', 'Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'MSTB1', 'MSTB2', 'Multisite']",Amardeep Kumar,Closed,Avril Bower
SEEN-888,https://miggbo.atlassian.net/browse/SEEN-888,[AWS-MS][Guardian]-SWIM CCO-test5_verify_upload_os_image_mark_golden,"Guardian Version used :2.1.515.70134

Script Name :  solution_test_3sites_sjc_nyc_sf.py

Branch Used:private/Guardian-ms/sanity_api_auto

Testcases Impacted : 

[Test_TC30_SWIM_UPGRADE_ECA_DEVICE|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=890481&size=2251148&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fauto_MS_job.2022Nov29_18:21:19.407709.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Fail Log:
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fauto_MS_job.2022Nov29_18:21:19.407709.zip&atstype=ATS]    -->Refer TC 30

Description:
 Hi Moe,
 While integrating SWIM CCO feature,we observed an issue. test5_verify_upload_os_image_mark_golden  is an old method of uploading images manually not through the CCO. 

So we are hitting an key error issue where the script is trying to fetch the image from image_dir path manually.

 
 6607: api_switch_call called:
 6608: {}
 6609: Resource path full url: [https://172.35.16.150/api/assurance/v1/time]
 6612: Current DNAC time is as follows {'version': '1.0', 'response': [

{'timeType': 'GLOBAL', 'time': 1669775580000}

, \{'timeType': 'CLIENT', 'time': 1669775820000}, \{'timeType': 'NETWORK', 'time': 1669775580000}, \{'timeType': 'CURRENT', 'time': 1669776079676}, \{'timeType': 'POE', 'time': 1669775340000}]}
 6613: Time in the Unix Time format 1669776079676
 6614: Action: Swim uploading with newe image
 6615: Traceback (most recent call last):
 6616: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 6617: result = testfunc(func_self, **kwargs)
 6618: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py"", line 1326, in test5_verify_upload_os_image_mark_golden
 6619: if (dnac_handle.swim_image_upload_assign_and_mark_golden(key=""image_dir"", site=""Global"")):
 6620: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 6621: result = method(*args, **kwargs)
 6622: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/api_groups/swim/group.py"", line 94, in swim_image_upload_assign_and_mark_golden
 6623: result = self.swim_image_upload_url(imageurl=imageurl,key=key)
 6624: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
 6625: result = method(*args, **kwargs)
 6626: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/Guardian-Multisite-Common-Multi-Job/services/dnaserv/lib/api_groups/swim/group.py"", line 18, in swim_image_upload_url
 {color:#de350b}6627: images = self.services.dnaconfig.testbed.custom[key]{color}
 {color:#de350b}6628: KeyError: 'image_dir'{color}
 {color:#de350b}6629: Test returned in 0:06:32.132750{color}
 {color:#de350b}6630: Errored reason: image_dir
 
 Please check on it whether the script needs enhancement so that it will try to upload images from CCO and mark the latest image available from CCO as golden.{color}",2022-12-05T18:16:57.533+0000,"The test case covers both image uploads from the FTP server and CCO. both need testing, so use the image_dir for the upload location.","['AWS_MSTB', 'Auton', 'Guardian', 'Issue']",Pawan Singh,Resolved,Avril Bower
SEEN-889,https://miggbo.atlassian.net/browse/SEEN-889,Auton:[Ghost-Upgrade]:Test_TC8_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision/test1_verify_configuration_on_devices_fabric1,"*Reporter Analysis:* 
 In the Presanity we are not having Tacacs Config enabled in DNAC-ISE side but while running upgrade script ,the script is adding tacacs to DNAC-ISE side.

If Tacacs is not present in presanity run after upgrade script also should not check or add tacacs

*Description:*  
 **
 TB7-SJ-eCA-BORDER-CP
 12632: {'result': True, 'output': 'show run | s aaa\naaa new-model\naaa group server radius dnac-client-radius-group\n server name dnac-radius_87.1.1.3\n key-wrap enable\n ip radius source-interface Loopback0\naaa group server radius dnac-network-radius-group\n server name dnac-radius_87.1.1.3\n key-wrap enable\n ip radius source-interface Loopback0\naaa group server radius dnac-rGrp-Radius_ssi-ab6415b1\n
  
 *Branch Name:  private/Ghost-ms/sanity_api_auto*
 *Script file/Usecase:* 
 after_upgrade_verify.py

*Source Team:  Upgrade-Sanity*

 

*TC's Affected:*

Test_TC10_DNAC_verifying_configuration_lisp_on_devices
Test_TC13_DNAC_verifying_configuration_lisp_on_devices

*Issue Seen first time or day0 issue:*

*Fail Log:*
 [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fenv_auto_job.2022Dec06_05:21:57.339228.zip&reqseq=&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=PYATS]

*Pass Log:*

*Testbed details:* NA",2022-12-07T10:25:13.805+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/services/dnaserv/lib/api_groups/device_config_validation/group.py?until=cdba88e267f90eb07ae8343e8f1e6f1f562d58e4]

 

FIxed.","['Auton', 'Ghost', 'Issue', 'Upgrade']",Pawan Singh,Resolved,Avril Bower
SEEN-891,https://miggbo.atlassian.net/browse/SEEN-891,[Auton][IBSTE] : 3.2 ISE integration with Halleck is failing,"Hi Team,

 

3.2 ISE integration with Halleck is failing with below error

Not supported to connect an existing deployment of Cisco DNA Center as a Reader node to Cisco Identity Services Engine (ISE). 

 

When checked with DE (Mark Bakinski) he shared API changes that needs to be incorporated for Multi DNAC. Currently on IBSTE we are running with 3 node DNAC

 

https://cisco-my.sharepoint.com/:w:/p/swashbur/EbziZ3id_lBHsZNljmBBnToBzzmAdOw1tE9Rry_NhuukLQ?e=fqeLhR",2022-12-07T15:52:19.805+0000,"No script change is needed. Please debug why there are 2 DNAC with single ise. IBSTE testbed is not for MDNAC, The ISE should not be shared with other clusters. Keep a dedicated ISE for one cluster.","['Auton', 'Halleck', 'IBSTE', 'Issue']",Pawan Singh,Resolved,Avril Bower
SEEN-892,https://miggbo.atlassian.net/browse/SEEN-892,[Auton] Ghost - Wireless Solution Sanity - Disable wireless on L2VN if device version is less than 17.10.1,"* *DNAC Release_Version Tested:* Ghost_2.1.610.70574
 * *Device Image Used:* 17.6.5
 * *Branch Used:* rcdn/Ghost-ms/api-auto (Latest sync to Main Branch done before reg run start)
 * *Script Name:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py
 * *Testcases Impacted:* Test_TC33_DNAC_assigne_dhcp_role_deploy_device_in_fabric gets errorred blocking other testcases
 * *Failed Trade Log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/22-12/sanitycombine.2022Dec05_10:30:20.782120.zip&atstype=ATS]
 * *Fix Required:* Disable wireless on L2VN if device version is less than 17.10.1
 * *Issue details/analysis:* Fabric Update on Device failed with reason ‘Invalid Message Code NCSO20858’
 NCSO20858: Cannot add Edge Node to fabric. Fabric contains wireless Layer2 Virtual Network along with Access Point Pool. Igmp Querier clis are supported on IOS-XE 17.10 and above. Upgrade devices %s to 17.10+ release or above Or disable wireless on Layer 2 Virtual Network [[Ljava.lang.Object;@6e864fd7] Format specifier '%s'
 *Fail Log Snapshot:*
 TC33 - test1_verify_assign_roles_and_deploy_devices_on_fabric1_site_san_jose
 38010: Config Preview Activity failed with reason: NCSP11000: Error occurred while processing the 'modify' request. Additional info for support: taskId: 'ea325b51-0537-4632-a773-753a1897d64e'.
 TC33 – test3_verify_assign_roles_and_deploy_devices_on_fabric1_site_new_york
 155713: Config Preview Activity failed with reason: NCSP11000: Error occurred while processing the 'modify' request. Additional info for support: taskId: 'ed4663ee-aeb1-4509-9427-153541ea46f8'.
 *DE Leena Comments (DNAC Wired Team):*
 This is a new validation error msg that has been introduced recently for the L2vn with wireless support.
 [https://cdetsng.cisco.com/summary/#/defect/CSCwd74548]
 [https://cdetsng.cisco.com/summary/#/defect/CSCwd15960] 
 The fix for CSCwd15960 root causes from a CFD found in field where the customer was able to enable wireless on l2vn in Guardian (feature is Ghost release). This led to flood access tunnel clis being configured on the device that led to a device bug which was ultimately fixed in 17.10 image. 
 L2vn wireless enablement (with ap pool in fabric) require flood access tunnel cli
 The flood access tunnel cli require the device image to be 17.10 and above
 Hence if someone now wants to enable wireless on l2vn along with ap pool in fabric they need to have a 17.10 image on the Edge node device.
 This is the bug for the device related issue observed in Guardian due to which the cli was reverted in previous releases [https://cdetsng.cisco.com/summary/#/defect/CSCwc78831] (see description)
 * *Testbed info:* Testbed is in regression currently
 * *Team/Source:* Wireless Solution Sanity Regression Testing Team under Loi",2022-12-08T05:58:50.493+0000,"PR: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4394/diff#configs/config_48hr_test/solution_test_input.json Is this fix permanent or temporary?

As current fix '""isWirelessPool"": ""false"" on JSON will disable wireless for L2 VN irrespective of device versions. 

But to test feature, we need it to be true for device version >= 17.10.1 and false for device version < 17.10.1 [~636ce33a6bbefce0aca3df70], this will be handled along with the l2only feature on a separate PR. but for default, we disable all of the wireless l2only.  ","['Auton', 'Ghost', 'Issue']",Moe Saeed,Resolved,Avril Bower
SEEN-893,https://miggbo.atlassian.net/browse/SEEN-893,[Auton] - Test_TC161_Brownfield_Workflow_9800 / test11_create_BF_profile,"*Reporter Analysis:* Getting time out error while creating brown field profile

*Description:*  

*!image-2022-12-08-11-58-12-903.png!*

*Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* testcases/forty_eight_hour/solution_test_sanityecamb_lan.py / [Test_TC161_Brownfield_Workflow_9800|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1293262&size=2080116&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fenv_auto_job.2022Dec07_03:54:59.633293.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:*  Sanity

*Issue Seen first time or day0 issue:* first time

*Fail Log:* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2280330&size=135436&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fenv_auto_job.2022Dec07_03:54:59.633293.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:* NA

*Testbed details:* AWS sanity",2022-12-08T06:30:44.863+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3bd5ac2148734a8f7d3806cda8d46c001c884fe9]

 

Seems like when model config getting added to the BF profile, somehow all the data in the profile is getting cleared instead. So added some taskstatus reading (since it was missing) to see if it shows an issue there.




This is the data in the NW profile before the model config addition:

{'siteProfileUuid': '2b3018f8-becb-4e12-8b60-2dc2c42a634e', 'version': 1, 'name': 'profile-brownfield', 'namespace': 'wlan', 'status': 'draft', 'lastUpdatedBy': 'admin', 'lastUpdatedDatetime': 1670415382010, 'profileAttributes': [\{'key': 'cli.templates', 'attribs': [{'key': 'device.family', 'value': 'Wireless Controller', 'attribs': [{'key': 'device.series', 'value': '', 'attribs': [{'key': 'device.type', 'value': '', 'attribs': [{'key': 'device.tag', 'value': ''}, \{'key': 'template.id', 'value': '975f6cd3-5ad9-4a1c-8ace-1ffc611ed3b8', 'attribs': [{'key': 'template.version', 'value': '1'}, \{'key': 'template.name', 'value': 'WLC-10.12.5.2-Unsupported'}]}]}]}]}, \{'key': 'device.family', 'value': 'Wireless Controller', 'attribs': [{'key': 'device.series', 'value': '', 'attribs': [{'key': 'device.type', 'value': '', 'attribs': [{'key': 'device.tag', 'value': ''}, \{'key': 'template.id', 'value': '6d67ae07-7def-478d-8d9b-9394e865153b', 'attribs': [{'key': 'template.name', 'value': 'WLC-10.12.5.2-Ignored'}, \{'key': 'template.version', 'value': '1'}]}]}]}]}]}]}

 

This is the payload getting sent to BF profile (basically whatevers above plus the model config stuff appended at the end)


 
{'siteProfileUuid': '2b3018f8-becb-4e12-8b60-2dc2c42a634e', 'version': 1, 'name': 'profile-brownfield', 'namespace': 'wlan', 'status': 'draft', 'lastUpdatedBy': 'admin', 'lastUpdatedDatetime': 1670415382010, 'profileAttributes': [\{'key': 'cli.templates', 'attribs': [{'key': 'device.family', 'value': 'Wireless Controller', 'attribs': [{'key': 'device.series', 'value': '', 'attribs': [{'key': 'device.type', 'value': '', 'attribs': [{'key': 'device.tag', 'value': ''}, \{'key': 'template.id', 'value': '975f6cd3-5ad9-4a1c-8ace-1ffc611ed3b8', 'attribs': [{'key': 'template.version', 'value': '1'}, \{'key': 'template.name', 'value': 'WLC-10.12.5.2-Unsupported'}]}]}]}]}, \{'key': 'device.family', 'value': 'Wireless Controller', 'attribs': [{'key': 'device.series', 'value': '', 'attribs': [{'key': 'device.type', 'value': '', 'attribs': [{'key': 'device.tag', 'value': ''}, \{'key': 'template.id', 'value': '6d67ae07-7def-478d-8d9b-9394e865153b', 'attribs': [{'key': 'template.name', 'value': 'WLC-10.12.5.2-Ignored'}, \{'key': 'template.version', 'value': '1'}]}]}]}]}]}, \{'key': 'NamedCapabilities', 'attribs': [{'key': 'NamedCapability.id', 'value': '', 'attribs': [{'key': 'NamedCapability.name', 'value': 'Advanced_SSID_Configuration'}, \{'key': 'NamedCapability.tag', 'value': ''}, \{'key': 'NamedCapability.description', 'value': 'Brownfield Advanced SSID config'}, \{'key': 'NamedCapability.instances', 'attribs': [{'key': 'NamedCapability.instanceid', 'value': 'b9bc5cd5-07dd-4314-afa7-1ceb665cfe21', 'attribs': [{'key': 'NamedCapability.name', 'value': 'Advanced_SSID_Configuration-bf_sup'}, \{'key': 'NamedCapability.description', 'value': 'Model Config'}, \{'key': 'NamedCapability.tag', 'value': ''}, \{'key': 'NamedCapability.applicability.SSID', 'value': ""$ssId=='bf_sup'""}]}]}]}]}]}
 
And this is the profile data the next time its retrieved after the above call:
 
{'siteProfileUuid': '2b3018f8-becb-4e12-8b60-2dc2c42a634e', 'version': 1, 'name': 'profile-brownfield', 'namespace': 'wlan', 'status': 'draft', 'lastUpdatedBy': 'admin', 'lastUpdatedDatetime': 1670415382010, 'profileAttributes': [\{'key': 'cli.templates', 'attribs': [{'key': 'device.family', 'value': 'Wireless Controller', 'attribs': [{'key': 'device.series', 'value': '', 'attribs': [{'key': 'device.type', 'value': ''}]}]}]}]}
 
As can be seen above, somehow all the data is gone from the profile (except this dummy cli template data). So somehow during the model config api call, everythings getting cleared.
 ","['AWS_Sanity', 'Auton', 'Issue', 'Sanity']",Andrew Chen,Resolved,Avril Bower
SEEN-897,https://miggbo.atlassian.net/browse/SEEN-897,[Auton] -Ghost-Test_TC124_verify_SDA_fabric_issue,"*Reporter Analysis:*   Need to enhance in Regular script & optimization code for generating  VN  DHCP issue.The device version should be validated before generating a VN reachability issue.
If the device image is greater than 17.6, the test should be skipped.

*Description:  The error from log or more info* 

*Branch Name:  private/Ghost -ms/sanity_api_auto*
                                                                                                             **                           *private/Groot -ms/sanity_api_auto***
                            **                           

*Script file/Usecase:* [SDAFabricAssurance|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-145-SDAFabricAssurance&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_optimized_auto_job.2022Nov10_09:50:57.701742.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] 
                                 testcases/forty_eight_hour/solution_test_sanityecamb.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*  

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-145-SDAFabricAssurance&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_optimized_auto_job.2022Nov29_23:03:46.612608.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

**Pass Log:*  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-145-SDAFabricAssurance&begin=834926&size=11140&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_optimized_auto_job.2022Nov10_09:50:57.701742.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS*]

*Testbed details:* NA

DEfect Webspace :webexteams://im?space=c241d550-7283-11ed-b9cb-e9268ee7fef9 ",2022-12-12T05:49:14.154+0000,"MOre detail is needed why is this not required? 

Provide defect/Mailthread with justification. Fixed already by Tran: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/services/dnaserv/lib/api_groups/assurance_fabric/group.py?until=48a432902362a3ad2e9f8759e8766dc8fbae8f9b&at=refs%2Fheads%2Fprivate%2FGhost-ms%2Fapi-auto

======
The version match for 17.10 and 17.6 was not enough for the string match.","['AWS_Sanity', 'Auton', 'Ghost', 'Issue', 'Optimized', 'Sanity']",Pawan Singh,Resolved,Avril Bower
SEEN-898,https://miggbo.atlassian.net/browse/SEEN-898,Auton:Ghost:Sanity:  Test_TC52_DNAC_verify_SSID_lan_on_ECA_device  /   test1_DNAC_verify_SSID_lan_on_ECA_device,"*Reporter Analysis:* 

We observed a recent change in Solution input json as CUSTOM_RF SSID was added and it is assigned to SF site which sanity does not use.
A recent commit where dns server was changed recently ""*[4980032fcd0|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/4980032fcd0c7bece5e7abec4a78be8c02a207b5]""*
SSID created is removed by script

*Description:*  
ERROR Following line of expected cli output not present on device:
27262:  \d+\s+.*custom_rf_ssidtb7\s+UP\s+
27324: 
27325:  ERROR Following line of expected cli output not present on device:
27326:  \d+\s+.*custom_rf_ssidtb7\s+UP\s+
27333:  Failed reason: Result: All WLAN LANS are not UP
*Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* 

solution_test_sanityecamb_lan.py

*Source Team:  Sanity*

*Testcases Impacted:*
Test_TC85_generate_exceutive_summary_report/test3_DNAC_verify_SSID_lan_on_device
Test_TC146_Random_mac_enable/test3_DNAC_verify_SSID_lan_on_device

*Issue Seen first time or day0 issue:*

*Fail Log for TC52:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=66356562&size=31932&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fenv_auto_job.2022Dec08_08:17:00.410325.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Failed Log for TC85:

[test3_DNAC_verify_SSID_lan_on_device|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=44788645&size=189826&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fenv_auto_job.2022Dec08_08:17:00.410325.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Failed log for TC 146:

[test3_DNAC_verify_SSID_lan_on_device|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=86975723&size=186197&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fenv_auto_job.2022Dec08_08:17:00.410325.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:* NA",2022-12-12T09:11:19.060+0000,"Fixed through another auton. This auton is duplicate.

FIx on all branches needed.

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9f2ef84f0a685b43ddc1a3c9efc416e52f56a6e1#services/dnaserv/lib/api_groups/cli_check/group.py]

 

 ","['Auton', 'Ghost', 'Issue', 'Sanity', 'optimized']",Pawan Singh,Resolved,Avril Bower
SEEN-901,https://miggbo.atlassian.net/browse/SEEN-901,[Auton] [Ghost] - [16ssid limit] Error in Adding GUEST SSID to Network profile,"* *DNAC Release_Version Tested:* Ghost RC3 Uber ISO - 2.1.610.70583, FIPS
 * *Device Image Used:* 17.6.5
 * *Testbed:* Multisite Non-DR setup
 * *Branch Used:* private/Ghost-ms/api-auto
 * *Script Name:* solution_test_3sites_sjc_nyc_sf.py
 * *Solution Input File :* dnac-auto/configs/config_48hr_test/solution_test_input_FIPS.json
 * *Testcases Impacted:* Test_TC22_DNAC_verify_creating_wireless_guest_portal
 * *Failed Trade Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1232792&size=2823108&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fauto_MS_job.2022Dec12_06:37:49.590043.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
 * *Issue details/analysis:* Adding SSID to the network profile is getting failed because more SSIDs were added to Network Profile. We are seeing this issue after recent changes in the solution input JSON file where a new SSID was added ""custom_rf_ssid"". Because of this, we have more than 16 SSIDs mapping to the same network profile.
For verifying the clear pass feature we need SSID ""CWA_GUEST_SSID_AAAMS2"" to be added to the network profile which is not happening because of network profile has more than 16 SSIDs. This issue is seen after the addition of a new SSID ""custom_rf_ssid"" in JSON. 
For making progress in Regression Execution, we chose to remove custom ""custom_rf_ssid"" from DNAC GUI and later when tried execution it went fine. but we would want this issue to be fixed.

 * *Failed Log Snapshot :* 
Message:\{""response"":{""errorCode"":""NCND00002"",""message"":""NCND00002: The Network Profile request has validation errors"",""detail"":""[\""NCND01175: This Profile cannot be assigned to more than 16 SSIDs\""]""},""version"":""1.0""}
 * *Solution Input File Changes:* !image-2022-12-14-10-37-26-369.png!

 

 ",2022-12-14T05:25:43.853+0000,"HI,

We are seeing the same issue while integrating the EDIT SITE NAME Feature from Groot. We have seen the same issue like being unable to add SSID because of having more than 16 SSID's to the networking profile. I have tried deleting the above added Clear Pass SSID and tried adding this SSID. It went fine because of having space to create one more ssid after deleting.

Because we cannot manually delete some SSIDs in each run to continue testing, and adding the ids as needed is not possible.

*Failed Log:* [[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4701974&size=159438&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fauto_MS_job.2022Dec15_01:49:26.381025.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


 *Error Snip :* 
 Message:{""response"":

{""errorCode"":""NCND00002"",""message"":""NCND00002: The Network Profile request has validation errors"",""detail"":""[\""NCND01175: This Profile cannot be assigned to more than 16 SSIDs\""]""}

,""version"":""1.0""} [~62d2fe9f8afb5805e5d5af49] / [~63f50bcece6f37e5ed93c87e], as discussed over the call, we need to review the test/use-cases that are creating the SSIDs during runtime and not cleaning up while concluding the same.
This will avoid hitting the limit of more than 16 SSIDs. This issue also observed on AWS-MS profile while testing Ghost RC4-2.1.610.70586.This is further impacting integration act activity where new SSID's has to be added 
Log:https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1380755&size=2832127&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fauto_MS_job.2023Jan05_00:59:40.343386.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS PR:
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4611/overview I have been observing the same issue on Ghost P1-2.1.613.70114.
Trade Log:https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1264309&size=2808851&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fauto_MS_job.2023Mar06_04:11:13.824960.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5114/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5114/overview] Same  issue is also observed during Halleck execution on Sanity AWS-cluster:
*Uber ISO Version tested :* Halleck 2.1.660.70326
*Script Name:* Optimized code 
\testcases\sanityusecases\advancedWlanConfigs\advanced_wlan_configs.py
*Testbed :*TB11(AWS)
*Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-1310-advancedWlanConfigs&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_optimized_auto_job.2023Mar31_07:00:57.378251.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-1310-advancedWlanConfigs&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_optimized_auto_job.2023Mar31_07:00:57.378251.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
*Error Snip :*  

{noformat}Message:{""response"":{""errorCode"":""NCND00002"",""message"":""NCND00002: The Network Profile request has validation errors"",""detail"":""[\""NCND01175: This Profile cannot be assigned to more than 16 SSIDs\""]""},""version"":""1.0""}
171:  Traceback (most recent call last):{noformat} Following PRs: to address this issue:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5114/diff#services/dnaserv/lib/api_groups/wireless_guest/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5114/diff#services/dnaserv/lib/api_groups/wireless_guest/group.py]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5260/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5260/overview] Hi [~accountid:63f50bfce8216251ae4d59d5] 
Same issue is also observed during Ghost  execution on Sanity Tb7 Cluster Ghost P1 RC5 (2.1.613.70190):
*Uber ISO Version tested :* Ghost P1 RC5 (2.1.613.70190)
*Script Name:* Optimized code 
\testcases\sanityusecases\advancedWlanConfigs\advanced_wlan_configs.py
*Testbed :*TB11(AWS)
*Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-1310-advancedWlanConfigs&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr27_23:38:27.229556.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-1310-advancedWlanConfigs&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr27_23:38:27.229556.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
*Error Snip :* 

{code:python}Message:{""response"":{""errorCode"":""NCND00002"",""message"":""NCND00002: The Network Profile request has validation errors"",""detail"":""[\""NCND01175: This Profile cannot be assigned to more than 16 SSIDs\""]""},""version"":""1.0""}{code}


Could you please check [~accountid:63f50bf5e8216251ae4d59cf] , can you push the changes of this PR to Ghost too? 

[Pull Request #5260: add ssid custom_rf_ssid for testcase SEEN-1379 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5260/diff#configs/config_48hr_test/solution_test_input.json] Hi [~accountid:63f50bfce8216251ae4d59d5] ,
we observed during the execution of Halleck on the Sanity Tb7 cluster Halleck Respin 2.1.660.70351
TC failed while adding SSID but I can add ssid manually,
Could you please check 

+*Failed Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-131-advancedWlanConfigs&begin=10874&size=658768&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May09_04:20:32.569486.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-131-advancedWlanConfigs&begin=10874&size=658768&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May09_04:20:32.569486.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

+*Manually added   ssid snap:*+


!image-20230509-121652.png|width=1247,height=142! [~accountid:620b8357878c2f00729881c8] the ssid got created fine based on the logs, but the issue with adding the segment to it and need further debugging","['AWS-Santiy', 'Auton', 'Ghost', 'Guardian', 'Halleck', 'Issue', 'MSTB2', 'Multisite', 'Optimized']",Moe Saeed,Resolved,Avril Bower
SEEN-902,https://miggbo.atlassian.net/browse/SEEN-902,Test_TC32_Compliance_verification/test7_verify_Fabric_compliance,"Cluster Upgraded from Ghost RC1<>Halleck

*Reporter Analysis:* 

Compliance TC is false passing even all devices are in non-compliance state Tc is passing

*Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* after_upgrade_verify.py

*Source Team: Upgrade Sanity*

*Issue Seen first time or day0 issue:*

*False Pass Log:*
**[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3032819&size=12783590&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F12%2F14%2F21%2F14%2Fenv_auto_job.2022Dec14_21:14:36.274043.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:* NA",2022-12-15T08:57:26.330+0000,"API logs do not show any non-compliance. So no false pass chance here.

 

click on each non-compliance and collect details of what is non-compliant.  Also, match the timing with the script, if you are checking later in UI, that won't be valid to decide the test case status, as it may become non-compliant later. you should check in detail and debug why did it become non-compliant. Info is not enough to determine the issue. Get more info with matching time for test case execution completion, and detailed compliance details by clicking on device details and seeing the compliance summary.

 

Test also on purpose make devices to non-compliant, so, don't check UI at that time as it is expected and checked for it. Just check right after test completion. Hi [~accountid:712020:beae19f6-b61e-4276-9a74-490d9a540df7] 

Closing this Jira as on Upgrade script side 

[dnac-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse?at=private/Ghost-ms/sanity_api_auto]/[testcases|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases?at=private/Ghost-ms/sanity_api_auto]/[upgrade|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/upgrade?at=private/Ghost-ms/sanity_api_auto]/*after_upgrade_verify.py*


Under Compliance TC sub tc:
def test10_verify_overall_compliance_status_for_devices(self, dnac_handle):
was not present
Added the subtc and now compliance is failing if issue is there on DNAC side related to Compliance

Thanks,
Anusha John Hi Anusha, This is wrong userid..you should sent “pawansi” not “pawans” Hi [~accountid:5f3c6ae932360700388f7b4b] 

Closing this Jira as on Upgrade script side

[dnac-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse?at=private/Ghost-ms/sanity_api_auto]/[testcases|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases?at=private/Ghost-ms/sanity_api_auto]/[upgrade|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/upgrade?at=private/Ghost-ms/sanity_api_auto]/*after_upgrade_verify.py*



Under Compliance TC sub tc:
def test10_verify_overall_compliance_status_for_devices(self, dnac_handle):
was not present
Added the subtc and now compliance is failing if issue is there on DNAC side related to Compliance

Thanks,
Anusha John","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'Upgrade']",Anusha John,Closed,Avril Bower
SEEN-927,https://miggbo.atlassian.net/browse/SEEN-927,Auton: Test_TC70_DNAC_verify_assurance_health_data_after_traffic_run  /   test1_verify_assurance_health_nw_health_border_node,"Affected TC's:
[Test_TC81_verify_nw_device_global_assurance_metrics|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=376162&size=27352&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fenv_auto_job.2022Dec16_21:20:20.821749.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&atstype=PYATS]  /   test1_verify_assurance_health_nw_health_ext_node 

 

*Reporter Analysis:* 
In DNAC ,all devices have health 10.Script is thrwoing sttribute error

*Description:*  
**
2188:  File ""/home/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Fury/Fury-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 1648, in verify_ext_node_in_assurance_by_mac_and_ip
2189:  mac= self.services.dnaconfig.testbed.devices[dev].inv_data[""macAddress""]
2190:  File ""/users/rvonmize/pyats-inst/lib/python3.7/site-packages/pyats/topology/device.py"", line 484, in __getattr__
2191:  % attr) from None
2192:  AttributeError: 'Device' object has no attribute 'inv_data'
2195:  Failed reason: Result: extended node assurance data failed.
** 

*Branch Name:  private/Fury-ms/sanity_api_auto*

*Script file/Usecase:*
solution_test_sanityecamb.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=342937&size=3943&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F12%2F16%2F21%2F20%2Fenv_auto_job.2022Dec16_21:20:20.821749.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS**",2022-12-19T13:08:47.557+0000,"PR link: https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4636/overview PR approved, raise for other branches if. needed. ","['Auton', 'Fury', 'Issue']",ThangQuoc Tran,Resolved,Avril Bower
SEEN-928,https://miggbo.atlassian.net/browse/SEEN-928,[Auton][Ghost]- Script changes need on Advanced WLAN Configs Feature script,"* *DNAC Release_Version Tested:* Ghost RC3 Uber ISO - 2.1.610.70583, FIPS
 * *Device Image Used:* 17.6.5
 * *Testbed:* Multisite Non-DR setup
 * *Branch Used:* ghost-handoff-111822
 * *Script Name:* solution_test_3sites_sjc_nyc_sf.py
 * *Testcases Impacted:* [Test_TC183_advanced_wlan_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1193793&size=890282&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F12%2F18%2F20%2F51%2Fsr_mb2_three_sites_FIPS.2022Dec18_20:51:43.697585.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]
 * *Failed Trade Log:(Please Refer TC183)* [https://earms-trade.cisco.com/tradeui/logs/details?ats=%2Fusers%2Frvonmize%2Fpyats-inst&client=web&host=st-ucs-sanity.cisco.com&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fsr_mb2_three_sites_FIPS.2022Dec18_20:51:43.697585.zip]
 * *Issue details/analysis:* In case of Sub TC failures in the Testcase, Instead of blocking the remaining subTC it was executing the next TC. Here in the above log test 2 Adding SSID got failed. But instead of blocking the script went forward to the next case and performed verified device configs and cleanup of wireless SSID configs. So need to enhance the script to make other related TCs blocked if the dependent TC Fails. Also this would save the run time execution, as in case of failures, the script keeps running in a loop trying to validate expected outputs.",2022-12-20T07:08:58.142+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4673/overview,"['Auton', 'Ghost', 'Issue', 'MSTB2', 'Multisite']",Andrew Chen,Resolved,Avril Bower
SEEN-929,https://miggbo.atlassian.net/browse/SEEN-929,Auton] - Script changes need on Edit sites name Feature from Groot,"* *DNAC Release_Version Tested:* Ghost RC3 Uber ISO - 2.1.610.70583, FIPS
 * *Device Image Used:* 17.6.5
 * *Testbed:* Multisite Non-DR setup
 * *Branch Used:* ghost-handoff-111822
 * *Script Name:* solution_test_3sites_sjc_nyc_sf.py
 * *Testcases Impacted:* Test_TC177_edit_site_name
 * *Failed Trade Log (Please Refer TC177):* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fauto_MS_job.2022Dec15_01:49:26.381025.zip&atstype=ATS]
 * *Issue details/analysis:* In case of Sub TC failures in the Testcase, Instead of blocking the remaining subTC it was executing the next TC. Here in the above Adding SSID test case got failed. But instead of blocking the script went forward to the next case and onboarding wireless segment on SSID, provision of device. Actually, without adding SSID it should not continue to the next sub Testcase. So need to enhance the script to make other related TCs blocked if the dependent TC Fails. Also, this would save the run time execution, as in case of failures, the script keeps running in a loop trying to validate expected outputs.",2022-12-20T07:15:43.073+0000,"This ticket is not applicable. SSID has nothing to do with verification of site, this is a {color:red}bug{color}!!!
You should have filed a defect. 
Also, you need to run the following cases even it failed so it can revert the sites back otherwise you will be blocked!!

I am junking this ticket!","['Auton', 'Ghost', 'Groot', 'Issue', 'MSTB2', 'Multisite']",Moe Saeed,Closed,Avril Bower
SEEN-939,https://miggbo.atlassian.net/browse/SEEN-939,Ghost RC4 build usecase:- LAN Automation Loopback ip address  is not taking up for TB12-NY-FIAB,"*Reporter Analysis:* 

*Description:* 
 Loopback ip address is not taking up

*IMAGE BUILD VERSION* : Ghost 2.1.610.70586

*Error:*

{color:#1e252c}2022-12-19T05:31:05: %CLIENTMANAGER-INFO: Resource path full url: [https://10.22.40.47/api/v1/network-device/101/100]{color}{color:#1e252c}2022-12-19T05:31:05: %API-GROUP-INVENTORY-INFO: Breaking, as no more entries left{color}{color:#1e252c}2022-12-19T05:31:05: %ATS-INFO: Library group ""inventory"" method ""update_stored_network_info"" returned in 0:00:00.144180{color}{color:#1e252c}2022-12-19T05:31:05: %API-GROUP-INVENTORY-ERROR: {color}{color:#cd0000}Not found... returning None{color}{color:#1e252c}2022-12-19T05:31:05: %ATS-INFO: Library group ""inventory"" method ""get_network_device_info"" returned in 0:00:00.144562{color}{color:#1e252c}2022-12-19T05:31:05: %ATS-INFO: Library group ""inventory"" method ""find_uuid_of_device"" returned in 0:00:00.144799{color}{color:#1e252c}2022-12-19T05:31:05: %ATS-INFO: Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:00.144940{color}{color:#1e252c}2022-12-19T05:31:05: %ATS-INFO: Library group ""cli_check"" method ""get_lb_ip"" returned in 0:00:00.145086{color}{color:#1e252c}2022-12-19T05:31:05: %SERVICES-INFO: Got loopback of device:*TB12-NY-FIAB*, loopback_ip *received:None*{color}{color:#1e252c}Processing usecase:LANAutomation{color}

*Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:* 

[TRADe | File Viewer: env_optimized_auto_job (cisco.com)|https://earms-trade.cisco.com/tradeui/logs/fileviewer?logfile=TB12-NY-FIAB-cli-1671455320.log&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fenv_optimized_auto_job.2022Dec19_05:06:15.514518.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=pyATS&from=trade&view=all]

 

[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/Express-Sanity/job/Ghost/job/Ghost-Optimized-Deployment_and_Express_Sanity/43/consoleFull]

 

*Testbed details:* 

telnet 10.22.45.9 2008

 

TB12-NY-FIAB#show running-config interface Loopback0 | include address

 *ip address 204.1.2.3* 255.255.255.255

TB12-NY-FIAB#

TB12-NY-FIAB#

 

2022-12-19 05:16:03,458: %UNICON-INFO: +++ TB12-NY-FIAB with via 'a': executing command 'show running-config interface Loopback0 | include address' +++ show running-config *interface Loopback0 | include address* *ip address 204.1.2.3* 255.255.255.255 TB12-NY-FIAB#

 

 ",2022-12-21T17:01:36.149+0000,"The loopback IP is learned from DNAC, the IP can be learned only if the device is in inventory. 

 

Per your logs, the device is not in inventory. so the error is expected.  get to the original issue whey is the device not in inventory? (did LAN automation not complete) if so raise product bug.

 ","['Auton', 'Issue', 'Sanity']",Divya Prabhalika,Resolved,Avril Bower
SEEN-1019,https://miggbo.atlassian.net/browse/SEEN-1019,[Auton]:Ghost:Test_TC94_generate_link_flap_issues/test1_generate_link_flap_issues,"*Reporter Analysis:* 

For any device role link flap issue should be generated

*Description:*  
Message:{""message"":""Role does not have valid permissions to access the API""}

*Branch Name: private/HulkPatch-ms/api-auto*

*Script file/Usecase:* 

generateLinkFlapIssues.py

*Source Team:  Sanity*

*Fail Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-generate_link_flap_issues.py-184-generateLinkFlapIssues&begin=4785&size=63560&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep25_21:04:32.896955.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-generate_link_flap_issues.py-184-generateLinkFlapIssues&begin=4785&size=63560&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep25_21:04:32.896955.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:* NA",2023-01-09T16:30:01.022+0000,"Guardian: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/1fa84aac7fdb38145ff5221aca343c9e4bfd3d8a]

Groot: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0bd9cdf03f261696dc3ad75255b772be7f37bba8]

Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ef307790a2846ecb1cfcecba4b7695265e731d38]

 

Fixed.","['Auton', 'Ghost', 'Issue', 'Sanity']",Pawan Singh,Resolved,Avril Bower
SEEN-1020,https://miggbo.atlassian.net/browse/SEEN-1020,Auton:[Ghost]Test_TC156_Validate_sda_multicast_external_apis/test5_add_multicast_with_asm_external_rp_method_ipv4,"*Reporter Analysis:* 

The Script is checking for wrong ""siteNameHierarchy = Global/USA/BayAreaGuest""
We have only SANJOSE/NEWYORK Sites

*Description:* 
**
120488:  ""description"" : ""multicast is not configured on given siteNameHierarchy. siteNameHierarchy = Global/USA/BayAreaGuest"",
 

*Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* 

solution_test_sanityecamb.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=43490741&size=910647&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_auto_job.2023Jan08_00:50:37.534107.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

 

*Testbed details:* NA",2023-01-09T16:38:48.067+0000,"This is a product issue:

==============

It is doing it on San Jose. 

Raise a bug on sda-api for failing to enable multicast through APIs. 

 
 Resource path full url: [https://10.195.227.80/dna/intent/api/v1/business/sda/multicast]
124566:  Error Code: 400 URL:https://10.195.227.80/dna/intent/api/v1/business/sda/multicast Data:\{'timeout': 60, 'data': '{""siteNameHierarchy"": ""Global/USA/SAN JOSE"", ""multicastMethod"": ""native_multicast"", ""multicastType"": ""asm_with_external_rp"", ""multicastVnInfo"": [{""virtualNetworkName"": ""WiredVNFBLayer2"", ""ipPoolName"": ""multicast_sub"", ""externalRpIpAddress"": [""204.192.3.40""], ""ssmInfo"": []}, \{""virtualNetworkName"": ""WirelessVNFB"", ""ipPoolName"": ""multicast3_sjc"", ""externalRpIpAddress"": [""204.192.3.40""], ""ssmInfo"": []}]}'} Headers:\{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2M2I4NTM0MDdiMzI0NDBkYmM0YzY2OWYiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjYzYjg1MzNmN2IzMjQ0MGRiYzRjNjY5ZSJdLCJ0ZW5hbnRJZCI6IjYzYjg1MzNlN2IzMjQ0MGRiYzRjNjY5YyIsImV4cCI6MTY3MzE5ODg1NiwiaWF0IjoxNjczMTk1MjU2LCJqdGkiOiI3MmY5ZmI2MC1kMzA4LTQ4OWYtYTZhYi1hNjQwZjE2NDRmNzciLCJ1c2VybmFtZSI6ImFkbWluIn0.OJYh19BtpO9nxLOP0prp1GLgqeKsFYFPxUrPgykA182QLHHWpi50xbPEXV3a99R9t-EWCv_134bPgxOACUj7OqfaZR_AG4Ki8ePkWG8dYLrEufqozpcu4YOIHcSjz2KLlWr4Su5w576qCGoFAmv_ZtoGkkQ7deDWMqzJJ7xgnmbPGJdmk5GW_qVtvWmdKfPjDryuFCIDlelW9spfUYOGIuj4gPaigWAmDyBgd4LItFyGZa1DM-XttwkkN-QU8uCfwdjEdstGk7r-knufBSPuhLEQNTdgwU1DzF4WwN2KU5hi42p5-VNmbGjn1G7osZdgMFFruBFL3LB39L5Sz2itmA;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:{
124567:  ""status"" : ""failed"",
124568:  ""description"" : ""'list' object has no attribute 'strip'"",
124569:  ""taskId"" : null,
124570:  ""taskStatusUrl"" : ""/dna/intent/api/v1/task/null"",
124571:  ""executionStatusUrl"" : ""/dna/intent/api/v1/dnacaap/management/execution-status/862237da-f45c-4442-8ee1-90dd258e4b34"",
124572:  ""executionId"" : ""862237da-f45c-4442-8ee1-90dd258e4b34""
124573:  }
 
124568: ""description"" : ""'list' object has no attribute 'strip'"",
 
 
 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=44355640&size=45569&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_auto_job.2023Jan08_00:50:37.534107.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
 
 
-Pawan
 
 ","['Auton', 'Ghost', 'Issue', 'Sanity']",Pawan Singh,Resolved,Avril Bower
SEEN-1054,https://miggbo.atlassian.net/browse/SEEN-1054,Syntax error in solution_test_sanityecamb_lan.py script,Syntax error in solution_test_sanityecamb_lan.py script. !Screenshot 2023-01-13 at 8.51.00 AM.png!,2023-01-13T16:58:57.521+0000,"Required changes have been pushed with below PR to Ghost branch and cherry-picked to Groot Branch:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4540/overview]

 ","['Auton', 'Ghost', 'Groot', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-1055,https://miggbo.atlassian.net/browse/SEEN-1055,Address UnboundLocalError  Test_TC30_SWIM_UPGRADE_ECA_DEVICE  /   test12_verify_upgrading_os_image,"Address UnboundLocalError Test_TC30_SWIM_UPGRADE_ECA_DEVICE / test12_verify_upgrading_os_image

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2504076&size=58748&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fauto_MS_job.2023Jan11_09:57:55.045503.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

!UnboundLocalError.png!",2023-01-13T22:54:32.632+0000,"PR for required changes have been raised against Ghost Branch: 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4543/overview]

 

Once approved and merge, will cherry-pick or double commit to other relevant branches. Required changes have been merged to Ghost, Groot and Guardian Branches.

Marking this ticket as ""Done"".","['Auton', 'Ghost', 'Groot', 'Guardian', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-1064,https://miggbo.atlassian.net/browse/SEEN-1064,[Auton][Ghost]- Script changes needed for AP Zone Feature,"* *DNAC Release_Version Tested:* Ghost RC4 Uber ISO - 2.1.610.70586, FIPS
 * *Device Image Used:* 17.6.5PRD7FC1
 * *Testbed:* Multisite Non-DR setup
 * *Branch Used:* private/Ghost-ms/api-auto
 * *Script Name:* solution_test_3sites_sjc_nyc_sf.py
 * *Failed Trade Log:(Please Refer TC187) : [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fauto_MS_job.2023Jan04_05:45:32.642205.zip&atstype=ATS]*
 * *Issue details/analysis:* While running the script from solution_test_3sites_sjc_nyc_sf.py file in private/Ghost-ms/api-auto we are seeing [Test_TC187_AP_Zone|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1693428&size=10517&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fauto_MS_job.2023Jan04_05:45:32.642205.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]/cleanup_AP_zone is running in all suites irrespective of test cases. In the above log, I have tried running TC10, TC11, and TC12, But TC187 also ran automatically and got errored. In Every run we are seeing TC187 is running automatically. We need this to be fixed not to run TC187 cleanup_AP_zone in all runs. ",2023-01-19T09:40:18.111+0000,"https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4571/overview Hi Raji

Still, we are seeing the issue. [cleanup_AP_zone|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7408158&size=927039&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fauto_MS_job.2023Jan25_01:20:18.464361.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] is still running in every suite after changing aetest.cleanup to aetest.test for AP zone cleanup also. Seems we have to add ""@TestWrapper()"" also to the test case.

Failed log(Please refer TC187) : [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fauto_MS_job.2023Jan25_01:20:18.464361.zip&atstype=ATS]




  Still, we are seeing the issue. [cleanup_AP_zone|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7408158&size=927039&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fauto_MS_job.2023Jan25_01:20:18.464361.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] is still running in every suite after changing aetest.cleanup to aetest.test for AP zone cleanup also. Seems we have to add ""@TestWrapper()"" also to the test case.

Failed log(Please refer TC187) : [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fauto_MS_job.2023Jan25_01:20:18.464361.zip&atstype=ATS]

  Hey Raji,

Re-opening the Jira ticket as the issue is still seen on *private/Halleck-ms/api-auto* branch.

*Failed log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fsr_mb_multi_sites_mdnac.2023Feb19_07:59:11.637006.zip&atstype=ATS] -> Refer TC251

 

Could you please add the fix on Halleck branch? [~62d2fec15d6f5fd2c3db8f9f] Halleck branch solution_test_3sites_sjc_nyc_sf_mdnac_dr.py has the fix.

@aetest.test
def cleanup_AP_zone(self,dnac_handle)

 

  Raji, can. you pull it in other scripts also?  [~5f3c6ae932360700388f7b4b] Fix is already added to all scripts in Ghost and Halleck","['Auton', 'Ghost', 'Halleck', 'Issue', 'MSTB1', 'MSTB2', 'MSTB3', 'Multisite']",Raji Mukkamala,Closed,Avril Bower
SEEN-1065,https://miggbo.atlassian.net/browse/SEEN-1065,[Auton][Groot] [Guardian] - Re-add device to update Fabric failing in ITSM feature testcase,"*Uber ISO Version tested :* 
Promoted Guardian Patch4 Uber ISO - *2.1.518.72230**, Non-FIPS, PUBSUB enabled*

*Script Name:*  solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py

*Testbed :* MSTB1
 

*Description :* 

This is related to ITSM ticket generation Guardian feature. We see all SubTCs got passed except for last subTC - test13_readd_dev (Re-adding the 5520-2 Aireos device to fabric), where Fabric Update update operation has got failed (Please refer screenshot).

!1316_613_1.5.png!

After the execution got completed when checked from DNAC GUI, we could see the 5520-2 device was added to fabric and when we did a Re-provisioning of the device, the operation was successful. So may be we need to wait for more time for the operation to complete.



*Failed log:* 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fsr_mb_multi_sites_mdnac.2023Jan19_02:28:23.778986.zip&atstype=ATS] -> Refer TC224

 

Please refer team space webexteams://im?space=bfcf9d60-903e-11ed-84cc-4b71e6df604f for more details.",2023-01-19T15:27:19.119+0000,"Looks like the task did fail though in ui, so this should be a product issue. If it was added to fabric, then it should not say operation failed. Since operation was failed on ui but device was added to fabric, should be a product issue. Currently closing the Jira as we were not able to reproduce the issue. However we will continue to monitor for the issue in next subsequent regression cycles.

*Uber ISO version tested* - Promoted Guardian Patch4 Pre-RC2 Interim Build : 2.1.518.72292

*Pass log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fsr_mb_multi_sites_mdnac.2023Feb06_23:25:56.345473.zip&atstype=ATS] (Refer TC224)","['AWS_MSTB', 'Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'MSTB1', 'Multisite']",Andrew Chen,Closed,Avril Bower
SEEN-1066,https://miggbo.atlassian.net/browse/SEEN-1066,[Auton] - Script changes need on ITSM feature TCs to handle failure scenarios,"*Uber ISO Version tested :* 
Promoted Guardian Patch4 Uber ISO - *2.1.518.72230**, Non-FIPS, PUBSUB enabled*

*Script Name:*  solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py

*Testbed :* MSTB1

*Description :* 

In case of TC failures in each of the sub TC under ITSM feature related TC, we need to add Action on Failure to be Blocked for all the subsequent subTCs as any subTC Failure is not expected behavior and could be a defect.

Failed log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fsr_mb_multi_sites_mdnac.2023Jan18_23:03:06.256350.zip&atstype=ATS] -> Refer TC224

This has been already discussed. Please refer team space webexteams://im?space=bfcf9d60-903e-11ed-84cc-4b71e6df604f for more details.",2023-01-19T15:43:04.969+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4601/overview] Guardian

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4603/overview]

Groot

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4602/overview]

Ghost [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4601/overview] Guardian

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4603/overview]

Groot

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4602/overview]

Ghost The code changes has required changes.

*Uber ISO version tested* - Promoted Guardian Patch4 Pre-RC2 Interim Build : 2.1.518.72292

*Pass log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fsr_mb_multi_sites_mdnac.2023Feb06_23:25:56.345473.zip&atstype=ATS] (Refer TC224)","['AWS_MSTB', 'Auton', 'Ghost', 'Groot', 'Guardian', 'Issue', 'MSTB1', 'MSTB2', 'Multisite']",Andrew Chen,Closed,Avril Bower
SEEN-1078,https://miggbo.atlassian.net/browse/SEEN-1078,[AUTON] [GHOST] SWIM Customer Tag -test11_check_tag_precedence_for_upgrade,"* *DNAC Release_Version Tested:* Ghost RC4 Uber ISO - 2.1.610.70586, Non-FIPS
 * *Device Image Used:* 17.10.1
 * *Testbed:* AWS-Multisite
 * *Branch Used:* private/Ghost-ms/sanity_api_auto
 * *Script Name:* solution_test_3sites_sjc_nyc_sf.py
 * *Solution Input File :* dnac-auto/configs/config_48hr_test/solution_test_input.json
 * *Testcases Impacted:* [Test_TC30_SWIM_UPGRADE_ECA_DEVICE|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=843635&size=3478500&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fauto_MS_job.2023Jan20_03:15:29.079267.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
 * *Failed Trade Log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-01/auto_MS_job.2023Jan24_05:53:24.040341.zip&atstype=ATS] --Refer TC30
 * *Issue Analysis*:*check_tag_precedence_for_upgrade is getting errored.*This use case to see if the customer tag has taken proper precedence over the golden image and also have negative test cases included in it. In this a random 9300 device is taken for testing the feature, then data is collected from inventory and image repository pages to use in main logic.",2023-01-24T06:50:01.240+0000,"Hi Moe,were you able to check on this issue? [~accountid:63f50bfce8216251ae4d59d5] ,Could you please check on this issue asap? Tried executting SWIM customer Tag fetaure on Ghost P1 RC1,we are hitting the same issue as above and also observed one more failure as we had lost connection to SWIM server abruptly and thus failing to fetch images.Gave a re execution again,but meanwhile we lose the cluster access as new AMI with Hulk is coming over.Need to check further.

Failed Log:[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-04/auto_MS_job.2023Apr11_09:08:27.210950.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-04/auto_MS_job.2023Apr11_09:08:27.210950.zip&atstype=ATS] PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5355/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5355/overview]","['AWS_MSTB', 'Auton', 'Ghost', 'Issue', 'MSTB2', 'MSTB3', 'Multisite']",Moe Saeed,Resolved,Avril Bower
SEEN-1079,https://miggbo.atlassian.net/browse/SEEN-1079,TB11(DMZ)WEB server  traffic not getting to wired client 1 & wired clent 2,"Hi Team,

TB11(DMZ) WEB server traffic not getting to wired client & wired client 2

VSphere   -->[https://10.4.1.250/            (admin|https://10.4.1.250/] [@cisco.|mailto:assurance@cisco.local] [cloud /C1sco123!)|https://10.4.1.250/]

+*wired client  Details*+
 +*wired Client 1*+ -------------> GigabitEthernet1/0/11(vmnic2)------------>TB2-DMZ-SJ-FIAB-ECA.cisco.com
  
 +*Wired client 2*+--------->     GigabitEthernet1/0/11  (vmnic-3)    ----------->TB2-DMZ-NY-FIAB.cisco.com

+*Webserver*+ ----ip 87.1.1.11(vlan 101)

wiki:
 [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-11]

 +**IP Address Distribution for each TB.xlsx*+
 [https://cisco-my.sharepoint.com/:x:/r/personal/rajsaran_cisco_com/Documents/Cloud%20DMZ/IP%20Address%20Distribution%20for%20each%20TB.xlsx?d=wcea0cf5e65c443be87644ba00ae07790&csf=1&web=1&e=dbIiHv]] [
could you please check
  
  Regards,
 Omkar ",2023-01-25T10:56:16.294+0000,"This is testbed issue. Please debug and fix on your testbed. Make sure you have right routing in fusion, and webserver.","['Auton', 'Groot', 'Issue']",Tran Lam,Resolved,Avril Bower
SEEN-1082,https://miggbo.atlassian.net/browse/SEEN-1082,Add more logging in Test_TC126_verify_inventory_insights / test2_verify_VLAN_mismatch,"Add better logging in Test_TC126_verify_inventory_insights / test2_verify_VLAN_mismatch:
 # increase wait time from 30 to 60 seconds to let DNAC Catch up with the logging.
 # output of - show running-config | sec logging

 

Will help to add more details in case of failures.",2023-01-26T17:47:05.383+0000,"Raised required PR with the working changes for Ghost Branch:
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4610/overview changes have been merged to Ghost and Halleck.

For Groot and Guardian, need to raise separate PR due to Conflicts.. Got pass log with Guardian p4 rc4-rspin,TB6: [Test_TC126_verify_inventory_insights|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3796841&size=181103&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May26_04:28:26.127136.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS].","['Auton', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-1083,https://miggbo.atlassian.net/browse/SEEN-1083,Test_TC136_enable_ICMP_ping_check_AP_reachability / test1_enable_icmp_verify_ap_reachability is applicable to WLC only,"Test_TC136_enable_ICMP_ping_check_AP_reachability / test1_enable_icmp_verify_ap_reachability is applicable to WLC only

Also add a try except block to handle the prompt issue with WLC device while submitting

""_config interface ap-manager management disable""_ command to it.",2023-01-26T18:46:48.944+0000,"raised PR for the required change:

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4605/overview PR got approved and merged to Halleck, Ghost, Groot & esxivm_branch.","['Auton', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-1091,https://miggbo.atlassian.net/browse/SEEN-1091,[Auton]Hulk: DNAC_Prime_Migration_DNAC_Server_Settings_NTP_DHCP_AAA_DNS_settings,"*Reporter Analysis:* The same testcase is been executed twice. under [ISEIntegration and |https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_integration_verification_aca_sync.py-32-ISEIntegration&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan26_14:51:55.458200.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [primeMigration of UC3.|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-prime_migration_validation.py-33-primeMigration&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan26_14:51:55.458200.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

So we need to skip the testcase under primeMigration.

*Description:  The error from log or more info* 

*Branch Name:  private/Ghost-ms/sanity_api_auto*
*Script file/Usecase:* [{color:#007ebd}primeMigration{color}|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-prime_migration_validation.py-33-primeMigration&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan26_14:51:55.458200.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-prime_migration_validation.py-33-primeMigration&begin=3919&size=344243&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan26_14:51:55.458200.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_integration_verification_aca_sync.py-32-ISEIntegration&begin=4128&size=329687&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan26_14:51:55.458200.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8]",2023-01-30T07:59:48.206+0000,"This is an expected failure. The message is clear that AAA server is already added. 

If you are intentionally trying to re-add then just ignore it.
632:  !!!!!!!!!!!!Error in Configuring AAA Server, Already Configured!!!!!!!!!!!!!!!!!!
731:  Failed reason: Result : Adding servers failed
  Hi Pawan 
yes it is expected failure. This testcase has been repeated and is included as part of *ISEIntegration* and also *primeMigration* use case. 

Can we plan to place this test in one of the usecases. So that we can avoid the break on fail during optimized runs [ENG-SDN / dnac-auto / 54725273844 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/54725273844adb053210751719f2c57d227b441f]

Ghost commit, also pushed to halleck and hulk



Moved server cases out of prime migration and moved iseIntegration to after primeMigration instead of parallel","['Auton', 'Ghost', 'Issue', 'optimized', 'sanity']",Andrew Chen,Resolved,Avril Bower
SEEN-1092,https://miggbo.atlassian.net/browse/SEEN-1092,[Auton]Ghost: Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x/Test_TC3_DNAC_configure_multicast_on_sites/configure_multicast_on_site_sf,"*Reporter Analysis:* while the script is trying to configure multicast on sites, as there is no sites configured under san francisco, it is throing an error like RP devices are required

Snippet : 
 14994: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/fabric_multicast/group.py"", line 76, in _multicast_config_ip_pool_vn_mapping
 14995: raise ValueError(msg)
 14996: ValueError: *RP devices are required for ASM multicast*
  

*Description:  The error from log or more info* 

*Branch Name:  private/Ghost-ms/sanity_api_auto*
 *Script file/Usecase:* [trafficdot1x|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan25_20:17:13.074906.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x&begin=4567990&size=14036&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan25_20:17:13.074906.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:* NA

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2]",2023-01-30T08:54:35.509+0000,"Tran, can you add a check to validate if the site has devices, if not devices or no need to configure multicast? Skip configuring multicast if no device in the site.

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/20c6eab4ad7de6100951d91e2f55df64b98023de]

 ","['Auton', 'Ghost', 'Issue', 'optimized', 'sanity']",Tran Lam,Resolved,Avril Bower
SEEN-1093,https://miggbo.atlassian.net/browse/SEEN-1093,[Auton]Ghost: Task-external_authentication_radius.py-201-externalAuthentication/Test_TC2_DNAC_External_Authentication/test4_connect_external_auth,"*Reporter Analysis:* Testcase is getting failed while checking the Authentication.
     Snippet : 
Type': 'application/json'} Message:Authentication has failed for user ""dnac-observer"". Please provide valid credentials
*Description:  The error from log or more info* 

*Branch Name:  private/Ghost-ms/sanity_api_auto*
*Script file/Usecase:* [externalAuthentication|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-external_authentication_radius.py-201-externalAuthentication&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan25_23:12:40.456870.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-external_authentication_radius.py-201-externalAuthentication&begin=118831&size=32825&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan25_23:12:40.456870.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:* NA

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2]",2023-01-30T09:27:57.001+0000,"Looks like duplicate of : https://jira-eng-sjc1.cisco.com/jira/browse/SEEN-1094

 ","['Ghost', 'Issue', 'auton', 'optimized', 'sanity']",Raji Mukkamala,Closed,Avril Bower
SEEN-1094,https://miggbo.atlassian.net/browse/SEEN-1094,[Auton]Ghost: Task-radius_profile_enable.py-196-radiusProfileEnable/Test_TC1_Radius_profile_enable/test1_Radius_profile_enable,"*Reporter Analysis:* Script failed while checking the provision status for the device TB2-DM-eCA-BORDER
*Description:  The error from log or more info* 

*Branch Name:  private/Ghost-ms/sanity_api_auto*
*Script file/Usecase:* [radiusProfileEnable|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-radius_profile_enable.py-196-radiusProfileEnable&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan25_23:12:40.456870.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-radius_profile_enable.py-196-radiusProfileEnable&begin=4631&size=1151208&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan25_23:12:40.456870.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-external_authentication_radius.py-201-externalAuthentication&begin=118831&size=32825&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan25_23:12:40.456870.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:* NA

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2]
h4. Smart Checklist",2023-01-30T09:31:45.759+0000,"Looks like Setup or product issue. After External Auth is enabled not able to login to the cluster using an external user/password. 

Raji: Check it once and if so asked Regression to debug on setup or raise a product bug. and Junk this ticket.

   [~63f50bd74c355259db9ccc50] Please check if user ""dnac-observer"" is configured in ISE. Looks like user is missing on ISE. Refer to this wiki to check user in ISE https://wiki.cisco.com/display/EDPEIXOT/DNAC+External+Authentication As Raji pointed out, fixed ISE with user details.

 [~63f50bd74c355259db9ccc50] Please check if user ""dnac-observer"" is configured in ISE. Looks like user is missing on ISE. Refer to this wiki to check user in ISE [https://wiki.cisco.com/display/EDPEIXOT/DNAC+External+Authentication]

 

If still fails raise DNAC defect.","['Auton', 'Ghost', 'Issue', 'optimized', 'sanity']",KALYANI ORUGANTI,Resolved,Avril Bower
SEEN-1095,https://miggbo.atlassian.net/browse/SEEN-1095,"Log the ""executions"" part of the Report in Test_TC148_ap_report_generation  /   generate_ap_report","As per discussion with Developer w.r.t. debugging the failure for [https://cdetsng.cisco.com/webui/#view=CSCwe13591,] currently the execution log does not record as where the Execution ID is coming from. It is just printing it.

Snippet from the [Execution log|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2384345&size=45401&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_auto_job.2023Jan19_17:01:51.504011.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]:
{code:java}
10149: Resource path full url: https://10.22.45.61/api/dnacaap/v1/daas/core/data-set10152: Report Generation Success {'version': '1.0', 'response': {'name': 'AP_Gobal_report_generated_at_2023-01-20_01_19', 'dataCategory': 'AP', 'viewGroupId': 'c8bfe5c9-4941-4251-8bf5-0fb643e90816', 'schedule': {'type': 'SCHEDULE_NOW', 'scheduledServiceId': 'b98a4614-22eb-4962-b94c-dfb2aa443391', 'dateTime': 1674177540}, 'deliveries': None, 'view': {'name': 'AP', 'description': 'This report contains a detailed list of Access Points in the network', 'fieldGroups': [{'fieldGroupName': 'apDetailByAP', 'fieldGroupDisplayName': 'AP Detail By AP', 'fields': [{'name': 'macAddress', 'displayName': 'AP MAC Address'}, {'name': 'upTime', 'displayName': 'Up Time'}, {'name': 'adminState', 'displayName': 'Admin State'}, {'name': 'mode', 'displayName': 'Mode'}, {'name': 'nwDeviceName', 'displayName': 'Device Name'}, {'name': 'opState', 'displayName': 'Operational State'}, {'name': 'clCount_avg', 'displayName': 'Average Client Count'}, {'name': 'cpu', 'displayName': 'CPU Usage (%)'}, {'name': 'memory', 'displayName': 'Memory Usage (%)'}, {'name': 'clCount_max', 'displayName': 'Max Client Count'}, {'name': 'deviceFamily', 'displayName': 'Device Family'}, {'name': 'osVersion', 'displayName': 'OS Version'}, {'name': 'nwDeviceType', 'displayName': 'Device Type'}, {'name': 'managementIpAddress', 'displayName': 'IP Address'}, {'name': 'platformId', 'displayName': 'Platform'}, {'name': 'deviceModel', 'displayName': 'Device Model'}, {'name': 'overallScore', 'displayName': 'Health Score'}, {'name': 'wlcName', 'displayName': 'WLC'}, {'name': 'siteHierarchy', 'displayName': 'Site'}]}], 'filters': [{'type': 'MULTI_SELECT_TREE', 'name': 'Location', 'displayName': 'Location', 'value': [{'value': 'sample-global-site-id', 'displayValue': 'Global'}]}, {'type': 'TIME_RANGE', 'name': 'TimeRange', 'displayName': 'Time Range', 'value': {'timeRangeOption': 'LAST_3_HOURS', 'startDateTime': 0, 'endDateTime': 0, 'timeZoneId': None}, 'timeOptions': None}], 'format': {'name': 'JSON', 'formatType': 'JSON'}, 'viewInfo': None, 'viewId': '05cfaed0-e1f8-4ee4-a596-fcaad23dda05'}, 'executionCount': 0, 'clientId': 'default0', 'reportId': '2045d0bb-96f7-4bb2-b6ba-2e9fe3395c1e', 'viewGroupVersion': '2.0.0', 'reportWasExecuted': False}}10153: Execution ID 38379216-dc45-4ebe-8c61-b52a4134cbf510154: Download URL /dnacaap/v1/daas/core/content/data-set/2045d0bb-96f7-4bb2-b6ba-2e9fe3395c1e/38379216-dc45-4ebe-8c61-b52a4134cbf5
{code}
 

Hence, add the logging statement that prints the ""executions"" part of the Report in Test_TC148_ap_report_generation / generate_ap_report.",2023-01-30T18:31:38.411+0000,"Committed the changes required to esxivm_branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3c3d11499c02870fe662c442326a277655791bb7]

Cherry-picked the same to Halleck, Ghost, Groot and Guardian Branches.

Snippet from Execution log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=837149&size=46745&archive=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats%2Fusers%2Famardkum%2Farchive%2F23-01%2Fenv_auto_job.2023Jan30_11:21:11.561444.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats&submitter=amardkum&from=trade&view=all&atstype=pyATS]
 4360: {color:#ff8b00} Report - AP_Gobal_report_generated_at_2023-01-30_19_25 specific info:{color}
 4361: {'name': 'AP_Gobal_report_generated_at_2023-01-30_19_25', 'dataCategory': 'AP', 'viewGroupId': 'c8bfe5c9-4941-4251-8bf5-0fb643e90816', 'schedule':

{'type': 'SCHEDULE_NOW', 'scheduledServiceId': '3d97d59b-5cbe-4283-b806-804a0342c429', 'dateTime': 1675106703}

, 'deliveries': None, 'view': {'name': 'AP', 'description': 'This report contains a detailed list of Access Points in the network', 'fieldGroups': [{'fieldGroupName': 'apDetailByAP', 'fieldGroupDisplayName': 'AP Detail By AP', 'fields': [

{'name': 'macAddress', 'displayName': 'AP MAC Address'}

, \{'name': 'upTime', 'displayName': 'Up Time'}, \{'name': 'adminState', 'displayName': 'Admin State'}, \{'name': 'mode', 'displayName': 'Mode'}, \{'name': 'nwDeviceName', 'displayName': 'Device Name'}, \{'name': 'opState', 'displayName': 'Operational State'}, \{'name': 'clCount_avg', 'displayName': 'Average Client Count'}, \{'name': 'cpu', 'displayName': 'CPU Usage (%)'}, \{'name': 'memory', 'displayName': 'Memory Usage (%)'}, \{'name': 'clCount_max', 'displayName': 'Max Client Count'}, \{'name': 'deviceFamily', 'displayName': 'Device Family'}, \{'name': 'osVersion', 'displayName': 'OS Version'}, \{'name': 'nwDeviceType', 'displayName': 'Device Type'}, \{'name': 'managementIpAddress', 'displayName': 'IP Address'}, \{'name': 'platformId', 'displayName': 'Platform'}, \{'name': 'deviceModel', 'displayName': 'Device Model'}, \{'name': 'overallScore', 'displayName': 'Health Score'}, \{'name': 'wlcName', 'displayName': 'WLC'}, \{'name': 'siteHierarchy', 'displayName': 'Site'}]}], 'filters': None, 'format': \{'name': 'JSON', 'formatType': 'JSON'}, 'viewInfo': None, 'viewId': '05cfaed0-e1f8-4ee4-a596-fcaad23dda05'}, 'executions': [{'{color:#ff8b00}executionId': 'e637507e-eef0-45b6-a149-361ec089edcb'{color}, 'startTime': 1675106707869, 'endTime': 0, 'processStatus': 'IN_PROGRESS', 'requestStatus': 'ACCEPT', 'downloadFileSize': 0.0, 'downloadFile': '', 'errors': ['']}], 'executionCount': 1, 'clientId': 'default0', 'reportId': '45923e9d-b027-43e5-8e21-c56e763afa8f', 'viewGroupVersion': '2.0.0', 'reportWasExecuted': True}
 4362: Execution ID: {color:#ff8b00}e637507e-eef0-45b6-a149-361ec089edcb{color} required logging statement is in place.","['Auton', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-1138,https://miggbo.atlassian.net/browse/SEEN-1138,NDP 3.0 related APIs need additional but mandate Header: X-DNA-SVC-INSTANCE-NAME,"As per comments in [CSCwe21194|https://cdetsng.cisco.com/webui/#view=CSCwe21194] NDP 3.0 related APIs need additional but mandate Header: X-DNA-SVC-INSTANCE-NAME

for Halleck / VM.

Also, DSL queries will no longer be supported. So DSL queries need to be replaced with the corresponding Gremlin queries.

Example:
 API: [https://{{dnac IP}}/api/ndp/v1/data/graph|https://10.22.45.61/api/ndp/v1/data/graph]
 \{'params':{'query': ""g.V().hasLabel('NetworkDevice').has('macAddress','<NETWORK DEVICE MAC ADDRESS>').id().toList()""}, 'headers': \{'X-DNA-SVC-INSTANCE-NAME': 'test'}{color}}
  
 This change will be affecting all calls related to [https://{{dnac IP}}/api/ndp/v1/data/graph|https://10.22.45.61/api/ndp/v1/data/graph] API.",2023-01-31T02:15:16.992+0000,"Execution log with the changes made so far to esxivm_branch:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=505051&size=72608&archive=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats%2Fusers%2Famardkum%2Farchive%2F23-01%2Fenv_auto_job.2023Jan31_00:14:06.252575.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats&submitter=amardkum&from=trade&view=all&atstype=pyATS PR raised for Halleck branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4652/overview]

  As per discussion with [~5f3c6ae932360700388f7b4b], limiting this change to esxivm_branch and private/Halleck-ms/api-auto branches only..","['Auton', 'ESXi', 'Halleck', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-1139,https://miggbo.atlassian.net/browse/SEEN-1139,[Auton]Ghost: Task-assurance_health_metric.py-175-assuranceHealthMetrics/Test_TC9_apply_custom_profile_issue_on_site_level/test3_apply_simulate_ping_traffic_on_all_switches,"*Reporter Analysis:* Testcase is getting failed while checking the Authentication.
     Snippet : 
Type': 'application/json'} Message:Authentication has failed for user ""dnac-observer"". Please provide valid credentials
*Description:  The error from log or more info* 

*Branch Name:  private/Ghost-ms/sanity_api_auto*
*Script file/Usecase:* [externalAuthentication|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-external_authentication_radius.py-201-externalAuthentication&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan25_23:12:40.456870.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-external_authentication_radius.py-201-externalAuthentication&begin=118831&size=32825&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan25_23:12:40.456870.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:* NA

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2]
h4. Smart Checklist",2023-01-31T07:22:02.943+0000,"Follow these steps in wiki to configure your ISE properly.

[https://wiki.cisco.com/display/EDPEIXOT/DNAC+External+Authentication]

 ","['Auton', 'Ghost', 'Issue', 'optimized', 'sanity']",Raji Mukkamala,Closed,Avril Bower
SEEN-1140,https://miggbo.atlassian.net/browse/SEEN-1140,[Auton]Ghost: Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x/Test_TC6_DNAC_configure_multicast_external_rp_on_sites/test1_configure_external_rp_multicast_on_site_sf,"*Reporter Analysis:* Multicast Provisioning is getting failed in fabric sites
*Description:  The error from log or more info* 

*Branch Name:  private/Ghost-ms/sanity_api_auto*
*Script file/Usecase:*[{color:#007ebd}trafficdot1x{color}|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan25_20:17:13.074906.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x&begin=9176631&size=588062&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan25_20:17:13.074906.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:* NA

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2]",2023-01-31T08:05:47.311+0000,"Duplicate of 

SEEN-1141

 

Same issue:

Fixed in Ghost: 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6e67e154dddbb3deca9474597c9a564161a54fad]

 ","['Ghost', 'Issue', 'auton', 'optimized', 'sanity']",Pawan Singh,Resolved,Avril Bower
SEEN-1141,https://miggbo.atlassian.net/browse/SEEN-1141,[Auton]Ghost: Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x/Test_TC9_DNAC_configure_ssm_muticast_and_verify_traffic/cleanup_existing_multicast_ASM,"*Reporter Analysis:* unable to clear the existing multicast

Snippet : 

43749: No response for activity ID: ""4b3c3ccb-3d78-427d-9fec-b8285d0cc730""

43751: Failed to receive config preview

44045: !!!!!!!! Clear Multicast RP failed from fabric site Global/USA/SAN-FRANCISCO_US_SJ_Fabric1 !!!!!!!!

45494: Failed reason: Failed to cleanup multicast configuration!


*Description:  The error from log or more info* 

*Branch Name:  private/Ghost-ms/sanity_api_auto*
*Script file/Usecase:* trafficdot1x

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x&begin=12202609&size=1091036&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan25_20:17:13.074906.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:* NA

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2]",2023-01-31T10:21:37.377+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6e67e154dddbb3deca9474597c9a564161a54fad]

 

Fixed on Ghost.

 

Will be taken to Hallack through forward merge.","['Auton', 'Ghost', 'Issue', 'Sanity', 'optimized']",Pawan Singh,Resolved,Avril Bower
SEEN-1142,https://miggbo.atlassian.net/browse/SEEN-1142,[Auton]Ghost:Task-cmx_config_push_and_device_validations.py-157-cmxConfigsAndValidations/Test_TC2_verify_cmx_configs_push/test3_provision_the_devices,"*Reporter Analysis:* After CMX server is added, provisioning of devices is getting failed
*Description:  The error from log or more info* 

*Branch Name:  private/Ghost-ms/sanity_api_auto*
*Script file/Usecase:* [cmxConfigsAndValidationsexternalAuthentication|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-external_authentication_radius.py-201-externalAuthentication&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan25_23:12:40.456870.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: { [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-cmx_config_push_and_device_validations.py-157-cmxConfigsAndValidations&begin=79689&size=1156436&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan25_23:12:40.456870.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-cmx_config_push_and_device_validations.py-157-cmxConfigsAndValidations&begin=79689&size=1156436&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan25_23:12:40.456870.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] }

*Pass Log:* NA

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2]",2023-01-31T11:22:17.314+0000,"It is a product issue. All IDref are correct and are as received from DNAC in GET just before reprovision. Since this is just reprovisioned, the script does not change the payload. just get it to update the same.  Collect RCA and raise a product defect for the same. Raised a defect for this: [https://cdetsng.cisco.com/webui/#view=CSCwe58033] ","['Auton', 'Ghost', 'Issue', 'Optimized', 'Sanity']",Pawan Singh,Closed,Avril Bower
SEEN-1143,https://miggbo.atlassian.net/browse/SEEN-1143,update Test_TC119_ap_attributes / test1_position_ap_on_floor as per CSCwd60283,"As per notes from [CSCwd60283|https://cdetsng.cisco.com/webui/#view=CSCwd60283] in ESXi VM, the rate limit for API - is hit within 35 seconds.

When compared to On-Prem, it is still within given limit of 500 request for api/v1/dna-maps-service per 60 seconds.

To address the ""rate limit"" issue, some wait time would be required.",2023-01-31T23:18:01.002+0000,"Required change has been pushed to esxivm_branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/5419e55af924d662c158a392cb033814c737c9aa]

Pass log with 3.660.75270:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=773737&size=3336798&archive=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats%2Fusers%2Famardkum%2Farchive%2F23-01%2Fenv_auto_job.2023Jan31_14:30:29.220699.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats&submitter=amardkum&from=trade&view=all&atstype=pyATS]

  The fix provided via this ticket was specific to ESXi. 

Marking this ticket as ""Done"".","['Auton', 'ESXi', 'Halleck', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-1162,https://miggbo.atlassian.net/browse/SEEN-1162,Test_TC94_generate_link_flap_issues & Test_TC142_Add_interace_description requires update as per CSCwe17073,"As per discussion with the Developer w.r.t. below use-cases:

[CSCwe17073:|https://cdetsng.cisco.com/webui/#view=CSCwe17073] [ Test_TC94_generate_link_flap_issues|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14273558&size=104422&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_auto_job.2023Jan27_11:45:04.193964.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] / test1_generate_link_flap_issues / [Execution log|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=157713765&size=107525&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb10_13:06:45.634564.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[CSCwd95327|https://cdetsng.cisco.com/webui/#view=CSCwd95327]: [Test_TC142_Add_interace_description|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=44674390&size=247436&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_auto_job.2023Jan21_22:23:55.910990.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  / test1_add_interface_description / [Execution log|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=202200943&size=248404&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb10_13:06:45.634564.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

would require an update where the interfaces picked for flap should be from the devices onboarded in Inventory and connected to other onboarded Device interface.",2023-02-01T18:32:46.504+0000,"TC Failed in  

Guardian (2336) version 2.1.517.70045 -HF1
Branch :+*Script:*+solution_test_sanityecamb_lan.py
FAiled Log:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=18439923&size=730797&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb23_08:45:14.493776.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS [~accountid:63f50bcf4e86f362d39acde5] , can we have an ETA for this Auton? Hi [~accountid:63f50bcf4e86f362d39acde5]  ,
TC Failed in Latest Ghost Run  (Ghost P1 RC5 (2.1.613.70190)

*Uber ISO Version tested :* Ghost P1 RC5 (2.1.613.70190)
*Script Name:* Optimized code 
\testcases\sanityusecases\
*Testbed :*TB11(AWS)
*Failed Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-add_interace_description.py-178-addInteraceDescription&begin=4741&size=60723&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr28_06:52:09.069773.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-add_interace_description.py-178-addInteraceDescription&begin=4741&size=60723&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr28_06:52:09.069773.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Code snip :* 

{code:python}           if device_name_list:
                break
        self.log.info(f""The ap {ap1} connected device {device_name} interface is {interface}"")
        cmd=""show run interface {}"".format(interface)
        out=self.services.execute_command_on_device(device_name,cli=cmd)
        res=out['output']
        pattern=R'description\s+(.*)'  
        match=re.search(pattern,res)              {code}

*Error Snip From failed  log:*


{noformat}244:  Traceback (most recent call last):
245:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper
246:      result = testfunc(func_self, **kwargs)
247:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/testcases/sanityusecases/addInteraceDescription/add_interace_description.py"", line 99, in test1_add_interface_description
248:      if (dnac_handle.add_interface_description()):
249:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/assurance/group.py"", line 5427, in add_interface_description
250:      self.log.info(f""The ap {ap1} connected device {device_name} interface is {interface}"")
251:  UnboundLocalError: local variable 'ap1' referenced before assignment
252:  Test returned in 0:00:09.115143
253:  Errored reason: local variable 'ap1' referenced before assignment{noformat}


Could you please check  # PR Ghost-ms/api_auto link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5605/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5605/overview]
# PR Guardian-ms/api_auto link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5607/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5607/overview]
# Test Case:  {{TC_Add_interace_description}}
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link Ghost-ms/api_auto: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-05/sanity_TB2.2023May10_19:39:50.770348.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-05/sanity_TB2.2023May10_19:39:50.770348.zip&atstype=ATS]
# Trade log link Guardian-ms/api_auto: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-05/sanity_TB2.2023May11_00:50:54.857402.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-05/sanity_TB2.2023May11_00:50:54.857402.zip&atstype=ATS] [~accountid:63f50bcafb3ac4003fa2c6dd] 

*Analysis:*

Observed the interface which looking for the description not found the device TB18-SJ-eCA-BORDER-CP  hence it’s got failed

*snippet:*

TB18-SJ-eCA-BORDER-CP#show run interface GigabitEthernet1/0/5
show run interface GigabitEthernet1/0/5
                                      ^
% Invalid input detected at '^' marker.

TB18-SJ-eCA-BORDER-CP#show run interface GigabitEthernet1/   
TB18-SJ-eCA-BORDER-CP#show run interface GigabitEthernet1/
show run interface GigabitEthernet1/
% Incomplete command.

TB18-SJ-eCA-BORDER-CP#show run interface G                

TB18-SJ-eCA-BORDER-CP#show run interface gigabitEthernet
TB18-SJ-eCA-BORDER-CP#show run interface gigabitEthernet 1/0/5
show run interface gigabitEthernet 1/0/5
                                       ^
% Invalid input detected at '^' marker.

TB18-SJ-eCA-BORDER-CP#



*Branch:* private/Hulk-ms/api-auto

*Script file:* solution_test_sanityecamb.py and solution_test_sanityecamb_lan.py

*input file:* solution_test_input.json

*ova file:* assembly_release_dnac_hulk-intg_converged_07-3.710.75408.ova

*failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=88434758&size=57652&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun28_06:31:28.059646.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=88434758&size=57652&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun28_06:31:28.059646.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Hi [~accountid:63f50bcf4e86f362d39acde5] ,

Failure   analysis : Tc failed for
 {{UnboundLocalError: local variable 'ap1' referenced before assignment}}
TC Failed in Hulk (2370.70389)

*Uber ISO Version tested :* Hulk-2370-70389
*Script Name:* Optimized code 
\testcases\sanityusecases\[addInteraceDescription|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-add_interace_description.py-178-addInteraceDescription&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul02_06:11:33.461321.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
*Testbed :*TB7 
*Failed Log:*
[Task-add_interace_description.py-178-addInteraceDescription|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-add_interace_description.py-178-addInteraceDescription&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul02_06:11:33.461321.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

+*Error snip :*+ 

{code:python}242:  Traceback (most recent call last):
243:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity@2/services/commonlibs/test_wrapper.py"", line 301, in wrapper
244:      result = testfunc(func_self, **kwargs)
245:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity@2/testcases/sanityusecases/addInteraceDescription/add_interace_description.py"", line 75, in test1_add_interface_description
246:      if (dnac_handle.add_interface_description()):
247:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity@2/services/dnaserv/lib/api_groups/assurance/group.py"", line 5500, in add_interface_description
248:      self.log.info(f""The ap {ap1} connected device {device_name} interface is {interface}"")
249:  UnboundLocalError: local variable 'ap1' referenced before assignmen{code} Behavior Change. Hi [~accountid:620b8357878c2f00729881c8], I raised PR to fix your error. Could you please have a look once?

PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6421/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6421/overview]

Test Case:  {{TC_Add_interace_description}}

Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py

Trade log link: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-07/sanity_TB1.2023Jul26_20:22:14.464205.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-07/sanity_TB1.2023Jul26_20:22:14.464205.zip&atstype=ATS] Hi [~accountid:63f50bcafb3ac4003fa2c6dd]  ,

TC142  still failing with below  error on NY_-FIAB:
5261: 
 Failed to get the interface or access point on the device TB7-NY-FIAB.

{code:python}5142:  Cannot track test: tracking auth info must be set in order to transfer test tracking data
5207:  The device: TB7-SJ-EDGE doesn't connect any Access Point
5260:  The device: TB7-NY-FIAB doesn't connect any Access Point
5261:  Failed to get the interface or access point on the device TB7-NY-FIAB.
5263:  Failed reason: interface description validation failed{code}

could  you  please  check,
*Failed log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=976670&size=28946&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul31_08:14:04.750450.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=976670&size=28946&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul31_08:14:04.750450.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]




*TB7-NY-FIAB#sh cdp neighbors*
Capability Codes: R - Router, T - Trans Bridge, B - Source Route Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater, P - Phone,
                  D - Remote, C - CVTA, M - Two-port Mac Relay 

Device ID        Local Intrfce     Holdtme    Capability  Platform  Port ID
APF01D.2D1C.1AA8 Ten 1/0/27        136              R T   C9130AXE- Gig 0
TB7-DM-TSIM      Gig 1/0/11        154               H    AIR-CT550 Gig 0/0/2
TB7-DM-TSIM      Gig 1/0/12        154               H    AIR-CT550 Gig 0/0/4
[TB7-Fusion.cisco.com|http://TB7-Fusion.cisco.com]
                 Ten 2/0/1         165             R S I  WS-C3850- Ten 1/1/2
SEPA4B439731D12  Gig 1/0/24        123             H P M  IP Phone  Port 1
[TB7-Transit.cisco.com|http://TB7-Transit.cisco.com]
                 Ten 2/0/3         165             R S I  C9300-24U Ten 1/1/4
APA00F.379C.3820 Ten 1/0/28        138              R T   C9120AXP- Gig 0
APDC8C.3796.20EC Ten 1/0/35        122              R T   AIR-AP480 Gig 0
AP70F3.5A7A.1470 Ten 1/0/37        118              R T   AIR-AP180 Gig 0
AP5CE1.7629.CEF0 Gig 1/0/14        158              R T   C9120AXP- Gig 0
MGMT_NW_123.x.x.x
                 Gig 0/0           121              S I   UA-C3850- Gig 1/0/9
AP687D.B45C.2054 Gig 1/0/13        129              R T   C9136I-B  Gig 1
AP5CE1.7629.C894 Gig 1/0/15        146              R T   C9120AXP- Gig 0
[SN-JAE242302CZ.cisco.com|http://SN-JAE242302CZ.cisco.com]
                 Ten 1/0/48        154              S I   C9200L-48 Gig 1/0/48
[SN-JAE242302CZ.cisco.com|http://SN-JAE242302CZ.cisco.com]
                 Ten 1/0/47        138              S I   C9200L-48 Gig 1/0/47

Total cdp entries displayed : 15
TB7-NY-FIAB#



*TB7-SJ-EDGE#sh cdp neighbors*
Capability Codes: R - Router, T - Trans Bridge, B - Source Route Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater, P - Phone,
                  D - Remote, C - CVTA, M - Two-port Mac Relay 

Device ID        Local Intrfce     Holdtme    Capability  Platform  Port ID
[SN-FOC2311Y129.cisco.com|http://SN-FOC2311Y129.cisco.com]
                 Ten 1/0/47        143              S I   WS-C3560C Gig 0/12
[SN-FOC2311Y129.cisco.com|http://SN-FOC2311Y129.cisco.com]
                 Ten 1/0/48        176              S I   WS-C3560C Gig 0/11
AP70F3.5A7A.13C8 Two 1/0/24        150              R T   AIR-AP180 Gig 0
AP1416.9D2E.1FD4 Two 1/0/14        120              R T   C9130AXE- Gig 0
TB7-DM-TSIM      Two 1/0/1         136               H    AIR-CT550 Gig 0/0/1
TB7-DM-TSIM      Two 1/0/2         136               H    AIR-CT550 Gig 0/0/3
[TB7-SJ-eCA-BORDER-CP.cisco.com|http://TB7-SJ-eCA-BORDER-CP.cisco.com]
                 For 1/1/1         166             R S I  C9500-48Y Hun 2/0/49
AP3C41.0EFE.20C0 Two 1/0/35        167              R T   C9130AXI- Gig 0
MGMT_NW_123.x.x.x
                 Gig 0/0           145              S I   UA-C3850- Gig 1/0/7
[SN-FDO2515JDSL.cisco.com|http://SN-FDO2515JDSL.cisco.com]
                 Ten 1/0/38        165              S I   IE-9320-2 Gig 1/0/16

Total cdp entries displayed : 10 [~accountid:620b8357878c2f00729881c8] , pls create separate Auton for your issues.

Appears like the discussion not relevant for the purpose for which this Auton was raised.

Automation engineer would get confused with multiple issues mentioned in the same Jira ticket. sure  [~accountid:62ab7a399cd13c0068b18fe0]  Another Auton Raised for this [https://miggbo.atlassian.net/browse/SEEN-1992|https://miggbo.atlassian.net/browse/SEEN-1992|smart-link]  [~accountid:63f50bf5e8216251ae4d59cf] , the fix for [SEEN-1162|https://miggbo.atlassian.net/browse/SEEN-1162] is expected as changing the way of selecting the ""interface"" for flap.

Based on the PR mentioned in [SEEN-1992|https://miggbo.atlassian.net/browse/SEEN-1992] does not seem to have this change.

I would like to understand more on how this ticket is getting concluded based on PR for [https://miggbo.atlassian.net/browse/SEEN-1992|https://miggbo.atlassian.net/browse/SEEN-1992|smart-link].

Re-opening this Auton until it’s concluded properly. Raised PR for “generate_link_flap_event”: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7258/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7258/overview] PR related to “generate_link_flap_event” has been merged to _private/HulkPatch-ms/api-auto_ and cherry-picked to _private/Hulk-ms/api-auto_ branch. [~accountid:62ab7a399cd13c0068b18fe0] i have executed this case _private/HulkPatch-ms/api-auto_ in this build  Hulk P1 2.1.713.70263  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=956515&size=1996720&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct05_22:47:29.368095.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=956515&size=1996720&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct05_22:47:29.368095.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] testcase is failed i have anlayzed the log one iterface flapping is failing inconsistently {'SFO-FIAB-9400': [
'GigabitEthernet1/0/48 not find the devices
'GigabitEthernet1/0/11- AP70F3.5A7A.1208
'GigabitEthernet1/0/12-AS-B1-AP3-3802I - Failed this interface flapping is failing inconsistently so it take long time

SJC-FE-9300-1' GigabitEthernet1/0/11 SR-MS-SJC-CL2- -Success

NYC-FE-9400': GigabitEthernet1/0/17 - SR-MS-NYC-CL1 Success',
NYC-FE-9300':GigabitEthernet1/0/11- NYC-AP1-9120AXEB success I’ll check on it.","['Auton', 'ESXi', 'Ghost', 'Guardian', 'Hulk', 'Issue', 'Optimized', 'Sanity']",Amardeep Kumar,In Progress,Avril Bower
SEEN-1163,https://miggbo.atlassian.net/browse/SEEN-1163,Test_TC128_verify_global_devices_count / test1_verify_global_devices_count would require update as per CSCwd96194,"As per F-comment in [CSCwd96194|https://cdetsng.cisco.com/webui/#view=CSCwd96194], In case of ESXi VM DNAC, it is working in UI from inventory page but not from script.

Hence, an upgrade would be required to address the change in Test_TC128_verify_global_devices_count / test1_verify_global_devices_count use-case.",2023-02-01T19:36:49.263+0000,PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5413/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5413/overview] PR has been approved and merged to {{Halleck-ms/api-auto}} and cherry-picked to {{Hulk-ms/api-auto}} branch.,"['Auton', 'ESXi', 'Halleck', 'Issue']",ThangQuoc Tran,Closed,Avril Bower
SEEN-1164,https://miggbo.atlassian.net/browse/SEEN-1164,Increase the pagination limit from 10 to 50 in verify_fabric_SD_access(),"With given number of network devices and no filters, limit = 10 is not enough to list all the devices.
 Hence, there's a need to increase the pagination limit from 10 to 50 in verify_fabric_SD_access() method under assurance_fabric/group.py.

Execution log with 3.660.75270 that has only 10 entries - [test1_verify_fabric_SD_access|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=852378&size=166228&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb01_11:44:43.030106.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS].

 

After increasing the limit to 50, the use-case execution progressed further: [test1_verify_fabric_SD_access|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=852378&size=119217&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb01_15:24:28.240252.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS].",2023-02-01T23:28:54.648+0000,"Required change has been pushed to esxivm_branch PR raised for Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4646/overview]

Cherry picked to Ghost and Groot branches.

 

Marking this ticket as ""Done"".","['Auton', 'Ghost', 'Halleck', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-1188,https://miggbo.atlassian.net/browse/SEEN-1188,[Auton] [Guardian] [Ghost] Need for change in configuration check for ip radius source on 9800 EWLC,"*Uber ISO Version tested :* 

Latest Promoted Guardian & Ghost

*Script Name:*
solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py, solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB1, MSTB2
 

*Description :* 

We observed configuration verification check for ""ip radius source-interface"" on 9800 EWLC device got failed with config missing. 

On trying to check further with Wireless Team (mogaur@cisco.com), it was confirmed by them that there was recent changes done from DNAC side as part of AAA Hardening feature. 

The design change is only applicable for C9800 eWLC devices and for all Polaris versions >= 16.11. We need to handle this part of Automation changes.

Please refer Auton for more details - [https://cdetsng.cisco.com/summary/#/defect/CSCwd75809] 

 

*Current Failed Log on Guardian Patch4 Pre-RC2:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=23244191&size=1196943&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fsr_mb_multi_sites_mdnac.2023Jan27_10:51:21.620727.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Previous Pass log on Guardian Patch3 RC2:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27262635&size=1179478&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-12%2Fsr_mb_multi_sites_mdnac.2022Dec15_23:33:18.210949.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ",2023-02-03T15:58:25.600+0000," 

Fixe in Ghost:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/eb37715d50f48b9b0cd9ee3bdbbb821730d2e14f]

 

  Groot:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/b54380bff6be85ecb44e50989b27928557715eec]

 

Guardian:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e7549d1f98db66978e7d65ab1be46aea7b2cd027]

  Issue is no more observed during *Guardian Patch4 RC3 - 2.1.518.72319* testing. 

Pass log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=26655139&size=1202257&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fsr_mb_multi_sites_mdnac.2023Feb24_23:50:13.089275.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] -> Refer TC49.5","['Auton', 'Ghost', 'Guardian', 'Issue', 'MSTB1', 'MSTB2', 'MSTB3', 'Multisite']",Pawan Singh,Closed,Avril Bower
SEEN-1189,https://miggbo.atlassian.net/browse/SEEN-1189,Regex used in Test_TC136_enable_ICMP_ping_check_AP_reachability / test1_enable_icmp_verify_ap_reachability need revisit,"Regex used in Test_TC136_enable_ICMP_ping_check_AP_reachability / test1_enable_icmp_verify_ap_reachability need revisit.

Execution log from esxivm_branch with ESXi VM DNAC Halleck - 3.660.75294:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=83121513&size=86767&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb02_19:33:13.088697.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

expected AP is there in the output but Regex is not able to search it:
{code:java}
293663: [{'deviceUuid': '8b291287-b3ad-4627-b65f-757d5e68378a', 'commandResponses': {'SUCCESS': {'show ap mode flexconnect': 'show ap mode flexconnect\n\nNumber of APs.................................... 1\n\nAP Name Slots AP Model Ethernet MAC Location Country IP Address Clients DSE Location \n------------------ ----- -------------------- ----------------- ---------------- ---------- --------------- -------- --------------\nAPDC8C.37BF.F3A6 3 AIR-AP2802I-B-K9 dc:8c:37:bf:f3:a6 Global/USA/New Y US 204.1.212.165 0 [0 ,0 ,0 ]\n\n(TB4-DM-WLC) >'}, 'FAILURE': {}, 'BLACKLISTED': {}}}]
 293664: Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:02.447301
 293665: No ap in flex connect mode!!! Have at least one flex connectsupported APs, 2800 or 9100 series.{code}

  ",2023-02-04T01:16:22.555+0000,"Required change was merged to esxivm_branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c24add168b6f04e0a8f9518669c2018004c1c257]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/08b3b57a8456e5e6bc1fa6dbf34fa2395ae4ea22]

  can you cherry-pick to other branches Ghost/groot/guardian

don't pull on Hallack or hulk, those will come through forward merge. raised PR for Guardian branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4762/overview]

  PR for Groot: 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4850/overview]

has been merged and same has been cherry-picked to Ghost, Halleck, hallack_esxivm_branch. required changes are in place across all branches from Guardian.","['Auton', 'ESXi', 'Halleck', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-1190,https://miggbo.atlassian.net/browse/SEEN-1190,[Ghost] - Key Error for group_pool_handle in Multicast Enhancement Feature,"* *DNAC Release_Version Tested:* Ghost Pre RC Uber ISO - 2.1.613.70079
 * *Device Image Used:* 17.9.3 Prd2 Fc1
 * *Testbed:* Multisite Non-DR setup
 * *Branch Used:* private/Ghost-ms/api-auto
 * *Script Name:* solution_test_3sites_multicast.py
 * *Testcases Impacted:*  [Test_TC19_DEV_STRESS_asm_ssm_mcast_traffic_convergence_test_primary_border_reload|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=35233994&size=35543&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fsr_mb2_three_sites.2023Feb02_03:58:34.085401.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&atstype=PYATS]
 * *Failed Trade Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=35234192&size=35128&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F02%2F02%2F03%2F58%2Fsr_mb2_three_sites.2023Feb02_03:58:34.085401.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]
 * *Issue details/analysis:* We are observing Key Error while configure 1K ip for each port and Verify Traffic convergence on border from IXIA.
Error : 
121113:  232.1.1.1
121114:  Traceback (most recent call last):
121115:  File ""/auto/dna-sol/ws/sr-mb2/Ghost/dnac-auto/services/commonlibs/test_wrapper.py"", line 301, in wrapper
121116:  result = testfunc(func_self, **kwargs)
121117:  File ""/auto/dna-sol/ws/sr-mb2/Ghost/dnac-auto/testcases/mega_topo/solution_test_3sites_multicast.py"", line 1677, in test1_static_ixia_noauth_traffic_convergence_test
121118:  ixia_client.configure_igmpv3_multicast_on_port(port)
121119:  File ""/auto/dna-sol/ws/sr-mb2/Ghost/dnac-auto/services/commonlibs/ixia_client.py"", line 443, in configure_igmpv3_multicast_on_port
121120:  result = self.emulation_igmp_group_config(port,ip_addr_start = DEFAULT_IGMPv3_GROUP_IP, source_ip=source_ip)
121121:  File ""/auto/dna-sol/ws/sr-mb2/Ghost/dnac-auto/services/commonlibs/ixia_client.py"", line 348, in emulation_igmp_group_config
121122:  group_pool_handle = self.mcastgrouphandles[ip_addr_start],
121123:  KeyError: '232.1.1.1'",2023-02-06T09:27:56.022+0000,"Use new IXIA environment for IXNETWORKRESTAPI lib. we no longer support oldhltapi. 

 

To switch to new IXIA libs, set env variable in your python/pyasts env. 

 

export NO_IXIA_ENV=""True""

Source just source env file after sourcing your pyats env.

source env.sh

 

 ","['Auton', 'Ghost', 'Issue', 'MSTB2', 'Multisite']",Pawan Singh,Resolved,Avril Bower
SEEN-1202,https://miggbo.atlassian.net/browse/SEEN-1202,[Auton]   TC101_DNAC_Policy_Extended_node," Issue seen In: Guardian :2.1.518.72310

*Reporter Analysis:* we tried manually, test case working, expect the config to present. device onboard_policy_extended_node_interface successfully done 

*Description:*  

*Branch Name:  private/Groot-ms/sanity_api_auto*

*Script file/Usecase:* private/Guardian-ms/sanity_api_auto

*Source Team:  solution_test_sanityecamb_lan,*

*Issue Seen first time or day0 issue:*first  time seen* * 

*Fail Log:* *[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=34102391&size=274173&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb10_12:30:16.742125.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS***]

*Pass Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=775036&size=379700&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb01_19:47:13.162923.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7]",2023-02-13T15:07:43.963+0000,"Looks like an incorrect function is called to fetch the SGT ID :
=====
 
4693:  Library group ""aca"" method ""get_sg_tag_by_name"" returned in 0:00:02.059165
sgt_id= 5f8474aa-be3f-4509-9362-02c095da3a7a
Failed case:
 
136062:  Library group ""aca"" method ""get_sg_tag_by_name"" returned in 0:00:00.043656
136063: 
136064:  sgt_id= 30001
 
========
Looks like the function is fixed to return correctly as per the description. the call lib need to change to right function
get_sg_id_by_name [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/76016cb0d2f982b11bff2b0724ea64990aff0fdd]

 

 

Will be merged to Hallack and Hulk through Weekly forward merge. Thanks, [~5f3c6ae932360700388f7b4b]
 The issue has been resolved in Guardian. Hence moving closed state. 
+*Pass Log:*+
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=776349&size=382860&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb13_21:06:48.120962.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Guardian', 'Issue', 'Sanity']",Pawan Singh,Closed,Avril Bower
SEEN-1209,https://miggbo.atlassian.net/browse/SEEN-1209,[Auton]  [Guardian] Test_TC34_Remove_Edge_from_fabric_and_add_back  /   test1_remove_an_edge_from_the_fabric," 

*Reporter Analysis:* Upgrade Sanity Test case failed due to  name 'border' is not defined

*Description:*  
 24701: Traceback (most recent call last):
 24702: File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/Guardian-Upgrade-Verify-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
 24703: result = testfunc(func_self, **kwargs)
 24704: File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/test/Guardian-Upgrade-Verify-common-Multi-job/testcases/upgrade/after_upgrade_verify.py"", line 1344, in test1_remove_an_edge_from_the_fabric
 24705: if ""EDGENODE"" in dnac_handle.dnaconfig.testbed.devices[border[""name""]].role:
 24706: NameError: name 'border' is not defined
 24707: Test returned in 0:00:00.558789
 24708: Errored reason: name 'border' is not defined
 *Branch Name:  private/Guardian-ms/sanity_api_auto*

*Script file/Usecase:* after_upgrade_verify.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: First Time running upgrade sanity on AWS TB* 

*Pass Log:*
 *Fail Log:*  **  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17108720&size=21729&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb15_14:13:01.159336.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-11]",2023-02-16T09:18:46.305+0000,"Fixed on Guardian also.

https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/testcases/upgrade/after_upgrade_verify.py?until=95f51fa302562416ba8893d96390e5568305a984&at=refs%2Fheads%2Fprivate%2FGuardian-ms%2Fapi-auto","['AWS_Sanity', 'Auton', 'Guardian', 'Issue']",Pawan Singh,Resolved,Avril Bower
SEEN-1210,https://miggbo.atlassian.net/browse/SEEN-1210,Changes to NDP DocStore URL in PnC3.0/VM; v1 to v2,"DocStore needs to be modified to use the new endpoint. This applies only on PnC3.0/VM. 

api/ndp/*v1*/data/store/app/\{id}/query

to

api/ndp/*v2*/data/store/app/\{id}/query

 

The URL change and the potential impacted query patterns are detailed in the wiki.

[https://wiki.cisco.com/display/ASSURWIKI/v2+endpoint+for+DocStore+Queries+For+VM] 

 

As this Assurance feature (F137890: [PnC 3.0 adaptation] - v2 endpoint for Datastore search queries ES-SQL library) requires upliftment of Assurance test scripts, the feature is targeted for completion from dev/FT by 03/31. It will be available for SIT/Sol testing by 1st week of Apr.",2023-02-17T17:23:40.015+0000,"Once we get the confirmation on the implementation, I’ll re-open it.","['Auton', 'ESXi', 'Issue']",Amardeep Kumar,Cancelled,Avril Bower
SEEN-1211,https://miggbo.atlassian.net/browse/SEEN-1211,TC67_DNAC_CriticalVLAN_onboarding_ixia_scale  /   test66_subtest2_dot1x_auth_ixia required better logging for more info,"TC67_DNAC_CriticalVLAN_onboarding_ixia_scale / test66_subtest2_dot1x_auth_ixia required better logging for more info.
 # Test-case code can be moved inside library like others - to have more centralized code
 # Log interface config per device - will help in debugging and reporting any bug/failure
 # Log dot1x sessions from all four devices - currently it's only logging for two

Execution log with esxivm_branch: [TC67_DNAC_CriticalVLAN_onboarding_ixia_scale|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=141564371&size=30753&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb10_13:06:45.634564.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

This would help in debugging the failures for this test-case.",2023-02-17T17:47:43.745+0000,"Required PR has been raised against `halleck_esxivm_branch`:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5271/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5271/overview] PR has been approved and merged to {{hallack_esxivm_branch}}. Will monitor the result of TC67 with new runs and then decide to push to other branches.","['Auton', 'Issue']",Amardeep Kumar,Closed,Avril Bower
SEEN-1222,https://miggbo.atlassian.net/browse/SEEN-1222,[Auton] [Halleck] - Need for script enhancement for enablement of mdnac feature on Halleck,"*Uber ISO Version tested :* 

Promoted Halleck - *2.1.660.70275,* *Non-FIPS, PUBSUB enabled, Keywrap enabled, CSRF enabled,* 

*ISE version used - 3.2 CCO*

*Script Name:*
solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py

*Testbed :* MSTB1

 

*Reference Failed logs:*

1)  [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fsr_mb_multi_sites_mdnac.2023Feb19_10:13:23.997002.zip&atstype=ATS]  -> Refer TC6

2) [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fsr_mb_multi_sites_mdnac.2023Feb19_19:26:27.680309.zip&atstype=ATS] -> Refer TC6 and TC8

*Description :* 

 

From Halleck onwards, the mdnac feature enablement on DNAC has changes. 

1)  By default under System->Settings->System Configuration-> Multiple Cisco DNA Centre Settings we do not see MDNAC states and roles. Instead, we have a banner and description stating how the mdnac feature has to be enabled (Refer screenshot attached). So the corresponding api response fails. This corresponds to Failed Log 1) shared above.

!image-2023-02-21-21-19-33-382.png!

 

2) MDNAC feature enablement has to be done during ISE integration on DNAC workflow by selecting ""*Enable Multiple Cisco DNAC Centre Operation""* option under Advanced Settings. This is newly introduced from Halleck onwards. It has to be handled for all our MDNAC scripts at the ISE integration step and corresponding verifications as well. This corresponds to Failed Log 2) shared above.

!image-2023-02-21-21-19-48-747.png!

 

3) For 3.2 ISE version and above, mdnac feature gets enabled automatically without installing mdnac packages on both Author and reader node DNAC clusters. So corresponding verifications checks also as to be added as an enhancement.

 

Also please refer defect - [https://cdetsng.cisco.com/summary/#/defect/CSCwd63951]
for more details on MDNAC enablement on Halleck.",2023-02-21T15:50:51.438+0000,"Updated new changes for Mdnac in Halleck.
 * Enable mdnac when integrating ise for author and reader cluster
 * skip check mdnac packages if ise >=3.2

Commit: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d9d813654968f40800a28be91ceb237149099713]

[~62d2fec15d6f5fd2c3db8f9f], can you please check and let me know if any issue or anything else to address? Reverted to previous In_Progress state as merge update by Ashok had caused auto resolve on Jira ticket.","['Auton', 'Halleck', 'Issue', 'MSTB1', 'Multisite']",Tran Lam,Resolved,Avril Bower
SEEN-1223,https://miggbo.atlassian.net/browse/SEEN-1223,update RBAC library specific for ESXI related API change,"Update RBAC library specific for ESXI related API change:

|*RBAC components*|*On-Prem*|*ESXi*|
|users_url|/system/v1/identitymgmt/users|/idm/v1/local/users|
|single_user_url|/system/v1/identitymgmt/user|/idm/v1/local/user|
|roles_url|/system/v1/identitymgmt/roles|/idm/api/v1/im/tenants/tenantID/roles|
|delete_role_url|/system/v1/identitymgmt/role|/idm/api/v1/im/tenants/tenantID/roles|
|role_resource_types_url|/system/v1/identitymgmt/role/resource-types|/idm/api/v1/im/tenants/tenantID/roles/roleName|
|resource_types_url|/system/v1/identitymgmt/resource-types|/idm/api/v1/im/resource-types|
|aaa_server_url|/system/v1/identitymgmt/aaa-server|/idm/api/v1/lauth/plugins/aaa/instance|
|external_authentication_url|/system/v1/identitymgmt/aaa-server/external-authentication|/idm/api/v1/lauth/plugins/aaa/instance|
|authorization_attribute_url|/system/v1/identitymgmt/aaa-server/authorization-attribute|/idm/api/v1/lauth/plugins/aaa/instance|

 

Available Wiki Reference: [https://confluence-eng-sjc1.cisco.com/conf/pages/viewpage.action?pageId=275041647|https://confluence-eng-sjc1.cisco.com/conf/pages/viewpage.action?pageId=275041647]",2023-02-21T23:14:13.834+0000,"Created SEEN-1223_ESXi branch.
 Execution log for TC6.1 - [Test_TC6_DNAC_RBAC_create_users_roles|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=228830&size=20782&archive=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats%2Fusers%2Famardkum%2Farchive%2F23-02%2Fenv_auto_job.2023Feb26_22:31:38.797201.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats&submitter=amardkum&atstype=PYATS] [CSCwe49419|https://cdetsng.cisco.com/webui/#view=CSCwe49419] - re-execution of system-orchestrator API is failing even though the token remains same Raised below PR against private/Halleck-ms/api-auto:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5035/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5035/overview] 

Once approved and merged, will compare hallack_esxivm_branch with latest content of private/Halleck-ms/api-auto and raise another PR for ESXi. Required changes have been pushed to ESXi’s hallack_esxivm_branch as well:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/580f4748c363307a439dc20020bea8451b7c5548|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/580f4748c363307a439dc20020bea8451b7c5548]","['Auton', 'ESXi']",Amardeep Kumar,Closed,Avril Bower
SEEN-1224,https://miggbo.atlassian.net/browse/SEEN-1224,[Auton] Ghost - TC33_DNAC_assigne_dhcp_role_deploy_device_in_fabric - Wireless Solution Sanity,"* *DNAC Release_Version Tested:* Ghost_P1_2.1.613.70116
 * *Device Image Used:* 17.9.3
 * *Branch Used:* rcdn/Ghost-ms/api-auto (Latest sync to Main Branch done before reg run start - Main Branch Used: Private/Ghost-ms/api-auto)
 * *Script Name:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py
 * *Testcases Impacted:* Test_TC33_DNAC_assigne_dhcp_role_deploy_device_in_fabric gets errorred blocking other testcases
 * *Error faced:* Roles does not have valid permissions to access API
 * *Failed Trade Log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/23-02/sanitycombine.2023Feb20_08:20:13.753811.zip&atstype=ATS]
 * *Failed Taas Log:* [https://ngdevx.cisco.com/services/taas/results/8b6830be-b05f-486b-886d-53872e03cf04]
 * *Testbed info:* Testbed is in regression run currently
 * *Team/Source:* Wireless Solution Sanity Regression Testing Team under Loi**
 * *Fail Log Screenshot: (TC33 - test1_verify_assign_roles_and_deploy_devices_on_fabric1_site_san_jose)*
 * !image-2023-02-22-11-05-22-887.png!",2023-02-22T05:36:08.019+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/54a79d15b388416d367865f9f2656ebfc3e36937,"['Auton', 'Ghost', 'Issue']",Pawan Singh,Resolved,Avril Bower
SEEN-1225,https://miggbo.atlassian.net/browse/SEEN-1225,[Auton] Halleck - TC43_DNAC_EXT_NODE_interface_config_verifications - Wireless Solution Sanity,"* *DNAC Release_Version Tested:* Halleck_2.1.660.70282
 * *Device Image Used:* 17.11.1
 * *Branch Used:* rcdn/Ghost-ms/api-auto (Latest sync to Main Branch done before reg run start - Main Branch Used: Private/Ghost-ms/api-auto)
 * *Script Name:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py
 * *Testcases Impacted:* TC43_DNAC_EXT_NODE_interface_config_verifications/ test2_dnac_ext_node_onboarding_verifications gets failed blocking other testcases
 * *Error faced:*
160889:  Result: Failed in extedned node workflow.-> check fabric->wroflow stats for more details
160890:  One or more ext node onboarding issues
 * *Failed Trade Log:* https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/23-02/sanitycombine.2023Feb22_04:53:50.346791.zip&atstype=ATS
 * *Failed Taas Log:* [https://ngdevx.cisco.com/services/taas/results/caa6ffaf-2e58-472c-a90a-a329ea7ae3f9]
 * *Analysis:* Provision status of extended node under Inventory/PlugNPlay/Fabric are success.
 * *Testbed info:* Cluster: 10.89.48.89 - admin/Maglev123 for UI and maglev/Maglev123 for SSH - Wiki for detailed testbed info: https://wiki.cisco.com/display/WNBURCDNST/SOL-REG+POD04
 * *Team/Source:* Wireless Solution Sanity Regression Testing Team under Loi**
 * *Fail Log Screenshot:***
 * *!image-2023-02-22-21-54-14-887.png!*
 * *DNAC UI Screenshot:*
 * 
 * *!image-2023-02-22-21-54-58-668.png!!image-2023-02-22-21-54-38-695.png!*",2023-02-22T16:25:06.558+0000,"Target device not showing in the api response for extended node workflow, please compare with previous pass logs and check what is missing. Looks like some fields are not getting populated. Please raise a defect for the missing values in the ""tasks"" key. There should be some fields like ""targets"" that contains the device info like ""deviceName"".",['Auton'],Andrew Chen,Resolved,Avril Bower
SEEN-1226,https://miggbo.atlassian.net/browse/SEEN-1226,[Auton]:Upgrade: Test_TC20_DNAC_provision_all_aps  /   test1_provision_all_aps,"*Reporter Analysis:* 

Issue is seen on Upgrade Combination of:
Shockwave P4 RC5 <> Ghost P2
Guardian P4 RC4<>Ghost P2
Ghost P1 RC6<>GhostP2

In upgrade sanity execution,
During provision of AP via script it is failing with error code 
""NCWL10307: APWirelessConfiguration FLOOR1_LEVEL1_TYPICAL_8989b with Id a8db85df-ec06-4b9f-b226-adc34b3f371b SiteId 6a9efffd-5c5c-4b31-b69f-aa43ecba1784 and RfProfile TYPICAL is already present in the database""

Raised a bug regarding same which went to J-state:
[https://cdetsng.cisco.com/webui/#view=CSCwe20354|https://cdetsng.cisco.com/webui/#view=CSCwe20354]

 

 

In Sanity,
We are seeing similar error snip when we are trying to rerun TC  :
[Test_TC53_DNAC_provision_all_aps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=501370&size=182366&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb18_23:27:29.183223.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_provision_all_aps
*Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=501934&size=177811&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb18_23:27:29.183223.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=501934&size=177811&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb18_23:27:29.183223.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Description:*  
""NCWL10307: APWirelessConfiguration FLOOR1_LEVEL1_TYPICAL_8989b with Id a8db85df-ec06-4b9f-b226-adc34b3f371b SiteId 6a9efffd-5c5c-4b31-b69f-aa43ecba1784 and RfProfile TYPICAL is already present in the database""

*Branch Name:*  private/Ghost-ms/sanity_api_auto

*Script file/Usecase:* 
after_upgrade_verify.py

*Source Team:  Upgrade Sanity and Sanity*

 

*Fail Log:*
*Script Used:*
*solution_test_sanityecamb_lan.py and solution_test_sanityecamb.py*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=263226&size=160447&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F01%2F27%2F06%2F37%2Fenv_auto_job.2023Jan27_06:37:00.702870.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=263226&size=160447&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F01%2F27%2F06%2F37%2Fenv_auto_job.2023Jan27_06:37:00.702870.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log from Shockwave P4RC5(*[*2.2.3.*|https://2.2.3.6/]*6)<>Ghost RC4 (2.3.5.0)*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=255590&size=368667&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F12%2F20%2F16%2F44%2Fenv_auto_job.2022Dec20_16:44:37.889566.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=255590&size=368667&archive=%2Fauto%2Fatslogs%2Fcl%2F2022%2F12%2F20%2F16%2F44%2Fenv_auto_job.2022Dec20_16:44:37.889566.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:* NA

 

*Affected Testcases:*
[*Test_TC45_DNAC_provision_all_aps*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=308874&size=163405&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F01%2F28%2F03%2F36%2Fenv_auto_job.2023Jan28_03:36:45.399597.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]*/*[*test1_provision_all_aps*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=309021&size=163093&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F01%2F28%2F03%2F36%2Fenv_auto_job.2023Jan28_03:36:45.399597.zip&ats=%2Fusers%2Frvonmize%2Fpyats-inst&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-02-23T06:13:37.554+0000,"h2. Description

* *DNAC Release_Version Tested:*  +*Ghost Patch 1 2.1.613.70190 RC5*+ 
* *Device Image Used:* 17.11.1, ISE -3.1P6 ,
* *Testbed:* Sanity Testbed 7
* *Branch Used:* private/Ghost-ms/sanity__api__auto
* *ScriptName:*\testcases\sanityusecases\FEWAccessPointAndCLients\ap_sync_provisioning_clients_roaming.py
* *Solution Input File :* dnac-auto/configs/config_48hr_test/solution_test_input.json
* *Testcases Impacted:* [Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr27_23:38:27.229556.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   [Test_TC2_DNAC_provision_all_aps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1730451&size=774129&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr27_23:38:27.229556.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_provision_all_aps
* *Failed Trade Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1731213&size=212789&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr27_23:38:27.229556.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1731213&size=212789&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr27_23:38:27.229556.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
* *Issue details/analysis:*

A similar issue hitting  in  on the TB7 Ghost P1 RC5 execution in optimized code, where the AP Provisioned failed for the Rf Profile LOW line which is already present in the database . This resulted in the failure of all AP Provisions.
*NCWL10307: APWirelessConfiguration FLOOR1_LEVEL1_LOW_da74e with Id a180558d-9060-4f36-b1ff-aeea48ea76a1 SiteId 956f341b-0baa-40e9-9c2a-1e3d6083c50b and RfProfile LOW is already present in the database*


* *Failed Log Snapshot :* 

{noformat}6574:  Provisioning AP of device failed for reason:NCWL10307: APWirelessConfiguration FLOOR1_LEVEL1_LOW_da74e with Id a180558d-9060-4f36-b1ff-aeea48ea76a1 SiteId 956f341b-0baa-40e9-9c2a-1e3d6083c50b and RfProfile LOW is already present in the database
6575:  Library group ""provision_wireless"" method ""provision_unified_ap_devices"" returned in 0:00:36.730724
6576:  Test returned in 0:00:36.795738
6577:  Failed reason: Result: AP Provision failed
6578:  The result of section test1_provision_all_aps is => FAILED{noformat}
* *Solution Input File Changes:*  Hi [~accountid:63f50be9e76fc61320f4eab3] 
Can you please prioritize this jira

Thanks,
Anusha John Similar issue has been seen on recent Ghost P2 70780 builds
Failed Log: [test1_provision_all_aps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=49958609&size=169027&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul06_03:33:51.998558.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  
{{Provisioning AP of device failed for reason:NCWL10307: APWirelessConfiguration FLOOR1_LEVEL1_LOW_c042a with Id a7ddf29b-6195-46cf-8eb5-7b33aa6daeec SiteId 3de600fe-cbd0-44c6-a3d1-805b8c6a0d8a and RfProfile LOW is already present in the database }}

{{137365: Failed reason: Result: AP Provision failed }} h2. Description

* *DNAC Release_Version Tested:* +*Hulk 2.3.7.0-70414.*+
* *Device Image Used:* 17.9.4 PRD12 FC1
* *Testbed:* Sanity Testbed 2
* *Branch Used:* private/Hulk-ms/sanity_delay_testing
* *Solution Input File :* dnac-auto/configs/config_48hr_test/solution_test_input.json
* *Testcases Impacted:* [Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr27_23:38:27.229556.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   [Test_TC2_DNAC_provision_all_aps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1730451&size=774129&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr27_23:38:27.229556.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_provision_all_aps
* *Failed Trade Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11906030&size=747220&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul11_10:04:54.988978.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11906030&size=747220&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul11_10:04:54.988978.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
* *Issue details/analysis:*
A similar issue hitting in on the TB2 Hulk Delay Testing execution, where the AP Provisioned failed for the Rf Profile LOW line which is already present in the database . This resulted in the failure of all AP Provisions.
NCWL10307: APWirelessConfiguration FLOOR1_LEVEL1_LOW_125de with Id eb4de52f-4d59-4532-b004-cea24a0866bb SiteId adc359b5-86e0-4b8f-ae51-1b079e7cf4ef and RfProfile LOW is already present in the database 

 36180: 
 Config Preview Activity failed with reason: NCWL10307: APWirelessConfiguration FLOOR1_LEVEL1_LOW_125de with Id eb4de52f-4d59-4532-b004-cea24a0866bb SiteId adc359b5-86e0-4b8f-ae51-1b079e7cf4ef and RfProfile LOW is already present in the database

{noformat}36181: 
{noformat}

{noformat}36182: 
 Activity: 9bf9e391-dda6-4b84-bc42-53a772e42c7a Trigger job: {'id': '141681e4-d541-4b77-96fd-576e98f4c16f', 'triggeredJobTaskId': '3ccb28ec-2cd4-40a3-87f5-c36904d6d61d', 'triggeredTime': 1689103106705, 'status': 'FAILED', 'failureReason': 'NCWL10307: APWirelessConfiguration FLOOR1_LEVEL1_LOW_125de with Id eb4de52f-4d59-4532-b004-cea24a0866bb SiteId adc359b5-86e0-4b8f-ae51-1b079e7cf4ef and RfProfile LOW is already present in the database', 'triggeredJobId': '141681e4-d541-4b77-96fd-576e98f4c16f'}{noformat}

{noformat}36269: 
 activity_id is False. Config preview task failed for description Provisioning Unified APs at time 1689103103.5437863 - Configuration Preview{noformat} This issue has been encountered on Hulk P1 - which is failing with below error and the test is not BLOCKING even after failure. 

Due to which there are lot of failures of other tests that were dependent on APs provisioning

Error snip: 
{{Config Preview Activity failed with reason: NCWL10307: APWirelessConfiguration FLOOR1_LEV_AI_RF_PROF_9f25e with Id 0d78a5db-90f0-46a9-9e75-5975da22c5cf SiteId 7b879bde-4af5-4922-b656-c574b0230805 and RfProfile AI_RF_PROFILE is already present in the database }} 

Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=617590&size=763352&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug09_08:53:33.507883.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=617590&size=763352&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug09_08:53:33.507883.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [ENG-SDN / dnac-auto / bab9e2c6573 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/bab9e2c657300da0ea1c2432cdac001b4b266d76]



In upgrade scenario, using reprovision method instead Closing Jira as we got pass log on Ghost P2 RC3 and will validate on hulk by next week
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=269781&size=435940&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug23_21:12:07.142891.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=269781&size=435940&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug23_21:12:07.142891.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Thanks [~accountid:63f50bcece6f37e5ed93c87e]  Jira was long pending This issue is still seen in the SANITY runs 
Recently on HULK P2 runs we are seeing AP provisioning failures
 LOG: [test1_provision_all_aps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=2290036&size=1472759&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep06_01:33:05.632831.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
{{failureReason': 'NCWL10307: APWirelessConfiguration FLOOR1_LEV_AI_RF_PROF_72755 with Id 492cf9dd-113b-413c-b814-2c5b114c2884 SiteId 8937534b-181f-4798-b63c-0d0871287774 and RfProfile AI_RF_PROFILE is already present in the database'}}

Please add the fix in REG and LAN script as well as in OPTIMIZED code

*Branch*: private/HulkPatch-ms/api-auto, private/HulkPatch2-ms/api-auto
 [~accountid:5e1415780242870e996f0b2f] is this re-opened one a rerun scenario? This is a different auton if so, and should be considered an enhancement. Hi [~accountid:63f50bcece6f37e5ed93c87e]  This is not a rerun scenario , The issue has been encountered in the initial runs itself.","['Auton', 'Ghost', 'Hulk', 'Sanity', 'Upgrade', 'exsi', 'hulk-vm-sanity']",Andrew Chen,Resolved,Avril Bower
SEEN-1227,https://miggbo.atlassian.net/browse/SEEN-1227,[Auton]   Test_TC202_site_tags_negative_operations /   test2_subtest1_verify_addition_of_wireless_profiles_to_site_bld_floor," 

*Reporter Analysis:* Manually is working Add a new site tag, then assign the new tag to the same WLAN profile .

*Description:*  I tried  New York, San Jose , and SAN-FRANCISCO, For your reference, I have attached a snapshot ** 

*Branch Name:  private/Ghost -ms/sanity_api_auto*

*Script file/Usecase:* *solution_test_sanityecamb_lan,*

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*  https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1607510&size=254733&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb21_22:29:14.545067.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log:N/A*

*Testbed details:* https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-3",2023-02-24T15:04:31.967+0000,"+*Pass log:*+
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1164132&size=58747&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb24_12:33:45.507706.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS","['Auton', 'Ghost', 'Issue', 'Sanity']",Moe Saeed,Closed,Avril Bower
SEEN-1232,https://miggbo.atlassian.net/browse/SEEN-1232,Device Installation on DNAC for Wireless Devices C9800-L not mapped correctly," 
 * *DNAC Release_Version Tested: Halleck 2.3.3.7-72319* 
 * *Device Image Used:*  BLD_V179_THROTTLE_LATEST_20230127_180724
 * *Testbed:* sytem-tb12
 * *Branch Used:* private/Halleck-ms/api-auto
 * *Script Name:*  
 * *Failed Trade Log:(Please Refer TC33) :* 
 * *Issue details/analysis:*

 

[https://cdetsng.cisco.com/summary/#/defect/CSCwe25507]

 

Issue seen while doing SWIM upgrade on C9800-L platform devices, script is mapping to different image for this C9800-L platform

During SWIM upgrade on 2.3.7.0 from DNAC, on platform C9800-L (KATAR) device installed with ""C9800-universalk9_wlc.BLD_V179_THROTTLE_LATEST_20230127_180724.SSA.bin"" which is wrongly mapped, according to the platform, actual image is to be ""C9800-L-universalk9_wlc.BLD_V179_THROTTLE_LATEST_20230127_180724.SSA.bin"".

I have attached the Screen-Shots for reference.",2023-02-27T23:16:03.370+0000,https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/365354d4c808e7a675e1978e2d7396f3139bceac,['Auton'],Pawan Singh,Resolved,Avril Bower
SEEN-1254,https://miggbo.atlassian.net/browse/SEEN-1254,[Auton][Guardian][Groot][Ghost] - Talos block list apply on cluster maglev fails with prompt issue,"*Uber ISO Version tested :* 
Promoted Guardian Patch4 RC2 and RC3 - *2.1.518.72310 and 2.1.518.72319 respectively, Non-FIPS, PUBSUB enabled*

*Script Name:* solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py, solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB1
 

*Description :* 

Tried executing the Talos TC on build using *talos_fixes* branch ([https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4159/overview]) using *private/Groot-ms/api-auto*

 

We see Verify Talos block list on cluster is failing due to maglev command apply unable to execute due to some prompt issue. So eventually subTC5 also fails. We have not made any changes with respect to session settings. From the log we see its timing out with in 1 min which is not expected and seems something wrong.
 

*Failed log:*  [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fsr_mb_multi_sites_mdnac.2023Mar01_04:10:51.559993.zip&atstype=ATS] -> Refer TC225.4

Please refer Team space - webexteams://im?space=73157080-560a-11ed-9ce3-0b88e3da78a1 for more details.",2023-03-01T14:31:00.481+0000,"PR:
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4869/overview https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4869/overview Based on the latest fix added - [31f41d0e313|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/31f41d0e3130ea024b3f3f1af06d8342350b3a37] , we are still observing the issue on subTC 225.4

Branch used - *private/Guardian-ms/api-auto*

Uber ISO tested - *Guardian Patch4 RC3 - 2.1.518.72319*

*Failed log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fsr_mb_multi_sites_mdnac.2023Mar04_10:09:28.059326.zip&atstype=ATS] -> Refer TC225.4

 

*Error snip:*
12211: Action: Get Ip blocked lists !
12212: 'MaglevSystemCommandLine' object has no attribute 'connection_time'
12213: Test returned in 0:00:00.011402
12214: Failed reason: Failed to configure Talos blocked list on the maglev cluster!!

 

More details in Webex Team space - webexteams://im?space=73157080-560a-11ed-9ce3-0b88e3da78a1

  This error is due to missing some changes not Guardian done on Magliv script. 
I have added the changes from Ghost to the Guardian that you need. If you see any magliv issue, then it should be missing changes from other branches. 
https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9e3c3ffba9a2b4c54b87243c3a9237e335e3cca9","['Auton', 'Ghost', 'Groot', 'Guardian', 'Halleck', 'Hulk', 'MSTB1', 'Multisite']",Moe Saeed,Resolved,Avril Bower
SEEN-1259,https://miggbo.atlassian.net/browse/SEEN-1259,False PASS - Test_TC150_Client_AP_360 / test4_Verify_wireless_client_onboarding_values,"False PASS - [Test_TC150_Client_AP_360 / test4_Verify_wireless_client_onboarding_values|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=101945552&size=9645&archive=env_auto_job.2023Feb27_20:31:52.712910.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
{code:java}
359394: Using the URL /assurance/v1/host/client/detail PAYLOAD {'filters': {'macAddress': '00:E0:4C:1A:B1:26'}, 'startTime': 1677559192977, 'endTime': 1677645593081}359395: Resource path full url: https://10.22.45.61/api/assurance/v1/host/client/detail359398: Client Details is as follows {'version': '1.0', 'page': None}359399: Unable to to find the client with MAC:00:E0:4C:1A:B1:26 and Name:TB4-wireless-client13{code}
Even though the API response has no info about the Client, the use-case got concluded as ""PASS"".

Here, the assurance data was not available due to [CSCwe33525|https://cdetsng.cisco.com/webui/#view=CSCwe33525].

Pass log where Client Details were available - [TC150_test4|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=204720365&size=30555&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb10_13:06:45.634564.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-03-03T14:24:24.370+0000,"PRs:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6179/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6179/overview]
* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6180/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6180/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6181/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6181/overview] Approved and merged all three PRs. Marking this “Auton” as “Resolved”.",['Auton'],ThangQuoc Tran,Resolved,Avril Bower
SEEN-1260,https://miggbo.atlassian.net/browse/SEEN-1260,took 41 minutes to conclude as Failure - Test_TC161_Brownfield_Workflow_9800 / test8_learn_device_config,"Took 41 minutes to conclude as Failure -

[Test_TC161_Brownfield_Workflow_9800 / test8_learn_device_config|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=117551807&size=120668&archive=env_auto_job.2023Feb27_20:31:52.712910.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Please review the use-case code and check if the failure can be detected earlier.",2023-03-03T14:29:02.884+0000,"Changing retries to ~12 min instead, and adding show running-config output upon failure

 

 

[+https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4896/overview+]

 

Ghost ^

 

[+https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4898/overview+|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4896/overview]

 

Groot ^

 

[+https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4897/overview+]

 

Guardian ^

 ",['Auton'],Andrew Chen,Closed,Avril Bower
SEEN-1261,https://miggbo.atlassian.net/browse/SEEN-1261,AP Power Profile feature issue on AWS-MSTB,"Hi Majlona Aliaj,
 * *DNAC Release_Version Tested:* Halleck-2.1.660.70232
 * *Device Image Used:* 17.11.03prd2
 * *Testbed:* AWS-MS 
 * *Branch Used:* private/Ghost-ms/sanity_api_auto
 * *Script Name:* solution_test_3sites_sjc_nyc_sf.py
 * *Solution Input File :* dnac-auto/configs/config_48hr_test/solution_test_input.json
 * *Failed trade Log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-03/auto_MS_job.2023Mar02_02:15:07.518822.zip&atstype=ATS]

I have tried executing AP power profile feature after upgrading wireless devices to 17.11prd2.We have ewlc and eca devices on SF and NY sites and we have normal WLC FW-5520-1 on SJC site and 9130 series AP on SF site.

So according to feature wiki ,in solution input file ,it is updated as SANJOSE site for site tags parameters but we dont have either 9130 AP or ewlc or eca devices on Sanjose site.
 I am thinking this is causing the issue for the failure.

Could you please check on the log and confirm?

 ",2023-03-03T16:24:40.642+0000,"Hi [~accountid:63f50be71223974bc04b0534] ,
Could you please check on this issue and update us? [~accountid:63f50bddc1685a24e1314c87] Hi, I have updated the PR with the new changes two weeks ago. [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5343/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5343/overview]
Could you try running with the new updated code and let me know if you still see any issues? [~accountid:63f50bddc1685a24e1314c87]  / [~accountid:641058d57222b08f3e7064d0] pls. share the update if you have tested with the latest code.. Hi [~accountid:63f50be71223974bc04b0534] .Thanks for the fix.We are no more seeing the issue,but have hit one more issue and raised a separate Jira for it.I will close this ticket. PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6933/diff#testcases/forty_eight_hour/solution_test_sanityecamb_cert.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6933/diff#testcases/forty_eight_hour/solution_test_sanityecamb_cert.py]","['AWS_MSTB', 'Auton', 'Halleck', 'Multisite']",Majlona 'Luna' Aliaj,Resolved,Avril Bower
SEEN-1278,https://miggbo.atlassian.net/browse/SEEN-1278,[Auton][Guardian]- Script execution failing at initial point due to code issues in services/iseserv/iseapi.py file,"*Uber ISO Version tested :* 
Promoted Guardian Patch4 RC4 - *2.1.518.72323 respectively, Non-FIPS, PUBSUB enabled*

*Script Name:* solution_test_3sites_sjc_nyc_sf_mdnac_dr.py, solution_test_3sites_sjc_nyc_sf_mdnac.py, solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB1
 

*Description :* 

Hi Tran,

Initial script execution is failing due to code issues at services/iseserv/iseapi.py file. Look like [73f864f4645|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/73f864f46457f4559ca02f97dafef4e35ea1e67b#services/iseserv/iseapi.py] commit by [majegath@cisco.com|https://directory.cisco.com/dir/reports/majegath] is causing this issue. I am unable to assign it to this engineer while reporting Jira , hence added you for the Jira ticket.

*Error snip:*

(pyats_internal) [admin@st-ds-2 sr-mb1_Guardian]$ easypy job/sr_mb/sr_mb_multi_sites_mdnac.py -mailto sandshiv@cisco.com
2023-03-06T04:51:05: %EASYPY-WARNING: Failed create standard user folder: /auto/dna-sol/ws/rajsaran/sanity/pyats_internal/users/admin
2023-03-06T04:51:05: %EASYPY-WARNING: Check your directory permissions and/or available disk quota/free-space
2023-03-06T04:51:05: %EASYPY-WARNING: Falling back to using /home/admin/.pyats.
2023-03-06T04:51:06: %EASYPY-INFO: Starting job run: sr_mb_multi_sites_mdnac
2023-03-06T04:51:06: %EASYPY-INFO: Runinfo directory: /home/admin/.pyats/runinfo/sr_mb_multi_sites_mdnac.2023Mar06_04:51:05.631388
2023-03-06T04:51:06: %EASYPY-INFO: --------------------------------------------------------------------------------
2023-03-06T04:51:11: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:51:11: %EASYPY-INFO: | Clean Information |
2023-03-06T04:51:11: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:51:11: %EASYPY-INFO: \{'bringup': {}, 'cleaners': {}, 'devices': {}}

2023-03-06T04:51:11: %PLUGIN_BUNDLE-ERROR: No testbed given. Cannot collect CRFT.
2023-03-06T04:51:11: %PLUGIN_BUNDLE-ERROR: No testbed given. Cannot collect Btrace.
2023-03-06T04:51:11: %ATS-INFO: Checking all devices are up and ready is disabled, '--check-all-devices-up' must be set to True in case of pyats runs or '-check_all_devices_up' set to True in case of legacy easypy runs
2023-03-06T04:51:11: %ATS-INFO: Not enabling ctc/cflow plugin as argument '-coverage' not provided by the user. Disabling ctc/cflow collection plugin.


Running the script from here:/auto/dna-sol/ws/sr-mb1_Guardian


2023-03-06T04:51:11: %EASYPY-INFO: Starting task execution: Task-1
2023-03-06T04:51:11: %EASYPY-INFO: test harness = pyats.aetest
2023-03-06T04:51:11: %EASYPY-INFO: testscript = /auto/dna-sol/ws/sr-mb1_Guardian/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
2023-03-06T04:51:17: %EASYPY-ERROR: Caught error during task execution: Task-1: solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
2023-03-06T04:51:17: %EASYPY-ERROR: Traceback (most recent call last):
2023-03-06T04:51:17: %EASYPY-ERROR: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/aetest/main.py"", line 191, in run
2023-03-06T04:51:17: %EASYPY-ERROR: testscript = TestScript.from_source(self.testable,
2023-03-06T04:51:17: %EASYPY-ERROR: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/aetest/script.py"", line 73, in from_source
2023-03-06T04:51:17: %EASYPY-ERROR: module = loader.load(testable)
2023-03-06T04:51:17: %EASYPY-ERROR: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/utils/import_utils/flex.py"", line 64, in load
2023-03-06T04:51:17: %EASYPY-ERROR: module = self.load_module_from_file(obj)
2023-03-06T04:51:17: %EASYPY-ERROR: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/utils/import_utils/flex.py"", line 117, in load_module_from_file
2023-03-06T04:51:17: %EASYPY-ERROR: module = loader.load_module()
2023-03-06T04:51:17: %EASYPY-ERROR: File ""<frozen importlib._bootstrap_external>"", line 462, in _check_name_wrapper
2023-03-06T04:51:17: %EASYPY-ERROR: File ""<frozen importlib._bootstrap_external>"", line 962, in load_module
2023-03-06T04:51:17: %EASYPY-ERROR: File ""<frozen importlib._bootstrap_external>"", line 787, in load_module
2023-03-06T04:51:17: %EASYPY-ERROR: File ""<frozen importlib._bootstrap>"", line 265, in _load_module_shim
2023-03-06T04:51:17: %EASYPY-ERROR: File ""<frozen importlib._bootstrap>"", line 702, in _load
2023-03-06T04:51:17: %EASYPY-ERROR: File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
2023-03-06T04:51:17: %EASYPY-ERROR: File ""<frozen importlib._bootstrap_external>"", line 783, in exec_module
2023-03-06T04:51:17: %EASYPY-ERROR: File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
{color:#de350b}2023-03-06T04:51:17: %EASYPY-ERROR: File ""/auto/dna-sol/ws/sr-mb1_Guardian/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py"", line 65, in <module>{color}
{color:#de350b}2023-03-06T04:51:17: %EASYPY-ERROR: from services.commonlibs.sftopology import SFTopology{color}
{color:#de350b}2023-03-06T04:51:17: %EASYPY-ERROR: File ""/auto/dna-sol/ws/sr-mb1_Guardian/services/commonlibs/sftopology.py"", line 29, in <module>{color}
{color:#de350b}2023-03-06T04:51:17: %EASYPY-ERROR: from services.iseserv.iseapi import IseApi{color}
{color:#de350b}2023-03-06T04:51:17: %EASYPY-ERROR: File ""/auto/dna-sol/ws/sr-mb1_Guardian/services/iseserv/iseapi.py"", line 75{color}
{color:#de350b}2023-03-06T04:51:17: %EASYPY-ERROR: ""X-CSRF-TOKEN"": ""fetch""{color}
{color:#de350b}2023-03-06T04:51:17: %EASYPY-ERROR: ^{color}
{color:#de350b}2023-03-06T04:51:17: %EASYPY-ERROR: SyntaxError: invalid syntax{color}
{color:#de350b}2023-03-06T04:51:17: %CISCO-INFO: Start Telemetry - Task{color}
2023-03-06T04:51:18: %EASYPY-INFO: TIMS result uploading is not enabled
2023-03-06T04:51:26: %CISCO-INFO: Start Telemetry - Test
2023-03-06T04:52:04: %CISCO-INFO: Posted data to Kafka returning status code 200 within time 0:00:08.670812
2023-03-06T04:52:04: %GENIE-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:04: %GENIE-INFO: | Starting Device Image MD5 Hash Plugin |
2023-03-06T04:52:04: %GENIE-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:04: %GENIE-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:04: %GENIE-INFO: | Ending Device Image MD5 Hash Plugin |
2023-03-06T04:52:04: %GENIE-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:04: %CONTRIB-INFO: WebEx Token not given as argument or in config. No WebEx notification will be sent
2023-03-06T04:52:04: %EASYPY-INFO: --------------------------------------------------------------------------------
2023-03-06T04:52:04: %EASYPY-INFO: Job finished. Wrapping up...
2023-03-06T04:52:05: %EASYPY-INFO: Creating archive file: /home/admin/.pyats/archive/23-03/sr_mb_multi_sites_mdnac.2023Mar06_04:51:05.631388.zip
2023-03-06T04:52:05: %EASYPY-INFO: Uploading logs to TRADe...
2023-03-06T04:52:22: %CISCO-INFO: Start Telemetry - Archive Log Upload
2023-03-06T04:52:22: %EASYPY-INFO: TRADe upload completed.
2023-03-06T04:52:30: %CISCO-INFO: Posted data to Kafka returning status code 200 within time 0:00:08.099234
2023-03-06T04:52:30: %EASYPY-INFO: Log Archive uploaded to: https://ngdevx.cisco.com/services/taas/results/ae751f0f-5e70-4a66-86be-6f7010f4ac97
2023-03-06T04:52:30: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:30: %EASYPY-INFO: | TaaS URL |
2023-03-06T04:52:30: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:30: %EASYPY-INFO: 
2023-03-06T04:52:30: %EASYPY-INFO: TaaS Log Viewer
2023-03-06T04:52:30: %EASYPY-INFO: -------------
2023-03-06T04:52:30: %EASYPY-INFO: https://ngdevx.cisco.com/services/taas/results/ae751f0f-5e70-4a66-86be-6f7010f4ac97
2023-03-06T04:52:30: %EASYPY-INFO: 
2023-03-06T04:52:30: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:30: %EASYPY-INFO: | TRADe URL |
2023-03-06T04:52:30: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:30: %EASYPY-INFO: 
2023-03-06T04:52:30: %EASYPY-INFO: Result Viewer
2023-03-06T04:52:30: %EASYPY-INFO: -------------
2023-03-06T04:52:30: %EASYPY-INFO: https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-03/sr_mb_multi_sites_mdnac.2023Mar06_04:51:05.631388.zip&atstype=ATS
2023-03-06T04:52:30: %EASYPY-INFO: 
2023-03-06T04:52:30: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:30: %EASYPY-INFO: | !!! Exception in Task - Task-1 !!! |
2023-03-06T04:52:30: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:30: %EASYPY-INFO: Traceback (most recent call last):
2023-03-06T04:52:30: %EASYPY-INFO: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/aetest/main.py"", line 191, in run
2023-03-06T04:52:30: %EASYPY-INFO: testscript = TestScript.from_source(self.testable,
2023-03-06T04:52:30: %EASYPY-INFO: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/aetest/script.py"", line 73, in from_source
2023-03-06T04:52:30: %EASYPY-INFO: module = loader.load(testable)
2023-03-06T04:52:30: %EASYPY-INFO: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/utils/import_utils/flex.py"", line 64, in load
2023-03-06T04:52:30: %EASYPY-INFO: module = self.load_module_from_file(obj)
2023-03-06T04:52:30: %EASYPY-INFO: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/utils/import_utils/flex.py"", line 117, in load_module_from_file
2023-03-06T04:52:30: %EASYPY-INFO: module = loader.load_module()
2023-03-06T04:52:30: %EASYPY-INFO: File ""<frozen importlib._bootstrap_external>"", line 462, in _check_name_wrapper
2023-03-06T04:52:30: %EASYPY-INFO: File ""<frozen importlib._bootstrap_external>"", line 962, in load_module
2023-03-06T04:52:30: %EASYPY-INFO: File ""<frozen importlib._bootstrap_external>"", line 787, in load_module
2023-03-06T04:52:30: %EASYPY-INFO: File ""<frozen importlib._bootstrap>"", line 265, in _load_module_shim
2023-03-06T04:52:30: %EASYPY-INFO: File ""<frozen importlib._bootstrap>"", line 702, in _load
2023-03-06T04:52:30: %EASYPY-INFO: File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
2023-03-06T04:52:30: %EASYPY-INFO: File ""<frozen importlib._bootstrap_external>"", line 783, in exec_module
2023-03-06T04:52:30: %EASYPY-INFO: File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
2023-03-06T04:52:30: %EASYPY-INFO: File ""/auto/dna-sol/ws/sr-mb1_Guardian/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py"", line 65, in <module>
2023-03-06T04:52:30: %EASYPY-INFO: from services.commonlibs.sftopology import SFTopology
2023-03-06T04:52:30: %EASYPY-INFO: File ""/auto/dna-sol/ws/sr-mb1_Guardian/services/commonlibs/sftopology.py"", line 29, in <module>
2023-03-06T04:52:30: %EASYPY-INFO: from services.iseserv.iseapi import IseApi
2023-03-06T04:52:30: %EASYPY-INFO: File ""/auto/dna-sol/ws/sr-mb1_Guardian/services/iseserv/iseapi.py"", line 75
2023-03-06T04:52:30: %EASYPY-INFO: ""X-CSRF-TOKEN"": ""fetch""
2023-03-06T04:52:30: %EASYPY-INFO: ^
2023-03-06T04:52:30: %EASYPY-INFO: SyntaxError: invalid syntax
2023-03-06T04:52:30: %EASYPY-INFO: 
2023-03-06T04:52:30: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:30: %EASYPY-INFO: | Easypy Report |
2023-03-06T04:52:30: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:30: %EASYPY-INFO: pyATS Instance : /auto/dna-sol/ws/rajsaran/sanity/pyats_internal
2023-03-06T04:52:30: %EASYPY-INFO: Python Version : cpython-3.8.2 (64bit)
2023-03-06T04:52:30: %EASYPY-INFO: CLI Arguments : /auto/dna-sol/ws/rajsaran/sanity/pyats_internal/bin/easypy job/sr_mb/sr_mb_multi_sites_mdnac.py -mailto sandshiv@cisco.com
2023-03-06T04:52:30: %EASYPY-INFO: User : admin
2023-03-06T04:52:30: %EASYPY-INFO: Host Server : st-ds-2.cisco.com
2023-03-06T04:52:30: %EASYPY-INFO: Host OS Version : Red Hat Enterprise Linux 8.4 Ootpa (x86_64)
2023-03-06T04:52:30: %EASYPY-INFO: 
2023-03-06T04:52:30: %EASYPY-INFO: Job Information
2023-03-06T04:52:30: %EASYPY-INFO: Name : sr_mb_multi_sites_mdnac
2023-03-06T04:52:30: %EASYPY-INFO: Start time : 2023-03-06 04:51:11.855009-08:00
2023-03-06T04:52:30: %EASYPY-INFO: Stop time : 2023-03-06 04:51:18.372627-08:00
2023-03-06T04:52:30: %EASYPY-INFO: Elapsed time : 6.517618
2023-03-06T04:52:30: %EASYPY-INFO: Archive : /home/admin/.pyats/archive/23-03/sr_mb_multi_sites_mdnac.2023Mar06_04:51:05.631388.zip
2023-03-06T04:52:30: %EASYPY-INFO: 
2023-03-06T04:52:30: %EASYPY-INFO: Total Tasks : 1 (1 failed to run)
2023-03-06T04:52:30: %EASYPY-INFO: 
2023-03-06T04:52:30: %EASYPY-INFO: Overall Stats
2023-03-06T04:52:30: %EASYPY-INFO: Passed : 0
2023-03-06T04:52:30: %EASYPY-INFO: Passx : 0
2023-03-06T04:52:30: %EASYPY-INFO: Failed : 0
2023-03-06T04:52:30: %EASYPY-INFO: Aborted : 0
2023-03-06T04:52:30: %EASYPY-INFO: Blocked : 0
2023-03-06T04:52:30: %EASYPY-INFO: Skipped : 0
2023-03-06T04:52:30: %EASYPY-INFO: Errored : 0
2023-03-06T04:52:30: %EASYPY-INFO: 
2023-03-06T04:52:30: %EASYPY-INFO: TOTAL : 0
2023-03-06T04:52:30: %EASYPY-INFO: 
2023-03-06T04:52:30: %EASYPY-INFO: Success Rate : 0.00 %
2023-03-06T04:52:30: %EASYPY-INFO: 
2023-03-06T04:52:30: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:30: %EASYPY-INFO: | Task Result Summary |
2023-03-06T04:52:30: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:30: %EASYPY-INFO: Empty - did something go wrong?
2023-03-06T04:52:30: %EASYPY-INFO: 
2023-03-06T04:52:30: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:30: %EASYPY-INFO: | Task Result Details |
2023-03-06T04:52:30: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-03-06T04:52:30: %EASYPY-INFO: Empty - did something go wrong?
2023-03-06T04:52:30: %EASYPY-INFO: Sending report email...
2023-03-06T04:52:39: %EASYPY-INFO: Done!

Pro Tip
-------
 Try the following command to view your logs:
 pyats logs view

(pyats_internal) [admin@st-ds-2 sr-mb1_Guardian]$

 ",2023-03-07T07:05:47.150+0000,"If you see this issue again, please reopen it.

I will close it for now.

thanks","['Auton', 'Guardian', 'MSTB1', 'Multisite']",Moe Saeed,Resolved,Avril Bower
SEEN-1279,https://miggbo.atlassian.net/browse/SEEN-1279,[Auton][IBSTE] : Site Tag feature integration on IBSTE,"Hi [~63f50bfce8216251ae4d59d5]

While integrating Site tag feature on IBSTE we are seeing below issue
2023-03-02T21:40:20: %AETEST-ERROR: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/IBSTE/optimizedIbste_regression/services/commonlibs/test_wrapper.py"", line 301, in wrapper
2023-03-02T21:40:20: %AETEST-ERROR: result = testfunc(func_self, **kwargs)
2023-03-02T21:40:20: %AETEST-ERROR: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/IBSTE/optimizedIbste_regression/testcases/ibsteusecases/siteTagsNegativeOperations/siteTagsNegativeOperations.py"", line 73, in test1_verify_creating_of_site_building_floor
2023-03-02T21:40:20: %AETEST-ERROR: new_floor = site_handle.input_data['site_tags']['new_site']
2023-03-02T21:40:20: %AETEST-ERROR: AttributeError: 'NoneType' object has no attribute 'input_data'

As discussed over Webex chat issue is due to Network Hierarchy created for IBSTE is different compared to Sanity IBSTE cluster. 

On IBSTE we have New York as main area as compared to Sanity IBSTE testbed as USA as main area..

Please move back the Jira to assign state once PR is merged.

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Foptimized_ibste_job.2023Mar02_21:38:36.970022.zip&atstype=ATS]

Tested Version#2.1.613.70106

Device Image# 17.9.3

Thanks,

Divakar",2023-03-07T07:25:32.483+0000,"Hi Pawan, 
Can you please help us in fixing this issue, It was blocking the integration of SIte Tags Negative Operation Feature on IBSTE Testbed. [~accountid:63f50bfce8216251ae4d59d5] pls. help checking on the issue reported here. Hi [~accountid:63f50bfce8216251ae4d59d5] ,
Any update on this Jira. Did you get chance to fix this?
Can you please help us in fixing this issue, It was blocking the integration of SIte Tags Negative Operation Feature on IBSTE Testbed [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5449/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5449/overview] Hi Moe,we are hitting this issue again on Hulk Patch2,Could you please check on it once.
Failed Log:[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-siteTagsNegativeOperations.py-171-siteTagsNegativeOperations&begin=7747&size=31349&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Foptimized_ibste_job.2023Nov26_19:27:52.202691.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-siteTagsNegativeOperations.py-171-siteTagsNegativeOperations&begin=7747&size=31349&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Foptimized_ibste_job.2023Nov26_19:27:52.202691.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Ghost', 'IBSTE']",Moe Saeed,Reopened,Avril Bower
SEEN-1280,https://miggbo.atlassian.net/browse/SEEN-1280,[Auton]:Test_TC60_Verify_DHCP_server_change_on_segments/test3_verify_change_sharedsecret/test4_verify_provision_the_devices_fabric1,"*Reporter Analysis:* 

*Test_TC60_Verify_DHCP_server_change_on_segments/test3_verify_change_sharedsecret*

TC 60 is configuring shared secret and saving in Global settings , after saving it will be pushing to devices and ISE side.Enhance the script as after shared secret key is pushed  we want shared key to be displayed in logs and  validate and match every device key  with *""newcisco""*

*Test_TC60_Verify_DHCP_server_change_on_segments/test4_verify_provision_the_devices_fabric1*

Once device provision is success  enhance the script  to print and verify *""sh run | i key""* to display *shared key* to understand whether the key is pushed corrctly or not

*Description:*   

Pass Log :

test3_verify_change_sharedsecret
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=28407686&size=56367&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_auto_job.2023Mar01_05:25:46.847642.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

test4_verify_provision_the_devices_fabric1:


[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=841601&size=1414385&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_auto_job.2023Mar01_22:27:12.148877.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

*Branch Name:  private/Guardian-ms/sanity_api_auto*

*Script file/Usecase:* solution_test_sanityecamb_lan.py  solution_test_sanityecamb.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

 

*Testbed details:* NA",2023-03-07T10:59:05.668+0000,"Perform a show to grep the. new key in both devices and ISE. 

if it is encoded, decode the key with base64 decoder. 

Validate it is correct. PRs:

* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5298/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5298/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5301/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5301/overview]
* Groot: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5314/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5314/overview]
* Guardian: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5319/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5319/overview]","['Auton', 'Ghost', 'Groot', 'Guardian', 'Halleck', 'Sanity', 'Shockwave']",ThangQuoc Tran,Resolved,Avril Bower
SEEN-1281,https://miggbo.atlassian.net/browse/SEEN-1281,Test_TC45_DNAC_TSIM_static_onboarding_verifications  /   test6_prepare_aps_dot1x,"*Reporter Analysis:* 

Script is taking wrong command to clear capwap .
It is using *""debug capwap console cli""* instead of *""capwap ap erase all""*

*Description:*  
AP687D.B402.648C#
149633:  +++ AP687D.B402.648C with via 'a': executing command 'debug capwap console cli' +++
debug capwap console cli  ^ % Invalid input detected at '^' marker.

*Branch Name:  private/Guardian-ms/sanity_api_auto*

*Script file/Usecase:* 
solution_test_sanityecamb_lan.py

*Source Team:  AWS Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=43227288&size=39404&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_auto_job.2023Mar06_03:23:27.437639.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Pass Log from another Cluster:*
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27650291&size=23118&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_auto_job.2023Mar06_10:56:12.503676.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Testbed details:* NA",2023-03-07T12:00:37.707+0000,"Hi [~accountid:61efa8c457b25b006877eda3], I didn’t see this issue on tb1 and it works fine. Please recheck it on your cluster.

Pass log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-thangqtr/users/thangqtr/archive/23-04/sanity_TB1.2023Apr04_02:55:41.304495.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-thangqtr/users/thangqtr/archive/23-04/sanity_TB1.2023Apr04_02:55:41.304495.zip&atstype=ATS] Hi [~accountid:61efa8c457b25b006877eda3], Can you please close this ticket if this issue did not be seen again? Hi [~accountid:63f50bd34c355259db9ccc4d] 
I am seeing issue again on my cluster:

*Description:*  
{{AP687D.B402.648C# debug capwap console cli ^ % Invalid input detected at '^' marker. AP687D.B402.648C# Trying 10.4.15.10... Connected to 10.4.15.10. Escape character is '^]'}}

*Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* 
solution_test_sanityecamb_lan.py

*Source Team:  AWS Sanity*

*Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=40438826&size=39732&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct31_00:53:57.843438.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=40438826&size=39732&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct31_00:53:57.843438.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Error from AP side while manually tried:
Username: wlcaccess
Password:
AP687D.B402.648C>en
Password:
AP687D.B402.648C#debug capwap console cli
                               ^
% Invalid input detected at '^' marker.
AP687D.B402.648C#debug capwap console% Unrecognized command
AP687D.B402.648C#debug capwap console % Unrecognized command
AP687D.B402.648C#debug capwap console 



To access AP and DNAC:

Login to RDP:

|10.22.45.112|administrator/C1sco123!|

telnet 10.4.15.10 2012(wlcaccess/Lablab#123,en:Lablab123)

Cluster IP:

[172.35.16.15|mailto:maglev@172.35.16.151]0 admin/maglev1@3
 Cluster is being used for execution. Will back to this issue when the testbed is in an ideal state.

[~accountid:61efa8c457b25b006877eda3] Could you please provide the correct cluster ip address if it’s available? Raised PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7933/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7933/overview]","['AWS_Sanity', 'Auton', 'Guardian']",ThangQuoc Tran,Pending Code Review,Avril Bower
SEEN-1282,https://miggbo.atlassian.net/browse/SEEN-1282,[Auton] Guardian - Wireless Solution Sanity - test4_ise_user_validation under TC3 gets errored,"* *DNAC Release_Version Tested:* Guardian_P4_2.1.518.72323
 * *Device Image Used:* 17.9.3
 * *Branch Used:* rcdn/Guardian-ms/api-auto (Latest sync to Main Branch done before reg run start today - Main Branch Used: Private/Guardian-ms/api-auto)
 * *Script Name:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py
 * *Error faced:* test4_ise_user_validation under TC3 gets errored
 * *Failed Trade Log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/23-03/sanitycombine.2023Mar06_10:56:44.674339.zip&atstype=ATS] 
 * *ISE IP:* 10.88.187.208 - admin/Maglev123
 * *ISE Info on Yamls Used:* [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/rcdn_solution/pod1/dnac_cat9k_tb4.yaml?at=refs%2Fheads%2Frcdn%2FGuardian-ms%2Fapi-auto] and [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/rcdn_solution/pod1/solution_sanitycombine.json?at=refs%2Fheads%2Frcdn%2FGuardian-ms%2Fapi-auto] 
 * *Team/Source:* Wireless Solution Sanity Regression Testing Team under Loi**",2023-03-07T18:18:53.892+0000,"Please Follow up with Pawan or Manimaran to check what you are missing in your ISE. As per offline discussion with Pawan, Looks like we need to do below pre-requisites, as per new testcase requirement 

1) create below username/password on ISE UI under Administration -> System -> Admin Access -> Administrators -> Admin Users

username=ersadmin ; password=Maglev123 and Admin Groups=ERS Admin

2) update testbed yaml for ISE (Under API section, update it ersadmin as api user.). 

devInfo of ISE -> Replace value of custom.api.username as ersadmin from admin","['Auton', 'Guardian']",Moe Saeed,Closed,Avril Bower
SEEN-1287,https://miggbo.atlassian.net/browse/SEEN-1287,[Auton] DNAC Version to be added into meta.json,"*Reporter Analysis:* 

We would need to print the DNAC label id into the meta.json file as currently it print the default value 1.5.1 

 

*Script file/Usecase:*  optimized code and legacy code

*Source Team:  Sanity*

 

Regular script log : [https://earms-trade.cisco.com/tradeui/logs/fileviewer?logfile=meta.json&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_auto_job.2023Feb23_08:05:10.521665.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=pyATS&from=trade&view=all]

Optimized log : [https://earms-trade.cisco.com/tradeui/logs/fileviewer?logfile=meta.json&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_auto_job.2023Mar06_10:56:12.503676.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=pyATS&from=trade&view=all]

 

Currently get_dnac_version_information API returns version which is 1.5.1 and in this place need to print the DNAC UBER Build .

From the API call : [https://10.30.0.100/api/system/v1/maglev/packages],

                              ""_labels"":

{                                            ""release"": [""dnac:2.3.3.7.72323""]                              } ",2023-03-08T06:13:07.968+0000,PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4931/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4931/overview],['Auton'],ThanhTan Nguyen,Closed,Avril Bower
SEEN-1336,https://miggbo.atlassian.net/browse/SEEN-1336,[Auton]Halleck: Test_TC4_Client_AP_360/test2_verify_AP360_missing_KPI Key Error: 'PowerStatus',"*Reporter Analysis:* For the below api call Powerstatus key is missing, Needs script enhancement for the api call

*Description:  The error from log or more info* 

*Branch Name:  private/Halleck-ms/sanity_api_auto*


 *Script file/Usecase:* [Client_AP_360|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_reachability_issue.py-164-FEWapReachabilityIssue&begin=332628&size=1698159&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_optimized_auto_job.2023Mar03_07:42:39.331944.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_reachability_issue.py-164-FEWapReachabilityIssue&begin=1914465&size=17656&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_optimized_auto_job.2023Mar03_07:42:39.331944.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Pass Log:* 

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8]",2023-03-09T12:57:37.467+0000,"Hi [~accountid:63f50be24e86f362d39acde8] ,

I was run this test case on Halleck and Ghost branches. Use Sanity TB1 and Sanity TB8. But all are passed.

I think there was a problem with the API before. But now it's back to normal.

Please check the trade log link below:

private/Ghost-ms/sanity_api_auto - Sanity TB1
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB1.2023Mar31_02:09:40.158530.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB1.2023Mar31_02:09:40.158530.zip&atstype=ATS]

private/Ghost-ms/sanity_api_auto - Sanity TB8:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB8.2023Mar31_02:18:43.535033.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB8.2023Mar31_02:18:43.535033.zip&atstype=ATS]

private/Halleck-ms/sanity_api_auto - Sanity TB1
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB1.2023Mar31_01:49:20.258271.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB1.2023Mar31_01:49:20.258271.zip&atstype=ATS]

private/Halleck-ms/sanity_api_auto - Sanity TB8:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB8.2023Mar31_01:38:39.574249.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-03/sanity_TB8.2023Mar31_01:38:39.574249.zip&atstype=ATS]

Thanks. Hi [~accountid:63f50be24e86f362d39acde8] , The Testcase works fine for me. Is it work for you? Could you check again? If there isn’t an issue, can you close this auton? Hello [~accountid:63f50bcafb3ac4003fa2c6dd] as informed, we am still observing the test case to be failing. Hi [~accountid:63f50be24e86f362d39acde8] as discussed before via Webex, you still see the Test case fail in the Halleck branch and on Sanity Testbed 2 (10.195.227.31).

However, When I pull the latest Halleck code and run the testcase, it passes without issue.

Test_TC150_Client_AP_360  /   test2_verify_AP360_missing_KPI

private/Halleck-ms/sanity_api_auto - Sanity TB2 - testcases/forty_eight_hour/solution_test_sanityecamb.py

The log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-04/sanity_TB2.2023Apr04_01:07:10.109750.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-04/sanity_TB2.2023Apr04_01:07:10.109750.zip&atstype=ATS]

Could you please check the testcase again with the latest Halleck code? Thanks. Hi [~accountid:63f50bcafb3ac4003fa2c6dd] 

Let me check this Test case and update this thread Hi [~accountid:63f50be24e86f362d39acde8], Are you still observing the test case to fail with the latest Halleck code?

If you still see the Test case fail in the Halleck branch and on Sanity Testbed 2, Can you share the Fail log? Hello [~accountid:63f50bcafb3ac4003fa2c6dd] Yes, we still observe this to be failing

Here is the latest Failed Log that you requested:

[test2_verify_AP360_missing_KPI|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_reachability_issue.py-164-FEWapReachabilityIssue&begin=1081772&size=18156&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr10_05:37:47.582884.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  Hi [~accountid:63f50be24e86f362d39acde8] Can you fix the device AP: {{AP6C8B.D3FA.353A}} to {{Reachable}}?

Because I see you run the testcase fails with that device AP, but now its status is {{Unreachable}} so I can’t use that device to debug. When I run testcase pass with all of another device AP. Hello [~accountid:63f50bcafb3ac4003fa2c6dd] the AP is now reachable.

I tried to rerun the Test case and could see it to fail  Thanks [~accountid:63f50be24e86f362d39acde8]. I will fix it. # PR Halleck-ms/api_auto link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5405/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5405/overview]
# PR Ghost-ms/api_auto link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5406/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5406/overview]
# Test Case:  {{TC_Client_AP_360}}
# Sanity Testbed 2
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link- Halleck-ms/sanity_api_auto: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-nhanhngu%2Fusers%2Fnhanhngu%2Farchive%2F23-04%2Fsanity_TB2.2023Apr13_20:13:08.533327.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-nhanhngu%2Fusers%2Fnhanhngu%2Farchive%2F23-04%2Fsanity_TB2.2023Apr13_20:13:08.533327.zip&atstype=ATS]
# Trade log link- Ghost-ms/sanity_api_auto: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-04/sanity_TB2.2023Apr13_23:42:41.977262.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-04/sanity_TB2.2023Apr13_23:42:41.977262.zip&atstype=ATS] Hi [~accountid:63f50be24e86f362d39acde8], Could you help me review this PR Auton? Hello [~accountid:63f50bcafb3ac4003fa2c6dd],

Will review this our latest Ghost Run and update Hello [~accountid:63f50bcafb3ac4003fa2c6dd] 

We are now observing this failure in Hulk Testbed as well.

Here is the Failed log for reference:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=47603261&size=15107&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul12_11:25:06.733047.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=47603261&size=15107&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul12_11:25:06.733047.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Release: Hulk

Version: 2.3.7.0-70414 # PR Hulk-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6284/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6284/overview]
# Test Case:  {{TC_Client_AP_360/test2_verify_AP360_missing_KPI}}
# Sanity Testbed 2
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link Hulk-ms/api_auto:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-07/sanity_TB2.2023Jul14_03:00:57.898395.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-07/sanity_TB2.2023Jul14_03:00:57.898395.zip&atstype=ATS] Hi [~accountid:63f50be24e86f362d39acde8], I already raised PR on the Hulk branch for this Auton. Please review it, Thanks. The PR was merged into {{private/Hulk-ms/api-auto}}.","['Auton', 'Ghost', 'Halleck', 'Hulk', 'Issue', 'Shockwave', 'optimized', 'sanity']",NhanHuu Nguyen,Resolved,Avril Bower
SEEN-1337,https://miggbo.atlassian.net/browse/SEEN-1337,[Auton]Halleck: Task-assurance_maintaince_mode_on_off.py-165-assuranceMaintanenceMode/test5_verify_ap_health_score_when_wlc_or_eca_under_maintenance | Change in Key Value,"*Reporter Analysis:* During the maintenance mode, Devices Health score will be negative, However in this case the AP's score was expected to be negative.

We see the AP health score  No health score records on some APs

As per the bug: [CSCwe19169|https://cdetsng.cisco.com/webui/#view=CSCwe19169], DE suggested that there is change is API payloads and the script needs to be enhanced accordingly

*Description:  The error from log or more info* 

*Branch Name:  private/Halleck-ms/sanity_api_auto*
 *Script file/Usecase:* [assuranceMaintanenceMode|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_maintaince_mode_on_off.py-165-assuranceMaintanenceMode&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_optimized_auto_job.2023Mar03_07:42:39.331944.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_maintaince_mode_on_off.py-165-assuranceMaintanenceMode&begin=1438013&size=17801&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_optimized_auto_job.2023Mar03_07:42:39.331944.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Pass Log:* https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_maintaince_mode_on_off.py-165-assuranceMaintanenceMode&begin=56390&size=304373&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan05_05:43:12.716711.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8]",2023-03-09T13:11:57.418+0000,"# PR Halleck-ms/sanity_api_auto link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5371/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5371/overview]
# PR Halleck-ms/api_auto link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5372/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5372/overview]
# Test Case:  {{TC_enable_maintenance_mode_on_devices_and_validate_maintenance_mode}}
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link- Halleck-ms/sanity_api_auto: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-04/sanity_TB2.2023Apr17_02:27:00.575564.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-04/sanity_TB2.2023Apr17_02:27:00.575564.zip&atstype=ATS]
# Trade log link- Halleck-ms/api_auto: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-04/sanity_TB2.2023Apr17_20:30:09.432272.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-04/sanity_TB2.2023Apr17_20:30:09.432272.zip&atstype=ATS]","['Auton', 'Ghost', 'Halleck', 'Issue', 'optimized', 'sanity']",NhanHuu Nguyen,Resolved,Avril Bower
SEEN-1338,https://miggbo.atlassian.net/browse/SEEN-1338,[Auton]Halleck: Test_TC3_wireless_policy_PSK/test4_deploy_wireless_policy_PSK | 'bool' object is not subscriptable,"*Reporter Analysis:* We see the below error when the script is executed

*Description:  The error from log or more info* 

*Branch Name:  private/Halleck-ms/sanity_api_auto*
 *Script file/Usecase:* 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=1709227&size=1294666&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_optimized_auto_job.2023Mar03_07:42:39.331944.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

 

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8]",2023-03-09T13:49:42.863+0000,"I am going to JUNK it as this is a product issue, the TASK waited for ~>600 secs but did not complete it. raise a product bug in it why the task is never completed.  Before raise a Jira you should analyze the logs why is the test cases failing or erroring out. ","['Auton', 'Ghost', 'Halleck', 'Issue', 'optimized', 'sanity']",Pawan Singh,Resolved,Avril Bower
SEEN-1346,https://miggbo.atlassian.net/browse/SEEN-1346,"sftopology / configure_dhcp_server_on_fusion / configure_static parameter should default to ""False""","Earlier, Static config was required to fix IP address of the client to be sure that it gets the same IP.
But we don't need it now. We could remove the static mapping.

Hence, set the sftopology / configure_dhcp_server_on_fusion / *configure_static* parameter default to ""False"".",2023-03-13T19:36:42.390+0000,"Raised required PR for Ghost branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4986/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4986/overview]

After merger, cherry-picked to Halleck, hallack_esxivm_branch Raised PR for Groot: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4987/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/4987/overview] and cherry-picked to Guardian Branch",['Auton'],Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1348,https://miggbo.atlassian.net/browse/SEEN-1348,[Auton]:Guardian: Test_TC104_wifi_readniness /test1_wifi_readniness,"*Reporter Analysis:*
In the DNAC-C  showing correctly. The script, it should check  wi-fi -6 & wi-fi-6E also ** 

*Description:*  **  
****

**95: Wi-Fi 6 APs: 6, Non Wi-Fi 6 APs: 43

4396: Wifi6 capable APs are 7 : ap names: ['AP1416.9D2E.1FD4', 'AP3C41.0EFE.20C0', 'AP5CE1.7629.C894', 'AP5CE1.7629.CEF0', 'AP687D.B45C.2054', 'APA00F.379C.3820', 'APF01D.2D1C.1AA8'],

4397: Wifi 6 Readiness is not as Expected!!  **

*it should check with MAUI ap with  Wi-fi-6E*
AP687D.B45C.2054 (maui AP )


*Branch Name:*  private/Guardian-ms/sanity_api_auto

*Script file/Usecase:* solution_test_sanityecamb_lan.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*
**

[Test_TC104_wifi_readniness|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=73229042&size=108722&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_auto_job.2023Mar06_10:56:12.503676.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-03-15T18:40:43.291+0000,"PRs link:

* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5203/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5203/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5205/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5205/overview]
* Groot: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5210/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5210/overview]
* Guardian: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5212/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5212/overview] TC Passed Ghost Run, Hence moving to  auton Close state : 
Pass Log:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-05/env_auto_job.2023May02_07:07:49.968959.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-05/env_auto_job.2023May02_07:07:49.968959.zip&atstype=ATS]","['Auton', 'Ghost', 'Guardian', 'Halleck', 'Optimized']",ThangQuoc Tran,Closed,Omkar Sharad Wagh
SEEN-1349,https://miggbo.atlassian.net/browse/SEEN-1349,"update dnac-auto lib with ESXi specific API equivalent for ""/v1/maglev/packages""",update dnac-auto lib with ESXi Specific API that is equivalent for {{/v1/maglev/packages}} - {{/v1/system-orchestrator/software-management/releases/installed}},2023-03-15T23:03:58.791+0000,"Raised required PR with ESXi equivalent APIs: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5465/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5465/overview] PR has been approved and merged to {{private/Halleck-ms/api-auto}}.

Same will get into {{private/Hulk-ms/api-auto}} with next sync.



Marking this ticket as “Done”.","['Auton', 'ESXi', 'Halleck', 'Hulk']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1354,https://miggbo.atlassian.net/browse/SEEN-1354,[Auton]:Ghost-Test_TC9_DNAC_Verify_adding_range_discovery_ssh_global_credentials/test_deploy_Wired_policy/test1_verify_adding_range_discovery_ssh_global_credentials,"*Reporter Analysis:* We have observed that testcase detects *only ewlc* according to the script. 
But we have ewlc or WLC devices in the sanity testbed. Script should handle discovery of these devices such as both WLC or EWLC based on the TB setup.

Sanity testbed’s device wlc & Ewlc details 
TB2,TB3,TB4,TB5,TB6=WLC 
TB7,TB8,TB13,TB11=eWLC
 

*Description*:  

{code:python}def get_seeds_by_level(self, level, wlc=False):
    """"""    Helper method to return lan auto seeds per level, will include wlcs for discovery if param wlc is true    """"""
    device_list = []
    level_seed_pattern = f""LEV{level}-SESS\d-(SEED|PEER)""
    for device in self.services.dnaconfig.fulllist:
        a = {'a': 1}
        if (type(device) == type(a)):
            device = device['name']
        if re.search(level_seed_pattern, self.services.dnaconfig.testbed.devices[device].enhancedpnprole):
            device_list.append(device)
        if wlc and ""EWLC"" in self.services.dnaconfig.testbed.devices[device].role: <<==
            device_list.append(device)
    return device_list{code}


*Branch Name:* Ghost -ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:

**Fail Log:**Ghost 2.1.613.70145+17.9.3+wlc(8.10.183.0)+2.7P9
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1903696&size=104283&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_auto_job.2023Mar13_12:55:32.140872.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1903696&size=104283&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_auto_job.2023Mar13_12:55:32.140872.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:* 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery.py-51-toolsDiscovery&begin=4448&size=204424&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan04_04:46:26.077736.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery.py-51-toolsDiscovery&begin=4448&size=204424&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-01%2Fenv_optimized_auto_job.2023Jan04_04:46:26.077736.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-03-17T06:39:01.343+0000,"PR raised for this issue:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5044/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5044/overview] PR merged and cherry picked to halleck","['Auton', 'Ghost', 'Optimized']",Omkar Sharad Wagh,Closed,Omkar Sharad Wagh
SEEN-1355,https://miggbo.atlassian.net/browse/SEEN-1355,[Auton] TC8_verify_WAN_issues /test2_verify_wan_issue  should verify issue for other device family like routers,"*Reporter Analysis:* In this script – currently it is verifying for the issue with ""*High input/output utilization on Switch WAN interface* "" irrespective of type of device like switchws or router. So the script should be able to handle other issues of devices like Router ""*High input/output utilization on Router WAN interface* 'GigabitEthernet0/0/0' (Interface description: --)"".
Script is restricting to verify only switches. 

{noformat}summ=""High input/output utilization on Switch WAN interface '{}'"".format(int)
flag = Falsefor i in res:
    if re.search(summ,i['summary']):
        self.log.info(""WAN Issues is as Expected!!! {}"".format(dev['name']))
        flag = True    else:
        self.log.error(""WAN Issue is not as Expected {}"".format(dev))
        failed_device.append(dev['name']){noformat}

As on Sanity TB8, we have ASR device on which we can see the WAN issue listed. 
{{Retrieving the Device Issues with the IP 204.1.2.3 qand response {'version': '1.0', 'response': [{'id': '9dd7f4d5-532d-4e66-b541-52e5b848e941', 'name': 'router_wan_interface_excess_rx_tx_util', 'flattened': True, 'enabled': True, 'entityType': 'Network Device', 'entityName': 'Unknown', 'entity': '204.1.2.3', 'groupBy': 'Unknown', 'severity': 'MEDIUM', 'category': 'Connected', 'summary': ""High input/output utilization on Router WAN interface 'GigabitEthernet0/0/0' (Interface description: --)"", 'rootCause': 'Unknown', 'timestamp': 1678650488647, 'startTime': 1678649288643, 'occurrences': 3, 'instances': 1, 'scope': 'GLOBAL', 'priority': 'P3', 'status': {'status': 'ACTIVE', 'updatedBy': 'Unknown', 'notes': 'Unknown', 'source': 'Unknown', 'updatedAt': None, 'ignoreSource': 'Unknown', 'ignoreValue': 'Unknown', 'ignoreStartTime': None, 'ignoreEndTime': None}, 'totalOccurrences': {'count': 3, 'startTime': 1678649288643, 'endTime': 1678650488647}}, {'id': 'd56b4129-7732-443f-864c-2c3a55baf1f2', 'name': 'radius_error_trigger', 'flattened': True, 'enabled': True, 'entityType': 'Network Device', 'entityName': 'Unknown', 'entity': '3b4e9311-74ed-45b7-83cf-9d999b3a3904', 'groupBy': 'Unknown', 'severity': 'HIGH', 'category': 'Device', 'summary': ""The RADIUS server '85.1.1.3:1812,1813' is not responding to requests being made from the network device 'TB8-SJ-BORDER-CP-ASR.cisco.com'"", 'rootCause': 'Unknown', 'timestamp': 1678649427775, 'startTime': 1678417577470, 'occurrences': 4, 'instances': 1, 'scope': 'GLOBAL', 'priority': 'P2', 'status': {'status': 'ACTIVE', 'updatedBy': 'Unknown', 'notes': 'Unknown', 'source': 'Unknown', 'updatedAt': None, 'ignoreSource': 'Unknown', 'ignoreValue': 'Unknown', 'ignoreStartTime': None, 'ignoreEndTime': None}, 'totalOccurrences': {'count': 4, 'startTime': 1678417577470, 'endTime': 1678649427775}}]}}}



*Description:  The error from log or more info* 

{noformat}Event Not Found on {'name': 'TB8-SJ-BORDER-CP-ASR', 'role': ['BORDERNODE', 'MAPSERVER'], 'type': ['INTERNAL', 'EXTERNAL'], 'sdainabox': False, 'sdatransit': True, 'bgp_as': 6100, 'remote_bgp_as': 200, 'mgmtIp': '10.10.10.10', 'lb_ip': None, 'client': [], 'border_fusion_intf': 'GigabitEthernet0/0/0'}{noformat}

{noformat}2572: 
 Verification Failed [{'TB8-SJ-BORDER-CP-ASR': {'version': '1.0', 'response': [{'id': '9dd7f4d5-532d-4e66-b541-52e5b848e941', 'name': 'router_wan_interface_excess_rx_tx_util', 'flattened': True, 'enabled': True, 'entityType': 'Network Device', 'entityName': 'Unknown', 'entity': '204.1.2.3', 'groupBy': 'Unknown', 'severity': 'MEDIUM', 'category': 'Connected', 'summary': ""High input/output utilization on Router WAN interface 'GigabitEthernet0/0/0' (Interface description: --)"", 'rootCause': 'Unknown', 'timestamp': 1678650488647, 'startTime': 1678649288643, 'occurrences': 3, 'instances': 1, 'scope': 'GLOBAL', 'priority': 'P3', 'status': {'status': 'ACTIVE', 'updatedBy': 'Unknown', 'notes': 'Unknown', 'source': 'Unknown', 'updatedAt': None, 'ignoreSource': {noformat}



*Branch Name:  private/Halleck-ms/sanity_api_auto*


*Script file/Usecase:* [applicationTelemetryAppTraffic|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-app_traffic_application_telemetry_dashboard.py-135-applicationTelemetryAppTraffic&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_optimized_auto_job.2023Mar12_12:16:36.951988.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: Yes*

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-app_traffic_application_telemetry_dashboard.py-135-applicationTelemetryAppTraffic&begin=1268012&size=130914&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_optimized_auto_job.2023Mar12_12:16:36.951988.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-app_traffic_application_telemetry_dashboard.py-135-applicationTelemetryAppTraffic&begin=1268012&size=130914&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_optimized_auto_job.2023Mar12_12:16:36.951988.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8]",2023-03-17T10:28:48.185+0000,"PRs:

* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5360/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5360/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5361/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5361/overview]
* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5746/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5746/overview]","['Auton', 'Ghost', 'Halleck', 'Optimized', 'Sanity']",ThangQuoc Tran,Resolved,Ashwini R Jadhav
SEEN-1356,https://miggbo.atlassian.net/browse/SEEN-1356,Test_TC208_l2_VNs_handoff_validation requires update in terms of API's payload,"While trying to integrate {{Test_TC208_l2_VNs_handoff_validation}}, we came across failure which is pointing to a change in the payload of the API used:

Execution log: [Test_TC208_l2_VNs_handoff_validation|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1290844&size=1075136&archive=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats%2Fusers%2Famardkum%2Farchive%2F23-03%2Fenv_auto_job.2023Mar17_11:14:06.990035.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats&submitter=amardkum&from=trade&view=all&atstype=pyATS]",2023-03-21T17:08:46.451+0000,PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5090/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5090/overview],"['Auton', 'Ghost']",Moe Saeed,Closed,Amardeep Kumar
SEEN-1357,https://miggbo.atlassian.net/browse/SEEN-1357,[Auton] Halleck - Wireless Solution Sanity - reconnect_clients_rbac takes 3 mins on all testcases increasing overall execution time,"*Regression:* Solution Sanity (SSR)

*Branch:* rcdn/Halleck-ms/api-auto synced to main branch private/Halleck-ms/api-auto yesterday before regression run start

*Script:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

*TCs Executed:* TC0 to TC41

*Observation: reconnect_clients_rbac takes some mins on all testcases that has increased execution time on comparison*

|*SSR*|*Last Week*|*Yesterday*|
|*Run Time*|2 hrs 15 Mins|4 hrs 39 Mins|
|*Taas Log*|[Log|https://ngdevx.cisco.com/services/taas/results/a6bd4522-1ffc-4a18-8d2a-653314fd4969/run-results]|[Log|https://ngdevx.cisco.com/services/taas/results/40f6fff0-a786-4af9-8fae-cadf38779895/run-results]|

*Analysis:*

*From Last Week Run Log:*

11715: 2023-03-16T09:39:54:  Executing testcase Test_TC13_DNAC_Device_Inventory_verifications_all_devices_in_managed_state test 13.1 ""test1_verifications_all_devices_in_managed_state"".

11716: 2023-03-16T09:39:54:  ****************************************************************

11717: 2023-03-16T09:39:54:  TRYING TO RECONNECT TO DNAC WITH USER PROVISION

11718: 2023-03-16T09:39:54:  ****************************************************************

11719: 2023-03-16T09:39:54: 

11720: 2023-03-16T09:39:54: 

11721: 2023-03-16T09:39:54:  Setting up maglev-based DNAC

11722: 2023-03-16T09:39:54: 

11723: 2023-03-16T09:39:54:  Connecting to the Apic-em northbound API client.

11724: 2023-03-16T09:39:54:  Resource path full url: [https://10.89.48.89/api/system/v1/identitymgmt/login|https://10.89.48.89/api/system/v1/identitymgmt/login]

11725: 2023-03-16T09:39:54:  Library method ""reconnect_clients_rbac"" returned in 0:00:00.812480

*From Yesterday Run Log:*

11703: 2023-03-20T08:22:41:  Executing testcase Test_TC13_DNAC_Device_Inventory_verifications_all_devices_in_managed_state test 13.1 ""test1_verifications_all_devices_in_managed_state"".

11704: 2023-03-20T08:22:41:  ****************************************************************

11705: 2023-03-20T08:22:41:  TRYING TO RECONNECT TO DNAC WITH USER PROVISION

11706: 2023-03-20T08:22:41:  ****************************************************************

11707: 2023-03-20T08:22:41: 

11708: 2023-03-20T08:22:41: 

11709: 2023-03-20T08:22:41:  Setting up maglev-based DNAC

11710: 2023-03-20T08:22:41: 

11711: 2023-03-20T08:22:41:  Traceback (most recent call last):

11712: 2023-03-20T08:22:41:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod4/services/dnaserv/dnaservices.py"", line 911, in reconnect_clients_rbac

11713: 2023-03-20T08:22:41:      connect=connect,

11714: 2023-03-20T08:22:41:  TypeError: __init__() missing 2 required positional arguments: 'cli_username' and 'cli_password'

11715: 2023-03-20T08:22:41: 

11716: 2023-03-20T08:22:41: 

11717: 2023-03-20T08:22:41:  DNAC not yet ready!

11718: 2023-03-20T08:22:41:     Waiting for DNAC to come back after restore from backup or from reload......

11719: 2023-03-20T08:22:41: 

11720: 2023-03-20T08:23:41:  ****************************************************************

11721: 2023-03-20T08:23:41:  TRYING TO RECONNECT TO DNAC WITH USER PROVISION

11722: 2023-03-20T08:23:41:  ****************************************************************

11723: 2023-03-20T08:23:41: 

11724: 2023-03-20T08:23:41: 

11725: 2023-03-20T08:23:41:  Setting up maglev-based DNAC

11726: 2023-03-20T08:23:41: 

11727: 2023-03-20T08:23:41:  Traceback (most recent call last):

11728: 2023-03-20T08:23:41:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod4/services/dnaserv/dnaservices.py"", line 911, in reconnect_clients_rbac

11729: 2023-03-20T08:23:41:      connect=connect,

11730: 2023-03-20T08:23:41:  TypeError: __init__() missing 2 required positional arguments: 'cli_username' and 'cli_password'

11731: 2023-03-20T08:23:41: 

11732: 2023-03-20T08:23:41: 

11733: 2023-03-20T08:23:41:  DNAC not yet ready!

11734: 2023-03-20T08:23:41:     Waiting for DNAC to come back after restore from backup or from reload......

11735: 2023-03-20T08:23:41: 

11736: 2023-03-20T08:24:41:  ****************************************************************

11737: 2023-03-20T08:24:41:  TRYING TO RECONNECT TO DNAC WITH USER PROVISION

11738: 2023-03-20T08:24:41:  ****************************************************************

11739: 2023-03-20T08:24:41: 

11740: 2023-03-20T08:24:41: 

11741: 2023-03-20T08:24:41:  Setting up maglev-based DNAC

11742: 2023-03-20T08:24:41: 

11743: 2023-03-20T08:24:41:  Traceback (most recent call last):

11744: 2023-03-20T08:24:41:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod4/services/dnaserv/dnaservices.py"", line 911, in reconnect_clients_rbac

11745: 2023-03-20T08:24:41:      connect=connect,

11746: 2023-03-20T08:24:41:  TypeError: __init__() missing 2 required positional arguments: 'cli_username' and 'cli_password'

11747: 2023-03-20T08:24:41: 

11748: 2023-03-20T08:24:41: 

11749: 2023-03-20T08:24:41:  DNAC not yet ready!

11750: 2023-03-20T08:24:41:     Waiting for DNAC to come back after restore from backup or from reload......

11751: 2023-03-20T08:24:41: 

11752: 2023-03-20T08:25:41:  Library method ""reconnect_clients_rbac"" returned in 0:03:00.147720

 ",2023-03-21T17:22:35.855+0000,"Required changes have been made via below PR:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5082/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5082/overview] [~accountid:636ce33a6bbefce0aca3df70] , below PR got approved and merged to Halleck-ms/api-auto and cherry-picked to hallack_esxivm_branch.

Please check the executions and let us know if we can close on this ticket. Fix works fine with updated workspace. Thanks for providing fix","['Auton', 'Halleck']",Amardeep Kumar,Closed,Yuvarani Iyamperumal
SEEN-1364,https://miggbo.atlassian.net/browse/SEEN-1364,Config Preview Activity failed with reason: NCSP11234 during device provision,"During Halleck testing, We have observed that Config Preview Activity failed while basic provision of device with the the reason ""Config Preview Activity failed with reason: NCSP11234: Not able to proceed with operation because of previous pending operations on the same intent. Further operations are not permitted until the existing ones are either deployed or discarded""

Found on:
Uber ISO : Halleck # 2.1.660.70310
Polaris version: 17.11.1 PRD4

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assign_devices_to_site_and_provision.py-62-provisioning&begin=2361657&size=4412900&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Foptimized_ibste_job.2023Mar13_08:31:02.211370.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assign_devices_to_site_and_provision.py-62-provisioning&begin=2361657&size=4412900&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Foptimized_ibste_job.2023Mar13_08:31:02.211370.zip&ats=%2Fws%2Fsjc-it%2Fpawan%2Fpyatsds3&submitter=admin&from=trade&view=all&atstype=pyATS]

We have Filed a bug for it and later confirmed that it was due to But later we came to know in webex teamspace for bug like this issue was because fix not there check for any pending operations before triggering a new operation on the same namespace.
Webex Team Space : webexteams://im?space=3983f980-c433-11ed-ae4d-814e0e82e714
Can you please help us in fixing these for IBSTE Script under “private/Halleck-ms/api-auto“.",2023-03-23T06:05:06.241+0000,"Issue is also observed during Halleck RC1 testing on MSTB1 profile.

*Uber ISO Version tested :* 
Promoted Halleck RC1 Uber ISO - *2.1.660.70326, Non-FIPS, PUBSUB enabled*

*Script Name:* solution_test_3sites_sjc_nyc_sf_mdnac_dr.py,  solution_test_3sites_sjc_nyc_sf.py

*Testbed :* MSTB1, MSTB2

*Description :* 

Config preview for Basic device provisioning operation for all the wired devices fails with error as *“Config Preview Activity failed with reason: NCSP11234: Not able to proceed with operation because of previous pending operations on the same intent. Further operations are not permitted until the existing ones are either deployed or discarded. Additional info for support:  Current operation:”*

Also we see the corresponding Tasks under Activities->“Work Items” are stuck in “*Pending Review”* status forever. 

*Failed log:*  [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fsr_mb_multi_sites_mdnac.2023Mar27_10:47:19.342001.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fsr_mb_multi_sites_mdnac.2023Mar27_10:47:19.342001.zip&atstype=ATS] → Refer TC28 Same issue is also observed during Halleck execution on Sanity AWS-cluster:
*Uber ISO Version tested :* Halleck 2.1.660.70326
*Script Name:* Optimized code 
\testcases\sanityusecases\randomMacEnable\random_mac_enable.py
*Testbed :*TB11(AWS)
*Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-random_mac_enable.py-194-randomMacEnable&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_optimized_auto_job.2023Mar31_09:45:26.074013.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-random_mac_enable.py-194-randomMacEnable&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_optimized_auto_job.2023Mar31_09:45:26.074013.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
*Error Snip :* 

{{4066: Config Preview Activity failed with reason: NCSP11234: Not able to proceed with operation because of previous pending operations on the same intent. Further operations are not permitted until the existing ones are either deployed or discarded. Additional info for support: Current operation: Provision Device - TB2-DMZ-SJ-FIAB-ECA at 1680303319.0795398 - Configuration Preview(bddfef1f-3cec-4474-8442-b4d4b78e82c4). Pending operation(s): [Provision Device - TB2-DMZ-SJ-FIAB-ECA at 1680302681.7040138 - Configuration Preview (13866ae1-7d4b-4ecf-a415-eefa331eebeb)].}} Same issue is observed during Halleck ESXI execution on MSTB2:
*Uber ISO Version tested :* Halleck 3.660.75447
*Script Name:* solution_test_3sites_sjc_nyc_sf.py
*Testbed :* MSTB2
*Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1410801&size=15883841&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fauto_MS_job.2023Apr11_22:53:09.069832.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1410801&size=15883841&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fauto_MS_job.2023Apr11_22:53:09.069832.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
*Error Snip :* 

{{Config Preview Activity failed with reason: NCSP11234: Not able to proceed with operation because of previous pending operations on the same intent. Further operations are not permitted until the existing ones are either deployed or discarded. Additional info for support: Current operation: Provision Device - SJC-IM-9300-1 at 1681280496.6585963 - Configuration Preview(0ab5503d-2e8d-4b64-8240-21136e2d2169). Pending operation(s): [Provision Device - SJC-IM-9300-1 at 1680278436.3991659 - Configuration Preview (5036a2ed-3b25-4860-8346-9ec51fd94aae)].}} Andrew enhanced provision in our lib to support VCR conflicts on May 2nd. Please try again. Resolved this ticket since Sathwick didnot have any update for a long time. Issue was not observed in recent regressions after code commit.","['AWS-Santiy', 'Auton', 'Halleck', 'IBSTE', 'MSTB1', 'MSTB2', 'Multisite', 'Optimized']",Sathwick Reddy Polamreddy,Closed,Sathwick Reddy Polamreddy
SEEN-1366,https://miggbo.atlassian.net/browse/SEEN-1366,call to connect_rbac() and verify_current_user() Methods in Multisite Scripts are not required,"MultiSite related Scripts do not create Custom Roles and Users.

So, there’s no need to call connect_rbac() and verify_current_user() methods inside testcases of Multisite Scripts.

These are unnecessary calls to IDM/RBAC APIs: [test3_extnode_ap_static_onboarding_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24902566&size=183099&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fauto_MS_job.2023Mar03_01:37:19.966391.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] taking 3 minutes per call.

solution_test_3sites_sjc_nyc_sf: Total 38 calls * 3 minutes ~ 2 hours of unnecessary execution time.",2023-03-24T19:48:55.947+0000,PR for the required changes: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5113/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5113/overview] have been merged to Halleck branch and cherry-picked to hallack_esxivm_branch and Ghost branch. PR raised for Groot: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5115/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5115/overview] PR raised for Guardian: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5138/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5138/overview],['Auton'],Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1371,https://miggbo.atlassian.net/browse/SEEN-1371, Test_TC174_syslog_server_event_notification  /   test3_connect_syslog_server_update_rsyslog_config_file systemctl command out is not getting captured,"While integrating  Test_TC174_syslog_server_event_notification  /   [test3_connect_syslog_server_update_rsyslog_config_file|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2765357&size=5788&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr03_11:22:42.553149.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS], observed that {{systemctl}} command out is not getting captured.
Below debug shows the same:

{noformat}> /auto/dna-sol/ws/amardkum/dnac-auto/services/commonlibs/sftopology.py(8037)connect_linux_syslog_server()
-> out=self.testbed.devices[device].execute(""sudo systemctl status rsyslog"")
(Pdb) 

2023-04-03 12:58:39,217: %UNICON-INFO: +++ nfs-virtual-machine with via 'linux': executing command 'sudo systemctl status rsyslog' +++
sudo systemctl status rsyslog
● rsyslog.service - System Logging Service
   Loaded: loaded (/lib/systemd/system/rsyslog.service; enabled; vendor preset: enabled)
   Active: active (running) since Mon 2023-04-03 15:24:13 EDT; 1min 15s ago
     Docs: man:rsyslogd(8)
           http://www.rsyslog.com/doc/
 Main PID: 7760 (rsyslogd)
    Tasks: 5 (limit: 4915)
   CGroup: /system.slice/rsyslog.service
           └─7760 /usr/sbin/rsyslogd -n

Apr 03 15:24:13 nfs-virtual-machine systemd[1]: Starting System Logging Service...
Apr 03 15:24:13 nfs-virtual-machine rsyslogd[7760]: imuxsock: Acquired UNIX socket '/run/systemd/journal/syslog' (fd 3) from systemd.  [v8.32.0]
Apr 03 15:24:13 nfs-virtual-machine systemd[1]: Started System Logging Service.
Apr 03 15:24:13 nfs-virtual-machine rsyslogd[7760]: rsyslogd's groupid changed to 106
Apr 03 15:24:13 nfs-virtual-machine rsyslogd[7760]: rsyslogd's userid changed to 102
Apr 03 15:24:13 nfs-virtual-machine rsyslogd[7760]:  [origin software=""rsyslogd"" swVersion=""8.32.0"" x-pid=""7760"" x-info=""http://www.rsyslog.com""] start
root@nfs-virtual-machine:~# 
> /auto/dna-sol/ws/amardkum/dnac-auto/services/commonlibs/sftopology.py(8038)connect_linux_syslog_server()
-> logger.info(out)
(Pdb) out
''
{noformat}



Solution: include additional operation like {{cat}} or {{tee}} to get it captured.
Example:
{{sudo systemctl status rsyslog | tee}}
or
{{sudo systemctl status rsyslog | cat}}",2023-04-03T22:29:08.909+0000,"Required change have been pushed to Halleck branch:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6a0f7976d36ec8a925283d4c97fae64b3f928159|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6a0f7976d36ec8a925283d4c97fae64b3f928159]
and cherry-picked to hallack_esxivm_branch, Ghost, Groot and Guardian branches. Marking this as “Done”.",['Auton'],Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1374,https://miggbo.atlassian.net/browse/SEEN-1374,update Syslog Config API for ESXi release,"On-Prem API to get syslog config: {{https://10.22.45.61/dna/intent/api/v1/event/syslogConfig}}

ESXi API to get syslog config: {{https://10.22.45.61/dna/intent/api/v1/event/syslog-config}}",2023-04-04T04:11:49.471+0000,"Required PR has been raised to consider the mentioned change:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5229/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5229/overview] PR has been approved and merged to Halleck-ms/api-auto branch.","['Auton', 'ESXi', 'automation']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1375,https://miggbo.atlassian.net/browse/SEEN-1375,current usage of verify_current_user() in Sanity script is not adding any value,"Current usage of {{verify_current_user()}} in Sanity scripts is not adding any value as the existing code is only getting list of users configured on DNAC, except increasing the overall runtime of the use-case.
{{solution_test_sanityecamb.py}} alone has 226 calls.

Moreover, calls to {{verify_current_user()}} method after {{connect_rbac()}} which in case of failure is going to default to “admin” user.

Unless {{verify_current_user()}} is enhanced and brings some value add, it should be commented out inside the scripts.",2023-04-04T18:02:29.376+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5252/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5252/overview]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5251/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5251/overview]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5250/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5250/overview]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5234/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5234/overview]",['Auton'],Raji Mukkamala,Closed,Amardeep Kumar
SEEN-1377,https://miggbo.atlassian.net/browse/SEEN-1377,[Auton]:Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients/Test_TC2_DNAC_provision_all/test1_provision_all_aps_aps/,"*Reporter Analysis:*
we observed sanity Ghost run, optimized code where two operations running at the same time, the first one already added AP zones using Test_TC1_AP_Zone, but the failure already has happened on other use cases where try to provision without adding AP Zone. When the multicast was executed parallel, the provisioning failure was already there. Due to this auton, 10 TC’s  are impacted  
*Uber ISO Version tested:* Ghost Patch 1 RC build 2.1.613.70170
*Script Name:* Optimized code 
testcases\sanityusecases\FEWAccessPointAndCLients\ap_sync_provisioning_clients_roaming.py
*Testbed :* TB7
*Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1285809&size=210302&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr04_00:45:51.564385.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1285809&size=210302&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr04_00:45:51.564385.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
*AP Zone UC failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_zones.py-1311-apZones&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr04_00:45:51.564385.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_zones.py-1311-apZones&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr04_00:45:51.564385.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
*Pass Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1196184&size=575869&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_optimized_auto_job.2023Feb28_19:21:43.817989.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1196184&size=575869&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fenv_optimized_auto_job.2023Feb28_19:21:43.817989.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
*Testbed Wiki:*
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-6]7
*Error Snip :* 


{code:python}5008:  {'version': 1680597284997, 'endTime': 1680597284997, 'startTime': 1680597284500, 'progress': 'TASK_PROVISION', 'data': 'workflow_id=bd369552-8972-4b88-a841-940da07859bd;cfs_id=0;rollback_status=not_supported;rollback_taskid=0;failure_task=Validation of business intent:83d59cdb-4135-42ef-817f-b421257fbcd6;processcfs_complete=false', 'errorCode': 'NCWL10325', 'isError': True, 'serviceType': 'NCSP', 'failureReason': 'NCWL10325: APWirelessConfiguration FLOOR1_LEVEL1_LOW_4433f with Id 13f95832-a914-4543-a4a1-9e57bd1179fd SiteId 2a4c7d7b-29e7-4de5-ac52-c65592855bca and Apzone Zone1_Enterprise is already present in the database.Please do wlancontroller provisioning for any changes in apzone.', 'lastUpdate': 1680597284982, 'instanceTenantId': '642abb5ae3ba6c5fe9fb8b61', 'id': '48dbadec-bf75-499a-a7ae-898d836c5e5f'}
5009:  Provisioning AP of device failed for reason:NCWL10325: APWirelessConfiguration FLOOR1_LEVEL1_LOW_4433f with Id 13f95832-a914-4543-a4a1-9e57bd1179fd SiteId 2a4c7d7b-29e7-4de5-ac52-c65592855bca and Apzone Zone1_Enterprise is already present in the database.Please do wlancontroller provisioning for any changes in apzone.
5010:  Library group ""provision_wireless"" method ""provision_unified_ap_devices"" returned in 0:00:34.381347
5011:  Test returned in 0:00:34.468208
5012:  Failed reason: Result: AP Provision failed{code}",2023-04-06T03:32:23.329+0000," 

Hi Team,

 

Can I get ETA for the ticket raised

 

Regards,

Omkar  [~accountid:63f50bfce8216251ae4d59d5]  could you please check  [~accountid:63f50bfce8216251ae4d59d5] , can we have an ETA or any update on this Auton? a fix: [Pull Request #5459: SEEN-1377-APs negative operations halleck - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5459/diff#services/dnaserv/lib/api_groups/provision_wireless/group.py]

however: this need more checking.

Can you run this use cases with AP Zones? [~accountid:620b8357878c2f00729881c8]  Hi [~accountid:63f50bfce8216251ae4d59d5]  ,
I ran TC in the Halleck cluster after fetching the current code; however, I am unable to figure out  issue the error message; I looked at the input file as per the wiki, and all data is correct.

+*Failed Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3048625&size=42425&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May12_04:13:55.826977.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3048625&size=42425&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May12_04:13:55.826977.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

+*Error Snip*+  :


{code:python}11900:  ERROR changing the site in the input file!!, please check if the input file has changed!!
11901:  Reason key error: list index out of range
11902:  Library group ""inventory"" method ""set_site_with_max_aps"" returned in 0:00:00.118139
11903:  Test returned in 0:00:03.212854
11904:  Failed reason: Failed setting up use case to choose a site that has APs!{code} [~accountid:620b8357878c2f00729881c8], This is part of Halleck new use cases, and merge multiple use cases from Ghost too. What branch did you used? Hi [~accountid:63f50bfce8216251ae4d59d5]  Below branch used

Branch: private/Halleck-ms/sanity_api_auto
PR: [c51d902f6a3|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c51d902f6a371603fdddc7f1abea3abdb288c069] [~accountid:620b8357878c2f00729881c8] 

Please treat the ap zones & site tags as new feature in halleck since it has changed a lot. 

but for the original issue in Ghost, please rerun again and send me the log to look at. 

If you still see issue in Halleck then, we can resolve it as a new feature since there is already another ticket with same issue and should be already resolved. Hi [~accountid:63f50bfce8216251ae4d59d5] 
Please find  below Ghost  execution  Log : 
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May30_21:01:13.505518.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May30_21:01:13.505518.zip&atstype=ATS]

[test3_create_AI_RF_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=4499138&size=268429&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May30_21:01:13.505518.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ==Failed  
*Error  snip* 

{noformat};Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:{""response"":{""errorCode"":""NCND00049"",""message"":""NCND00049: Invalid request. Please check the request parameters and resubmit"",""detail"":""Rf profile names has to be unique. Duplicate entries are not allowed""},""version"":""1.0""}{noformat}


[Task-ap_zones.py-1310-apZones|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_zones.py-1310-apZones&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May30_21:01:13.505518.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] /Test_TC1_AP_Zone 
[test5_provision_aps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_zones.py-1310-apZones&begin=1538016&size=154211&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May30_21:01:13.505518.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ==Failed 


{noformat} Provisioning AP of device failed for reason:NCSP11104: Error occurred while processing the 'modify' request. Additional info for support: taskId: '0bd9c758-0b59-4629-9746-2b8589fb69ef'. Internal error while attempting to transform the object for further processing.{noformat}


 [~accountid:63f50bf5e8216251ae4d59cf] 

Can you check bellow error in Ghost:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May30_21:01:13.505518.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May30_21:01:13.505518.zip&atstype=ATS] [~accountid:63f50bfce8216251ae4d59d5] / [~accountid:620b8357878c2f00729881c8]  For AP zones, It failed at AP provisioning, Looking at error message “{{ Provisioning AP of device failed for reason:NCSP11104: Error occurred while processing the 'modify' request. Additional info for support: taskId: '0bd9c758-0b59-4629-9746-2b8589fb69ef'. Internal error while attempting to transform the object for further processing.}} “ 

It could be either a defect or need to check DNAC logs  [~accountid:620b8357878c2f00729881c8] , Please follow up with the DE and see why this error and get updates or open a defect. 

Also, please do not dump all the errors here. If you got a new error open a new ticket. 

Another suggestion:

If the issue is related to fabirc zones: add [~accountid:63f50bf5e8216251ae4d59cf] 

if the issue related to ap profiles and power profiles: add [~accountid:63f50be71223974bc04b0534] 

if the issue related to site tags: add [~accountid:63f50bfce8216251ae4d59d5]  Thanks  [~accountid:63f50bfce8216251ae4d59d5]  for Giving me info. Upon receiving the latest promoted Ghost, we'll check it in and update you. [~accountid:620b8357878c2f00729881c8] , did you get the execution log for Ghost?

How’s the result in Hulk?","['AWS-Santiy', 'Auton', 'Ghost', 'Integration', 'Optimized', 'Sanity']",Moe Saeed,Closed,Omkar Sharad Wagh
SEEN-1378,https://miggbo.atlassian.net/browse/SEEN-1378,[SR-IBSTE] : SWIM CCO Guardian Feature - Script Enhancement,"Subject : Device families details for upgrading from CCO needs to provide from JSON file .


Hi Moe,
While integrating the SWIM CCO Feature on SR-IBSTE, only 9800 EWLC Devices are successfully upgrading from CCO. The remaining devices, such as cat9k, are taking images from uploading to the swim server and upgrading.
Because we can see in the testcase, only these ""C9800-CL, AIR-CT8540, C9800"" devices were marked as supported families. The remaining devices are not taking images from the CCO, instead, they were taking images accordingly from our swim server.
Since We only have three EWLC devices in the IBSTE testbed, and upgrading fine from CCO. But there are still more devices like Cat9k ISR and ASR that are upgrading via regular uploads from the swim server.
Is there any chance of enhancing the script so that we can send the input from a json file including the supported family models. so that it will be simpler to update device models to upgrade it from CCO. Because normal image upload to swim and upgrade is taking more time then upgrading the image from CCO.
Script Execution was taking around 6hrs. As we have more number of cat9k devices and upgrading through normal image upload process. It would be better if we pass the device model input from json.",2023-04-06T18:15:15.625+0000,"Hi Moe,
Can you please double commit the fixes to Ghost branch (private/Ghost-ms/api-auto) also. Seems it was committed only to Hulk branch. Required PR has been raised, approved and merged for Ghost branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6168/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6168/overview] [~accountid:63f50bfce8216251ae4d59d5] 
tried running the script getting below error. Couldnt get much info from log.


Failed, these DeviceFamilys do not have any fetched versions for recommendation from CCO: ['Cisco Catalyst 9407R Switch', 'Cisco Catalyst 9606R Switch', 'Cisco Catalyst C9500 SVL Switch']

8912:

8913: Result: After syncing CCO data, there is no CCO image recommendation or latests!!! Check why CCO cannot download latest images to DNAC.


On IBSTE testbed:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/divayada-sjc/Solution_pyatsenv/users/admin/archive/23-07/sr_ibste.2023Jul06_00:56:30.028527.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/divayada-sjc/Solution_pyatsenv/users/admin/archive/23-07/sr_ibste.2023Jul06_00:56:30.028527.zip&atstype=ATS]
 [~accountid:63f50bfce8216251ae4d59d5]  any update on this? waiting for the testbed, expected by Friday 07/14 Testbed  has been shared on  7/14. Moe couldn't able to look into. will  try to provide next slot, I used the Hulk script and no issues found, and I found a lot of changes have not pushed to Ghost. I opened a PR for that:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6355/diff#services/dnaserv/lib/api_groups/swim/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6355/diff#services/dnaserv/lib/api_groups/swim/group.py]","['Auton', 'EXECUTION', 'Execution', 'Ghost', 'Hulk', 'IBSTE']",Moe Saeed,Resolved,Sathwick Reddy Polamreddy
SEEN-1379,https://miggbo.atlassian.net/browse/SEEN-1379,Create custom_rf_ssid for N+1 WLC test ,"custom_rf_ssid ssid should be created at the test level, Add ssid creation for specific test N+1 WLC",2023-04-06T22:12:02.655+0000,,['Auton'],Raji Mukkamala,Resolved,Raji Mukkamala
SEEN-1385,https://miggbo.atlassian.net/browse/SEEN-1385,Test_TC62_DEV_STRESS_mcast_traffic_convergence_test_primary_border_reload ,"* *DNAC Release_Version Tested:* Ghost P1 RC1-2.1.613.70170
* *Device Image Used:* 17.10.01FC2
* *Testbed:* AWS-Multisite
* *Branch Used:* private/Ghost-ms/sanity_api_auto
* *Script Name:* solution_test_3sites_sjc_nyc_sf.py
* *Solution Input File :* dnac-auto/configs/config_48hr_test/solution_test_input_FIPS.json
* *Testcases Impacted:* Test_TC62_DEV_STRESS_mcast_traffic_convergence_test_primary_border_reload
* *Failed Trade Log:*[*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3690139&size=17206&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fauto_MS_job.2023Apr03_22:43:13.608300.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3690139&size=17206&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fauto_MS_job.2023Apr03_22:43:13.608300.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
* *Issue details/analysis:* After executing {color:#bf2600}Test_TC62_DEV_STRESS_mcast_traffic_convergence_test_primary_border_reload,we observed the below issue:{color}
{color:#bf2600}15758:  Traceback (most recent call last):{color}
{color:#bf2600}15759:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/AWS-Multisite-Common-Job/services/commonlibs/test_wrapper.py"", line 301, in wrapper{color}
{color:#bf2600}15760:      result = testfunc(func_self, **kwargs){color}
{color:#bf2600}15761:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/AWS-Multisite-Common-Job/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py"", line 3835, in test1_static_ixia_noauth_traffic_convergence_test{color}
{color:#bf2600}15762:      ixia_client = IxiaClient(chassis_ip=ixiaip,port_list=dnac_handle.dnaconfig.ixiaports,serverip=serveripi,port_config=port_config){color}
{color:#bf2600}15763:  NameError: name 'serveripi' is not defined{color}
{color:#bf2600}15764:  Test returned in 0:00:04.814056{color}
{color:#bf2600}15765:  Errored reason: name 'serveripi' is not defined{color}
{color:#bf2600}15766:{color}
{color:#bf2600}15767:  Exception:{color}
{color:#bf2600}15768:  Traceback (most recent call last):{color}
{color:#bf2600}15769:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/AWS-Multisite-Common-Job/services/commonlibs/test_wrapper.py"", line 301, in wrapper{color}
{color:#bf2600}15770:      result = testfunc(func_self, **kwargs){color}
{color:#bf2600}15771:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/AWS-Multisite-Common-Job/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py"", line 3835, in test1_static_ixia_noauth_traffic_convergence_test{color}
{color:#bf2600}15772:      ixia_client = IxiaClient(chassis_ip=ixiaip,port_list=dnac_handle.dnaconfig.ixiaports,serverip=serveripi,port_config=port_config){color}
{color:#bf2600}15773:  NameError: name 'serveripi' is not defined{color}
15774:  The result of section test1_static_ixia_noauth_traffic_convergence_test is => ERRORED
15775:  The result of testcase Test_TC62_DEV_STRESS_mcast_traffic_convergence_test_primary_border_reload is => ERRORED

Looks like,some script chnages have been made and thus causing the issue.Normal IXIA cases went fine.
Could you please help checking on the issue ?",2023-04-10T05:54:49.295+0000,"Commited to Ghost/Halleck.
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/55f35b0b442400416b7f280269d6c14cb0f0b105|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/55f35b0b442400416b7f280269d6c14cb0f0b105]","['AWS_MSTB', 'Auton', 'Ghost', 'MSTB3']",Tran Lam,Resolved,Neelima Doddipalli
SEEN-1386,https://miggbo.atlassian.net/browse/SEEN-1386,  [Ghost][AUTON]-Test_TC0_dnac_initial_cleanup  / test1dnac_initial_cleanup,"* *DNAC Release_Version Tested:* Ghost P1 RC1-2.1.613.70170
* *Device Image Used:* 17.10.01FC2
* *Testbed:* AWS-Multisite
* *Branch Used:* private/Ghost-ms/sanity_api_auto
* *Script Name:* solution_test_3sites_sjc_nyc_sf.py
* *Solution Input File :* dnac-auto/configs/config_48hr_test/solution_test_input_FIPS.json
* *Testcases Impacted:* Test_TC0_dnac_initial_cleanup  / test1dnac_initial_cleanup
* *Failed Trade Log:*[*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=623575&size=798988&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fauto_MS_job.2023Mar29_03:58:51.818257.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=623575&size=798988&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fauto_MS_job.2023Mar29_03:58:51.818257.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]
* Issue Analysis/Snippet:
During ongoing Solution regression testing on Ghost P1 RC1,after executing TC0,we have observed following issue:
6736:  Resource path full url: [https://172.35.16.151/api/system/v1/maglev/proxy|https://172.35.16.151/api/system/v1/maglev/proxy]
6737:  Library group ""catalog_server"" method ""is_proxy_exsits"" returned in 0:00:00.039403
6738:  Resource path full url: [https://172.35.16.151/api/system/v1/maglev/proxy|https://172.35.16.151/api/system/v1/maglev/proxy]
{color:#bf2600}6739:  Error Code: 400 URL:{color}[{color:#bf2600}https://172.35.16.151/api/system/v1/maglev/proxy{color}|https://172.35.16.151/api/system/v1/maglev/proxy]{color:#bf2600} Data:{'timeout': 60} Headers:{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2MzlkOGQ3NmVhODUyYjEzZDg4YjU2NDAiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjYzOWQ4ZDc1ZWE4NTJiMTNkODhiNTYzZiJdLCJ0ZW5hbnRJZCI6IjYzOWQ4ZDc0ZWE4NTJiMTNkODhiNTYzZCIsImV4cCI6MTY4MDA5MTYwOSwiaWF0IjoxNjgwMDg4MDA5LCJqdGkiOiI0MDc5YjY5OC01OTUzLTQyYjAtOTEyNC02YTMxMmYzYjljMWUiLCJ1c2VybmFtZSI6ImFkbWluIn0.2hj20nafkobEC3YrKkEywhxeb9gTCyKXz8jIwfP55NA18Az0uoC4xhV2Xy-oZRw3RHYegKX2z8MFMxwUSH1qqGMEjJNyAsyxrXlKTAMCA8rV_BwDuFLge0B95oOzRl8SmncKoWwJJmkYbQvTEJjdaNTq9aCqv38oEArS1O7RcBCznqU87WaVYmBFadH1ipbQuIEswbRLT1TvjSgm22Izy9sOFSxrQFD6ShVNCJHPJsBjixme4uQ3G-R9eVaoC2VLcuMp5vTRhXhMplGBOSdImt9_AVDf1vsjs9Chz-uvEn7dohnjsDQvAjPkozDhGT8lcgkOpQnkpozfd7cw43o1VA;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:{""response"": {""error"": ""Proxy Settings not Registered yet"", ""errorCode"": ""UNABLE_TO_DELETE_ERROR""}, ""version"": ""1.5.1""}{color}
{color:#bf2600}6740:  Traceback (most recent call last):{color}
{color:#bf2600}6741:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/AWS-Multisite-Common-Job/services/dnaserv/client_manager.py"", line 295, in call_api{color}
{color:#bf2600}6742:      response.raise_for_status(){color}
{color:#bf2600}6743:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status{color}
{color:#bf2600}6744:      raise HTTPError(http_error_msg, response=self){color}
{color:#bf2600}6745:  requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: {color}[{color:#bf2600}https://172.35.16.151/api/system/v1/maglev/proxy{color}|https://172.35.16.151/api/system/v1/maglev/proxy]
{color:#bf2600}6746:  Encountered unhandled HTTPError in group ""catalog_server"" method ""configure_proxy_settings""!{color}
{color:#bf2600}6747:  Flagging result as FAIL!{color}
{color:#bf2600}6748:  	Reason: 400 Client Error: Bad Request for url: {color}[{color:#bf2600}https://172.35.16.151/api/system/v1/maglev/proxy{color}|https://172.35.16.151/api/system/v1/maglev/proxy]
{color:#bf2600}6749:  	Args:   (<services.dnaserv.lib.api_groups.catalog_server.group.Group object at 0x7fbcea9735e0>,){color}
{color:#bf2600}6750:  Kwargs:{color}
{color:#bf2600}6751:  {'overwrite': True, 'remove_proxy': True}{color}
{color:#bf2600}6752:  Traceback:{color}
{color:#bf2600}6753:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/AWS-Multisite-Common-Job/services/dnaserv/lib/decorators.py"", line 32, in wrapper{color}
{color:#bf2600}6754:      result = method(*args, **kwargs){color}
{color:#bf2600}6755:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/AWS-Multisite-Common-Job/services/dnaserv/lib/api_groups/catalog_server/group.py"", line 96, in configure_proxy_settings{color}
{color:#bf2600}6756:      response = self.services.base.gvclient.call_api({color}
{color:#bf2600}6757:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/AWS-Multisite-Common-Job/services/dnaserv/client_manager.py"", line 498, in call_api{color}
{color:#bf2600}6758:      response = super(ApicemClientManager, self).call_api(method=method,{color}
{color:#bf2600}6759:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/AWS-Multisite-Common-Job/services/dnaserv/client_manager.py"", line 300, in call_api{color}
{color:#bf2600}6760:      raise e{color}
{color:#bf2600}6761:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/AWS-Multisite-Common-Job/services/dnaserv/client_manager.py"", line 295, in call_api{color}
{color:#bf2600}6762:      response.raise_for_status(){color}
{color:#bf2600}6763:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status{color}
{color:#bf2600}6764:      raise HTTPError(http_error_msg, response=self){color}
{color:#bf2600}6765:  Encountered unhandled HTTPError in group ""cleanup"" method ""cleanup_all_config""!{color}
{color:#bf2600}6766:  Flagging result as FAIL!{color}
{color:#bf2600}6767:  	Reason: 400 Client Error: Bad Request for url: {color}[{color:#bf2600}https://172.35.16.151/api/system/v1/maglev/proxy{color}|https://172.35.16.151/api/system/v1/maglev/proxy]
{color:#bf2600}6768:  	Args:   (<services.dnaserv.lib.api_groups.cleanup.group.Group object at 0x7fbcea96c2e0>,){color}
{color:#bf2600}6769:  Kwargs:{color}

Could you please help checking on the issue?
",2023-04-10T06:07:03.953+0000,,"['AWS_MSTB', 'Auton', 'Ghost', 'MSTB3']",NhanHuu Nguyen,In Progress,Neelima Doddipalli
SEEN-1406,https://miggbo.atlassian.net/browse/SEEN-1406,update RBAC lib w.r.t. DNAC VM Version check to accommodate future versions,"Update RBAC lib w.r.t. DNAC VM Version check to accommodate future versions.

Currently it is only check if “3.660” is in DNAC Version String.

With Hulk, it is {{3.710.xxxx}} series.",2023-04-11T04:42:18.815+0000,"Required PR raised: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5309/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5309/overview]

Approved and merged, hence marking this as “Done”.",['Auton'],Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1414,https://miggbo.atlassian.net/browse/SEEN-1414,[Auton] Test_TC1_enable_fabric_wireless_eca /test5_enable_dtls_version Failed due to un-expected prompt on device,"*ISO:* Halleck 70349 (TB2)

*Script:* dnac-auto\job\api_auto\env_optimized_auto_job.py (optimized run)
*Branch:* private/Halleck-ms/sanity_api_auto

*Description:* 

The TC added to address this issue has failed with latest code pull (SEEN-183)

*Please Note: Script needs to handle below scenario:* 
The config that is being pushed as part of this script should be done only to devices whose images are >=17.9 version 



*Impacted Testcase*  

Usecase: [FEWembededWirelessECA|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eca_enable_wireless_in_fabric.py-111-FEWembededWirelessECA&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr01_00:53:43.782267.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Testscript : :[test5_enable_dtls_version|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eca_enable_wireless_in_fabric.py-111-FEWembededWirelessECA&begin=2543127&size=12530&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr01_00:53:43.782267.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Error Snip:* 

Error snippet from the log:

TB2-DM-eCA-BORDER(config)# ap dtls-version DTLS_1_0
Any change in this config will disconnect existing APs
TB2-DM-eCA-BORDER(config)# crypto pki certificate map map1 1
TB2-DM-eCA-BORDER(ca-certificate-map)#
5748: Timeout of 120 seconds has been reached.
5749: Prompt Recovery has commenced. Total timeout occurs in 100 seconds.
5750: Sending prompt recovery command: b'\r'

TB2-DM-eCA-BORDER(ca-certificate-map)#
5752: Sending prompt recovery command: b'\x15'
5753: Sending prompt recovery command: b'\x1a'
^Z
TB2-DM-eCA-BORDER#
5755: Exception occured Expected device to reach 'config' state, but landed on 'enable' state.

*Failed log*: 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eca_enable_wireless_in_fabric.py-111-FEWembededWirelessECA&begin=2543127&size=12530&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr01_00:53:43.782267.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eca_enable_wireless_in_fabric.py-111-FEWembededWirelessECA&begin=2543127&size=12530&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr01_00:53:43.782267.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed wiki:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2]",2023-04-12T05:34:50.494+0000,"* Halleck run causes the same issue 
*DNAC Release_Version Tested: Halleck Respin 2.1.660.70351*
* *Device Image Used: 17.11.1*
* 
* *Testbed: Sanity Testbed*
* *Branch Used: private/HAlleck -ms/sanity-api-auto*
* *Script Name:Optimized code* 
*\testcases\sanityusecases\*
* *Testbed :*TB7
* 
* *Testcases Impacted:* [*Task-eca_enable_wireless_in_fabric.py-111-FEWembededWirelessECA*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eca_enable_wireless_in_fabric.py-111-FEWembededWirelessECA&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May05_08:10:21.735814.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  */*   [*Test_TC1_enable_fabric_wireless_eca*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eca_enable_wireless_in_fabric.py-111-FEWembededWirelessECA&begin=4128&size=2867029&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May05_08:10:21.735814.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  */   test5_enable_dtls_versio*
* *Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eca_enable_wireless_in_fabric.py-111-FEWembededWirelessECA&begin=2835996&size=34940&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May05_08:10:21.735814.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eca_enable_wireless_in_fabric.py-111-FEWembededWirelessECA&begin=2835996&size=34940&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May05_08:10:21.735814.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Error Snip :*

{noformat}TB7-eWLC#
config term
Enter configuration commands, one per line.  End with CNTL/Z.
TB7-eWLC(config)#
TB7-eWLC(config)#                    ap dtls-version DTLS_1_0
Any change in this config will disconnect existing APs
TB7-eWLC(config)#                    crypto pki certificate map map1 1
TB7-eWLC(ca-certificate-map)#
6616:  Timeout of 120 seconds has been reached.
6617:  Prompt Recovery has commenced. Total timeout occurs in 100 seconds.

TB7-eWLC(ca-certificate-map)#
^Z
TB7-eWLC#
6623:  Exception occured Expected device to reach 'config' state, but landed on 'enable' state.
6626:  Failed reason: Result:Failed to configure DTLS_1_0 on ECA and EWLC{noformat} Ghost  P2 runs the same issue seen  

+*Failed log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eca_enable_wireless_in_fabric.py-111-FEWembededWirelessECA&begin=3062893&size=46969&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May19_20:37:56.562262.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eca_enable_wireless_in_fabric.py-111-FEWembededWirelessECA&begin=3062893&size=46969&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May19_20:37:56.562262.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 
During Ghost P2 runs the same issue seen

+*Failed log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eca_enable_wireless_in_fabric.py-111-FEWembededWirelessECA&begin=4284136&size=27996&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun28_23:46:37.757123.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eca_enable_wireless_in_fabric.py-111-FEWembededWirelessECA&begin=4284136&size=27996&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun28_23:46:37.757123.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] This issue has occured on current Ghost P2 70780 runs 
Log: [test5_enable_dtls_version|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=29723477&size=4629&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul06_03:33:51.998558.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]   Hi [~accountid:63f50bf5e8216251ae4d59cf]  
Could you please share latest progress details here Added device version check yet to commit , working on the prompt recovery as device is stuck after executing “crypto pki certificate map map1 1” PIe Stack ticket file by Ashwini [https://piestack.cisco.com/23413/cat9300-creating-certificate-adding-device-getting-recovered|https://piestack.cisco.com/23413/cat9300-creating-certificate-adding-device-getting-recovered] Piestack DE Kameswara Rao Tullimilli, has suggested to update PyATS version to the latest and see if the issue persists or not.

our current PyATS version is: 23.3.1
Latest version to be installed is 23.11
 Currently Sanity is going on for Hulk P2 RC1 -70391 
Once completed with reports -will attempt to upgrade the PyATS version","['Auton', 'Ghost', 'Halleck', 'Hulk', 'Optimized', 'Sanity']",Ashwini R Jadhav,In Progress,Ashwini R Jadhav
SEEN-1415,https://miggbo.atlassian.net/browse/SEEN-1415,[Auton] Ghost/Hulk - Wireless Solution Sanity - Enhancement generation executive summary report,"*Regression:* Solution Sanity (SSR)

*Branch:* rcdn/Ghost-ms/api-auto synced to main branch private/Ghost-ms/api-auto and rcdn/Hulk-ms/api-auto synced to main branch private/Hulk-ms/api-auto - yesterday before regression run start 

*Script:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

*Issue Faced:* TC85 creates file with 0B at 1st run. On Reruns, get errored eventhough file gets created with some size

*Cluster:*

* Hulk (2.1.710.70195) loaded cluster 10.88.187.190
* Ghost P2 (2.1.614.70641) loaded cluster 10.89.48.89
* For both clusters, login is admin/Maglev123 for UI and maglev/Maglev123 for SSH

*Taas Log:*

* Hulk Run: [https://ngdevx.cisco.com/services/taas/results/5ef7ca3e-ab54-4597-ad8d-1ada8f9b3074|https://ngdevx.cisco.com/services/taas/results/5ef7ca3e-ab54-4597-ad8d-1ada8f9b3074]
* Ghost Run: [https://ngdevx.cisco.com/services/taas/results/f56e8fa8-25c2-4294-921f-1e6236da1070|https://ngdevx.cisco.com/services/taas/results/f56e8fa8-25c2-4294-921f-1e6236da1070]    ",2023-04-12T14:08:38.789+0000,"Resoved at PR (Ghost): [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5327/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5327/overview]
PR (Halleck): [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5347/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5347/overview]

Hulk is merged in commit: bf0b05c480291b36d8d16fd287c65c45cae4e435 by Tran also","['Auton', 'Ghost', 'Hulk']",ThanhTan Nguyen,Resolved,Yuvarani Iyamperumal
SEEN-1433,https://miggbo.atlassian.net/browse/SEEN-1433,"[Auton] Hulk - Wireless Solution Sanity - Fabric Update fails with NCSP11234 on TC33, blocking all other TCs","*Regression:* Solution Sanity (SSR)

*Branch:* rcdn/Hulk-ms/api-auto synced to main branch private/Hulk-ms/api-auto - yesterday before regression run start

*Script Used:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

*Issue Faced:* Fabric Update fails with NCSP11234 during TC33, blocking all other TCs

*FIPS Hulk + 17.6.5*

* *Cluster:* 10.88.187.195 – admin/Maglev123maglev for UI and maglev/Maglev123 for SSH – Loaded with Hulk *2.1.710.70212*.iso
** For device access, SSH to device ip from DNAC CLI via login cisco/ Cisco#123maglev1/Cisco#123maglev1 or wlcaccess/Lablab#123maglev1/Cisco#123maglev1
* *Trade Log (TC33):* [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/23-04/sanitycombine.2023Apr12_01:17:50.006754.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/23-04/sanitycombine.2023Apr12_01:17:50.006754.zip&atstype=ATS]  

38605: The Schedduled Job failed: with reason {'id': '420905e9-1d6d-44ed-80bc-a6ba7d88d493', 'triggeredJobTaskId': '372dd204-64dc-448e-9a51-c7a1947a05f8', 'triggeredTime': 1681284631474, 'status': 'FAILED', 'failureReason': 'NCSP11234: Not able to proceed with operation because of previous pending operations on the same intent. Further operations are not permitted until the existing ones are either deployed or discarded. Additional info for support: Current operation: Fabric update on Device(s) for False on time: 1681284631.0095356(98368b7e-19b9-4358-b850-dace23ed793c). Pending operation(s): [null (null)].', 'triggeredJobId': '420905e9-1d6d-44ed-80bc-a6ba7d88d493'}

*Hulk + 17.11.1*

* *Cluster:* 10.88.187.190 – admin/Maglev123 for UI and maglev/Maglev123 for SSH – Loaded with Hulk *2.1.710.70213*.iso
** For device access, SSH to device ip from DNAC CLI via login cisco/Cisco#123/Cisco#123 or wlcaccess/Lablab#123/Cisco#123
* *Trade Log (TC33):* [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/23-04/sanitycombine.2023Apr13_08:32:50.617210.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/23-04/sanitycombine.2023Apr13_08:32:50.617210.zip&atstype=ATS]

*DNAC SPF DE Ram Comments on Hulk + 17.6.5 testbed:*

It looks like a script issue.

 

From the logs, I can see this happening:

 

# A preview request is made for namespaces b59473ef-d1b9-4f60-b0ad-9723518001 and/ or 00140382-3df3-4fbd-b142-ef9b364ab303
# A cleanup request is sent for the above preview request
# At the same time when the cleanup is ongoing, a “deploy” request was sent for the same (or one of the) namespaces
## This resulted in the error (which we call recycle prevention) seen below and in the UI since the cleanup hadn’t completed yet.

 

Can the script owner please check the sequence provided above?

 

Ram",2023-04-13T16:29:41.973+0000,"The VCR changes  Facing the same issue in TC28 itself and run is getting blocked after that 

DNAC+ Device version : Hulk(2.1.710.70212)+ V179_THROTTLE_LATEST_20230403_150730_V17_9_3_3

Error Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9575149&size=887711&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-04%2Fsanitycombine.2023Apr13_22:49:59.004075.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9575149&size=887711&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-04%2Fsanitycombine.2023Apr13_22:49:59.004075.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

Full Log : [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-04%2Fsanitycombine.2023Apr13_22:49:59.004075.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-04%2Fsanitycombine.2023Apr13_22:49:59.004075.zip&atstype=ATS] Merged to Halleck and Hulk.

PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5370/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5370/overview]

Quick/temp fix for the provision and assign roles issue.

* Follow current code logic to provide quick fix for now.
* Support deploy or discard
* Handle activity_id completed/deleted in case deployment or discarding or failure.
* Not cover new devices' preview config check yet that will be covered in Andrew VCR.

Note: Andrew are working non VCR so we will have more changes/code refractoring in the area. So should we consider to use this fix for now or wait for Andrew's PR?

Only tested the following test with Halleck cluster since I don't have Hulk cluster. Should it be ok?

* Test_TC27_DNAC_Device_Provisioning: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/tranlam-sjc/testenv/users/tranlam/archive/23-04/multisite_ams2_jenkins_job.2023Apr17_17:10:12.336687.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/tranlam-sjc/testenv/users/tranlam/archive/23-04/multisite_ams2_jenkins_job.2023Apr17_17:10:12.336687.zip&atstype=ATS]
* TC32.2 test2_verify_assign_roles_and_deploy_devices_on_fabric1_site_new_york
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/tranlam-sjc/testenv/users/tranlam/archive/23-04/multisite_ams2_jenkins_job.2023Apr17_17:35:53.919876.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/tranlam-sjc/testenv/users/tranlam/archive/23-04/multisite_ams2_jenkins_job.2023Apr17_17:35:53.919876.zip&atstype=ATS]
* [https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/tranlam-sjc/testenv/users/tranlam/archive/23-04/multisite_ams2_jenkins_job.2023Apr17_21:20:23.352080.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/tranlam-sjc/testenv/users/tranlam/archive/23-04/multisite_ams2_jenkins_job.2023Apr17_21:20:23.352080.zip&atstype=ATS] Fix for similar issue with {{onboard_device_on_interface}}
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/bdcbc2ad62bf6168f6abd44b32473816481e02f5|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/bdcbc2ad62bf6168f6abd44b32473816481e02f5] Fix for similar issue with {{update_wireless_segment}}

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/be6a34238f8de0adc3bd4213d2b12471645d74c6|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/be6a34238f8de0adc3bd4213d2b12471645d74c6] Hi [~accountid:62d2fe9f8afb5805e5d5af49]  ,
Same issue is also observed during Hulk execution on Sanity on-prem 2.1.710.70276,
Devices provision failed with {{NCSP11234: Not able to proceed with operation because of previous pending operations on the same intent. }}
Could you please check on the log and confirm?
*Uber ISO Version tested:* Hulk 2.1.710.70276
*Device Image* :17.12.1
+*Branch:*+   *private/Hulk-ms/sanity_api_auto*
*Script Name:* Optimized code
testcases\sanityusecases\provisionReprovisionConfigValidation\reprovisioning_provision_conifg_validation.py
+*Usecasemaps:*+
usecasemaps\lansanity\lansanity_usecases_maps.yaml

+*Error snip:*+

{noformat}he Schedduled Job failed: with reason {'id': '6f541eb7-030b-4cad-ae4d-f7e3c2348e22', 'triggeredJobTaskId': '49f2aa36-7803-40d1-907b-d3acb12dc185', 'triggeredTime': 1684264555769, 'status': 'FAILED', 'failureReason': 'NCSP11234: Not able to proceed with operation because of previous pending operations on the same intent. Further operations are not permitted until the existing ones are either deployed or discarded. Additional info for support:  Current operation: Provision Device - TB7-SJ-eCA-BORDER-CP at 1684264553.0758862 - Regular Schedule(ef5c16ba-87f4-41ff-9214-9a4ad57fd208). Pending operation(s): [Deploy Scheduling task for connectivity domain  update for fabric Global/USA/SAN_JOSE_US_SJ_Fabric1 at time 1684264509.7651012 (ce144805-62bc-46a0-b0aa-16f12b936d4e)].', 'triggeredJobId': '6f541eb7-030b-4cad-ae4d-f7e3c2348e22'}
1738:  Checking schedule job status was failed. {'response': [{'runNow': True, 'startTime': 1684264555709, 'externalSchedule': {'notificationURL': '/api/v2/data/customer-facing-service/DeviceInfo', 'notificationMethod': 'PUT', 'notificationBody': [{'id': '426f057e-0ccc-454e-9857-1ef7e3903033', 'instanceId': 306508, 'instanceCreatedOn': 1684263797944, 'instanceUpdatedOn':{noformat}


+*Failed Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-reprovisioning_provision_conifg_validation.py-102-provisionReprovisionConfigValidation&begin=357357&size=1180450&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May16_11:59:35.822359.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-reprovisioning_provision_conifg_validation.py-102-provisionReprovisionConfigValidation&begin=357357&size=1180450&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May16_11:59:35.822359.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


!image-20230517-122817.png|width=1030,height=178! Please file defect for it. Based on the log it failed to clear the conflicting.
1282: 
 {'progress': 'NCSS10077: Failed to delete task schedule: [ce144805-62bc-46a0-b0aa-16f12b936d4e]', 'startTime': 1684264553425, 'version': 1684264553449, 'errorCode': 'NCSS10077', 'endTime': 1684264553448, 'serviceType': 'NCSS', 'lastUpdate': 1684264553432, 'isError': True, 'username': 'sysadmin', 'failureReason': 'NCSS10077: Failed to delete task schedule: ce144805-62bc-46a0-b0aa-16f12b936d4e', 'instanceTenantId': '64628f5ff2bd5d3c3314c3b2', 'id': 'a155ed4f-ebaa-4251-b858-57cda6a5e6b2'}

{noformat}1283: 
 Failed to delete schedule task: ce144805-62bc-46a0-b0aa-16f12b936d4e. Reason NCSS10077: Failed to delete task schedule: ce144805-62bc-46a0-b0aa-16f12b936d4e{noformat}

{noformat}1285: 
 Some issue in deleting conflicting actionable items{noformat}

{noformat}1287: 
 Failed to clear provisioning conflicts{noformat}","['AWS-Santiy', 'Auton', 'Blocked', 'Hulk', 'Optimized', 'Sanity']",Yuvarani Iyamperumal,Resolved,Yuvarani Iyamperumal
SEEN-1435,https://miggbo.atlassian.net/browse/SEEN-1435,Test_TC197_wireless_client_data_troubleshoot,"* *DNAC Release_Version Tested:* Ghost P1 RC1-2.1.613.70170
* *Device Image Used:* 17.10.1FC2
* *Testbed:* AWS-Multisite setup
* *Branch Used:* private/Ghost-ms/sanity_api_auto
* *Script Name:* solution_test_3sites_sjc_nyc_sf.py
* *Solution Input File :* dnac-auto/configs/config_48hr_test/solution_test_input_FIPS.json
* *Testcases Impacted:* Test_TC197_wireless_client_data_troubleshoot
* *Failed Trade Log:* [https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/SR-CL-MS/job/AWS-Multisite-Common-Job/454/console|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/SR-CL-MS/job/AWS-Multisite-Common-Job/454/console]   --please refer TC 197
* *Issue details/analysis:*While executing setup_parameter TC,we are hitting one script issue with the below error:
{color:#bf2600}2023-04-05T05:43:58: %SERVICES-ERROR:   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/AWS-Multisite-Common-Job/services/dnaserv/lib/api_groups/assurance/group.py"", line 9616, in get_client_mac_address_in_ewlc
2023-04-05T05:43:58: %SERVICES-ERROR:     if ""AP"" + result not in results[""output""]:
2023-04-05T05:43:58: %SERVICES-ERROR: TypeError: list indices must be integers or slices, not str{color}
",2023-04-14T04:43:12.738+0000,"!Wireless_trouble_Shooting issue.png|width=932,height=391! Autons Info:
===============

# PR link:

* [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5363/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5363/overview]
* [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5364/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5364/overview]

# Trade log: [+https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1291191&size=157821&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr17_02:45:51.914044.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS+|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1291191&size=157821&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr17_02:45:51.914044.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Hi Quang,
I have tried executing Wireless Troubleshooting feature and seeing this sub test case {{test3_get_log_file_content_and_validate}} getting errored out due to below reason:

  {{AttributeError: 'DnaServices' object has no attribute 'get_log_file_content'}}

Please find the Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1844451&size=168004&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May16_09:15:03.304529.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1844451&size=168004&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May16_09:15:03.304529.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Hi [~accountid:63f50bcf4e86f362d39acde5]  ,
Same issue seen  Ghost P2 run : 
FAiled Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-wireless_client_data_troubleshoot.py-1713-wirelessClientTroubleshoot&begin=4318&size=52354&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May20_09:42:25.776261.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-wireless_client_data_troubleshoot.py-1713-wirelessClientTroubleshoot&begin=4318&size=52354&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May20_09:42:25.776261.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Error snip* 

{noformat}89:      result = method(*args, **kwargs)
90:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/assurance/group.py"", line 9645, in get_client_mac_address_in_ewlc
91:      if ""AP"" + result not in results[""output""]:
92:  TypeError: list indices must be integers or slices, not str{noformat} [~accountid:63f50bcf4e86f362d39acde5] , do we have any update on below observed issue? New PR has been Raised: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6038/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6038/overview] [~accountid:63f50bcf4e86f362d39acde5] , I reviewed the PR and used it locally for testing. It required some more work, which I did and have committed and validated too.

PR raised, approved and merged for:

Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6141/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6141/overview]

Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6144/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6144/overview]

[~accountid:620b8357878c2f00729881c8] , I have validated it on ESXi Sanity testbed. You too, pls. go ahead and validate the same on On-Prem Sanity testbed as well.","['AWS_MSTB', 'Auton', 'Ghost', 'Integration', 'Optimized', 'Sanity']",QuangVinh Nguyen,Resolved,Neelima Doddipalli
SEEN-1436,https://miggbo.atlassian.net/browse/SEEN-1436,[Auton]Halleck: Test_TC144_ISE_PAN_failover / test2_syncup_secondary_ISE_node,"*Reporter Analysis:* In Halleck Testing we are observing the ISE Pan Failover Feature to be failing. Need to understand if the PPAN is shut down as we see the uptime to be changing for primary ISE and then the secondary ISE or SPAN is to take the role and sync with the PPAN. However, we see that the connection to the PPAN itself is failing.

*Description:  The error from log or more info* 

*Branch Name:  private/Halleck-ms/sanity_api_auto*

*Script file/Usecase:* [test1_ISE_PAN_failover1|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_pan_failover_takeover.py-143-ISEPANFailover&begin=4587&size=13063&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr10_05:37:47.582884.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_pan_failover_takeover.py-143-ISEPANFailover&begin=17650&size=203180&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr10_05:37:47.582884.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_pan_failover_takeover.py-143-ISEPANFailover&begin=17650&size=203180&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr10_05:37:47.582884.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:*

Test Cases Affected due to this:

[Test_TC1_ISE_PAN_failover|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_pan_failover_takeover.py-143-ISEPANFailover&begin=3900&size=220926&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr10_05:37:47.582884.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test2_syncup_secondary_ISE_node

[Test_TC1_ISE_PAN_failover|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_pan_failover_takeover.py-143-ISEPANFailover&begin=3900&size=220926&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr10_05:37:47.582884.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test3_ISE_PAN_failover2

[Test_TC1_ISE_PAN_failover|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_pan_failover_takeover.py-143-ISEPANFailover&begin=3900&size=220926&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr10_05:37:47.582884.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test4_syncup_secondary_ISE_node_again

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8]",2023-04-14T06:10:33.633+0000,"Hello [~accountid:63f50bcece6f37e5ed93c87e] 

Could you please review the below Auton  Hello [~accountid:63f50bcece6f37e5ed93c87e] 

Could you please help us with this Auton.","['Auton', 'Ghost', 'Guardian', 'Halleck', 'Optimized', 'Sanity']",Andrew Chen,Backlog,DeepakPratap Shinde
SEEN-1439,https://miggbo.atlassian.net/browse/SEEN-1439,Create/Use a flag that can help in getting rid of ESXi specific branches,"Create/Use a flag that can help in getting rid of ESXi specific branches.

This will save us lot of time while maintaining separate branches for ESXi and keep pulling latest content from the regular branches.",2023-04-17T22:27:24.258+0000,"Raised below PR for Halleck branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5388/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5388/overview] Halleck related PR has been merged.

Raised another PR for Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5390/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5390/overview] Hulk specific PR has also been approved and merged.

Marking this ticket as “Done”.","['Auton', 'automation']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1440,https://miggbo.atlassian.net/browse/SEEN-1440,correct the Testcase name convention,"Correct the naming conventions used across dnac-auto files, specially the testscripts:

# class Test_TC\d+_

This standardization in naming convention would be useful while working on any future automation work meant to process testcase data.

Removed {{uid}} entries as by default {{uid}} will be set to the test-case’s Class name.",2023-04-18T05:06:20.722+0000,"Raised PR for Halleck branch with required changes: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5377/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5377/overview] Required PR has been merged to Halleck and will be synced to Hulk during regular syncs.

Marking this ticket as “Done”.","['Auton', 'automation']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1441,https://miggbo.atlassian.net/browse/SEEN-1441,"[Auton] [Hulk] - Interface onboarding operation fails with ""NCSP11234 Pending operation"" errors","*Regression:* Solution Regression (Multisite and IBSTE)

*Branch:* private/Hulk-ms/api-auto 

*Script Used:* 

dnac-auto/testcases/forty_eight_hour/solution_test_3sites_sjc_nyc_sf_mdnac_dr.p

*Issue Faced:* Interface onboarding operation fails with NCSP11234 error with Pending operations issue. Due to this configs are not getting pushed. 
This issue is similar to Automation Jira - [https://miggbo.atlassian.net/browse/SEEN-1433|https://miggbo.atlassian.net/browse/SEEN-1433|smart-link]
 

*Failed logs:*
*Log 1 -* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fsr_mb_multi_sites_mdnac.2023Apr18_08:40:36.583719.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fsr_mb_multi_sites_mdnac.2023Apr18_08:40:36.583719.zip&atstype=ATS] -> Refer TC40

*Log 2 -* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fsr_mb_multi_sites_mdnac.2023Apr18_21:00:41.530215.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fsr_mb_multi_sites_mdnac.2023Apr18_21:00:41.530215.zip&atstype=ATS] -> TC40 Re-execution and failed with same issue",2023-04-19T13:19:56.321+0000,"Automation fix was committed for the issues as per this - [+bdcbc2ad62b+|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/bdcbc2ad62bf6168f6abd44b32473816481e02f5] & [be6a34238f8|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/be6a34238f8de0adc3bd4213d2b12471645d74c6] 

Post using this commit, issue is no more observed. *Pass logs:* 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Ftranlam-sjc%2Ftestenv%2Fusers%2Ftranlam%2Farchive%2F23-04%2Fsr_mb_multi_sites_mdnac.2023Apr18_23:35:04.259619.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Ftranlam-sjc%2Ftestenv%2Fusers%2Ftranlam%2Farchive%2F23-04%2Fsr_mb_multi_sites_mdnac.2023Apr18_23:35:04.259619.zip&atstype=ATS] → Refer TC40

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fsr_mb_multi_sites_mdnac.2023Apr19_02:00:14.684364.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fsr_mb_multi_sites_mdnac.2023Apr19_02:00:14.684364.zip&atstype=ATS] → Refer TC40","['Auton', 'Hulk', 'IBSTE', 'MSTB1', 'MSTB2', 'MSTB3', 'Multisite']",Tran Lam,Closed,SANDEEP SHIVARAMAREDDY
SEEN-1442,https://miggbo.atlassian.net/browse/SEEN-1442,skip calls to connect_rbac() until CSCwf02465 is fixed for Hulk ESXi,skip calls to connect_rbac() until [CSCwf02465|https://cdetsng.cisco.com/webui/#view=CSCwf02465] is fixed for Hulk ESXi,2023-04-19T18:14:05.022+0000,"PR for the required change: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5403/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5403/overview] PR approved and merged to {{Hulk-ms/api-auto}} branch and cherry-picked to `hulk_esxivm` branch.

Marking this ticket as “Done”.","['Auton', 'ESXi', 'automation']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1443,https://miggbo.atlassian.net/browse/SEEN-1443,test8_connect_linux_syslog_server_check_event_logs  - verify_rogue_awips_events_subscription() requires update while validating expected output,"[test8_connect_linux_syslog_server_check_event_logs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2687449&size=69081&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr20_12:12:11.761499.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] is failing due to improper handling of expected output.

Couple of issues:

# Trying to find MAC Address with “:” using “.” formatted MAC address

{noformat}11381:  res output [('00:50:56:A0:98:EF', 'TB7-SJ-EDGE'), ('00:50:56:A0:67:2C', 'TB7-NY-FIAB'), ('00:50:56:A7:B6:3F', 'TB7-SJ-EDGE')]
11382:  checking event for device TB7-SJ-EDGE and connected mac 00.50.56.A7.B6.3F{noformat}

2. The logic to update and use of {{faileddev}} variable is not appropriate.",2023-04-21T05:13:28.285+0000,"Required PR has been raised for Guardian branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5412/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5412/overview] Guardian specific PR has been approved and merged into Guardian branch..

Waiting for execution logs from other releases. Based on the data, will proceed further. Added same to Ghost branch and have got 100% pass result with Ghost P1 RC5 image: [Test_TC171_syslog_server_event_notification|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3123164&size=1389797&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr29_11:27:52.704962.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [~accountid:620b8357878c2f00729881c8] , pls. share the log from Hulk release to confirm on the closure of this ticket. TC passed in Hulk  ,Hence  moving to close state:
Pass Log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-06/env_auto_job.2023Jun16_01:59:20.156114.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-06/env_auto_job.2023Jun16_01:59:20.156114.zip&atstype=ATS]
 ","['Auton', 'Guardian']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1444,https://miggbo.atlassian.net/browse/SEEN-1444,[Auton]:Ghost-Task-ext_node_link_failover_ip_addr_change.py-151-SDAExtendedNodelinkfailoverIpChange/test1_verify_assurance_health_nw_health_border_node,"*Reporter Analysis:*
During solution sanity on  AWS-Ghost Uber(P1RC2) #2.1.660.70179. We are observing device health score is going to 1. Under DNAC Device’s 360 (eCA, NY-FIAB)Page issue is observed like "" MulticastRP_WirelessVNFB”  and “MulticastRP_WiredVNFBLayer2” for Virtual Network are down.

*Found on:*
Uber ISO : Ghost P1 RC2 # 2.3.5.3-70179
Polaris version: 17.11.1 
TB2-DMZ-SJ-FIAB-ECA : 10.12.64.65
[TB2-DMZ-NY-FIAB.cisco.com|http://TB2-DMZ-NY-FIAB.cisco.com] : 10.12.80.65 

*Description:*  **  

MulticastRP_WirelessVNFB-204.192.3.40-Overlay
MulticastRP_WiredVNFBLayer2-204.192.3.40-Overlay 
both VN services are  down 

*Branch Name:*  private/Ghost-ms/sanity_api_auto

*Script file/Usecase:* solution_test_sanityecamb_lan.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: 1)*[*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ext_node_link_failover_ip_addr_change.py-151-SDAExtendedNodelinkfailoverIpChange&begin=517558&size=137189&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr20_07:42:56.388654.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ext_node_link_failover_ip_addr_change.py-151-SDAExtendedNodelinkfailoverIpChange&begin=517558&size=137189&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr20_07:42:56.388654.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*Testbed details:*[*https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-11*|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-11] 
Cluster ip -172.35.16.151 (admin/maglev1@3) cli-maglev/Maglev123

----

 ",2023-04-23T05:25:35.275+0000,"Hi [~accountid:620b8357878c2f00729881c8], This issue happened due to the border lost connectivity in the VN WiredVNFBLayer2 and WirelessVNFB to Multicast RP 204.192.3.40. Please check the device configuration on the AWS cluster TB11.

!image-20230427-063910.png|width=75%!

Test case {{test1_verify_assurance_health_nw_health_border_node}} is working as expected. It failed because the device’s Health Score is 1.

I have checked on Sanity TB1 and it works fine. My execution log is as follows:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-thangqtr%2Fusers%2Fthangqtr%2Farchive%2F23-04%2Fsanity_TB1.2023Apr25_00:00:18.798496.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-thangqtr&submitter=thangqtr&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-thangqtr%2Fusers%2Fthangqtr%2Farchive%2F23-04%2Fsanity_TB1.2023Apr25_00:00:18.798496.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-thangqtr&submitter=thangqtr&from=trade&view=all&atstype=PYATS]","['AWS-Santiy', 'Auton', 'Ghost', 'Optimized', 'Sanity']",ThangQuoc Tran,Resolved,Omkar Sharad Wagh
SEEN-1446,https://miggbo.atlassian.net/browse/SEEN-1446,check the usage of get_cd_name_from_device() method across dnac-auto and cleanup the dups,"Check the usage of {{get_cd_name_from_device()}} method across dnac-auto and cleanup the dups.

Remove the local definition of {{get_cd_name_from_device()}} and make use of the one that {{services/dnaserv/lib/api_groups/inventory/group.py}} has and can be called using {{dnac_handle}}.",2023-04-25T20:50:40.677+0000,"Raise PR for the required changes: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5484/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5484/overview] for Halleck branch. Required PR has been merged into {{Halleck-ms/api-auto}} branch.

Marking this enhancement ticket as “Done”.","['Auton', 'Enhancement']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1464,https://miggbo.atlassian.net/browse/SEEN-1464,[Auton]:Ghost : Task-rback_roles_users_configs.py-31-systemUserAndRoleback  /   Test_TC1_DNAC_RBAC_create_users_roles  /   test4_Upload_CA_trusted_certificate ,"*Reporter Analysis:*
we observed sanity Ghost-optimized code.
If  ISE is less than 3.0, Test case should be skipped. we are getting the below error.


*Testbed Wiki:*
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-11|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-11]

*Error Snip:*  **  


{noformat}1420: 
1421:  Exception:
1422:  Traceback (most recent call last):
1423:    File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/Ghost_Optimized-Sanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper
1424:      result = testfunc(func_self, **kwargs)
1425:    File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/Ghost_Optimized-Sanity/services/dnaserv/testplans/testplan_groups/common_testplan.py"", line 464, in test4_Upload_CA_trusted_certificate
1426:      if dnac_handle.upload_trusted_certifcate_to_ISE():
1427:    File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/Ghost_Optimized-Sanity/services/dnaserv/lib/decorators.py"", line 32, in wrapper
1428:      result = method(*args, **kwargs)
1429:    File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/Ghost_Optimized-Sanity/services/dnaserv/lib/api_groups/certifiates/group.py"", line 203, in upload_trusted_certifcate_to_ISE
1430:      if self.services.dnaconfig.iseadminapi.upload_CA_trusted_certifcate(certificate):
1431:    File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/Ghost_Optimized-Sanity/services/iseserv/ise_admin_api.py"", line 328, in upload_CA_trusted_certifcate
1432:      response = res.json()[""response""]
1433:  AttributeError: 'NoneType' object has no attribute 'json'{noformat}



*Branch Name:*  private/Ghost-ms/sanity_api_auto

*Script file/Usecase:* [systemUserAndRoleback|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-rback_roles_users_configs.py-31-systemUserAndRoleback&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr19_00:31:49.398990.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-rback_roles_users_configs.py-31-systemUserAndRoleback&begin=129844&size=199330&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr19_00:31:49.398990.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-rback_roles_users_configs.py-31-systemUserAndRoleback&begin=129844&size=199330&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr19_00:31:49.398990.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:*
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-04/env_auto_job.2023Apr19_00:54:59.750565.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-04/env_auto_job.2023Apr19_00:54:59.750565.zip&atstype=ATS]
",2023-04-26T12:28:03.089+0000,"This issue has been encountered in recent Hulk P1 runs where we are using ISE 2.7 P10 
Please help to fix this  Fix [c62f61d9fb4|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c62f61d9fb416ce61063f4e0db64f750e1f0229d]","['AWS-Santiy', 'Auton', 'Ghost', 'Hulk', 'HulkPatch', 'Optimized', 'Sanity']",Raji Mukkamala,Resolved,Omkar Sharad Wagh
SEEN-1465,https://miggbo.atlassian.net/browse/SEEN-1465,[Auton]:Ghost:Task-dhcp_server_sahred_secrect_change.py-134-designNWSettingsEditsDhcp  /   Test_TC2_Verify_DHCP_server_change_on_segments ,"*Uber ISO Version tested:* Ghost Patch 1 RC 2 #2.1.613.70179
*Reporter Analysis:* We observed a sanity Ghost AWS run, optimizing code where TC’s fails for key errors.


*Testbed Wiki:*
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-6]11
Note: On AWS cluster we are not using internal ip for ISE 
*Error Snip :* 

{noformat}    @library_wrapper
    def verify_radius_shared_secret_on_dev(self) -> bool:
        """"""
        This lib is used to validate shared secret config on devices
        """""" 

 result = True
        radius_ip = self.services.dnaconfig.testbed.custom[self.services.dnaconfig.sid]['servers']['AAA']['authServers'][0]
        key = self.services.input_data[""NEW_SECRET_KEY""]{noformat}

*Script Name:* Optimized code 
\testcases\sanityusecases\designNWSettingsEditsDhcp\dhcp_server_sahred_secrect_change.py
*Testbed :* TB11
*Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-dhcp_server_sahred_secrect_change.py-134-designNWSettingsEditsDhcp&begin=85221&size=4392159&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr20_03:25:22.018309.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-dhcp_server_sahred_secrect_change.py-134-designNWSettingsEditsDhcp&begin=85221&size=4392159&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr20_03:25:22.018309.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-04-26T13:00:29.829+0000,"The similar issue is seen on TB3 as well - Ghost P1 RC2 (70179)
*Branch:* private/Ghost-ms/sanity_api-auto
*Test script*: testcases/forty_eight_hour/solution_test_sanityecamb.py

*Log:* [Test_TC60_Verify_DHCP_server_change_on_segments|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10951261&size=2839838&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr20_00:31:11.675683.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] / [test5_verify_devices_status_after_change_shared_secret|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12184543&size=1606378&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr20_00:31:11.675683.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

where the error snip is: 
{{radius_ip = self.services.dnaconfig.testbed.custom[self.services.dnaconfig.sid]['servers']['AAA']['authServers'][0] }}

{{32611: KeyError: 'authServers' }}  PRs:

* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5568/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5568/overview]
* Ghost/sanity: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5567/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5567/overview]
* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5569/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5569/overview]
* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5570/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5570/overview] Hi [~accountid:63f50bd34c355259db9ccc4d]  ,
Hulk  2370-70389

+*Failed log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-dhcp_server_sahred_secrect_change.py-134-designNWSettingsEditsDhcp&begin=84994&size=50268136&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun28_21:52:28.570522.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-dhcp_server_sahred_secrect_change.py-134-designNWSettingsEditsDhcp&begin=84994&size=50268136&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun28_21:52:28.570522.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

!image-20230703-091801.png|width=743,height=204!

!image-20230703-091513.png|width=786,height=292!

!image-20230703-091738.png|width=905,height=319! This issue has been fixed in PRs:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6190/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6190/overview]
* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6191/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6191/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6192/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6192/overview] [~accountid:63f50bd34c355259db9ccc4d] , I have approved and merged all your PRs.

[~accountid:620b8357878c2f00729881c8] , pls. validate the change from your end as well and confirm here.

If issue is still not resolved for you, pls. reopen this Auton.","['AWS-Santiy', 'Auton', 'Ghost', 'Hulk', 'Optimized', 'Sanity']",ThangQuoc Tran,Resolved,Omkar Sharad Wagh
SEEN-1466,https://miggbo.atlassian.net/browse/SEEN-1466,[Auton]: Task-ext_node_link_failover_ip_addr_change.py-151-SDAExtendedNodelinkfailoverIpChange ,"Please add the below-mentioned sub test case in optimized sanity code. 

Sanity_script_file: solution_test_sanityecamb_lan.py

* *test70_reassign_ext_node_ip*

Please double commit this in Groot & Ghost , Halleck  branches


*Script Name:* Optimized code
*Optimized  Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ext_node_link_failover_ip_addr_change.py-151-SDAExtendedNodelinkfailoverIpChange&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr20_07:42:56.388654.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ext_node_link_failover_ip_addr_change.py-151-SDAExtendedNodelinkfailoverIpChange&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr20_07:42:56.388654.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Legacy  Pass Script Log:*
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-04/env_auto_job.2023Apr25_09:40:06.380581.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-04/env_auto_job.2023Apr25_09:40:06.380581.zip&atstype=ATS]
",2023-04-26T14:05:30.354+0000,,"['AWS-Santiy', 'Auton', 'Ghost', 'Halleck', 'Optimized', 'Sanity']",Moe Saeed,Resolved,Omkar Sharad Wagh
SEEN-1469,https://miggbo.atlassian.net/browse/SEEN-1469," Test_TC176_ITSM_ticket_generation_test / test3_approve_SGT_request declare failure in case of ""latest_id"" is equal to ""None"""," [Test_TC176_ITSM_ticket_generation_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=126605772&size=165381&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr26_02:33:32.936669.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test3_approve_SGT_request

Declare graceful failure in case of ""latest_id"" is equal to ""None"" rather than hitting exception.

{noformat}419400:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Guardian/Guardian-sanity-common-Multi-job/services/dnaserv/lib/api_groups/itsm/group.py"", line 153, in get_integration_event_status
419401:      if id in event[""ITSMLink""]:
419402:  TypeError: 'in <string>' requires string as left operand, not NoneType
419403:  Test returned in 0:01:41.912829
419404:  Errored reason: 'in <string>' requires string as left operand, not NoneType{noformat}",2023-04-27T18:38:22.663+0000,"PR to handle the exception along with some refactoring and updates for Halleck Branch:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5461/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5461/overview] Separate PR for Guardian with just the required change: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5462/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5462/overview] Guardian specific PR got approved and merged, cherry-picked the same to Groot and Ghost branches as well. Halleck specific PR also got approved and merged.

Hulk will get it with next sync from Halleck to Hulk. Marking it as “Done”.","['Auton', 'automation']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1471,https://miggbo.atlassian.net/browse/SEEN-1471,[Auton]:Test_TC129_Disconnect_Delete_Reason /test2_Disconnect_Delete_Reason/ test3_reconnect_client ,"*Reporter Analysis:* We have observed that testcase is not able to connect to SSIDDot1xIndiatb6  according to the script on Wireless-Client1
Tried adding password manually also it is  showing “authentication failure”

TC 123:

[Test_TC123_aaa_per_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27481732&size=1827953&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May31_20:37:18.432295.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test5_connect_clients_to_ssids 

{noformat} #!!!Client TB6-wireless-client1 : Failed to connect to ssid test_AAAtb6 or couldnot get ip address!!!#{noformat}

We have observed that testcase is not able to connect to {{test_AAAtb6}}  according to the script on Wireless-Client1
Tried adding password manually also it is  showing “authentication failure”

*Failed Logs TC129:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=92325475&size=29455&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr12_05:53:39.895308.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=92325475&size=29455&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr12_05:53:39.895308.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Failed Logs TC123:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=29215888&size=30457&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May31_20:37:18.432295.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=29215888&size=30457&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May31_20:37:18.432295.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Error msg from Failed Logs:*
518412: 
 Failed to connect client TB6-wireless-client1 to ssid SSIDDot1XIndiatb6. Check log for more information
 



*Testbed wiki:*
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-6|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-6]

 

*Description*:  

{{518414: #!!!Client TB6-wireless-client1 : Failed to connect to ssid SSIDDot1XIndiatb6 or couldnot get ip address!!!#}}

*Branch Name:* Guardian-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue",2023-04-28T07:47:54.118+0000,"Hi [~accountid:62d2fe9f8afb5805e5d5af49] 

Can you please Prioritize this Issue as issue is seen only on TB6 for above testcases.


Thanks,
Anusha John [~accountid:61efa8c457b25b006877eda3] for test_AAAtb6 SSID  , On the secondary AAA radius server. Directly login to the secondary radius server VM, And verify the configuration from the wiki under the section *2 - Configure* is present 

[https://wiki.cisco.com/pages/viewpage.action?pageId=736849131|https://wiki.cisco.com/pages/viewpage.action?pageId=736849131] (Refer to  section *2 - Configure*)

For {{SSIDDot1XIndiatb6}} SSID connection, verify Radius logs on ISE. This is not script issue. Hi [~accountid:63f50bf5e8216251ae4d59cf] 
I checked on my aaa vm and other sanity TB’s vm where testcase is passing.
each sanity vm has this file which you mentioned.

Can we please have a syncup on this to close on this.

Thanks,

Anusha John Hi [~accountid:61efa8c457b25b006877eda3] , As discussed. Please verify all the configs are present on the AAA server as per the wiki. This is not script issue/Auton. Hi [~accountid:63f50bf5e8216251ae4d59cf] 

I made all changes on TB6 aaa and retried operations but again failed, please let me know whenever you are available for call  as i compared with other TB logs testcase not even attemption to connect to SSID test_AATB6 or any Tb

Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3066002&size=613317&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct08_22:54:10.677396.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3066002&size=613317&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct08_22:54:10.677396.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

 Assigning back to [~accountid:63f50bf5e8216251ae4d59cf] ","['Auton', 'Ghost', 'Guardian', 'Halleck', 'Hulk', 'Sanity', 'Shockwave']",Raji Mukkamala,Open,Anusha John
SEEN-1472,https://miggbo.atlassian.net/browse/SEEN-1472,[Auton][MSTB2] : Test_TC194_ap_profiles  /   test7_verify_ssh_enable_and_country_code,"Hi [~accountid:63f50be71223974bc04b0534] ,

I've triggered ""Test_TC194_ap_profiles"" execution. But one of subtestcase is failing as we have Aireos controller on San jose site.

In input file we have defined main site as """"main_site"": ""Global/USA/SAN JOSE/BLD23"",""

(FW-3504-1) >show run| sec ap profile

Incorrect usage. Use the '?' or <TAB> key to list commands.

(FW-3504-1) >

11526: Traceback (most recent call last):
11527: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 738, in call_service
11528: self.result = self.get_service_result()
11529: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 235, in get_service_result
11530: raise SubCommandFailure(
11531: unicon.core.errors.SubCommandFailure: ('sub_command failure, patterns matched in the output:', ['^Incorrect usage.'], 'service result', ""\r\nUpdating HBL license statistics file\r\n Done.\r\nshow run?\r\r\nrun-config running-config \r\r\n(FW-3504-1) >show run| sec ap profile\r\r\n\r\r\nIncorrect usage. Use the '?' or <TAB> key to list commands.\r\r\n\r\r\n(FW-3504-1) >"")
Failed log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2077120&size=47594&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fauto_MS_job.2023Apr28_01:56:06.185467.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2077120&size=47594&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fauto_MS_job.2023Apr28_01:56:06.185467.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Expected Behavior : Script should be able to verify command on other site where we have IOS-XE controller

From input file we can see that main
site_tags"": {
""main_site"": ""Global/USA/SAN JOSE/BLD23"",

We need to modify such that the automation script checks for each site and if the site has ECA device or EWLC then only the feature related checks should proceed, else it should skip.",2023-04-28T16:02:01.760+0000,[~accountid:63f50be71223974bc04b0534] Could you pritoize this [~accountid:63f50bf5e8216251ae4d59cf] I took care of this already: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6933/diff#testcases/forty_eight_hour/solution_test_sanityecamb_cert.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6933/diff#testcases/forty_eight_hour/solution_test_sanityecamb_cert.py],"['Auton', 'Integration', 'MSTB2']",Majlona 'Luna' Aliaj,Resolved,Divakar Kumar Yadav
SEEN-1474,https://miggbo.atlassian.net/browse/SEEN-1474,[Auton] Test_TC160_Brownfield_Workflow_Aireos / test14_onboard_BF_wireless_segments_and_provision_APs failed while provisioning APs that are part of WLC device,"*Reporter Analysis:* 

After learning brownfield configs from WLC device, while provisioning WLC is successful but onboarding BF wireless_segments and then provisioning APs has failed with error reason as: {{Provisioning AP of device failed for reason:NCWL10323: Access Point 00:0a:bc:00:18:00 is not assigned to site FLOOR1. Please make sure all Access Points part of this provision payload are assigned to site. }}

Script is picking an AP {{Access Point 00:0a:bc:00:18:00}} which is associated with ECA and not with WLC. 
This AP {{00:0a:bc:00:18:00}} is assigned to site Global/USA/SAN JOSE/BLD23/FLOOR1_LEVEL1


To provision APs - script should look for APs that are associated with WLC and is assigned to site {{FLOOR1}} of Global/USA/NEW YORK/BLDNYC/FLOOR1



*Description:  The error from log or more info* 
{{Provisioning AP of device failed for reason:NCWL10323: Access Point 00:0a:bc:00:18:00 is not assigned to site FLOOR1. Please make sure all Access Points part of this provision payload are assigned to site. }}

*Branch Name:*  private/Ghost-ms/sanity_api_auto

*Script file/Usecase:* testcases/forty_eight_hour/solution_test_sanityecamb.py

Test_TC160_Brownfield_Workflow_Aireos/ test14_onboard_BF_wireless_segments_and_provision_APs 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=972675&size=1990671&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May02_00:56:07.455822.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=972675&size=1990671&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May02_00:56:07.455822.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-3|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-3]",2023-05-02T09:26:21.387+0000,"[~accountid:63f50bcece6f37e5ed93c87e]  
I have executed this testcase on TB3 - Testbed is available with WLC device 
I have proceeded with provisioning of APs manually - But as we have provisioning of APs multiple times after learning brownfield configs - Please look into this issue 


!image-20230502-100235.png|width=1307,height=474! Luna has a fix for this, will add here when done [Pull Request #5599: Provision file modification - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5599/diff#services/dnaserv/lib/api_groups/provision_wireless/group.py] Luna’s fix Hi [~accountid:63f50bcece6f37e5ed93c87e] 
Please find the latest log of this testcase executed on TB3
[Test_TC160_Brownfield_Workflow_Aireos|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=928879&size=5756319&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May14_23:04:57.810717.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

where the sub-tc: [test25_onboard_wireless_segments_and_provision_APs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5341541&size=1342628&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May14_23:04:57.810717.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 
Failed due to {{15555: SSID SSIDSchedulertb3 is not enabled}}

When i checked on the cluster -the SSID was missing with Pool under fabric site Seems theres no mapping for segment in input file wiht this ssid, will check with Raji on it Executed this on TB3 with Ghost 
[Test_TC160_Brownfield_Workflow_Aireos|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=919640&size=7114444&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun19_01:07:15.434603.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

But subtest [test25_onboard_wireless_segments_and_provision_APs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5686223&size=2346838&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun19_01:07:15.434603.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] - has failed with reason as 

{noformat}15552: SSID Adv_wlan_configs_1 is not enabled
15553: SSID Adv_wlan_configs_2 is not enabled
15554: SSID SSIDSchedulertb3 is not enabled
15555:  SSID test_AAAtb3 is not enabled
 
Failed reason: Onboarding wireless segments and provisioning aps failure{noformat}

Lets not bother of these SSIDs Adv_wlan_configs_1 & Adv_wlan_configs_2 is not enabled as it is related to Advanced wlan feature. 
but please check on SSID SSIDSchedulertb3 & test_AAAtb3

[~accountid:63f50bcece6f37e5ed93c87e] Did you get chance to check on mapping for segment in input file? [~accountid:63f50bcece6f37e5ed93c87e] 
Executed on TB2- Got a pass log
[Test_TC160_Brownfield_Workflow_Aireos|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7297515&size=5293375&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun20_22:49:10.970977.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

This has been executed on TB where only first 60 TCs were executed which means - The SSIDs that are created are onboarded with segments. Hence there are no failures 
But when we do the full run i.e from TC1- TC159 - then there will be failures wrt to SSIDs segment onboarding. 
Please check on this Executed on TB3 with Ghost P2 70759 where WLC Aireos version is 8.10.185.0
Got a Pass log :  [Test_TC160_Brownfield_Workflow_Aireos|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=62582133&size=5504705&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun21_22:49:41.752625.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Ghost', 'Guardian', 'Issue']",Andrew Chen,Closed,Ashwini R Jadhav
SEEN-1478,https://miggbo.atlassian.net/browse/SEEN-1478,[Auton][Halleck][Hulk] - Failures in Wireless AP data Collection TC Validation,"*Regression profile:* Solution Regression Multisite

*Branch:* private/Hulk-ms/api-auto 

*Script Used:* 

dnac-auto/testcases/forty_eight_hour/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

dnac-auto/testcases/forty_eight_hour/solution_test_3sites_sjc_nyc_sf.py

*Issue Faced:* 

The wireless troubleshooting AP log collection related TC is failing with “This run does not meet requirement” error.  Also from the error its not clear as to what parameters are missing which does not meet the requirements. 
 

*Failed log:*
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May02_01:25:35.062494.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May02_01:25:35.062494.zip&atstype=ATS] → Refer TC260",2023-05-02T13:08:23.172+0000,"* PR - Link:
** *private/Hulk-ms/api-auto*: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5552/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5552/overview]
** *private/Halleck-ms/api-auto*: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5555/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5555/overview]
* Test Cases Summary: add try catch statement to improve code and handle some error scenario
* Trade log link: [+https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-05/sanity_TB8.2023May07_20:42:32.865805.zip&atstype=ATS+|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-05/sanity_TB8.2023May07_20:42:32.865805.zip&atstype=ATS] Issue is no more observed during execution on latest code base from *private/Halleck-ms/api-auto* branch.

*Branch used:* private/Halleck-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

Pass log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May30_08:24:23.273613.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May30_08:24:23.273613.zip&atstype=ATS] → Refer TC260","['AWS_MSTB', 'Auton', 'Halleck', 'Hulk', 'MSTB1', 'MSTB2', 'MSTB3', 'Multisite']",QuangVinh Nguyen,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-1479,https://miggbo.atlassian.net/browse/SEEN-1479,[Auton][Halleck][Hulk] - Failures in configure device into ISE NDG TC Validation,"*Regression profile:* Solution Regression Multisite

*Branch:* private/Hulk-ms/api-auto 

*Script Used:* 

dnac-auto/testcases/forty_eight_hour/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

dnac-auto/testcases/forty_eight_hour/solution_test_3sites_sjc_nyc_sf.py

*Issue Faced:* 

During config device into ISE NDG TC Validation, we see TC failures due to variable assignment 
 

*Failed log:*
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May02_01:25:35.062494.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May02_01:25:35.062494.zip&atstype=ATS] → Refer TC263

*Error snip:*


{noformat}12322: 
   File ""/auto/dna-sol/ws/sr-mb1_Hulk_fix/services/dnaserv/lib/api_groups/inventory/group.py"", line 2282, in get_all_device_in_inventory{noformat}

{noformat}12323: 
     data=all_device_payload){noformat}

{noformat}12324: 
 UnboundLocalError: local variable 'all_device_payload' referenced before assignment{noformat}

{noformat}12325: 
 Test returned in 0:00:00.278028{noformat}

{noformat}12326: 
 Errored reason: local variable 'all_device_payload' referenced before assignment{noformat}",2023-05-02T14:37:42.019+0000,"* PR - Link: 
** *private/Hulk-ms/api-auto*: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5510/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5510/overview]
** *private/Halleck-ms/api-auto*: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5512/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5512/overview]
* Test Cases Summary: some variable haven't assigned to a value, just validate it throught conditional statement. More than that, i fixed some further error that can happend in future
* Trade log link: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-05/sanity-intg2.2023May04_01:21:19.394843.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-05/sanity-intg2.2023May04_01:21:19.394843.zip&atstype=ATS]
* Wiki link: [https://wiki.cisco.com/display/EDPEIXOT/Change+and+Verify+Network+device+configuration+in+ISE+NDG+-+Network+Device+Group|https://wiki.cisco.com/display/EDPEIXOT/Change+and+Verify+Network+device+configuration+in+ISE+NDG+-+Network+Device+Group] Same issue is still observed using the latest code base from private/Halleck-ms/api-auto.

*Branch used:* private/Halleck-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Failed log on Halleck* - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May30_08:24:23.273613.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May30_08:24:23.273613.zip&atstype=ATS] → Refer TC263 affected by PR: [+https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5869/overview+|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5869/overview], just pull code from main branch an then it’s work

Below PR also has been resolved conflict [~accountid:62d2fec15d6f5fd2c3db8f9f] , pls. update the result with lasted content from Hulk branch. Will be verifying on this during our ongoing Hulk testing on DR testbed. Tentative ETA - 20th July 2023 Observing a different script error for assigning tag to the device TC.


*Error snip:*

{{12296: TypeError: list indices must be integers or slices, not str}}



*Failed log:* [Test_TC263_configure_device_into_specific_NDG|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2380749&size=153392&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_mb_multi_sites_mdnac.2023Jul24_20:29:07.107045.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Uber ISO tested - *Hulk RC1 - 2.1.710.70446* [~accountid:62d2fec15d6f5fd2c3db8f9f] , do you see the issue with the content of the PR shared by [~accountid:63f50bcf4e86f362d39acde5] ?

If not, can you pls. validate the change using the PR branch? since no update from reporter, i change the Jira status to “close”. you can reopen it whenever you hit the error again Test scenario was tried during Recent Hulk Patch1 - 2.1.713.70147 testing. The older error is not observed, but a different issue is observed during device verification on ISE side. 
The failure error message just says “ It's not Match” . The expected and actual values are not printed. So not clear on what is the reason for error.

Branch used - *private/HulkPatch-ms/api-auto* 

*Failed log -* [Test_TC264_configure_device_into_specific_NDG|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9235718&size=273780&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac.2023Sep06_08:05:18.884114.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] Close this auton because new auton has been raised: [https://miggbo.atlassian.net/browse/SEEN-2371|https://miggbo.atlassian.net/browse/SEEN-2371|smart-link] ","['AWS_MSTB', 'Auton', 'Halleck', 'Hulk', 'Integration', 'MSTB1', 'MSTB2', 'MSTB3', 'Multisite']",QuangVinh Nguyen,Closed,SANDEEP SHIVARAMAREDDY
SEEN-1480,https://miggbo.atlassian.net/browse/SEEN-1480,disable secure shell on ESXi VM DNAC with Hulk release requires update with new behavior,"Disable secure shell on ESXi VM DNAC with Hulk release requires update with new behavior.
[TC8_DNAC_Server_Settings_verify_server_confis_in_dnac|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1026488&size=49277&archive=%2Froot%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr20_13:50:28.209366.zip&ats=%2Fws%2Favdas-sjc%2Fpyats-new&submitter=root&atstype=PYATS] 

{noformat}6262:  Maglev Restricted Shell is active
6263:  Last login: Thu Apr 20 21:06:45 2023 from 172.29.76.64
6264: 
6265:  [Thursday Apr 20 21:06:47 UTC] maglev@169.254.6.66 (maglev-master-169-254-6-66)
6266:  Disabling secure shell
6267:  Sending line: `_shell -c 'sudo magctl ssh shell bash'`

6268:  Encountered Secure shell prompt for password
6269:  Context:

6270:  Automatically sending line: `xxxxxx`
6271:  Parsed timeout: sending interrupt.
6272:  Session closed.
6273:  ('10.195.214.103', 'maglev', 'xxxxxxxx')
6274:  Encountered error on login. Check login details or try again.  Error details:
6275:  The pid member must be None.
6276:  Encountered error on login. Check login details or try again.  Error details:
6277:  The pid member must be None.
6278:  Encountered error during excecution of `send_cmd`
6279:  The pid member must be None.
6280:  Traceback (most recent call last):
6281:    File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/VM-Sanity-for-Platform/services/maglev_cli/maglevclihandler.py"", line 151, in connect
6282:      if not self._send(""_shell -c \'sudo magctl ssh shell bash\'"", timeout=1):
6283:    File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/VM-Sanity-for-Platform/services/maglev_cli/maglevclihandler.py"", line 502, in _send
6284:      raise PexpectTimeout('Timeout exceeded.')
6285:  pexpect.exceptions.TIMEOUT: Timeout exceeded.
6286: 
6287:  During handling of the above exception, another exception occurred:
6288: 
6289:  Traceback (most recent call last):
6290:    File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/VM-Sanity-for-Platform/services/maglev_cli/utils.py"", line 34, in wrapper
6291:      self.connect()
6292:    File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/VM-Sanity-for-Platform/services/maglev_cli/maglevclihandler.py"", line 160, in connect
6293:      return self.connect(retry=retry - 1)
6294:    File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/VM-Sanity-for-Platform/services/maglev_cli/maglevclihandler.py"", line 141, in connect
6295:      if not self.ssh.login(*args, **kwargs):
6296:    File ""/ws/avdas-sjc/pyats-new/lib/python3.6/site-packages/pexpect/pxssh.py"", line 401, in login
6297:      spawn._spawn(self, cmd)
6298:    File ""/ws/avdas-sjc/pyats-new/lib/python3.6/site-packages/pexpect/pty_spawn.py"", line 283, in _spawn
6299:      assert self.pid is None, 'The pid member must be None.'
6300:  AssertionError: The pid member must be None.
6301:  Last ssh response:
6302:  ^C^C
6303: 
6304:  [Thursday Apr 20 21:06:50 UTC] maglev@169.254.6.66 (maglev-master-169-254-6-66)
6305:  $ ^C
6306: 
6307:  [Thursday Apr 20 21:07:19 UTC] maglev@169.254.6.66 (maglev-master-169-254-6-66)
6308:  $ exit
{noformat}",2023-05-03T04:20:15.121+0000,PR for the required change: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5496/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5496/overview] has been approved and merged to {{private/Hulk-ms/api-auto}} branch.,"['Auton', 'ESXi', 'Hulk']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1482,https://miggbo.atlassian.net/browse/SEEN-1482,[Auton]Test_TC161_Brownfield_Workflow_9800 / test8_learn_device_config - learning device configs such as supported wlans are missed and are added into unsupported configs,"*Reporter Analysis:* 

During learning device configs for 9800 device- the supported wlans *bf_sup* are listed under unsupported wlans. 

{noformat}learned wlans: []
learned aaa: []
Config check starting:
 Unsupported wlan bf_unsup.* not in learned configs 
 Supported wlan bf_sup not found in learned configs!{noformat}

*Description:  The error from log or more info* 
unsupported wlans: ['GUESTdmz_tb2', 'GUEST', 'Single5KBanddmz_tb2', 'Single5KBan', 'SSIDDUAL BANDdmz_tb2', 'SSIDDUAL BAND', 'SSIDSchedulerdmz_tb2', 'SSIDScheduler', 'GUEST2dmz_tb2', 'GUEST', 'posturedmz_tb2', 'posture', 'OPENdmz_tb2', 'OPEN', 'CiscoSensorProvisioning', 'SSIDDot1XIndiadmz_tb2', 'SSIDDot1XIndia', *'bf_unsup.*dmz_tb2', 'bf_unsup.*', 'bf_sup'* ]

{noformat}Unsupported wlan bf_unsup.* found in unsupported configs.
Found supported wlan bf_sup in unsupported configs{noformat}

*Branch Name:*  private/Ghost-ms/sanity_api_auto

*Script file/Usecase:* testcases/forty_eight_hour/solution_test_sanityecamb.py

Test_TC161_Brownfield_Workflow_9800 / test8_learn_device_config

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:* 

*Fail Log:* [Test_TC161_Brownfield_Workflow_9800|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1295329&size=904749&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr27_05:35:01.463448.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-11|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-11]",2023-05-04T12:53:48.373+0000,"Spoke with DE and looks like the issue is SSIDs on devices getting “FLEX central switching: disabled” even though it should be enabled. Therefore when BF learning occurs its expecting ssid vlans to be in flex profile, which its not. Ashwini or I will send the RCA depending on who can recreate the issue. Hi [~accountid:63f50bcece6f37e5ed93c87e] ,

Issue is reproduced on AWS Sanity Cluster,
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2430705&size=16313&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May15_03:53:36.645262.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2430705&size=16313&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May15_03:53:36.645262.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


RCA after issue is seen:
[http://172.21.236.183/sanity_rca/TB8/maglev-169.254.6.66-rca-2023-05-15_15-47-39_UTC.tar.gz|http://172.21.236.183/sanity_rca/TB8/maglev-169.254.6.66-rca-2023-05-15_15-47-39_UTC.tar.gz]
Testbed wiki:
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-11|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-11]

Wiki where logging to DMZ-cluster is explained:
[https://wiki.cisco.com/display/EDPEIXOT/Guidelines+-+DMZ+WoW|https://wiki.cisco.com/display/EDPEIXOT/Guidelines+-+DMZ+WoW]


Thanks,
Anusha John Forwarded RCA to DE [puslatha@cisco.com|mailto:puslatha@cisco.com], lets see her response [~accountid:61efa8c457b25b006877eda3] / [~accountid:5e1415780242870e996f0b2f] , DE is available in IST. Try to reach out directly during IST time and have it concluded if the observation reported is a bug or not. Created webex space to discuss DE Pushpalatha concluded on this issue, we have SSID *‘bf_sup’* that is pushed as a fabric SSID to the controller. They dont support the bf learn for fabric. 
we are yet to conclude on the exact behavior required for this feature. 

Webex space: webexteams://im?space=b8b788f0-f3a6-11ed-8b2a-1f4c74c62985  Previously it was working with fabric (although it wasnt technically supported), so need to update script to make sure ssid is not fabric. Hi [~accountid:63f50bcece6f37e5ed93c87e]  Could you please help to fix this. Please mention ETA for this fix Hi [~accountid:5e1415780242870e996f0b2f] my testbed is kept in earlier state due to my current feature being lan automation. ETA would be monday for this fix if theres no other testbed available. Please let me know if you have one, I will also ask for one. Using tb17, Moe’s testbed. [~accountid:63f50bcece6f37e5ed93c87e] do we have any progress update on this Auton? Hi [~accountid:63f50bcece6f37e5ed93c87e] 

Is there any fix committed for this issue ? The Jira is in open state
Can you please share the PR for the fix 
 Hi [~accountid:63f50bcece6f37e5ed93c87e]  
Could you please share latest progress on this jira Hi [~accountid:63f50bcece6f37e5ed93c87e] 
Could you please share latest update on this jira Hi [~accountid:63f50bcece6f37e5ed93c87e] 
Could you please share latest update on this jira Hi [~accountid:63f50bcece6f37e5ed93c87e]  
Could you please let me know - If you are in need of the testbed to resolve this issue
If so please make use of Sanity AWS and Sanity TB7 which have ewlc devices, As it is idle during PST 
Once fix has been added -we can give a run Hi [~accountid:63f50bcece6f37e5ed93c87e]  
Please let us know latest update on this fix! [~accountid:63f50bcece6f37e5ed93c87e] , pls. make use of TB11 if you need one to conclude on this Auton. Hi [~accountid:63f50bcece6f37e5ed93c87e] 
Could you let us know If any progress on this Jira ticket?","['Auton', 'Ghost', 'GhostPacth3', 'Guardian', 'Hulk', 'HulkP2', 'HulkP3', 'Integration', 'sanity']",Andrew Chen,Backlog,Ashwini R Jadhav
SEEN-1483,https://miggbo.atlassian.net/browse/SEEN-1483,[Auton]:Ghost C9136I-B connected to eWLC(9800) Radio Channel for 6GHZ (Maui AP) which is DOWN,"*Reporter Analysis:*
We observed during ghost solution sanity testing in the AI_RF_Profile configuration that TC is  administratively not enabling 6ghz Radio. It should be enabled in the Sanity testbed, we have some ap that support 6 GHz radio.Due this issue 1 tc failed 

* *DNAC Release_Version Tested:*  *Ghost P1 RC5 (2.1.613.70190)*
* *Device Image Used: 17.11.1* 
* *AP Model-C9136I-B(17.11.0.155)*
* *Testbed: Sanity Testbed* 
* *Branch Used: private/Ghost-ms/sanity-api-auto*
* *Script Name:Optimized code* 
*\testcases\sanityusecases\*
* *Testbed :*TB7
* *Failed Log:*
* *Solution Input File : dnac-auto/configs/config_48hr_test/solution_test_input.json*
* *Testcases Impacted:* {{Test_TC86_validate_AP_KPI_graphs}}
* *Failed Trade Log:* [*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-175-assuranceHealthMetrics&begin=1949013&size=1764058&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr28_06:52:09.069773.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-175-assuranceHealthMetrics&begin=1949013&size=1764058&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr28_06:52:09.069773.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

+*Snip From Failed Log:*+


{noformat}'totalResults': None}}
5241:  Calculating If the required result found  in the Response!!
5242:  Result Generated is ['0', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '1', '0', '1', '0', '1', '0', '0', '0', '0', '0', '1', '0', '1', '0', '1', '0', '1', '0', '1', '1', '1', '0', '1', '0', '1', '0', '1', '0', '1', '0', '1']
5243:  Success Verified Radio Utilization on AP AP687D.B45C.2054 on Slot 3
5244:  Following APs doesn't have Memory Health Score ['AP687D.B45C.2054']
5245:  Library group ""assurance"" method ""verify_AP_radio_utilization"" returned in 0:00:00.804186
5246:  Library group ""assurance"" method ""verify_AP_radio_utilization"" returned in 0:05:03.502003
5247:  Test returned in 0:05:03.562847
5248:  Failed reason: Validation AP RADIO failed{noformat}",2023-05-04T18:00:52.069+0000,"Same issue is seen, Halleck 
*Failed Trade Log:*
[Task-assurance_health_metric.py-175-assuranceHealthMetrics|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-175-assuranceHealthMetrics&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May05_22:22:14.013507.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] +*Ghost P2   Failed Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-175-assuranceHealthMetrics&begin=1942818&size=1922653&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May20_09:42:25.776261.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-175-assuranceHealthMetrics&begin=1942818&size=1922653&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May20_09:42:25.776261.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Ghost  P2 2.3.5.4-70759 
+*Failed Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-175-assuranceHealthMetrics&begin=1950328&size=2238967&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun22_09:23:42.801285.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-175-assuranceHealthMetrics&begin=1950328&size=2238967&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun22_09:23:42.801285.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] PR: 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6072/diff#configs/config_48hr_test/solution_test_input.json|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6072/diff#configs/config_48hr_test/solution_test_input.json]","['Auton', 'Ghost', 'Halleck', 'Hulk', 'Optimized', 'Sanity']",Moe Saeed,Resolved,Omkar Sharad Wagh
SEEN-1484,https://miggbo.atlassian.net/browse/SEEN-1484, Test_TC124_verify_SDA_fabric_issue,"*Sub Testcase Failed :* test2_verify_fabric_dhcp_issue.

*Error:*
TB4-DM1-B95K - Version from device: 17.6.20230417:185341. Version to compare: 17.6.2. Check: >=
65248: Traceback (most recent call last):
65249: File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/commonlibs/test_wrapper.py"", line 301, in wrapper
65250: result = testfunc(func_self, **kwargs)
65251: File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 9089, in test2_verify_fabric_dhcp_issue
65252: if not dnac_handle.verify_fabric_issues_health_scores():
65253: File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/assurance_fabric/group.py"", line 83, in verify_fabric_issues_health_scores
65254: check = self.services.check_ver(dev, version, compare='>=')
65255: File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/assurance_fabric/group.py"", line 56, in check_ver
65256: return compare_ios_version(sversion,version)
65257: File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/assurance_fabric/group.py"", line 729, in compare_ios_version
65258: if int(version1[i]) > int(version2[i]):
65259: ValueError: invalid literal for int() with base 10: '20230417:185341'
65260: Test returned in 0:00:00.118680
65261: Errored reason: invalid literal for int() with base 10: '20230417:185341'
65262:
65263: Exception:
65264: Traceback (most recent call last):
65265: File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/commonlibs/test_wrapper.py"", line 301, in wrapper
65266: result = testfunc(func_self, **kwargs)
65267: File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 9089, in test2_verify_fabric_dhcp_issue
65268: if not dnac_handle.verify_fabric_issues_health_scores():
65269: File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/assurance_fabric/group.py"", line 83, in verify_fabric_issues_health_scores
65270: check = self.services.check_ver(dev, version, compare='>=')
65271: File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/assurance_fabric/group.py"", line 56, in check_ver
65272: return compare_ios_version(sversion,version)
65273: File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/assurance_fabric/group.py"", line 729, in compare_ios_version
65274: if int(version1[i]) > int(version2[i]):
65275: ValueError: invalid literal for int() with base 10: '20230417:185341'
65276: The result of section test2_verify_fabric_dhcp_issue is => ERRORED
 
*Trade Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10684083&size=28380&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May05_16:06:06.054713.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10684083&size=28380&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May05_16:06:06.054713.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]
 
*Hulk Branch Used:* 
Branchbgl/Hulk-ms/api-auto
 
*Test Suite:*
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=refs%2Fheads%2Fbgl%2FHulk-ms%2Fapi-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=refs%2Fheads%2Fbgl%2FHulk-ms%2Fapi-auto]",2023-05-06T02:23:35.286+0000,PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5562/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5562/overview],"['Auton', 'Hulk']",Moe Saeed,Resolved,SAINATH CHATHARASI
SEEN-1485,https://miggbo.atlassian.net/browse/SEEN-1485,Test_TC0_dnac_initial_cleanup,"TC0 is errored due to below error, Attaching the full log here and run is still in progress but it is stuck at the TC1 since 12 Hrs.

*Error Log:*

[^Log.txt]

*Hulk Branch:*

bgl/Hulk-ms/api-auto


*Test Suite:*

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=refs%2Fheads%2Fbgl%2FHulk-ms%2Fapi-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=refs%2Fheads%2Fbgl%2FHulk-ms%2Fapi-auto]",2023-05-08T04:11:50.858+0000,"[~accountid:62ab7a399cd13c0068b18fe0] 

Your commit broke this area where it needs to use [https://10.106.133.152/api/system/v1/maglev/packages|https://10.106.133.152/api/system/v1/maglev/packages] but because of the changes it used wrong url [https://10.106.133.152/api/v1/maglev/packages|https://10.106.133.152/api/v1/maglev/packages]

It should use self.services.base.system.call_api which uses gv_client with base_url=""/api/system""

You changed it to use self.services.api_switch_call which currently uses apicem_client with base_url=""/api"" only.

Can you update it to use the correct one with base_url=""/api/system""?

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/4a81b480af2f359642cf690928e09fedc6d1bdd0#services/dnaserv/lib/api_groups/upgrade/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/4a81b480af2f359642cf690928e09fedc6d1bdd0#services/dnaserv/lib/api_groups/upgrade/group.py]

  Raised PR for Halleck-ms/api-auto branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5561/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5561/overview] PR got approved and merged to {{Halleck-ms/api-auto}} branch, cherry-picked to {{Hulk-ms/api-auto}} branch.

Committed separately to {{hulk_esxivm}} branch as well.



Marking this ticket as “Done”.","['Auton', 'Hulk']",Amardeep Kumar,Closed,SAINATH CHATHARASI
SEEN-1486,https://miggbo.atlassian.net/browse/SEEN-1486,[Auton] [Halleck] - Getting packages response failing with 404 Client error,"*Regression:* Solution Regression Multisite

*Branch:* private/Halleck-ms/api-auto 

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Issue Faced:* Observing failure at response for getting details of package details. This is affecting multiple TCs where this check has been implemented which is causing TC/subTC erroring out. Hence the subsequent  actions such as validations and configurations post this step are not getting performed.

The same is working fine using *private/Hulk-ms/api-auto* branch codebase.

*Error snip:*
34147: 2023-05-07T02:52:02: %SERVICES-6-INFO:  -------------------------------
34148: 2023-05-07T02:52:02: %SERVICES-6-INFO:  main is Active
34149: 2023-05-07T02:52:02: %SERVICES-6-INFO:  -------------------------------
34150: 2023-05-07T02:52:02: %ATS-6-INFO:  Library method ""reconnect_clients"" returned in 0:00:00.645769
34151: 2023-05-07T02:52:02: %SERVICES-6-INFO:  Initializing function group ""upgrade""
34152: 2023-05-07T02:52:02: %SERVICES-6-INFO:  Group ""upgrade"" initialized successfully
34153: 2023-05-07T02:52:02: %API-GROUP-UTILS-6-INFO:
34154: 2023-05-07T02:52:02: %API-GROUP-UTILS-6-INFO:
34155: 2023-05-07T02:52:02: %API-GROUP-UTILS-6-INFO:   api_switch_call called:
34156: 2023-05-07T02:52:02: %API-GROUP-UTILS-6-INFO:  {}
34157: 2023-05-07T02:52:02: %CLIENTMANAGER-6-INFO:  Resource path full url: [https://10.195.243.109/api/v1/maglev/packages|https://10.195.243.109/api/v1/maglev/packages]
34158: 2023-05-07T02:52:02: %CLIENTMANAGER-3-ERROR:  Error Code: 404 for
34159: 2023-05-07T02:52:02: %CLIENTMANAGER-3-ERROR:  URL:[https://10.195.243.109/api/v1/maglev/packages|https://10.195.243.109/api/v1/maglev/packages] Data:{'timeout': 60} Headers:{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2NDU1NDY5ODM5ODJmYjE0OTBkYmU4ZGUiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjY0NTU0Njk3Mzk4MmZiMTQ5MGRiZThkZCJdLCJ0ZW5hbnRJZCI6IjY0NTU0Njk2Mzk4MmZiMTQ5MGRiZThkYiIsImV4cCI6MTY4MzQ1NjcyMSwiaWF0IjoxNjgzNDUzMTIxLCJqdGkiOiJhYzVkMjc5Ni1lYmYwLTQzODEtOWRjMS03NTE3ZTRhZmNmZmEiLCJ1c2VybmFtZSI6ImFkbWluIn0.cq_n_L6H7BaDq3exgI4uA7GX2Bvc21TWyWl_NokHROFFscQ_8Vj2h_vtUTbqiZidPczv86_bc887CHL8uPUzVBKAXScZ1AgUFJaJ-37mkCB-GkgbII_IkyC6p5B-dZbhtOXRw_V0JSPuYCwnqmW9rEc0VhVxYEtCk7h7b93j2DLOeHMnvaOt4o1TJdRlHkSWTCAqvOt_o3glkLFC34fZTa6BQqQcXB5XJ1FGzlgvDOAWvT0EOnL0ROwu5VI2uoM0uZr1faIlFJ6ERQG9gw_mDeIF_viD_06xUeCKh3hJZSiL97952_Mfie46vuDo-1Lm-tffqzrchJckcD2ukgsZEA;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:Not Found
34160: 2023-05-07T02:52:02: %CLIENTMANAGER-3-ERROR:  Traceback (most recent call last):
34161: 2023-05-07T02:52:02: %CLIENTMANAGER-3-ERROR:    File ""/auto/dna-sol/ws/sr-mb1_Halleck/services/dnaserv/client_manager.py"", line 326, in call_api
34162: 2023-05-07T02:52:02: %CLIENTMANAGER-3-ERROR:      response.raise_for_status()
34163: 2023-05-07T02:52:02: %CLIENTMANAGER-3-ERROR:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status
34164: 2023-05-07T02:52:02: %CLIENTMANAGER-3-ERROR:      raise HTTPError(http_error_msg, response=self)
34165: 2023-05-07T02:52:02: %CLIENTMANAGER-3-ERROR:  requests.exceptions.HTTPError: 404 Client Error: Not Found for url: [https://10.195.243.109/api/v1/maglev/packages|https://10.195.243.109/api/v1/maglev/packages]
34166: 2023-05-07T02:52:02: %API-GROUP-UTILS-3-ERROR:  Encountered unhandled HTTPError in Internal API Call
34167: 2023-05-07T02:52:02: %API-GROUP-UTILS-3-ERROR:  Flagging result as FAIL!
34168: 2023-05-07T02:52:02: %API-GROUP-UTILS-3-ERROR:  	Reason: 404 Client Error: Not Found for url: [https://10.195.243.109/api/v1/maglev/packages|https://10.195.243.109/api/v1/maglev/packages]
34169: 2023-05-07T02:52:02: %API-GROUP-UTILS-3-ERROR:  Kwargs:
34170: 2023-05-07T02:52:02: %API-GROUP-UTILS-3-ERROR:  {}
34171: 2023-05-07T02:52:02: %API-GROUP-UTILS-6-INFO:  Error Caught While Querying the Internal API
34172: 2023-05-07T02:52:02: %ATS-3-ERROR:  Encountered unhandled HTTPError in group ""upgrade"" method ""is_packages_exist""!
34173: 2023-05-07T02:52:02: %ATS-3-ERROR:  Flagging result as FAIL!
34174: 2023-05-07T02:52:02: %ATS-3-ERROR:  	Reason: 404 Client Error: Not Found for url: [https://10.195.243.109/api/v1/maglev/packages|https://10.195.243.109/api/v1/maglev/packages]
34175: 2023-05-07T02:52:02: %ATS-3-ERROR:  	Args:   (<services.dnaserv.lib.api_groups.upgrade.group.Group object at 0x7f600c837fa0>, ['disaster-recovery'])
34176: 2023-05-07T02:52:02: %ATS-3-ERROR:  Kwargs:
 

*Failed logs:*
*Log 1 -* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May07_01:53:20.381807.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May07_01:53:20.381807.zip&atstype=ATS] → Refer TC4 and TC5

*Log 2 -* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May07_10:38:02.260175.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May07_10:38:02.260175.zip&atstype=ATS] → Refer TC31



*Pass log using Hulk code base on DNAC with Halleck Uber ISO:* 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May07_06:30:33.549055.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May07_06:30:33.549055.zip&atstype=ATS] → Refer TC4 and TC5",2023-05-08T04:25:08.843+0000,"Required PR for Halleck-ms/api-auto branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5561/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5561/overview] has been approved and merged.

PR got approved and merged to {{Halleck-ms/api-auto}} branch, cherry-picked to {{Hulk-ms/api-auto}} branch.

Committed separately to {{hulk_esxivm}} branch as well.

Marking this ticket as “Done”.","['Auton', 'Halleck', 'MSTB1', 'Multisite']",Amardeep Kumar,Closed,SANDEEP SHIVARAMAREDDY
SEEN-1487,https://miggbo.atlassian.net/browse/SEEN-1487,"[Auton] [Halleck] - Device re-provision TC fails with error ""KeyError: 'profileAttributes'""","*Regression:* Solution Regression Multisite

*Branch:* private/Halleck-ms/api-auto 

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Issue Faced:* Observing script failure under re-provisioning of devices subTC due to KeyError: 'profileAttributes' error.

Looks like recent commit - [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/be615dda4b7dbd57439aab1e6a588a6dee93e318#services/dnaserv/lib/api_groups/template_programmer/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/be615dda4b7dbd57439aab1e6a588a6dee93e318#services/dnaserv/lib/api_groups/template_programmer/group.py] has caused the breakage. 



*Error snip:*

42735: 2023-05-07T09:55:56: %API-GROUP-UTILS-6-INFO:
42736: 2023-05-07T09:55:56: %API-GROUP-UTILS-6-INFO:
42737: 2023-05-07T09:55:56: %API-GROUP-UTILS-6-INFO:   api_switch_call called:
42738: 2023-05-07T09:55:56: %API-GROUP-UTILS-6-INFO:  {}
42739: 2023-05-07T09:55:56: %CLIENTMANAGER-6-INFO:  Resource path full url: [https://10.195.243.109/api/v1/siteprofile/site/956aae47-ccfa-4d5f-b933-c856182c9302|https://10.195.243.109/api/v1/siteprofile/site/956aae47-ccfa-4d5f-b933-c856182c9302]
42740: 2023-05-07T09:55:56: %ATS-6-INFO:  Library group ""network_profile"" method ""get_site_profile_by_site"" returned in 0:00:00.082130
42741: 2023-05-07T09:55:56: %SERVICES-3-ERROR:  Traceback (most recent call last):
42742: 2023-05-07T09:55:56: %SERVICES-3-ERROR:    File ""/auto/dna-sol/ws/sr-mb1_Halleck/services/commonlibs/test_wrapper.py"", line 301, in wrapper
42743: 2023-05-07T09:55:56: %SERVICES-3-ERROR:      result = testfunc(func_self, **kwargs)
42744: 2023-05-07T09:55:56: %SERVICES-3-ERROR:    File ""/auto/dna-sol/ws/sr-mb1_Halleck/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py"", line 3031, in test1_verify_provision_the_devices_fabric1
42745: 2023-05-07T09:55:56: %SERVICES-3-ERROR:      if(dnac_handle.provision_devices(scheduler=True)):
42746: 2023-05-07T09:55:56: %SERVICES-3-ERROR:    File ""/auto/dna-sol/ws/sr-mb1_Halleck/services/dnaserv/lib/decorators.py"", line 32, in wrapper
42747: 2023-05-07T09:55:56: %SERVICES-3-ERROR:      result = method(*args, **kwargs)
42748: 2023-05-07T09:55:56: %SERVICES-3-ERROR:    File ""/auto/dna-sol/ws/sr-mb1_Halleck/services/dnaserv/lib/api_groups/provision/group.py"", line 462, in provision_devices
42749: 2023-05-07T09:55:56: %SERVICES-3-ERROR:      payload = self.services.get_template_payload_from_nw_profile(id_to_device[device_id])
42750: 2023-05-07T09:55:56: %SERVICES-3-ERROR:    File ""/auto/dna-sol/ws/sr-mb1_Halleck/services/dnaserv/lib/decorators.py"", line 32, in wrapper
42751: 2023-05-07T09:55:56: %SERVICES-3-ERROR:      result = method(*args, **kwargs)
42752: 2023-05-07T09:55:56: %SERVICES-3-ERROR:    File ""/auto/dna-sol/ws/sr-mb1_Halleck/services/dnaserv/lib/api_groups/template_programmer/group.py"", line 390, in get_template_payload_from_nw_profile
42753: 2023-05-07T09:55:56: %SERVICES-3-ERROR:      for att in profile_data[0][""profileAttributes""]:
42754: 2023-05-07T09:55:56: %SERVICES-3-ERROR:  KeyError: 'profileAttributes'
42755: 2023-05-07T09:55:56: %SERVICES-6-INFO:  Test returned in 0:19:05.287902
42756: 2023-05-07T09:55:56: %AETEST-3-ERROR:  Errored reason: profileAttributes
42757: 2023-05-07T09:55:56: %AETEST-3-ERROR:
42758: 2023-05-07T09:55:56: %AETEST-3-ERROR:  Exception:
42759: 2023-05-07T09:55:56: %AETEST-3-ERROR:  Traceback (most recent call last):

*Failed logs:*
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May07_08:45:41.277274.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May07_08:45:41.277274.zip&atstype=ATS] → Refer TC28.1",2023-05-08T05:34:24.545+0000,"[ENG-SDN / dnac-auto / cc5038ffcbe - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/cc5038ffcbebf5766685f17c670cd3bf9e1edb40]


Fix merged here, try pulling latest. Fixed earlier today.","['Auton', 'Halleck', 'MSTB1', 'Multisite']",Andrew Chen,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-1493,https://miggbo.atlassian.net/browse/SEEN-1493,AP Maintenance mode related use-cases should be included as part of Sanity Scripts,"AP Maintenance mode related use-cases should be included as part of Sanity Scripts as it’s overall execution time is approx. 2 hours.

For reference, same got added as part of below PR:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5479/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5479/overview]

{{testcases/sanityusecases/assuranceMaintanenceMode/assurance_maintaince_mode_on_off.py}}",2023-05-08T22:16:43.176+0000,,"['Auton', 'automation']",Majlona 'Luna' Aliaj,Resolved,Amardeep Kumar
SEEN-1505,https://miggbo.atlassian.net/browse/SEEN-1505,TC97_wireless_policy_PSK,"*Sub TC Failed:*

TC97_wireless_policy_PSK-          test7_delete_existing_wireless_policy

*Error snip shot:*

!image-20230509-050000.png|width=1175,height=574!

Analysis:

From DNAC UI all the three wireless policies has been deleted successfully .

!image-20230509-050113.png|width=1280,height=163!

*Trade Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=114316667&size=3770611&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May08_12:05:26.435543.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=114316667&size=3770611&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May08_12:05:26.435543.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

*Branch Used:*

bgl/Hulk-ms/api-auto

*Test Suite:*

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto]",2023-05-09T05:05:57.346+0000,"Based on the log, the task didnot complete within 240 seconds. Please raise defect for it.","['Auton', 'Hulk']",SAINATH CHATHARASI,Closed,SAINATH CHATHARASI
SEEN-1506,https://miggbo.atlassian.net/browse/SEEN-1506,Test_TC107_Compliance_verification,"*Sub TC Failed:*

test4_verify_VLAN_Virtual_ports

*Error Snap shot:*

!image-20230509-051108.png|width=947,height=282!

*Trade Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=155815593&size=834728&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May08_12:05:26.435543.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=155815593&size=834728&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May08_12:05:26.435543.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

*Branch Used:*

bgl/Hulk-ms/api-auto

*Test Suite:*

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto]",2023-05-09T05:12:49.016+0000,"# PR Hulk-ms/api_auto link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5638/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5638/overview]
# Test Case:  {{TC_Compliance_verification}}
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link  Hulk-ms/api_auto: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-05/sanity_TB1.2023May14_20:23:27.464633.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-05/sanity_TB1.2023May14_20:23:27.464633.zip&atstype=ATS] Hi [~accountid:642a816bd774ab7297295df0], Could you help me review this PR Auton?","['Auton', 'Hulk']",NhanHuu Nguyen,Resolved,SAINATH CHATHARASI
SEEN-1507,https://miggbo.atlassian.net/browse/SEEN-1507,Test_TC73_DNAC_verify_aaa_lisp_radius_configuration_on_devices_through_scheduled_job,"*Sub TC Failed:*

Test_TC73_DNAC_verify_aaa_lisp_radius_configuration_on_devices_through_scheduled_job-test1_verify_provision_the_devices_schedular_after_reload

*Error snip shot:*

!image-20230509-051839.png|width=970,height=357!

*Trade Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=82784012&size=5190340&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May08_12:05:26.435543.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=82784012&size=5190340&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May08_12:05:26.435543.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

*Branch Used:*

bgl/Hulk-ms/api-auto

*Test Suite:*

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto]",2023-05-09T05:19:55.042+0000,,"['Auton', 'Hulk']",SAINATH CHATHARASI,Closed,SAINATH CHATHARASI
SEEN-1508,https://miggbo.atlassian.net/browse/SEEN-1508,Auton:[Ghost]Test_TC156_Validate_sda_multicast_external_apis/test5_add_multicast_with_asm_external_rp_method_ipv4,"*Reporter Analysis:* 

The Script is using array instead of string “externalRpIpAddress”

*Description:* 
API is working as designed.

In payload, you are not using need following element correctly:

externalRpIpAddress ==> is string

And you are using it as array.

*Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* 

solution_test_sanityecamb.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=51607877&size=45693&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May02_09:08:55.164981.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=51607877&size=45693&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May02_09:08:55.164981.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-05-09T05:53:57.906+0000,"Fabric support multiple external rp address now. Ask him to either also accept list or add another parameter in API to take multiple RP addresses. reopen the bug with this info.  Hi Pawan,
We Reopened the bug with the comments you mentioned above
They again moved the bug to Junk with comments:
” please open a new sev6/enhancement bug.'' convert the same bug to enhancement.  Hi [~accountid:5f3c6ae932360700388f7b4b] ,

As per DE(Pooja ) analysis we are  not passing string value for  ""externalRpIpAddress""
Request:

{
""siteNameHierarchy"": ""Global/san jose"",
""multicastMethod"": ""native_multicast"",
""multicastType"": ""asm_with_external_rp"",
""multicastVnInfo"": [
{
""virtualNetworkName"": ""VN1"",
""ipPoolName"": ""10lan"",
""internalRpIpAddress"": [],
""externalRpIpAddress"": ""204.192.3.40"",
""ssmInfo"": []
},
{
""virtualNetworkName"": ""VN2"",
""ipPoolName"": ""11lan"",
""internalRpIpAddress"": [],
""externalRpIpAddress"": ""204.192.3.40"",
""ssmInfo"": []
}
]
}

h1. Response:

{
""status"": ""pending"",
""description"": ""API execution is in progress."",
""taskId"": ""9d08e593-2866-4113-b764-5651bf5d235d"",
""taskStatusUrl"": ""/dna/intent/api/v1/task/9d08e593-2866-4113-b764-5651bf5d235d"",
""executionStatusUrl"": ""/dna/intent/api/v1/dnacaap/management/execution-status/27f070ec-47aa-469e-b83f-785948c483ee"",
""executionId"": ""27f070ec-47aa-469e-b83f-785948c483ee""
}

Again bug was it is going back to script side , can you please help  Hi Team,

We are observing this issue in our Latest runs on 

Build: Ghost P2 RC1

Version;2.3.5.4-70815

Failed Log:[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3699967&size=45152&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug01_23:29:09.442567.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3699967&size=45152&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug01_23:29:09.442567.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Hi [~accountid:63f50bd640328c12e4ec5b00] ,
can you please look into the issue. I’ve submitted PR to fix this issue: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6988/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6988/overview]
+ [~accountid:5f3c6ae932360700388f7b4b] please review it","['Auton', 'Ghost', 'Sanity']",ThanhTan Nguyen,Resolved,Tulasi Reddy
SEEN-1512,https://miggbo.atlassian.net/browse/SEEN-1512,Floors not getting added after executing TC16_DNAC_Site_Building_Floor_Addition,"* *DNAC Release_Version Tested:* Ghost P1 RC5-2.1.613.70190
* *Device Image Used:* 17.10.1FC2
* *Testbed:* AWS-Multisite
* *Branch Used:* private/Ghost-ms/sanity_api_auto
* *Script Name:* solution_test_3sites_sjc_nyc_sf.py
* *Solution Input File :* dnac-auto/configs/config_48hr_test/solution_test_input.json
* *Testcases Impacted:* {{TC16_DNAC_Site_Building_Floor_Addition}}
* *Failed Trade Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=976478&size=110448&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May05_07:24:38.279607.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=976478&size=110448&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May05_07:24:38.279607.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-05-09T09:53:34.303+0000,"* PR - Link:
** *private/Ghost-ms/api-auto*: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5619/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5619/overview]
** *private/Ghost-ms/sanity_api-auto*: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5618/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5618/overview]
* Test Cases Summary: Missing HTTPError lib so it's cause error when trigger testcase
* Trade log link: [+https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-05/sr_mb2_three_sites.2023May11_23:29:19.766518.zip&atstype=ATS+|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-05/sr_mb2_three_sites.2023May11_23:29:19.766518.zip&atstype=ATS] While your update was able to help resolve some of the errors, it appears that the root cause remains unresolved.
The {{Group does not exist}} error that we're seeing is related to the {{parentGroupUuid}} value, which for some reason, is taking a {{name}} instead of a {{valid UUID}}. Please refer to your log pass, where the value appears to be valid.
I would recommend double-checking this root cause analysis to ensure that we're addressing the correct issue.
*from log pass* 

{code:json}'parentGroupUuid': '005d2c45-b9a4-4e37-a39a-2f1645fcd367' {code}","['Auton', 'Ghost', 'MSTB3', 'Multisite']",QuangVinh Nguyen,Resolved,Neelima Doddipalli
SEEN-1519,https://miggbo.atlassian.net/browse/SEEN-1519,Test_TC3_generate_dhcp_server_config_on_fusion,"*Sub TC Failed:*

Test_TC3_generate_dhcp_server_config_on_fusion - test4_update_tsim_config_for_wireless_ap_clients

*Snapshot from Logs:*

!image-20230509-150359.png|width=1062,height=147!

!image-20230509-150415.png|width=917,height=307!

*Analysis:*

When we dont have input for tsim port2 and has inputs only for port1,3,4, script (TC3) is doing tsim sim config for AP_HOST_DIST_LIST incorrectly - it does port3 mapped device configs for port2 , and port4 mapped device configs for port3.So,when port2 info not given , it should do port3 and port4 config as per expected.

*Trade Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2207867&size=18640&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May08_12:05:26.435543.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2207867&size=18640&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May08_12:05:26.435543.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

*Branch Used:*

bgl/Hulk-ms/api-auto

*Test Suite:*

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto]",2023-05-09T15:13:39.573+0000,Connect the TSIM port 2 to the expected edge. You are not supposed to make random connections. The sanity TSIM connection is to be matched. There is no use of leaving 1 connection. Fix this in testbed. ,['Auton'],Pawan Singh,Resolved,SAINATH CHATHARASI
SEEN-1520,https://miggbo.atlassian.net/browse/SEEN-1520,Test_TC138_system_health_assurance_checks,"*Sub Test Case Failed:*

{{test1_verify_devices_categorized_health_no_health}}

*Error Snap Shot:*

!image-20230511-123239.png|width=930,height=285!

*Trade Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=198724939&size=18343&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May08_12:05:26.435543.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=198724939&size=18343&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May08_12:05:26.435543.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

*Branch Used:*

bgl/Hulk-ms/api-auto

*Test Suite:*

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto]",2023-05-11T12:34:10.290+0000,"* PR - Link:
** *bgl/Hulk-ms/api-auto*: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5622/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5622/overview]
* Test Cases Summary: add try except to prevent tc failed
* Trade log link: [+https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-05/sanity_TB1.2023May12_01:01:19.915124.zip&atstype=ATS+|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-05/sanity_TB1.2023May12_01:01:19.915124.zip&atstype=ATS]","['Auton', 'Hulk']",QuangVinh Nguyen,Resolved,SAINATH CHATHARASI
SEEN-1529,https://miggbo.atlassian.net/browse/SEEN-1529,[Auton][Halleck] - Enable ICAP url fails with 400 Client Error and also TC false passes,"*Regression:* Solution Regression Multisite

*Branch:* private/Halleck-ms/api-auto 

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

*Script Used:* dnac-auto/testcases/mega_topo/solution_assurance_test.py

*Issue Faced:* 
a) Observing failures during Enable of ICAP for APs. Further even though we have ICAP enable failures, we see the corresponding TC gets marked as Pass. 

b) Also subsequent ICAP related TC verifications are getting failed due to Key errors. This might be as a consequence of issue as in a) above.

The same has worked fine using *private/Ghost-ms/api-auto* branch codebase.

*Error snip:*

4032:  Error Code: 400 for
4033:  URL:[https://10.195.243.123/api/assurance/v1/airsense/createIcapSession|https://10.195.243.123/api/assurance/v1/airsense/createIcapSession] Data:{'timeout': 60, 'data': '{""type"": ""ICAP_SPECTRUM"", ""apInfos"": [{""macAddress"": ""10:B3:D5:5D:5A:40"", ""apName"": ""SJ-AP1-3802E-RMA""}], ""spectrumSlot"": 0}'} Headers:{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2NDU1NDY5ODM5ODJmYjE0OTBkYmU4ZGUiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjY0NTU0Njk3Mzk4MmZiMTQ5MGRiZThkZCJdLCJ0ZW5hbnRJZCI6IjY0NTU0Njk2Mzk4MmZiMTQ5MGRiZThkYiIsImV4cCI6MTY4Mzg3MjMwOSwiaWF0IjoxNjgzODY4NzA5LCJqdGkiOiIxN2IxMjE4MC1lMzU1LTRlOTgtYjkzNC1iMmVjYzU3NjUzYzUiLCJ1c2VybmFtZSI6ImFkbWluIn0.SUFM_nzQT85JS7266fFrjU1s_YFGwocKUbPCW8CclgN5QQusNVKnOM7HbFjWuUu3L06oaAXiQFE7uyXByV5u0seKZ0X6vpV8b2zawnSf6Wi4ZQccIfwknmhRShq4qWrm-ZI6DPx5e6kGCaRwT2phhACqBOXpe9SaAe6CIpDiBZuVSFNN0GGUd-IjwyFR3oHbdq4sDVKaH6PUbJDPcZXR34bp6oqLSLbT0uJb07a_jTXjTG21Iiw_PyLFpaFhNoBXagWyS1A8WMtjhPavze0aIZO6RpiOwFqzcA0B9kvlgYHOYzgvvxcsCmUVyp1K5MJ_jtaz32Mj8anhioqsFkVWtg;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:{""icapFeatureConfigList"":[{""type"":""ICAP_SPECTRUM"",""command"":null,""state"":""WRONG_PARAMS"",""errorString"":""Session is missing spectrum slot number."",""apInfos"":[{""macAddress"":""10:B3:D5:5D:5A:40"",""apName"":""SJ-AP1-3802E-RMA"",""otaChannel"":0,""otaBand"":"""",""channelWidth"":20,""otaSlot"":"""",""xorRadio"":0,""otaApMode"":0,""etherMac"":"""",""uuid"":""""}],""startTimeSecs"":0,""configDuration"":0,""actualDuration"":0,""remainingDuration"":0,""sessionId"":0,""owner"":"""",""estimatedTime"":0,""originatedAp"":null,""otaDestination"":""""}]}
4034:  Traceback (most recent call last):
4035:    File ""/auto/dna-sol/ws/sr-mb1_Halleck_SEEN-1485/services/dnaserv/client_manager.py"", line 326, in call_api
4036:      response.raise_for_status()
4037:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status
4038:      raise HTTPError(http_error_msg, response=self)
4039:  requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: [https://10.195.243.123/api/assurance/v1/airsense/createIcapSession|https://10.195.243.123/api/assurance/v1/airsense/createIcapSession]
4040:  Encountered unhandled HTTPError in Internal API Call
4041:  Flagging result as FAIL!
4042:  	Reason: 400 Client Error: Bad Request for url: [https://10.195.243.123/api/assurance/v1/airsense/createIcapSession|https://10.195.243.123/api/assurance/v1/airsense/createIcapSession]
4043:  Kwargs:
4044:  {'data': {'apInfos': [{'apName': 'SJ-AP1-3802E-RMA',
4045:                         'macAddress': '10:B3:D5:5D:5A:40'}],
4046:            'spectrumSlot': 0,
4047:            'type': 'ICAP_SPECTRUM'}}

Failed log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites.2023May11_22:18:14.946789.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites.2023May11_22:18:14.946789.zip&atstype=ATS] → Refer TC25, TC29, TC30, TC31, TC32 & TC33

*Pass log using Ghost code base on DNAC with Ghost Patch1 RC1 Uber ISO:*
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fsr_mb_multi_sites.2023Apr06_02:09:47.815987.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fsr_mb_multi_sites.2023Apr06_02:09:47.815987.zip&atstype=ATS] →  *Refer TC25, TC29, TC30, TC31, TC32 & TC33*",2023-05-12T14:01:53.856+0000,"[~accountid:62d2fec15d6f5fd2c3db8f9f] hey Sandeep, for this I want the release number and if possible the cluster that hit this issue.



thanks

regards [~accountid:63f50bfce8216251ae4d59d5] : The release info I have already added in defect description:

*Branch:* private/Halleck-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

About the cluster in problem state, currently we not have it in that state. It is a bug: [CDETS (cisco.com)|https://cdetsng.cisco.com/webui/#view=CSCwf35748]","['Auton', 'Halleck', 'MSTB1', 'Multisite']",Moe Saeed,Closed,SANDEEP SHIVARAMAREDDY
SEEN-1536,https://miggbo.atlassian.net/browse/SEEN-1536,[Auton] - Fabric Health score verification changes needed for tcpConnScore value from 10 to -1 as valid,"*Regression:* Solution Regression Multisite

*Branch:* private/Halleck-ms/api-auto, private/Ghost-ms/api-auto, private/Guardian-ms/api-auto, private/Hulk-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351, Ghost Patch1 RC1

*Script Used:* dnac-auto/testcases/mega_topo/solution_assurance_test.py

*Issue Faced:* 
Fabric Health score for devices getting failed if tcpConnScore is -1 for devices with Fabric Health score as 10. But as per the Assurance DE team, they have confirmed that this is the expected behavior for Polaris release 17.6.2 and higher versions and with DNAC versions Shockwave and higher. 

Automation scrips has to be modified accordingly to accommodate this.

*Comment from DE team* - “This particular KPI has been depreciated since device version 17.6.2 on switches/routers (since we depreciated snmp polling for ipsla data)”
[https://cdetsng.cisco.com/summary/#/defect/CSCwe95564/note?noteTitle=Eng-notes|https://cdetsng.cisco.com/summary/#/defect/CSCwe95564/note?noteTitle=Eng-notes]

*Failed log on Ghost Patch1 -*  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=732548&size=238953&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fsr_mb_multi_sites.2023Apr06_08:41:22.967994.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=732548&size=238953&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fsr_mb_multi_sites.2023Apr06_08:41:22.967994.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] → Refer TC50

Failed log on Halleck - [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1111602&size=238165&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites.2023May11_22:18:14.946789.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1111602&size=238165&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites.2023May11_22:18:14.946789.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] → Refer TC50",2023-05-12T14:40:06.188+0000,"PR:

# Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5735/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5735/overview]
# Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5734/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5734/overview]
# Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5736/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5736/overview]
# Groot: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5737/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5737/overview]
# Guardian: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5738/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5738/overview]
# Frey: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5739/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5739/overview] Hi [~accountid:62d2fec15d6f5fd2c3db8f9f], Could you please help me review this PR Auton? [~accountid:63f50bcafb3ac4003fa2c6dd] : I have reviewed the PRs for Hulk and Ghost branches. The same is applicable for other release PRs as well. Please go ahead and merged the code changes. Hi [~accountid:62d2fec15d6f5fd2c3db8f9f], Could you help me approve these PRs below and I will merge those? I can’t approve my PR by myself [~accountid:63f50bcafb3ac4003fa2c6dd] : Sorry I had somehow missed to approve the PRs. I have Approved the PRs for Halleck and Hulk. But for Ghost, Groot, Guardian and Frey it looks like there some discrepancy in modifying the code. Instead of changing cp score alone dhcp=10, dns=10, options are also removed. 

Could you please cross check and see it this is correct? Hi [~accountid:62d2fec15d6f5fd2c3db8f9f], I already merged the PRs for Halleck and Hulk. You can confirm these two branches first.

And I don’t have any cluster of these old versions (Ghost, Groot, Guardian, and Frey) to check.

So I think we can decline PRs of these old versions. Hi [~accountid:63f50bcafb3ac4003fa2c6dd]  ,

After merging this PR in Sanity, the Test_TC1_verify_border_edge_kpi test case is failing. Could you please check it
PR:[6603cfb811d|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6603cfb811d07ccdc0190eb41af324f16f272732]


*Found on:*
Uber ISO : Hulk  RC3
Polaris version: 17.12

*Script file/Usecase :* lansanity_usecases_maps.yaml

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0 issue*

+*Failed  Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_border_edge_kpi.py-176-verifyBorderEdgeKPI&begin=3957&size=104807&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug03_18:37:05.750930.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_border_edge_kpi.py-176-verifyBorderEdgeKPI&begin=3957&size=104807&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug03_18:37:05.750930.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Hulk RC2 PAss log:
+*Old  PAss Log:*+

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_border_edge_kpi.py-176-verifyBorderEdgeKPI&begin=4653&size=27720&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul26_08:32:30.057971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_border_edge_kpi.py-176-verifyBorderEdgeKPI&begin=4653&size=27720&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul26_08:32:30.057971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]   [~accountid:63f50bcafb3ac4003fa2c6dd] 

Same issue on DNAC EXSI setup as well on latest Hulk RC2 VM *3.710.75509* build

*OVA build:* assembly_release_dnac_hulk-intg_converged_07-3.710.75509.ova

*Branch:* private/Hulk-ms/api-auto

*Script file:* solution_test_sanityecamb.py

*Failed log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=953137&size=86587&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug04_01:05:23.652827.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=953137&size=86587&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug04_01:05:23.652827.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Hi [~accountid:63f50bd68ab3d6a635ecc29b] and [~accountid:620b8357878c2f00729881c8], After checking two failed logs of yours and debug on TB7 with latest Hulk code. I find out the root cause: the old library used one variable to compare with the {{tcpConnScore}}value and the {{overallScore}} value. So it makes fail with {{EDGE}} and {{eWLC}}.

I created a new PR to fix it.

PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6535/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6535/overview]

*Testbed:* Testbed 7 Version 2.3.7.0-70479

*Branch:* private/Hulk-ms/api-auto

*Script file:* solution_test_sanityecamb.py

Pass log:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-08/sanity_TB7.2023Aug07_02:24:30.103026.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-08/sanity_TB7.2023Aug07_02:24:30.103026.zip&atstype=ATS] [~accountid:63f50bcafb3ac4003fa2c6dd] : Thanks for looking into the issue further and applying fix! I have approved the PR. Could you please merge the fix?

Also , what shall we do with both these fixes for Ghost, Groot, Guardian and Frey? (Please note that Ghost and Guardian are the Active releases so far on DNAC along with Hulk) [~accountid:62d2fec15d6f5fd2c3db8f9f]: The PR was merged. About these old versions, I will update existing PRs including apply fix the issue. But I don’t have any cluster of these old versions (Ghost, Groot, Guardian, and Frey) to get new trade logs.","['Auton', 'Frey', 'Ghost', 'Groot', 'Guardian', 'Halleck', 'Hulk', 'MSTB1', 'Optimized', 'Sanity', 'Shockwave', 'hulk-vm-sanity']",NhanHuu Nguyen,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-1589,https://miggbo.atlassian.net/browse/SEEN-1589,[HULK]:Test_TC85_generate_exceutive_summary_report,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*Branch*: private/Hulk-ms/api-auto

*Script Name :* solution_test_sanity_ecamb.py

*Testbed :* TB4

*Testcases Impacted :*  Test_TC85_generate_exceutive_summary_report

During Hulk ESXI testing : ""TC85_generate_exceutive_summary_report"" the assurance data not matching to the output.

Please find the Logs attached

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=694595&size=33715&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May14_23:49:40.998379.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=694595&size=33715&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May14_23:49:40.998379.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[Error SNIP …]

3728: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 3733, in generate_exceutive_summary_report
3729: executions = report['executions']
3730: KeyError: 'executions'",2023-05-15T17:21:40.059+0000,"Looks like this is Defect and it did pass on Release 3.710.75307, hence closing the auton
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=26707741&size=1773174&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May21_02:06:42.902551.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=26707741&size=1773174&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May21_02:06:42.902551.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] The reported script issue in this Jira ticket got addressed by the fix provided for [https://miggbo.atlassian.net/browse/SEEN-1705|https://miggbo.atlassian.net/browse/SEEN-1705|smart-link] via below commit:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/572a137abe99e414ecbe0ddf7a49ea4a01da085f|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/572a137abe99e414ecbe0ddf7a49ea4a01da085f] Marking this Jira ticket as Resolved and Closed.",['Auton'],KRISHNA MUKKU,Closed,KRISHNA MUKKU
SEEN-1590,https://miggbo.atlassian.net/browse/SEEN-1590, Test_TC93_generate_ap_reachability_events,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI
*Branch*: private/Hulk-ms/api-auto
*Script Name :* solution_test_sanity_ecamb.py
*Testbed :* TB4
*Testcases Impacted :* [Test_TC93_generate_ap_reachability_events|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=715780&size=83587&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May15_11:46:44.720613.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

During Hulk ESXI testing : ""[TC93_generate_ap_reachability_events|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=715780&size=83587&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May15_11:46:44.720613.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]” the assurance data shows null data but the UI page shows that AP is down.

Please find the Logs attached and SNAP shot from Cluster.

*Logs:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=716344&size=82853&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May15_11:46:44.720613.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=716344&size=82853&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May15_11:46:44.720613.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-05-15T19:13:40.160+0000,"[~accountid:63f50bf9e8216251ae4d59d4] based on the latest execution log with 3.710.75307 - [test1_generate_ap_reachability_events|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=29316512&size=62693&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May21_02:06:42.902551.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] and when compared with the provided fail log, it seems to be a timing issue.

Please monitor this use-case for two more cycles and then conclude on this Jira ticket.

Until then, I’m moving this to “In-Progress” state. Did not see issue on Build: 3.710.75307, hence closing the auton
Pass Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=29283518&size=32248&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May21_02:06:42.902551.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=29283518&size=32248&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May21_02:06:42.902551.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",['Auton'],KRISHNA MUKKU,Closed,KRISHNA MUKKU
SEEN-1594,https://miggbo.atlassian.net/browse/SEEN-1594," Test_TC109_DNAC_maps:: test3_import_Ekahau_file[ekahau_type=ekahau_with_lat_long,expect_failure=False]","*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*Branch*: private/Hulk-ms/api-auto

*Script Name :* solution_test_sanity_ecamb.py

*Testbed :* TB4

*Testcases Impacted :*  [Test_TC109_DNAC_maps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=116113989&size=47171&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May08_22:30:57.365391.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

During Hulk ESXI testing : ""[Test_TC109_DNAC_maps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=116113989&size=47171&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May08_22:30:57.365391.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]"" found “Error importing Ekahau file. 409 Client Error”

Please find the Logs attached

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=116148916&size=12094&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May08_22:30:57.365391.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=116148916&size=12094&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May08_22:30:57.365391.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[Error SNIP …]

406156: Step 3: Start Ekahu file import

406157: *************************************************
406158: ------------Import Ekahau file ------------------

406159: *************************************************

406160:

406161:

406162: api_switch_call called:

406163: {'response_dict': False}

406164: Resource path full url: [https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async|https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async]

406165: Error Code: 409 for

406166: URL:https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async Data:{'timeout': 60} Headers:{'Content-Type': 'application/json', 'X-Auth-Token': 'eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJDRE5BIiwiYXV0aFNvdXJjZSI6ImxlZ2FjeSIsImNsaWVudElkIjoiYWRtaW4iLCJlbWFpbCI6ImFkbWluQGxvY2FsdXNlci5jb20iLCJleHAiOjE2ODM2NjEwMzEsImZpcnN0TmFtZSI6ImFkbWluIiwiaWF0IjoxNjgzNjU3NDMxLCJpc3MiOiJkbmFjIiwicm9sZXMiOlsiU1VQRVItQURNSU4iXSwic2Vzc2lvbklkIjoiMzNiZWNlOTAtOTY3MS01M2FkLThjMTItOWQ1ODZjMjQ1NDYyIiwic3ViIjoiYWRtaW4iLCJ0ZW5hbnRJZCI6IjY0NTM3ZDc4ZmQzZjVkMDAxMzc1MjI0MCIsInRlbmFudE5hbWUiOiJUTlQwIiwidXNlcm5hbWUiOiJhZG1pbiJ9.cQOMVvTuWVbrq9qL8R2P2okwIcB04bjMEIi_JAc3JOO4JL0Gbm9WHYgL1Eq8twfWEESCBk7Dl-0pJwGA2CMRDw'} Message:{""response"":{""errorCode"":""Conflict"",""message"":""Error: Context is in invalid state: VALIDATING"",""detail"":""maps.import-export.context.state.invalid:[VALIDATING]"",""href"":""""},""version"":""1.0""}

406167: Traceback (most recent call last):

406168: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 326, in call_api

406169: response.raise_for_status()

406170: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status

406171: raise HTTPError(http_error_msg, response=self)

406172: requests.exceptions.HTTPError: 409 Client Error: Conflict for url: [https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async|https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async]

406173: Encountered unhandled HTTPError in Internal API Call

406174: Flagging result as FAIL!

406175: Reason: 409 Client Error: Conflict for url: [https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async|https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async]

406176: Kwargs:

406177: {'response_dict': False}

406178: Error Caught While Querying the Internal API

406179: !!!!!!!! Error importing Ekahau file. 409 Client Error: Conflict for url: [https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async|https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async] !!!!!!!!

406180: Test returned in 0:00:00.203841

406181: Failed reason: Failed to import Ekahau file",2023-05-15T21:57:03.606+0000,Check with DE if there is a behaviour change for Ekahau,['Auton'],Raji Mukkamala,Cancelled,KRISHNA MUKKU
SEEN-1605,https://miggbo.atlassian.net/browse/SEEN-1605,Test_TC134_poor_RF_AP_Issue,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*Branch*: private/Hulk-ms/api-auto

*Script Name :* solution_test_sanity_ecamb.py

*Testbed :* TB4

*Testcases Impacted :*   [Test_TC134_poor_RF_AP_Issue|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1020032&size=23907&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May15_01:19:07.186587.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

During Hulk ESXI testing : ""[Test_TC134_poor_RF_AP_Issue|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1020032&size=23907&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May15_01:19:07.186587.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]"" the assurance data errored out.

Please find the Logs attached

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1020596&size=23186&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May15_01:19:07.186587.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1020596&size=23186&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May15_01:19:07.186587.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[Error SNIP …]

3728: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 3733, in generate_exceutive_summary_report
3729: executions = report['executions']
3730: KeyError: 'executions'Error Code: 500 for

4947: URL:https://10.22.45.61/api/assurance/v1/issue/global-detail Data:{'timeout': 60, 'data': '{""issueStatus"": ""active"", ""filters"": {""issueName"": ""global_radio_5_poor_rf_trigger"", ""priority"": ""P3"", ""deviceRole"": ""Access Point"", ""category"": ""Availability"", ""issueCategory"": ""Network"", ""isAIDriven"": ""false"", ""siteId"": ""__global__"", ""siteType"": ""Global""}, ""sortBy"": ""issueMessage"", ""sortOrder"": ""ASC""}'} Headers:{'Content-Type': 'application/json', 'X-Auth-Token': 'eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJDRE5BIiwiYXV0aFNvdXJjZSI6ImxlZ2FjeSIsImNsaWVudElkIjoiYWRtaW4iLCJlbWFpbCI6ImFkbWluQGxvY2FsdXNlci5jb20iLCJleHAiOjE2ODQxNDM3MzMsImZpcnN0TmFtZSI6ImFkbWluIiwiaWF0IjoxNjg0MTQwMTMzLCJpc3MiOiJkbmFjIiwicm9sZXMiOlsiU1VQRVItQURNSU4iXSwic2Vzc2lvbklkIjoiYmQ5OTA2MDUtZjk3ZC01MTQ4LWIzNGQtZTljNWViZWJkYzMyIiwic3ViIjoiYWRtaW4iLCJ0ZW5hbnRJZCI6IjY0NTM3ZDc4ZmQzZjVkMDAxMzc1MjI0MCIsInRlbmFudE5hbWUiOiJUTlQwIiwidXNlcm5hbWUiOiJhZG1pbiJ9.2ORQMrtCVwuIxMFbnu3GGtXUwMJFgAee6qHdcPmjbo4t72vlekodjpAsxBxBOgQxfIcnY8aCFkEzSmfpQ3U8pg'} Message:{""response"":{""errorCode"":5000,""message"":""An internal has error occurred while processing this request."",""detail"":""An internal has error occurred while processing this request.""}}

4948: Traceback (most recent call last):

4949: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 326, in call_api

4950: response.raise_for_status()

4951: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status

4952: raise HTTPError(http_error_msg, response=self)

4953: requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: [https://10.22.45.61/api/assurance/v1/issue/global-detail|https://10.22.45.61/api/assurance/v1/issue/global-detail]

4954: Encountered unhandled HTTPError in Internal API Call

4955: Flagging result as FAIL!

4956: Reason: 500 Server Error: Internal Server Error for url: [https://10.22.45.61/api/assurance/v1/issue/global-detail|https://10.22.45.61/api/assurance/v1/issue/global-detail]

4957: Kwargs:

4958: {'data': {'filters': {'category': 'Availability',

4959: 'deviceRole': 'Access Point',

4960: 'isAIDriven': 'false',

4961: 'issueCategory': 'Network',

4962: 'issueName': 'global_radio_5_poor_rf_trigger',

4963: 'priority': 'P3',

4964: 'siteId': '__global__',

4965: 'siteType': 'Global'},

4966: 'issueStatus': 'active',

4967: 'sortBy': 'issueMessage',

4968: 'sortOrder': 'ASC'}}

4969: Error Caught While Querying the Internal API

4970: Traceback (most recent call last):

4971: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper

4972: result = testfunc(func_self, **kwargs)

4973: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 9141, in test1_poor_rf_issue

4974: if (dnac_handle.enable_poor_rf_issue()):

4975: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 5772, in enable_poor_rf_issue

4976: response_rd_issue = self.services.api_switch_call(method=""POST"", resource_path=url, data=payload)

4977: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/lib/api_groups/utils/group.py"", line 48, in api_switch_call

4978: response = self.services.base.NB_API.call_api(method, resource_path, **kwargs)

4979: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 584, in call_api

4980: response = super(ApicemClientManager, self).call_api(

4981: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 332, in call_api

4982: raise e

4983: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 326, in call_api

4984: response.raise_for_status()

4985: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status

4986: raise HTTPError(http_error_msg, response=self)

4987: requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: [https://10.22.45.61/api/assurance/v1/issue/global-detail|https://10.22.45.61/api/assurance/v1/issue/global-detail]

4988: Test returned in 0:00:11.874991

4989: Errored reason: 500 Server Error: Internal Server Error for url: [https://10.22.45.61/api/assurance/v1/issue/global-detail|https://10.22.45.61/api/assurance/v1/issue/global-detail]

4990:

4991: Exception:

4992: Traceback (most recent call last):

4993: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper

4994: result = testfunc(func_self, **kwargs)

4995: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 9141, in test1_poor_rf_issue

4996: if (dnac_handle.enable_poor_rf_issue()):

4997: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 5772, in enable_poor_rf_issue

4998: response_rd_issue = self.services.api_switch_call(method=""POST"", resource_path=url, data=payload)

4999: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/lib/api_groups/utils/group.py"", line 48, in api_switch_call

5000: response = self.services.base.NB_API.call_api(method, resource_path, **kwargs)

5001: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 584, in call_api

5002: response = super(ApicemClientManager, self).call_api(

5003: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 332, in call_api

5004: raise e

5005: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 326, in call_api

5006: response.raise_for_status()

5007: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status

5008: raise HTTPError(http_error_msg, response=self)

5009: requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: [https://10.22.45.61/api/assurance/v1/issue/global-detail|https://10.22.45.61/api/assurance/v1/issue/global-detail]",2023-05-15T22:12:52.466+0000,"The reported script issue in this Jira ticket got addressed by the fix provided for [https://miggbo.atlassian.net/browse/SEEN-1705|https://miggbo.atlassian.net/browse/SEEN-1705|smart-link] via below commit: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/572a137abe99e414ecbe0ddf7a49ea4a01da085f|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/572a137abe99e414ecbe0ddf7a49ea4a01da085f]

Marking this Auton as “Resolved” and “Closed”.",['Auton'],Amardeep Kumar,Closed,KRISHNA MUKKU
SEEN-1662,https://miggbo.atlassian.net/browse/SEEN-1662,[Auton][GHOST] Ap Profile and/or site tag validation failed.,"DNAC Release_Version Tested: Ghost RC5 Uber ISO - 2.1.610.70190, Non-FIPS

Device Image Used: 17.10.1

Testbed: AWS-Multisite

Branch Used: private/Ghost-ms/sanity_api_auto

Script Name: solution_test_3sites_sjc_nyc_sf.py

Solution Input File : dnac-auto/configs/config_48hr_test/solution_test_input.json

Testcases Impacted: Test_TC194_ap_profiles

Failed Trade Log:  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16096994&size=1014768&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May10_02:23:35.993147.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16096994&size=1014768&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May10_02:23:35.993147.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Issue Analysis: After adding AP Power profile to validate the AP tag validation is failed.",2023-05-16T09:23:58.063+0000,"Hi Majilona, Executed the cases in this build Ghost P1 RC5-2.1.613.70190 repovisioning ap is failed.  Failed Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2170404&size=62271&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fauto_MS_job.2023Jun16_02:17:13.273720.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2170404&size=62271&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fauto_MS_job.2023Jun16_02:17:13.273720.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Hi [~accountid:641058d57222b08f3e7064d0] 
From the logs, looks like there are some issues with AI RF Profile. I am looking into it, however there has been no code changes in this particular method at all with respect to this usecase. [~accountid:63f50be71223974bc04b0534] , do we have any update on this Auton? [~accountid:641058d57222b08f3e7064d0] Regarding AP Provision Failure please follow up with Raji, issue seems to be related to AP Zone.
As of {{test6_verify_ap_calendar_profile_and_site_tag}} failure that is related to controller not present in the specific site - AP Profile works with IOS-XE only and not AIR-OS so I am working on the code to validate the type of controller in the site. [~accountid:63f50bf5e8216251ae4d59cf] Can you please check and update on this Was AP zone test run prior to this test? I don’t see it in the logs. from the {{test6_verify_ap_calendar_profile_and_site_tag}} logs, it does not look like AP zone related [~accountid:63f50bf5e8216251ae4d59cf] This cases failed AP power profile, [~accountid:63f50be71223974bc04b0534]  checked the logs and updated issue is related to AP Zone. [~accountid:63f50be71223974bc04b0534] Can you please check the below raji comments and update on this PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6933/diff#testcases/forty_eight_hour/solution_test_sanityecamb_cert.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6933/diff#testcases/forty_eight_hour/solution_test_sanityecamb_cert.py]","['Auton', 'Ghost', 'Integration']",Majlona 'Luna' Aliaj,Resolved,Balaji Raju
SEEN-1668,https://miggbo.atlassian.net/browse/SEEN-1668,[GHOST] Failed to provision AP with custom RF profile,"DNAC Release_Version Tested: Ghost RC5 Uber ISO - 2.1.610.70190, Non-FIPS

Device Image Used: 17.10.1

Testbed: AWS-Multisite

Branch Used: private/Ghost-ms/sanity_api_auto

Script Name: solution_test_3sites_sjc_nyc_sf.py

Solution Input File : dnac-auto/configs/config_48hr_test/solution_test_input.json

Testcases Impacted: Test_TC201_Provision_single_AP_with_custom_rf_profile

Failed Trade Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2210410&size=146010&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May16_03:46:55.339623.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2210410&size=146010&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May16_03:46:55.339623.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Issue Analysis: No wireless controller device items found, so  Failed to provision AP with custom RF profile",2023-05-16T16:12:14.999+0000,"From log, the testbed didnot have a right condition for the script so it returned failed.
It looked for a WLC that have APs joined and not associated with {{AI_RF_PROFILE}}

All of the wlcs were either having no AP or associated with {{AI_RF_PROFILE}}. Due to thisSEEN-1708 feature is blocked","['Auton', 'Ghost', 'Integration']",Balaji Raju,Resolved,Balaji Raju
SEEN-1670,https://miggbo.atlassian.net/browse/SEEN-1670,Common Cleanup,"Common Cleanup Errored:


!image-20230517-065848.png|width=729,height=209!

!image-20230517-065933.png|width=761,height=193!

Error Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=213331660&size=24000&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May16_11:26:31.394991.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=213331660&size=24000&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May16_11:26:31.394991.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

Branch Used:
bgl/Hulk-ms/api-auto

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse?at=refs%2Fheads%2Fbgl%2FHulk-ms%2Fapi-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse?at=refs%2Fheads%2Fbgl%2FHulk-ms%2Fapi-auto]",2023-05-17T07:01:31.782+0000,"Hi [~accountid:642a816bd774ab7297295df0],
Please fix your issue by installing the proper library: {{pdfplumber}} in your pyats environment

{noformat}pip install pdfplumber
# OR
pip3 install pdfplumber{noformat}

I found another issue after fixing the current issue, like missing {{get_dnac_version_information()}},then I will raise PR to fix them one time Hi [~accountid:63f50bd640328c12e4ec5b00] , Thanks for the workaround will try that after the current run gets completed. h2. Contents

* update [dnaservices.py|http://dnaservices.py]: replace not existing attribute to an existing one to avoid getattr
* give function {{get_dnac_version_information}} back to ./services/dnaserv/lib/api_groups/upgrade/group.py
* add ""pdfplumber"" to requirements.txt file

h2. PASS log:

# Not install library yet: [Log Link|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fsjc-it%2Fthanhtn2%2Fpyats%2Fusers%2Fthanhtn2%2Farchive%2F23-05%2Fsanity_TB1.2023May18_23:51:17.869863.zip&reqseq=&ats=%2Fws%2Fsjc-it%2Fthanhtn2%2Fpyats&submitter=thanhtn2&from=trade&view=all&atstype=PYATS]
# Installed library: [Log Link|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=215773&size=23350&archive=%2Fws%2Fsjc-it%2Fthanhtn2%2Fpyats%2Fusers%2Fthanhtn2%2Farchive%2F23-05%2Fsanity_TB1.2023May18_23:52:46.865590.zip&ats=%2Fws%2Fsjc-it%2Fthanhtn2%2Fpyats&submitter=thanhtn2&from=trade&view=all&atstype=pyATS]

h2. PR Link: 

* [PR Link|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5731/overview]","['Auton', 'Hulk', 'auton']",ThanhTan Nguyen,Closed,SAINATH CHATHARASI
SEEN-1686,https://miggbo.atlassian.net/browse/SEEN-1686,Auton:Ghost:Sanity:Test_TC41_DNAC_Configuring_bgp_on_border_fusion_router   Test_TC77_DNAC_Configuring_L3Handoff_and_bgp_on_border_fusion_router_anchoredvn/test1_configure_loopback_bgp_on_border_SJ/ test1_configure_loopback_bgp_on_border_NY ,"*Reporter Analysis:*  In TB5 we have observed TC41 and TC77 was failing due to adding configs on Fusion device , the error code we seeing was “{{%Insertion failed - prefix-list entry exists: seq 70 permit 40.50.0.0/16 le 32 TB5-DM-Fusion(config)#end}}“ after getting error code immediately it going to end command and other TB’s also we are seeing same error code but the script was continuing 

In TB5 from TC31 to TC40 somewhere configs was removing on the  fusion device  duo to this TC41 was failing 

*Description*:  
{{TB5-DM-Fusion(config)#ip prefix-list VRf_WirelessVN seq 390 permit 204.1.32.0/20 le 32 TB5-DM-Fusion(config)#ip prefix-list VRf_WirelessVN seq 400 permit 40.50.0.0/16 le 32 %Insertion failed - prefix-list entry exists: seq 70 permit 40.50.0.0/16 le 32 TB5-DM-Fusion(config)#end TB5-DM-Fusion# }}

{noformat}56819: 
 Traceback (most recent call last):{noformat}



*Branch Name:* Ghost -ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:

**Fail Log:*
TC41:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19892521&size=520640&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May12_00:13:51.340224.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19892521&size=520640&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May12_00:13:51.340224.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
TC77:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=30378777&size=2494239&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May15_22:18:27.221709.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=30378777&size=2494239&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May15_22:18:27.221709.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Pass log for TB3 with Ghost
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10815030&size=781567&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr19_02:23:55.549462.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10815030&size=781567&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr19_02:23:55.549462.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-05-17T15:49:48.302+0000,"Hi [~accountid:62d2fe9f8afb5805e5d5af49] 

Can you please prioritize the bug as it is difficult to add configs manully everytime and we are getting back to back request for Upgrades

Thanks,
Anusha John Can you share the tesbed since the issue only happened on this testbed? Hi [~accountid:62d2fe9f8afb5805e5d5af49] ,

We are seeing only in TB5 from Ghost ,
Execution is in progress now , will share you the testbed in next testing  Hi [~accountid:62d2fe9f8afb5805e5d5af49] ,
Cluster is in issue state :
10.22.40.63(admin/Maglev123)
Trade log:
[https://ngdevx.cisco.com/services/taas/results/ad870dfe-6a3b-4cb7-953e-1d4eeb1a936d|https://ngdevx.cisco.com/services/taas/results/ad870dfe-6a3b-4cb7-953e-1d4eeb1a936d]

Can you please look into the issue In your fabric json input '{{""fusion_route_vrftoglobal"": ""204.1.0.0/16,40.50.0.0/16"".}}
We don’t need to include {{40.50.0.0/16}} since it is belongs to a pool that will be added by script. Since you have this in fabric input, it created duplicated entry.

Added handle for duplicated pools scenarios.

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/79de2119263b6354ea0f5b40e772656ad47f3d39|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/79de2119263b6354ea0f5b40e772656ad47f3d39] Hi [~accountid:62d2fe9f8afb5805e5d5af49] ,
Thanks for Fixing the issue, i'm closing the JIRA.
Pass log:
[Test_TC41_DNAC_Configuring_bgp_on_border_fusion_router|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=29901679&size=754142&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul25_00:09:09.530519.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
[Test_TC77_DNAC_Configuring_L3Handoff_and_bgp_on_border_fusion_router_anchoredvn|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=25875601&size=2780160&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul25_02:18:10.478149.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Execution', 'Ghost', 'Halleck', 'Hulk', 'sanity']",Tran Lam,Closed,Tulasi Reddy
SEEN-1698,https://miggbo.atlassian.net/browse/SEEN-1698,Test_TC0_dnac_initial_cleanup,"*TC0_dnac_Initial_cleanup Failed:*

{noformat}2023-05-17T22:22:24: %CLIENTMANAGER-ERROR: [31mURL:https://10.106.133.152/api/v1/image/repository/summary/site/-1 Data:{'timeout': 60} Headers:{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2NDY0YmVkODA1MjZmZjBmZTI3OTkzMTAiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjY0NjRiZWQ1MDUyNmZmMGZlMjc5OTMwZiJdLCJ0ZW5hbnRJZCI6IjY0NjRiZWQ0MDUyNmZmMGZlMjc5OTMwZCIsImV4cCI6MTY4NDM0NTY5NCwiaWF0IjoxNjg0MzQyMDk0LCJqdGkiOiIyMjhmZWU0ZS1iYmY2LTQxYWYtYTJjMC1lZmMwNTMyZTI0OGQiLCJ1c2VybmFtZSI6ImFkbWluIn0.a-aaSnMEqpcogrWuS86P_ZRPGyvtKHhJaBluKH_q8Dtf48B82VBiTRli5nHhFKFobP6J4Vg-re_Qe-YqMFlnNv9Myg-Fh8hxLWcYzBFQqS4SuaIlxT2Rb3DwBcbzFb0ThKA-HyDba_uZpIw0NzxSVatkfkjQ1I0uyufDY_XXWY6Oomz174tTehNAXc4fGaRdJI1h4U4MbSLY1VUzeX_e89Vx1bwfDO20nserV_SGpPE0QeptUPVHRX81Os1QG-FmrzTZVgOaQpiwR5Z7nTPMgJx59shwFkz-KCZAgDCC2_PfQYs3pWj9UwAqH9JjG2Q3xqneLoM0E4GP0rm8Od4j6A;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:An invalid response was received from the backend service. Please refer to the backend service's logs for more details.[0m[39m
2023-05-17T22:22:24: %CLIENTMANAGER-ERROR: [31mTraceback (most recent call last):[0m[39m
2023-05-17T22:22:24: %CLIENTMANAGER-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 326, in call_api[0m[39m
2023-05-17T22:22:24: %CLIENTMANAGER-ERROR: [31m    response.raise_for_status()[0m[39m
2023-05-17T22:22:24: %CLIENTMANAGER-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/pyats/lib/python3.8/site-packages/requests/models.py"", line 953, in raise_for_status[0m[39m
2023-05-17T22:22:24: %CLIENTMANAGER-ERROR: [31m    raise HTTPError(http_error_msg, response=self)[0m[39m
2023-05-17T22:22:24: %CLIENTMANAGER-ERROR: [31mrequests.exceptions.HTTPError: 502 Server Error: Bad Gateway for url: https://10.106.133.152/api/v1/image/repository/summary/site/-1[0m[39m
2023-05-17T22:22:24: %API-GROUP-UTILS-ERROR: [31mEncountered unhandled HTTPError in Internal API Call[0m[39m
2023-05-17T22:22:24: %API-GROUP-UTILS-ERROR: [31mFlagging result as FAIL![0m[39m
2023-05-17T22:22:24: %API-GROUP-UTILS-ERROR: [31m	Reason: 502 Server Error: Bad Gateway for url: https://10.106.133.152/api/v1/image/repository/summary/site/-1[0m[39m
2023-05-17T22:22:24: %API-GROUP-UTILS-ERROR: [31mKwargs:[0m[39m
2023-05-17T22:22:24: %API-GROUP-UTILS-ERROR: [31m{}[0m[39m
2023-05-17T22:22:24: %API-GROUP-UTILS-INFO: Error Caught While Querying the Internal API
2023-05-17T22:22:24: %ATS-ERROR: [31mEncountered unhandled HTTPError in group ""swim"" method ""get_family_images""![0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31mFlagging result as FAIL![0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m	Reason: 502 Server Error: Bad Gateway for url: https://10.106.133.152/api/v1/image/repository/summary/site/-1[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m	Args:   (<services.dnaserv.lib.api_groups.swim.group.Group object at 0x7f47eb25ec10>,)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31mKwargs:[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m{}[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31mTraceback:[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    result = method(*args, **kwargs)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/swim/group.py"", line 2087, in get_family_images[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    response = self.services.api_switch_call(method=""GET"", resource_path=url)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/utils/group.py"", line 48, in api_switch_call[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    response = self.services.base.NB_API.call_api(method, resource_path, **kwargs)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 584, in call_api[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    response = super(ApicemClientManager, self).call_api([0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 332, in call_api[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    raise e[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 326, in call_api[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    response.raise_for_status()[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/pyats/lib/python3.8/site-packages/requests/models.py"", line 953, in raise_for_status[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    raise HTTPError(http_error_msg, response=self)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31mEncountered unhandled HTTPError in group ""swim"" method ""remove_marked_golden_images_from_DNAC""![0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31mFlagging result as FAIL![0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m	Reason: 502 Server Error: Bad Gateway for url: https://10.106.133.152/api/v1/image/repository/summary/site/-1[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m	Args:   (<services.dnaserv.lib.api_groups.swim.group.Group object at 0x7f47eb25ec10>,)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31mKwargs:[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m{}[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31mTraceback:[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    result = method(*args, **kwargs)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/swim/group.py"", line 693, in remove_marked_golden_images_from_DNAC[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    image_families = self.services.get_family_images()[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 44, in wrapper[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    raise err[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    result = method(*args, **kwargs)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/swim/group.py"", line 2087, in get_family_images[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    response = self.services.api_switch_call(method=""GET"", resource_path=url)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/utils/group.py"", line 48, in api_switch_call[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    response = self.services.base.NB_API.call_api(method, resource_path, **kwargs)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 584, in call_api[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    response = super(ApicemClientManager, self).call_api([0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 332, in call_api[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    raise e[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 326, in call_api[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    response.raise_for_status()[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/pyats/lib/python3.8/site-packages/requests/models.py"", line 953, in raise_for_status[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    raise HTTPError(http_error_msg, response=self)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31mEncountered unhandled HTTPError in group ""cleanup"" method ""cleanup_all_config""![0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31mFlagging result as FAIL![0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m	Reason: 502 Server Error: Bad Gateway for url: https://10.106.133.152/api/v1/image/repository/summary/site/-1[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m	Args:   (<services.dnaserv.lib.api_groups.cleanup.group.Group object at 0x7f47eb25e4f0>,)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31mKwargs:[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m{}[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31mTraceback:[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    result = method(*args, **kwargs)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/cleanup/group.py"", line 349, in cleanup_all_config[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    r = self.services.remove_marked_golden_images_from_DNAC()[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 44, in wrapper[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    raise err[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    result = method(*args, **kwargs)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/swim/group.py"", line 693, in remove_marked_golden_images_from_DNAC[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    image_families = self.services.get_family_images()[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 44, in wrapper[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    raise err[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    result = method(*args, **kwargs)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/swim/group.py"", line 2087, in get_family_images[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    response = self.services.api_switch_call(method=""GET"", resource_path=url)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/utils/group.py"", line 48, in api_switch_call[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    response = self.services.base.NB_API.call_api(method, resource_path, **kwargs)[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 584, in call_api[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    response = super(ApicemClientManager, self).call_api([0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 332, in call_api[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    raise e[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 326, in call_api[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    response.raise_for_status()[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/pyats/lib/python3.8/site-packages/requests/models.py"", line 953, in raise_for_status[0m[39m
2023-05-17T22:22:24: %ATS-ERROR: [31m    raise HTTPError(http_error_msg, response=self)[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31mTraceback (most recent call last):[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/commonlibs/test_wrapper.py"", line 301, in wrapper[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    result = testfunc(func_self, **kwargs)[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 197, in test1dnac_initial_cleanup[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    if (dnac_handle.cleanup_all_config()):[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 44, in wrapper[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    raise err[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    result = method(*args, **kwargs)[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/cleanup/group.py"", line 349, in cleanup_all_config[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    r = self.services.remove_marked_golden_images_from_DNAC()[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 44, in wrapper[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    raise err[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    result = method(*args, **kwargs)[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/swim/group.py"", line 693, in remove_marked_golden_images_from_DNAC[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    image_families = self.services.get_family_images()[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 44, in wrapper[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    raise err[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    result = method(*args, **kwargs)[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/swim/group.py"", line 2087, in get_family_images[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    response = self.services.api_switch_call(method=""GET"", resource_path=url)[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/utils/group.py"", line 48, in api_switch_call[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    response = self.services.base.NB_API.call_api(method, resource_path, **kwargs)[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 584, in call_api[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    response = super(ApicemClientManager, self).call_api([0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 332, in call_api[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    raise e[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 326, in call_api[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    response.raise_for_status()[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/pyats/lib/python3.8/site-packages/requests/models.py"", line 953, in raise_for_status[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31m    raise HTTPError(http_error_msg, response=self)[0m[39m
2023-05-17T22:22:24: %SERVICES-ERROR: [31mrequests.exceptions.HTTPError: 502 Server Error: Bad Gateway for url: https://10.106.133.152/api/v1/image/repository/summary/site/-1[0m[39m
2023-05-17T22:22:24: %SERVICES-INFO: Test returned in 0:04:48.500798
2023-05-17T22:22:24: %AETEST-ERROR: [31mErrored reason: 502 Server Error: Bad Gateway for url: https://10.106.133.152/api/v1/image/repository/summary/site/-1[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31mException:[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31mTraceback (most recent call last):[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/commonlibs/test_wrapper.py"", line 301, in wrapper[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    result = testfunc(func_self, **kwargs)[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 197, in test1dnac_initial_cleanup[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    if (dnac_handle.cleanup_all_config()):[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 44, in wrapper[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    raise err[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    result = method(*args, **kwargs)[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/cleanup/group.py"", line 349, in cleanup_all_config[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    r = self.services.remove_marked_golden_images_from_DNAC()[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 44, in wrapper[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    raise err[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    result = method(*args, **kwargs)[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/swim/group.py"", line 693, in remove_marked_golden_images_from_DNAC[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    image_families = self.services.get_family_images()[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 44, in wrapper[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    raise err[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    result = method(*args, **kwargs)[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/swim/group.py"", line 2087, in get_family_images[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    response = self.services.api_switch_call(method=""GET"", resource_path=url)[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/lib/api_groups/utils/group.py"", line 48, in api_switch_call[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    response = self.services.base.NB_API.call_api(method, resource_path, **kwargs)[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 584, in call_api[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    response = super(ApicemClientManager, self).call_api([0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 332, in call_api[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    raise e[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/Hulk/dnac-auto/services/dnaserv/client_manager.py"", line 326, in call_api[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    response.raise_for_status()[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m  File ""/ws/yiyamper-bgl/dnac_soln_reg/pyats/lib/python3.8/site-packages/requests/models.py"", line 953, in raise_for_status[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31m    raise HTTPError(http_error_msg, response=self)[0m[39m
2023-05-17T22:22:24: %AETEST-ERROR: [31mrequests.exceptions.HTTPError: 502 Server Error: Bad Gateway for url: https://10.106.133.152/api/v1/image/repository/summary/site/-1[0m[39m
2023-05-17T22:22:24: %AETEST-INFO: The result of section test1dnac_initial_cleanup is => ERRORED
2023-05-17T22:22:24: %AETEST-INFO: The result of testcase Test_TC0_dnac_initial_cleanup is => ERRORED{noformat}

*Branch Used:*

bgl/Hulk-ms/api-auto

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse?at=refs%2Fheads%2Fbgl%2FHulk-ms%2Fapi-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse?at=refs%2Fheads%2Fbgl%2FHulk-ms%2Fapi-auto]



Complete Log is Also attached with the issue, Please check.",2023-05-17T17:38:54.429+0000,"Request URL:

[https://10.195.227.14/api/v1/image/repository/summary/site/-1?__preventCache=1687296060851|https://10.195.227.14/api/v1/image/repository/summary/site/-1?__preventCache=1687296060851]

Request Method:

GET

Status Code:

200 OK

Remote Address:

10.195.227.14:443

Referrer Policy:

strict-origin-when-cross-origin

=====

Response:

{
    ""response"": [
        {
            ""deviceFamily"": ""Cisco 8540 Series Wireless Controllers"",
            ""deviceTypeOrdinal"": ""286284599"",
            ""softwareType"": ""Cisco Controller"",
            ""productType"": ""Cisco 8540 Series Wireless Controllers"",
            ""productTypeOrdinal"": 286284599,
            ""supervisorType"": null,
            ""supervisorTypeOrdinal"": -1,
            ""baseImages"": {
                ""Cisco Controller (8.10.183.0)"": ""8.10.183.0""
            },
            ""kickstartImages"": {},
            ""smuImages"": {},
            ""rommonImages"": {},
            ""apspImages"": {},
            ""apdpImages"": {},
            ""deviceUuids"": [
                ""5a562568-ee9b-4424-bd55-17f97d3b1618""
            ],
            ""goldenImageUuids"": [],
            ""productFamily"": ""Wireless Controller"",
            ""productFamilyOrdinal"": 978875243,
            ""productSeries"": null,
            ""productSeriesOrdinal"": -1,
            ""runningImageList"": [],
            ""virtualVendors"": [],
            ""deviceAvailable"": true
        },
        {
            ""deviceFamily"": ""Cisco Catalyst 9300 Switch"",
            ""deviceTypeOrdinal"": ""286315874"",
            ""softwareType"": ""IOS-XE"",
            ""productType"": ""Cisco Catalyst 9300 Switch"",
            ""productTypeOrdinal"": 286315874,
            ""supervisorType"": null,
            ""supervisorTypeOrdinal"": -1,
            ""baseImages"": {
                ""cat9k_iosxe.17.12.01prd2.SPA.bin"": ""17.12.01.0.213"",
                ""Install Mode (17.11.01.0.1324)"": ""17.11.01.0.1324"",
                ""Install Mode (17.10.01.0.1444)"": ""17.10.01.0.1444""
            },
            ""kickstartImages"": {},
            ""smuImages"": {},
            ""rommonImages"": {},
            ""apspImages"": {},
            ""apdpImages"": {},
            ""deviceUuids"": [
                ""ed520954-597b-49d0-8e95-ac3d7dc73121"",
                ""33ea3b45-b7e2-41ba-b1d1-f28e4eea941a"",
                ""d6628114-8b02-49ad-ae21-e938e1c77008"",
                ""1d877c75-2962-4b2d-9536-93891673a5e4"",
                ""e80ec794-faa3-48d8-903f-8a4d111e15b8""
            ],
            ""goldenImageUuids"": [],
            ""productFamily"": ""Switches and Hubs"",
            ""productFamilyOrdinal"": 268438038,
            ""productSeries"": null,
            ""productSeriesOrdinal"": -1,
            ""runningImageList"": [],
            ""virtualVendors"": [],
            ""deviceAvailable"": true
        },
        {
            ""deviceFamily"": ""Cisco Catalyst 9800-CL Wireless Controller for Cloud"",
            ""deviceTypeOrdinal"": ""286322605"",
            ""softwareType"": ""IOS-XE"",
            ""productType"": ""Cisco Catalyst 9800-CL Wireless Controller for Cloud"",
            ""productTypeOrdinal"": 286322605,
            ""supervisorType"": null,
            ""supervisorTypeOrdinal"": -1,
            ""baseImages"": {
                ""Install Mode (17.11.1)"": ""17.11.1""
            },
            ""kickstartImages"": {},
            ""smuImages"": {},
            ""rommonImages"": {},
            ""apspImages"": {},
            ""apdpImages"": {},
            ""deviceUuids"": [
                ""a50bc179-ee03-463c-9cf2-3b8b00a8c704""
            ],
            ""goldenImageUuids"": [],
            ""productFamily"": ""Wireless Controller"",
            ""productFamilyOrdinal"": 978875243,
            ""productSeries"": null,
            ""productSeriesOrdinal"": -1,
            ""runningImageList"": [],
            ""virtualVendors"": [],
            ""deviceAvailable"": true
        },
        {
            ""deviceFamily"": ""Unassigned"",
            ""deviceTypeOrdinal"": ""-100"",
            ""softwareType"": null,
            ""productType"": null,
            ""productTypeOrdinal"": -1,
            ""supervisorType"": null,
            ""supervisorTypeOrdinal"": -1,
            ""baseImages"": {
                ""cat9k_iosxe.17.12.01prd2.SPA.bin"": ""17.12.01.0.213"",
                ""cat9k_iosxe.BLD_V179_THROTTLE_LATEST_20220602_212900_V17_9_0_33.SSA.bin"": ""17.09.01.0.868""
            },
            ""kickstartImages"": {},
            ""smuImages"": {},
            ""rommonImages"": {},
            ""apspImages"": {},
            ""apdpImages"": {},
            ""deviceUuids"": [],
            ""goldenImageUuids"": [],
            ""productFamily"": null,
            ""productFamilyOrdinal"": -1,
            ""productSeries"": null,
            ""productSeriesOrdinal"": -1,
            ""runningImageList"": [],
            ""virtualVendors"": [],
            ""deviceAvailable"": false
        }
    ],
    ""version"": ""1.0""
}





=======

The API is support to work, Raise a product defect for it. ","['Auton', 'Hulk']",Moe Saeed,Closed,SAINATH CHATHARASI
SEEN-1700,https://miggbo.atlassian.net/browse/SEEN-1700,ncp-node/graphql giving empty response - TC36_DNAC_verify_device_stauts_after_adding_fabric Test_TC45_DNAC_TSIM_static_onboarding_verifications Test_TC51_DNAC_Verify_provision_status_after_segments_added Test_TC60_Verify_DHCP_server_change_on_segments,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*EXSI OVA build:* assembly_release_dnac_hulk-intg_converged_07-3.710.75408.ova
*Branch*: private/Hulk-ms/api-auto
*Script Name :* solution_test_sanity_ecamb.py
*Testbed :* TB4
*Testcases Impacted :*  “TC36_DNAC_verify_device_stauts_after_adding_fabric” and “Test_TC45_DNAC_TSIM_static_onboarding_verifications”

During Hulk ESXI testing : ""TC36_DNAC_verify_device_stauts_after_adding_fabric"" the assurance data not matching to the output.

Failure Logs: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=22341420&size=1198301&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May15_23:39:17.715766.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=22341420&size=1198301&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May15_23:39:17.715766.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

“Test_TC45_DNAC_TSIM_static_onboarding_verifications”

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14315662&size=1187077&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May16_09:37:54.634231.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14315662&size=1187077&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May16_09:37:54.634231.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

“Test_TC51_DNAC_Verify_provision_status_after_segments_added”

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=22441182&size=1203086&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May16_09:37:54.634231.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=22441182&size=1203086&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May16_09:37:54.634231.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Latest #3.710.75408 ova build fail log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8016567&size=1503204&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_07:01:32.510414.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8016567&size=1503204&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_07:01:32.510414.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

“Test_TC60_Verify_DHCP_server_change_on_segments”

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=31633565&size=1218437&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May16_09:37:54.634231.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=31633565&size=1218437&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May16_09:37:54.634231.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*Log output:*

97127: Method: POST, url:/v1/ncp-node/graphql, data:{'operationName': '', 'variables': {}, 'query': '{\n allDeviceStore(filters: [{field: ""graphIds"", value: ""8a1a1927-e6cb-48cc-ad5a-72e764be789e""}]) {\n items {\n id\n hostname\n family\n managementIpAddress\n softwareVersion\n reachabilityStatus\n upTime\n uptimeSeconds\n deviceSupportLevel\n macAddress\n role\n platformId\n serialNumber\n series\n collectionStatus\n lastUpdated\n site {\n groupNameHierarchy\n __typename\n }\n provisionStatus {\n aggregatedStatus\n __typename\n }\n deviceImage {\n deviceImageUpgradeStatus\n upgradeStatus\n __typename\n }\n deviceMetrics {\n trend {\n cpuScore\n memoryScore\n tcpConnScore\n errorScore\n upLinkScore\n allMetricValue\n freeTimerScore\n freeMbufScore\n wqePoolScore\n packetPoolScore\n interferenceScore\n channelNoiseScore\n utilizationScore\n channelAirQualityScore\n overallScore\n memory\n cpu\n entity_information\n time\n __typename\n }\n __typename\n }\n __typename\n }\n __typename\n }\n}\n'} for device :TB4-DM-Transit

97130: api_switch_call called:

97131: {'headers': {'Content-type': 'application/json'}, 'data': {'operationName': '', 'variables': {}, 'query': '{\n allDeviceStore(filters: [{field: ""graphIds"", value: ""8a1a1927-e6cb-48cc-ad5a-72e764be789e""}]) {\n items {\n id\n hostname\n family\n managementIpAddress\n softwareVersion\n reachabilityStatus\n upTime\n uptimeSeconds\n deviceSupportLevel\n macAddress\n role\n platformId\n serialNumber\n series\n collectionStatus\n lastUpdated\n site {\n groupNameHierarchy\n __typename\n }\n provisionStatus {\n aggregatedStatus\n __typename\n }\n deviceImage {\n deviceImageUpgradeStatus\n upgradeStatus\n __typename\n }\n deviceMetrics {\n trend {\n cpuScore\n memoryScore\n tcpConnScore\n errorScore\n upLinkScore\n allMetricValue\n freeTimerScore\n freeMbufScore\n wqePoolScore\n packetPoolScore\n interferenceScore\n channelNoiseScore\n utilizationScore\n channelAirQualityScore\n overallScore\n memory\n cpu\n entity_information\n time\n __typename\n }\n __typename\n }\n __typename\n }\n __typename\n }\n}\n'}}

97132: Resource path full url: [https://10.22.45.61/api/v1/ncp-node/graphql|https://10.22.45.61/api/v1/ncp-node/graphql]

97133: {'response': []}



=====================================================================

*Postman Output*

Please find the Postman logs attached, Postman Query and Response shown below

*Query:*

'{\n allDeviceStore(filters: [{field: ""graphIds"", value: ""11857caa-762e-4a94-8ebf-c350a89b84d6""}]) {\n items {\n id\n hostname\n family\n description\n managementIpAddress\n softwareVersion\n reachabilityStatus\n upTime\n uptimeSeconds\n deviceSupportLevel\n macAddress\n role\n platformId\n serialNumber\n series\n collectionStatus\n lastUpdated\n site {\n groupNameHierarchy\n __typename\n }\n provisionStatus {\n aggregatedStatus\n __typename\n }\n latestProvisionStatus {\n status\n __typename\n }\n deviceImage {\n deviceImageUpgradeStatus\n upgradeStatus\n __typename\n }\n deviceHealth {\n score\n __typename\n }\n __typename\n }\n __typename\n }\n}\n'

*Response:*

{""data"":{""allDeviceStore"":{""items"":[{""id"":""11857caa-762e-4a94-8ebf-c350a89b84d6"",""hostname"":""[SN-FOC2350L1BR.cisco.com|http://SN-FOC2350L1BR.cisco.com]"",""family"":""Switches and Hubs"",""description"":""Cisco IOS Software, C3560CX Software (C3560CX-UNIVERSALK9-M), Version 15.2(7)E6, RELEASE SOFTWARE (fc2) Technical Support: [http://www.cisco.com/techsupport|http://www.cisco.com/techsupport|smart-link]  Copyright (c) 1986-2022 by Cisco Systems, Inc. Compiled Tue 22-Mar-22 11:14 by mcpre"",""managementIpAddress"":""204.1.32.151"",""softwareVersion"":""15.2(7)E6"",""reachabilityStatus"":""Reachable"",""upTime"":""21:55:14.27"",""uptimeSeconds"":78903,""deviceSupportLevel"":""Supported"",""macAddress"":""a4:b2:39:a6:25:00"",""role"":""ACCESS"",""platformId"":""WS-C3560CX-12PC-S"",""serialNumber"":""FOC2350L1BR"",""series"":""Cisco Catalyst 3560-CX Series Switches"",""collectionStatus"":""Managed"",""lastUpdated"":""2023-05-18 00:45:00"",""site"":{""groupNameHierarchy"":""Global/USA/SAN JOSE/BLD23"",""__typename"":""SiteStore""},""provisionStatus"":{""aggregatedStatus"":""Success"",""__typename"":""ProvisionStatusStore""},""latestProvisionStatus"":{""status"":""Success"",""__typename"":""LatestProvisionStatusStore""},""deviceImage"":{""deviceImageUpgradeStatus"":""UNKNOWN"",""upgradeStatus"":""Tag Golden"",""__typename"":""SoftwareImageStore""},""deviceHealth"":{""score"":10,""__typename"":""DeviceHealthScore""},""__typename"":""DeviceStore""}],""__typename"":""allDeviceStoreResponse""}},""metadata"":{""stores"":{""DeviceStore"":""Thu May 18 2023 00:38:08 GMT+0000 (Coordinated Universal Time)"",""SiteStore"":""Thu May 18 2023 00:38:07 GMT+0000 (Coordinated Universal Time)"",""ProvisionStatusStore"":""Thu May 18 2023 00:38:07 GMT+0000 (Coordinated Universal Time)"",""LatestProvisionStatusStore"":""Thu May 18 2023 00:38:12 GMT+0000 (Coordinated Universal Time)"",""SoftwareImageStore"":""Thu May 18 2023 00:38:23 GMT+0000 (Coordinated Universal Time)"",""DeviceHealthScore"":""Thu May 18 2023 00:37:49 GMT+0000 (Coordinated Universal Time)""},""query"":{""query"":""{ allDeviceStore(filters: [{field: \""graphIds\"", value: \""11857caa-762e-4a94-8ebf-c350a89b84d6\""}]) { items { id hostname family description managementIpAddress softwareVersion reachabilityStatus upTime uptimeSeconds deviceSupportLevel macAddress role platformId serialNumber series collectionStatus lastUpdated site { groupNameHierarchy __typename } provisionStatus { aggregatedStatus __typename } latestProvisionStatus { status __typename } deviceImage { deviceImageUpgradeStatus upgradeStatus __typename } deviceHealth { score __typename } __typename } __typename } } "",""timeTaken"":""0.00secs""}}}",2023-05-18T00:54:28.599+0000,"Raised PR for Hulk where {{ncp_graph_check}} flag has been set to {{False}} while making calls to {{verify_devices_aggregated_status()}} , thus avoiding calls to “ncp-node/graphql” API method:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6122/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6122/overview]",['Auton'],Amardeep Kumar,Closed,KRISHNA MUKKU
SEEN-1702,https://miggbo.atlassian.net/browse/SEEN-1702,TC197 AttributeError on sub testcase 3,"* *DNAC Release_Version Tested:* Ghost P1 RC1-2.1.613.70170
* *Device Image Used:* 17.10.1FC2
* *Testbed:* AWS-Multisite setup
* *Branch Used:* private/Ghost-ms/sanity_api_auto
* *Script Name:* solution_test_3sites_sjc_nyc_sf.py
* *Solution Input File :* dnac-auto/configs/config_48hr_test/solution_test_input_FIPS.json
* *Testcases Impacted:* Test_TC197_wireless_client_data_troubleshoot
{{test3_get_log_file_content_and_validate}} getting errored out due to below reason:
{{AttributeError: 'DnaServices' object has no attribute 'get_log_file_content'}}
Please find the Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1844451&size=168004&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May16_09:15:03.304529.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1844451&size=168004&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May16_09:15:03.304529.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-05-18T06:58:12.516+0000,"Hi Quang, Executed this cases in Hulk RC1 getting errored out: Please find the attached errored snapshot: [Test_TC194_wireless_client_data_troubleshoot|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=20462755&size=199628&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul27_04:53:23.524842.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

!errored_out_wireless.png|width=876,height=248! [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6458/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6458/overview]

affect by above PR, can you please check again to see if it’s okay or not

if it’s failed, please share the log and let me know which branch did you use

thank you ❤️ PR is merged on Hulk.","['AWS_MSTB', 'MSTB3', 'auton']",QuangVinh Nguyen,Resolved,Neelima Doddipalli
SEEN-1704,https://miggbo.atlassian.net/browse/SEEN-1704, CSCwf27891 - LLDP device discovery taking 40 minutes to get complete,"Hulk build *3.710.75272 & 3.710.75296*  ESXi VM observed the LLDP discovery device taking 40 minutes to get complete for Transit, EDGE & FIAB device

Issue seen on Hulk release only, not seen in Halleck build 3660.75447 

*Branch:* private/Hulk-ms/api-auto

*Script file:*  solution_test_sanityecamb.py 

Hulk log: *3.710.75272* *VA build*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=382892&size=5701632&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May08_01:25:07.500468.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=382892&size=5701632&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May08_01:25:07.500468.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Hulk log: *3.710.75296* *VA build*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2551524&size=7530312&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May16_00:16:08.296839.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2551524&size=7530312&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May16_00:16:08.296839.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Halleck pass log:

Issue not seen in Halleck release :  In LLDP discovery gets completed within 7 minutes
log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2417352&size=367289&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr05_16:25:20.339270.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2417352&size=367289&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_auto_job.2023Apr05_16:25:20.339270.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-05-18T11:52:26.756+0000,"There is no change in automation for discovery.

Please debug and raise defect if you see it took longer for DNAC to discover devices. [~accountid:62d2fe9f8afb5805e5d5af49] 
Pls take look on the Defect eng-notes *CSCwf27891*
On the Halleck release using branch is *“hallack_esxivm_branch”* didn’t seen delay discovery device works fine.

In Halleck release lldp discovery taking 1 count but in hulk taking 5 this is causing delay discovery 

*Halleck results:*

13986: Resource path full url: [+https://10.22.45.217/api/v1/discovery/46/network-device/count+|https://10.22.45.217/api/v1/discovery/46/network-device/count]
13987: Discovery successfully created, number of devices found in discovery: 1.
13988: Library group ""discovery"" method ""lldp_discovery"" returned in 0:01:20.390535
13989: LLDP Discovery is successful, and state is complete, all devices discovered.

*“private/Hulk-ms/api-auto“ Hulk results:*

29301: Resource path full url: [+https://10.22.45.217/api/v1/discovery/73/network-device/count+|https://10.22.45.217/api/v1/discovery/73/network-device/count]
29302: Discovery successfully created, number of devices found in discovery: 5.
29303: Library group ""discovery"" method ""lldp_discovery"" returned in 2:02:19.483725
29304: LLDP Discovery is successful, and state is complete, all devices discovered.

Since it is expected that discovery takes more time with high lldp level value and with more number of credentials.



Still observing same issue Duplicate Autons: [https://miggbo.atlassian.net/browse/SEEN-2102|https://miggbo.atlassian.net/browse/SEEN-2102|smart-link] [https://miggbo.atlassian.net/browse/SEEN-1934|https://miggbo.atlassian.net/browse/SEEN-1934|smart-link]  In Latest Hulk P1 build #*3.713.75106* also observing the same issue  

Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2486352&size=11394083&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep03_06:18:33.596485.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2486352&size=11394083&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep03_06:18:33.596485.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Addressed: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/610cabaeb46e24eb83d7a4d273020ef79140e6e0|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/610cabaeb46e24eb83d7a4d273020ef79140e6e0]

Cherry picked to all Hulk Patch branches Thank you [~accountid:63f50bf5e8216251ae4d59cf] for the fix
Will try to validate the fix in upcoming runs Added some fix for the commit below:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f20b1c4e01f9b4a23c9c99cb7db37ab288f5e0fc#testcases/forty_eight_hour/solution_test_sanityecamb.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f20b1c4e01f9b4a23c9c99cb7db37ab288f5e0fc#testcases/forty_eight_hour/solution_test_sanityecamb.py]","['Auton', 'Hulk', 'auton', 'sanity']",Raji Mukkamala,Resolved,Raghavendrachar Baraguru Mallesha Char
SEEN-1705,https://miggbo.atlassian.net/browse/SEEN-1705,Generate Executive  Summary  Report Verification Errored ,"*Analysis:* 

In DNAC is showing the executive summary report showing but via script executive summary report verification got errored from day one issue

Attached screenshot of the executive summary on the DNAC

Hulk Log: *3.710.75296* *VA build*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2765430&size=20354&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May17_11:46:38.035367.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2765430&size=20354&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May17_11:46:38.035367.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-05-18T12:26:27.855+0000,"h2. Contents:

* can NOT reproduce the issue but added one more validation for case 'executions' not present in the response

h2. Pass log:

* [TRADe Log link|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fsjc-it%2Fthanhtn2%2Fpyats%2Fusers%2Fthanhtn2%2Farchive%2F23-05%2Fsanity_TB10.2023May19_01:10:13.369373.zip&reqseq=&ats=%2Fws%2Fsjc-it%2Fthanhtn2%2Fpyats&submitter=thanhtn2&from=trade&view=all&atstype=PYATS]

h2. PR Link:

* [PR Link:|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5733/overview]

h2. Images:

!image-20230519-082514.png|width=1546,height=962!

!image-20230519-082519.png|width=1792,height=809!

!image-20230519-082524.png|width=1895,height=456!","['Auton', 'Hulk', 'auton', 'sanity']",ThanhTan Nguyen,Resolved,Raghavendrachar Baraguru Mallesha Char
SEEN-1706,https://miggbo.atlassian.net/browse/SEEN-1706,Test_TC107_Compliance_verification,"*Sub TC Failed:*

test8_verify_Networkprofile_compliance

*Error Snap shot:*

!image-20230518-170655.png|width=573,height=305!

*Trade Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=143749202&size=3253121&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May17_23:26:20.434379.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=143749202&size=3253121&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May17_23:26:20.434379.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

*Branch Used:*

bgl/Hulk-ms/api-auto

*Test Suite:*

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto]",2023-05-18T17:09:25.216+0000,,"['Auton', 'Hulk']",ThangQuoc Tran,Cancelled,SAINATH CHATHARASI
SEEN-1707,https://miggbo.atlassian.net/browse/SEEN-1707,[Auton] Discovery ID is not validated and None value is passed as parameter to API call,"Discovery ID is not validated and None value is passed as parameter to API call 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery_brownfield.py-51-toolsDiscoveryBrownfield&begin=0&size=-1&archive=%2Froot%2F.pyats%2Farchive%2F23-05%2Fsanity_TB1_cert.2023May18_13:05:11.655217.zip&ats=%2Fws%2Favdas-sjc%2Fpyats-new&submitter=root&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery_brownfield.py-51-toolsDiscoveryBrownfield&begin=0&size=-1&archive=%2Froot%2F.pyats%2Farchive%2F23-05%2Fsanity_TB1_cert.2023May18_13:05:11.655217.zip&ats=%2Fws%2Favdas-sjc%2Fpyats-new&submitter=root&atstype=PYATS]



{noformat} 
 {}{noformat}

{noformat}931: 
 Resource path full url: https://10.195.214.103/api/v1/discovery/None{noformat}

{noformat}932: 
 Error Code: 400 for{noformat}

{noformat}933: 
 URL:https://10.195.214.103/api/v1/discovery/None Data:{'timeout': 60} Headers:{'Content-Type': 'application/json', 'X-Auth-Token': 'eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJDRE5BIiwiYXV0aFNvdXJjZSI6ImxlZ2FjeSIsImNsaWVudElkIjoiYWRtaW4iLCJlbWFpbCI6ImFkbWluQGxvY2FsdXNlci5jb20iLCJleHAiOjE2ODQ0NDQ4MDMsImlhdCI6MTY4NDQ0MTIwMywiaXNzIjoiZG5hYyIsInJvbGVzIjpbIlNVUEVSLUFETUlOIl0sInNlc3Npb25JZCI6IjBkNTBkMTQ0LTFjMmMtNTAwNi05MzBkLTRjOTE3ZTc1NGJmNiIsInN1YiI6ImFkbWluIiwidGVuYW50SWQiOiI2NDVkOWQwZWMxZTUxODAwMTNlOWM3YjYiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInVzZXJuYW1lIjoiYWRtaW4ifQ.-Uyb64bihNlIWQX7-p6b58sjDWyomrK0v48hwAS104-JbXaukXUrgV0rb1QSl4hhbziACprPQUgRh5zKZmqXag'} Message:{""response"":{""errorCode"":""Bad request"",""message"":""Invalid input request"",""detail"":""Discovery Id is not in correct format None""},""version"":""1.0""}{noformat}

{noformat}934: 
 Traceback (most recent call last):{noformat}

{noformat}935: 
   File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/OptimizedSanity/services/dnaserv/client_manager.py"", line 326, in call_api{noformat}

{noformat}936: 
     response.raise_for_status(){noformat}

{noformat}937: 
   File ""/ws/avdas-sjc/pyats-new/lib/python3.6/site-packages/requests/models.py"", line 960, in raise_for_status{noformat}

{noformat}938: 
     raise HTTPError(http_error_msg, response=self){noformat}

{noformat}939: 
 requests.exceptions.HTTPError: 400 Client Error:  for url: https://10.195.214.103/api/v1/discovery/None{noformat}

{noformat}940: 
 Encountered unhandled HTTPError in Internal API Call{noformat}

{noformat}941: 
 Flagging result as FAIL!{noformat}

{noformat}942: 
 	Reason: 400 Client Error:  for url: https://10.195.214.103/api/v1/discovery/None{noformat}

{noformat}943: 
 Kwargs:{noformat}

{noformat}944: 
 {}{noformat}

{noformat}945: 
 Error Caught While Querying the Internal API{noformat}

{noformat}946: 
 HTTPError: None{noformat}

{noformat}947: 
 HTTPError: b'{""response"":{""errorCode"":""Bad request"",""message"":""Invalid input request"",""detail"":""Discovery Id is not in correct format None""},""version"":""1.0""}'{noformat}

{noformat}948: 
 Library group ""discovery"" method ""configure_verify_discovery_range"" returned in 0:00:02.460880{noformat}

{noformat}949: 
 Traceback (most recent call last):{noformat}

{noformat}950: 
   File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/OptimizedSanity/testcases/sanityusecases/toolsDiscoveryBrownfield/discovery_brownfield.py"", line 113, in test1_verify_adding_range_discovery_ssh_global_credentials{noformat}

{noformat}951: 
     self.failed(""Result: Device Discovery is failed"", goto=['common_cleanup']){noformat}

{noformat}952: 
   File ""/ws/avdas-sjc/pyats-new/lib/python3.6/site-packages/pyats/aetest/base.py"", line 545, in failed{noformat}

{noformat}953: 
     raise signals.AEtestFailedSignal(reason, goto, from_exception, data){noformat}

{noformat}954: 
 pyats.aetest.signals.AEtestFailedSignal: ('Result: Device Discovery is failed', ['common_cleanup'], None, None){noformat}

{noformat}955: {noformat}",2023-05-18T21:49:16.857+0000,[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5725/diff#services/dnaserv/lib/api_groups/discovery/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5725/diff#services/dnaserv/lib/api_groups/discovery/group.py],"['Auton', 'Halleck-ESXI', 'Issue']",Raji Mukkamala,Resolved,Raji Mukkamala
SEEN-1708,https://miggbo.atlassian.net/browse/SEEN-1708,[Ghost]Assigning AI RF Profile to site fails with Internal Error,"*DNAC Release_Version Tested*: Ghost RC5 Uber ISO - 2.1.610.70190, Non-FIPS

*Device Image Used*: 17.10.1

*Branch Used*: private/Ghost-ms/sanity_api_auto

*Script Name:* solution_test_3sites_sjc_nyc_sf.py

*Solution Input File* : dnac-auto/configs/config_48hr_test/solution_test_input.json

*Testbed :* AWS-Multisite MSTB3

*Testcases Impacted :*  

 [Test_TC51_DNAC_provision_all_aps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1027725&size=932216&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul27_03:51:39.928547.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
-> [test4_assign_AI_profile_site|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1781079&size=178701&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-07%2Fsr_mb_multi_sites_mdnac.2022Jul27_03:51:39.928547.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Failed Logs:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1323220&size=93543&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May07_18:14:35.578192.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1323220&size=93543&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May07_18:14:35.578192.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-05-19T03:49:55.910+0000,"[~accountid:641058d57222b08f3e7064d0] , I see couple of things:

# {{FW-9800-11}}Does not have any AP associated with it. Is it expected?
# below excerpt shows some Error Code:
{noformat}8459:  {'version': 1683510119709, 'startTime': 1683510119688, 'errorCode': 'NCWS90501', 'endTime': 1683510119709, 'serviceType': 'NCWS', 'username': 'admin', 'failureReason': 'Internal Error', 'isError': True, 'instanceTenantId': '6444471500b4294bbd7bb2f3', 'id': '5783325f-fb87-4e1d-9644-5549c45fcef5'}{noformat}

which clearly indicate, there’s issue. Could be a bug.



Please do some analysis before raising an Auton. Hi Amardeep, cluster is down from lastweek, on  due to new deployment, I got a cluster on 26/6/2023. I will work on this and update [~accountid:641058d57222b08f3e7064d0] Sure [~accountid:641058d57222b08f3e7064d0] , pls. try with the available Cluster and share the update on test result.","['AWS_MSTB', 'Auton', 'Ghost', 'Integration', 'MSTB2', 'MSTB3', 'hulk']",Balaji Raju,Open,Balaji Raju
SEEN-1710,https://miggbo.atlassian.net/browse/SEEN-1710,[Auton] Missing read loopback interface in provisionInventoryBrownfield usecase,"Missing read loopback interface in provisionInventoryBrownfield usecase

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-inventory_checks_brownfield.py-61-provisionInventoryBrownfield&begin=53313&size=25067&archive=%2Froot%2F.pyats%2Farchive%2F23-05%2Fsanity_TB1_cert.2023May18_21:13:29.289248.zip&ats=%2Fws%2Favdas-sjc%2Fpyats-new&submitter=root&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-inventory_checks_brownfield.py-61-provisionInventoryBrownfield&begin=53313&size=25067&archive=%2Froot%2F.pyats%2Farchive%2F23-05%2Fsanity_TB1_cert.2023May18_21:13:29.289248.zip&ats=%2Fws%2Favdas-sjc%2Fpyats-new&submitter=root&from=trade&view=all&atstype=pyATS]

339:  Traceback (most recent call last):

340:    File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/OptimizedSanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper

341:      result = testfunc(func_self, **kwargs)

342:    File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/OptimizedSanity/testcases/sanityusecases/provisionInventoryBrownfield/inventory_checks_brownfield.py"", line 121, in test1_verifications_configure_roles_on_devices

343:      if(dnac_handle.configure_device_role()):

344:    File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/OptimizedSanity/services/dnaserv/lib/decorators.py"", line 32, in wrapper

345:      result = method(*args, **kwargs)

346:    File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/OptimizedSanity/services/dnaserv/lib/api_groups/inventory/group.py"", line 131, in configure_device_role

347:      dev_ip=self.services.dnaconfig.testbed.devices[name].lb_ip

348:    File ""/ws/avdas-sjc/pyats-new/lib/python3.6/site-packages/pyats/topology/device.py"", line 560, in *getattr*

349:      % attr) from None

350:  AttributeError: 'Device' object has no attribute 'lb_ip'

351:  Test returned in 0:00:00.408640

352:  Errored reason: 'Device' object has no attribute 'lb_ip'",2023-05-19T06:16:18.981+0000,[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d914b57670b8a631fc00dfd22118d434932a0715|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d914b57670b8a631fc00dfd22118d434932a0715],"['Auton', 'ESXi', 'Hulk', 'Optimized']",Raji Mukkamala,Resolved,Raji Mukkamala
SEEN-1714,https://miggbo.atlassian.net/browse/SEEN-1714,[Auton] [Halleck] [Hulk] - Collection of testbed metadata TC getting Errored,"*Regression:* Solution Regression Multisite 

*Branch:* private/Halleck-ms/api-auto, private/Hulk-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351, Hulk - 2.1.710.70276

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Issue Faced:* 
Collection of testbed meta testcase is getting errored with module path missing.



*Failed log on Halleck* - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May21_08:44:04.413506.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May21_08:44:04.413506.zip&atstype=ATS] → Refer common_cleanup TC

*Failed log on Hulk* - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May16_06:04:09.971163.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May16_06:04:09.971163.zip&atstype=ATS] → Refer common_cleanup TC",2023-05-22T11:12:38.340+0000,"This is same issue to: [https://miggbo.atlassian.net/browse/SEEN-1670|https://miggbo.atlassian.net/browse/SEEN-1670|smart-link] 
 Hulk is fixed by this PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5731/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5731/overview] PR Link to fix in Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5766/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5766/overview]","['Auton', 'Halleck', 'Hulk', 'MSTB1', 'Multisite']",ThanhTan Nguyen,Closed,SANDEEP SHIVARAMAREDDY
SEEN-1715,https://miggbo.atlassian.net/browse/SEEN-1715,[Auton]:Ghost-Test_TC109_DNAC_maps  /   test2_ap_bulk_import ,"*Reporter Analysis:* 

For any device, role should be perform the operation 
+*Description:*+  
Message:{""message"":""Role does not have valid permissions to access the API""}

*Branch Name:  private/Ghost-ms/sanity_api_auto*

*Script file/Usecase:* 

solution_test_sanityecamb.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10350269&size=37583&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May22_07:35:22.840473.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10350269&size=37583&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May22_07:35:22.840473.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-05-23T05:10:58.452+0000,[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/2d78d678e7d143f54c03c17b0a927873a49f169b|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/2d78d678e7d143f54c03c17b0a927873a49f169b],"['Auton', 'Ghost', 'Optimized', 'Sanity']",Raji Mukkamala,Resolved,Omkar Sharad Wagh
SEEN-1717,https://miggbo.atlassian.net/browse/SEEN-1717,[Auton] - Verify sgt mapping fails for the client corresponding to ise end point dictionary,"*Regression:* Solution Regression Multisite

*Branch used:* private/Halleck-ms/api-auto, private/Hulk-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351, Hulk - 2.1.710.70276

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Issue Faced:* 
1) When integrating the feature corresponding to ise endpoint dictionary, we see that verify of sgt for the wired client configured for ise end point dictionary fails. We see that this sub TC gets passed if clear of access-session method for dot1x is performed before the verification. This has to be implemented.

*Below is the verification snippet:*
SJ-FE-9300-1#sh cts role-based sgt-map vrf WiredVNFBLayer2 all                   
Active IPv4-SGT Bindings Information

IP Address              SGT     Source

204.1.96.170            4       LOCAL

IP-SGT Active Bindings Summary

Total number of LOCAL    bindings = 1
Total number of active   bindings = 1

SJ-FE-9300-1#
SJ-FE-9300-1#
SJ-FE-9300-1#clear access-session method dot1x interface FiveGigabitEthernet1/0/7
SJ-FE-9300-1#
SJ-FE-9300-1#
SJ-FE-9300-1#sh cts role-based sgt-map vrf WiredVNFBLayer2 all                   
Active IPv4-SGT Bindings Information

IP Address              SGT     Source

204.1.96.170            13      LOCAL

IP-SGT Active Bindings Summary

Total number of LOCAL    bindings = 1
Total number of active   bindings = 1

SJ-FE-9300-1#
SJ-FE-9300-1#sho access-session method dot1x interface FiveGigabitEthernet1/0/7
Interface                MAC Address    Method  Domain  Status Fg  Session ID

Fi1/0/7                  0050.56a3.90c6 dot1x   DATA    Auth        3E0101CC0000002946C7D017

Key to Session Events Blocked Status Flags:

  A - Applying Policy (multi-line status for details)
  D - Awaiting Deletion
  F - Final Removal in progress
  I - Awaiting IIF ID allocation
  P - Pushed Session
  R - Removing User Profile (multi-line status for details)
  U - Applying User Profile (multi-line status for details)
  X - Unknown Blocker

SJ-FE-9300-1#

2) Also after running the cleanup for this TC, also we need to add another subTC to clear of access-session method for dot1x and verify if the sgt is reverted back to original value which was before endpoint dictionary is configured.

*Below is the verification snippet:*


SJ-FE-9300-1#
SJ-FE-9300-1#sh cts role-based sgt-map vrf WiredVNFBLayer2 all
Active IPv4-SGT Bindings Information

IP Address              SGT     Source

204.1.96.170            13      LOCAL

IP-SGT Active Bindings Summary

Total number of LOCAL    bindings = 1
Total number of active   bindings = 1

SJ-FE-9300-1#                                                              
SJ-FE-9300-1#
SJ-FE-9300-1#
SJ-FE-9300-1#clear access-session method dot1x interface FiveGigabitEthernet1/0/7
SJ-FE-9300-1#
SJ-FE-9300-1#
SJ-FE-9300-1#sh cts role-based sgt-map vrf WiredVNFBLayer2 all                   
Active IPv4-SGT Bindings Information

IP Address              SGT     Source

204.1.96.170            4       LOCAL

IP-SGT Active Bindings Summary

Total number of LOCAL    bindings = 1
Total number of active   bindings = 1

SJ-FE-9300-1#

*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/DCS+-+ISE+endpoint+dictionary|https://wiki.cisco.com/display/EDPEIXOT/DCS+-+ISE+endpoint+dictionary]

*Failed log on Halleck* - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May22_07:48:53.666751.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May22_07:48:53.666751.zip&atstype=ATS] → Refer TC98

*Pass log after manually performing steps as mentioned in 1) above:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May23_01:08:09.407607.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May23_01:08:09.407607.zip&atstype=ATS] → Refer TC98.8 & TC99",2023-05-23T15:25:33.223+0000,"Add reconnection clients after classified.

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a2327472ff26a68d31f814634517ff34c882848f|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a2327472ff26a68d31f814634517ff34c882848f]","['Auton', 'Ghost', 'Groot', 'Guardian', 'Halleck', 'Hulk', 'Integration', 'MSTB1', 'Multisite']",Tran Lam,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-1720,https://miggbo.atlassian.net/browse/SEEN-1720,[Auton] - Verify of kairos baseline and AP daily heatmap chart fails inconsistently,"*Regression:* Solution Regression Multisite

*Branch used:* private/Halleck-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351, Hulk - 2.1.710.70276

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Description:* For Kairos base line and Ap daily heat charts, the verifications are failing inconsistently. On try to check with cloud team, they have suggested us to use the payload with time stamp with more accuracy instead of just date. Also not sure why the inconsistent data coming in.

*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/Kairos+Enhancements+2|https://wiki.cisco.com/display/EDPEIXOT/Kairos+Enhancements+2]

*Failed logs:*

# [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May22_03:40:07.379870.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May22_03:40:07.379870.zip&atstype=ATS] → Refer TC244, TC245, TC246, TC247; Here TC244 and TC247 failed
#  [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May23_01:43:48.053544.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May23_01:43:48.053544.zip&atstype=ATS] →  Refer TC244, TC245, TC246, TC247; Here TC244 failed and TC247 passed.",2023-05-23T16:42:12.649+0000,,"['Auton', 'Ghost', 'Guardian', 'Halleck', 'Hulk', 'MSTB1', 'Multisite']",Raji Mukkamala,Open,SANDEEP SHIVARAMAREDDY
SEEN-1722,https://miggbo.atlassian.net/browse/SEEN-1722,[Auton][MSTB2] : RLAN feature integration,"Hi [~accountid:63f50bcece6f37e5ed93c87e] ,

We are trying to integrate RLAN feature on MSTB2. But script fails due to below error
{'params': {'name': '[FW-9800-11.cisco.com|http://FW-9800-11.cisco.com]'}}
9721:  Resource path full url: [https://10.195.243.37/api/system/v1/auth/token|https://10.195.243.37/api/system/v1/auth/token]
9722:  Resource path full url: [https://10.195.243.37/api/v2/data/customer-facing-service/DeviceInfo|https://10.195.243.37/api/v2/data/customer-facing-service/DeviceInfo]
9723:  device_response for FW-9800-11
9724:  Could not find the device: MSTB2-VEWLC details

As per logger info script is expecting all controller to be part of Fabric. But MSTB2 has one non-fabric site too. So MSTB2-VEWLC is not part of any fabric site. So script fails complaining couldnot find device details.

Expected Behaviour : Script should skip Non-fabric controller

Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1622696&size=25546&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May15_00:11:04.196702.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1622696&size=25546&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May15_00:11:04.196702.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Branch Name : private/Halleck-ms/api-auto
Script Name : solution_test_3sites_sjc_nyc_sf.py
Testbed : MSTB2-Non-DR
Wiki : [https://wiki.cisco.com/display/EDPEIXOT/S.R.+Multisite+TB2+Non-DR+Inventory|https://wiki.cisco.com/display/EDPEIXOT/S.R.+Multisite+TB2+Non-DR+Inventory]",2023-05-24T03:52:43.982+0000,"[Pull Request #5860: Check for other devices besides the first in hte device list if details arent present - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5860/overview]

Changes pushed to halleck as well Fixed two separate issues:

# Typo in printing the device name, it was previously just printing the last device in devlist, not the device it was looking for the details for. eg. MSTB2-VEWLC instead of FW-9800-11.
# If device details from any device in the list are not found, before it used to just return False. Now have made it so it checks through the device list for other device infos. But in any case, the issue is that FW-9800-11 on SF site is not in fabric/does not have device details, which is not fixed. Hi [~accountid:63f50bcece6f37e5ed93c87e] ,

I’ve tried executing feature on latest Hulk-ESXI-3.710.75393 build, but script still complains device detail is not present.

Failed log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1630499&size=19820&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fauto_MS_job.2023Jun27_22:19:43.971037.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1630499&size=19820&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fauto_MS_job.2023Jun27_22:19:43.971037.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Script complains below error
device_response for FW-9800-12
9855:  Could not find the device: FW-9800-12 details
9856:  Library group ""inventory"" method ""verify_ewlc_rlan_supported_aps"" returned in 0:00:00.090609
9857:  Test returned in 0:00:01.939323
9858:  Skipped reason: Could not find the EWLC device or supported RLAN ports in setup [ENG-SDN / dnac-auto / c7e6ee77baf - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c7e6ee77baf640008457923059e7256598cbeccd]

Added retry for api without domain name in params. Please give this a try. RLAN ap is associated with 9800-11
[FW-9800-11.cisco.com|http://FW-9800-11.cisco.com]> sh ap summary
Number of APs: 3

CC = Country Code
RD = Regulatory Domain

h2. AP Name Slots AP Model Ethernet MAC Radio MAC CC RD IP Address State Location

AP00DF.1D86.2A1C 2 C9105AXW-B 00df.1d86.2a1c 00df.1d87.89a0 US -B 204.1.213.153 Registered Global/USA/SAN-FRANCISCO/BLD_SF/FLOOR1
APA488.73CE.9AF4 2 C9120AXE-B a488.73ce.9af4 a488.73d4.d7a0 US -B 204.1.213.154 Registered Global/USA/SAN-FRANCISCO/BLD_SF/FLOOR1
AP1416.9D2E.1FC8 3 C9130AXE-B 1416.9d2e.1fc8 ac4a.56ac.9260 US -B 204.1.213.155 Registered Global/USA/SAN-FRANCISCO/BLD_SF/FLOOR2

[FW-9800-11.cisco.com|http://FW-9800-11.cisco.com]>

Failed log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1629944&size=48811&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fauto_MS_job.2023Jun29_10:09:06.598667.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1629944&size=48811&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fauto_MS_job.2023Jun29_10:09:06.598667.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS].

But still script complains that RLAN supported AP is not present Hi [~accountid:63f50bcece6f37e5ed93c87e],

Can you please let us know the latest on the failure?

Thanks [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1454299&size=29540&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul20_16:07:15.760218.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1454299&size=29540&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul20_16:07:15.760218.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

This commit to use a different part of the api response to determine if ewlc has RLAN supported APs.  [~accountid:63f50bf0e8216251ae4d59ca] I think I forgot to mark this one resolved, last commit. Please give it a try.","['Auton', 'IntegrationIssue', 'MSTB2']",Andrew Chen,Resolved,Divakar Kumar Yadav
SEEN-1723,https://miggbo.atlassian.net/browse/SEEN-1723,TC71_DNAC_extended_node_link_failover_test,"*Sub TC failed*:
test70_reassign_ext_node_ip

*Error snip shot* :

!image-20230524-094030.png|width=571,height=478!

Here connection is getting closed by foreign host can we add clear line for the device {{SN-FDO2351J7F7}} before script starts for this TC .

*Device console details :*

*IP:* 10.106.133.135 

*Port:* 2012

*Failed Log first run :*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=87007751&size=75369&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May20_14:14:48.112300.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=87007751&size=75369&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May20_14:14:48.112300.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

*Failed log re-run after doing clear line manually:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3147728&size=164006&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May24_00:39:57.863581.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3147728&size=164006&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May24_00:39:57.863581.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

Passed Log after doing the steps in log manually and reran the script by waiting  sometime:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1218368&size=197688&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May24_11:58:42.989794.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1218368&size=197688&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May24_11:58:42.989794.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

Please add delay in the timer to get the updated ip to the device.",2023-05-24T09:46:51.277+0000,"I see the ext-node IP address is changing but the script validation says IP-address not changed (might be need to wait for some more time to grep the value) [~accountid:642a816bd774ab7297295df0] , do you still see this issue of Connection error? yes [~accountid:62ab7a399cd13c0068b18fe0]  still the issue is there i am seeing all my current runs Latest Log:[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=55982263&size=74507&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-07%2Fsanitycombine.2023Jul17_12:29:56.570216.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=55982263&size=74507&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-07%2Fsanitycombine.2023Jul17_12:29:56.570216.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS] [~accountid:642a816bd774ab7297295df0] , we do not see this issue with our testbed. So could not check more on it.

I am assigning this ticket to [~accountid:62d2fe9f8afb5805e5d5af49]  if she can comment on it. PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6677/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6677/overview] [~accountid:642a816bd774ab7297295df0], pls. make use of below PR specific branch - *ghost-fixes-ext* for testing the change and confirm if this fixes your issue. [~accountid:62ab7a399cd13c0068b18fe0] still the issue persists can you check TC71 in below log
Taas Log : [https://ngdevx.cisco.com/services/taas/results/e6f81276-5fd9-410d-80e9-a5a9c26d4fe1/run-results|https://ngdevx.cisco.com/services/taas/results/e6f81276-5fd9-410d-80e9-a5a9c26d4fe1/run-results] [~accountid:62ab7a399cd13c0068b18fe0] 

From the logs, I do not see the console issue. Can you merge the fix and resolve this?  [~accountid:642a816bd774ab7297295df0], as Moe mentioned, your current log does not complain on Console issue.

If you have any more script issue, pls. raise a new Auton with initial analysis from your end.

[~accountid:63f50bfce8216251ae4d59d5], I have approved and merged the PR.
Marking this Auton as “Resolved”.","['Auton', 'ESxi', 'Ghost', 'Hulk', 'Optimized', 'sanity']",Moe Saeed,Resolved,SAINATH CHATHARASI
SEEN-1724,https://miggbo.atlassian.net/browse/SEEN-1724,CSCwf37486[SS] Functional: Devices Provision failed for NCSS10077: Failed to delete task schedule,"DNAC Release: Hulk  ISO: 2.3.7.0-70276

*Reporter Analysis:*
*Engg Note :* 
Root cause of this issue is that the task delete API called in automation script is not setting the sws-id-type header,  so the api fails to cleanup the conflicting task.
[+https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/services/dnaserv/lib/api_groups/orchestration_engine/group.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto#295+|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/services/dnaserv/lib/api_groups/orchestration_engine/group.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto#295]

Since this conflict is not cleared due to the failure, subsequent tasks on the same device/namespace are failing with the recycle prevention error.



+*Here is what the UI does:*+ [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dna-core-apps/browse/ui-plugins/core/app/baseAutomation/src/activity/scheduledTasks/queries.js#36|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dna-core-apps/browse/ui-plugins/core/app/baseAutomation/src/activity/scheduledTasks/queries.js#36]

+*Failed  Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-reprovisioning_provision_conifg_validation.py-102-provisionReprovisionConfigValidation&begin=357357&size=1180450&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May16_11:59:35.822359.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-reprovisioning_provision_conifg_validation.py-102-provisionReprovisionConfigValidation&begin=357357&size=1180450&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May16_11:59:35.822359.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

+*Defect Team Sapce:*+   webexteams://im?space=3dfbdde0-f587-11ed-b60a-0df5d01d789b

!09_46_05.jpg|width=719,height=516!",2023-05-24T15:24:14.185+0000,"[ENG-SDN / dnac-auto / 9c4f3244b1e - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9c4f3244b1e89ff5ab1aea2f9250e0f4102e7748]

This was merged, please check if the issue is resolved [~accountid:620b8357878c2f00729881c8] ","['Auton', 'Hulk', 'Optimized', 'Sanity']",Andrew Chen,Resolved,Omkar Sharad Wagh
SEEN-1725,https://miggbo.atlassian.net/browse/SEEN-1725,[Auton] - Hydra trustscore validation and apply ANC policy on hydra fails for DCS hydra feature Testcase,"*Regression:* Solution Regression Multisite

*Branch used:* private/Halleck-ms/api-auto, private/Hulk-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Issue Faced:* 

When integrating the feature corresponding to DCS hydra, we see that total trustscore and mfc trustscore verifications are working fine, where as hydra trustscore verification Fails for the client - 00:50:56:A3:90:C6 (host name - SR-MB1-CL-2). Also ANC policy apply on hydra enabled client - 00:50:56:A3:90:C6 (host name - SR-MB1-CL-2) is also failing.

On trying to check from AI-analytics section from DNAC GUI, it looks fine as expected. Since the feature does not have steps to verify Hydra trustscore, not sure if this is script side issue or its a DNAC defect. Please refer the below attached recording taken from DNAC UI under AI-analytics section for more details:

!DCS_Hydra_Related_Recording.mp4|width=1912,height=968!

*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/DCS+-+Hydra+%2C+MFC+Change|https://wiki.cisco.com/display/EDPEIXOT/DCS+-+Hydra+%2C+MFC+Change]



*Failed log on Halleck* - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May24_19:43:41.814413.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May24_19:43:41.814413.zip&atstype=ATS] → Refer TC100

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May24_22:00:36.386191.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May24_22:00:36.386191.zip&atstype=ATS] → Refer TC101",2023-05-25T11:28:50.629+0000,"[~accountid:62d2fec15d6f5fd2c3db8f9f] , Please share the testbed, I will look into. It could be behavior changes or bug.","['Auton', 'Halleck', 'Hulk', 'Integration', 'MSTB1', 'Multisite']",Tran Lam,Backlog,SANDEEP SHIVARAMAREDDY
SEEN-1726,https://miggbo.atlassian.net/browse/SEEN-1726,[Auton] - Verification of Auth failure devices fails with Ixia api related issues,"*Regression:* Solution Regression Multisite

*Branch used:* private/Halleck-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

*Script Used:* dnac-auto/testcases/mega_topo/solution_assurance_test.py

*Issue Faced and Suggested Enhancements:* 

# When integrating this assurance TC,  related to verify auth failure assurance event, we see script failures due to ixia api related calls.
# Also script needs to be enhanced to have clear ixia onboard interface followed by ixia onboard interfaces. Because this TC is loading a preconfigured ixia dot1x file for which sessions are being checked (similar to TC63 - DNAC_dot1x_onboarding_ixia_scale under solution_test_3sites_sjc_nyc_sf_mdnac_dr.py script) 

*Failed log on Halleck* - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites.2023May24_10:31:10.828203.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites.2023May24_10:31:10.828203.zip&atstype=ATS] → Refer TC20 both sub TCs",2023-05-25T15:46:46.735+0000,"This seem like not a new usecase, why is it have ‘integration’ label? [~accountid:62d2fe9f8afb5805e5d5af49] : This is older usecase which was not integrated on the testbed. Hence added the integration label. Please suggest if an alternate label needs to be used for older usecases which in case are not integrated being looked into now. Raised PRs:

* HulkPatch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7275/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7275/overview]
* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7276/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7276/overview]
* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7277/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7277/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7278/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7278/overview]","['Auton', 'Ghost', 'Halleck', 'Hulk', 'Integration', 'MSTB1', 'Multisite']",ThangQuoc Tran,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-1727,https://miggbo.atlassian.net/browse/SEEN-1727,[Auton] - Stack device verification in Assurance script giving false verification pass,"*Regression:* Solution Regression Multisite

*Branch used:* private/Halleck-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

*Script Used:* dnac-auto/testcases/mega_topo/solution_assurance_test.py

*Issue Faced and Suggested Enhancements:* 

# We see that stack device verification check is not happening on Stack device despite we have SVL device part of Multisite testbed. For SVL device, “show switch detail” command does not support, instead “show stackwise-virtual” has to be used or any supporting alternative command has to be used. However the proc used for verification has GET REST api call from device 360 page of the device, which is correct. 
# Also the negative scenario should also include failure message even if its a stack device and device 360 page does not have right values.

*Error snippet:*

5506:  Resource path full url: [https://10.195.243.109/api/v1/file/107e7f02-b4c7-43a6-b695-51769456be2b|https://10.195.243.109/api/v1/file/107e7f02-b4c7-43a6-b695-51769456be2b]
5507:  Response received against GET Operation: [{'deviceUuid': '19c09f2f-8c9c-488a-9798-8836003560c3', 'commandResponses': {'SUCCESS': {}, 'FAILURE': {'show switch detail': ""Error occurred while executing command : show switch detail\nshow switch detail\n            ^\n% Invalid input detected at '^' marker.\n\nNY-CP-9300#""}, 'BLACKLISTED': {}}}]

*Failed log on Halleck* - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites.2023May24_08:44:02.573800.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites.2023May24_08:44:02.573800.zip&atstype=ATS] → Refer TC23



Attaching HAR capture for device 360 of SVL device - NY-CP-9300 which has stack related api call values.

[^Assurance_stack_Details_query_HAR_Capture_Halleck.har]

-------------------------------------------------------------------------

Also attaching the device side log for the SVL device for reference.

[^NY-CP-9300_device_logs.log]

",2023-05-25T17:04:32.205+0000,"[~accountid:62d2fec15d6f5fd2c3db8f9f] , this '[Test_TC1_verify_healthy_device_metrics|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=546327&size=723&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites.2023May24_08:44:02.573800.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] is not a new testcase. Why is there Integration label? [~accountid:62d2fe9f8afb5805e5d5af49] : Looks like you have mentioned a different TC by mistake in the below comment. This Jira was reported for TC -  *Test_TC23_verify_stack_device_details.* Yes its a older TC. But since it was passing fine and did not have failure, we had not looked into the pass criteria. Recently when were having a look into the each TC Pass/Fail criteria in detail, we observed this TC was passing with out actual stack verification despite we have SVL device in testbed. Hence reported JIRA. This we have considered as a Re-integration of existing TC. 

Hence added the integration label. Please suggest if an alternate label needs to be used for older usecases. Raised PRs:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6227/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6227/overview]
* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6228/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6228/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6229/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6229/overview] [~accountid:63f50bd34c355259db9ccc4d] : I have added my comments for the PR review with respect to Ghost PR link. The same is applicable for PR links of Halleck and Hulk. Could you please have a look into it? Hi [~accountid:62d2fec15d6f5fd2c3db8f9f] , I’ve updated my PRs and addressed your comments, please recheck them. Thanks [~accountid:63f50bd34c355259db9ccc4d] : Thanks for addressing the comments. I have approved all the 3 PRs. Could you please have a look and merge the fixes ?  All PRs have been merged. Thanks [~accountid:62d2fec15d6f5fd2c3db8f9f]  [~accountid:62d2fec15d6f5fd2c3db8f9f] Could you please close this ticket? Thanks","['Auton', 'Halleck', 'Hulk', 'Integration', 'MSTB1', 'Multisite']",ThangQuoc Tran,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-1728,https://miggbo.atlassian.net/browse/SEEN-1728,Test_TC110_ssid_edit,"*Sub TC failed*:
test2_verify_modify_ssid

*Error snip shot* :

!image-20230525-181226.png|width=581,height=147!

*Failed Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=171060587&size=482909&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May20_14:14:48.112300.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=171060587&size=482909&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May20_14:14:48.112300.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

*Branch used:*

bgl/Ghost-ms/api-auto_New

*Test Suite:*

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Ghost-ms/api-auto_New|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Ghost-ms/api-auto_New]",2023-05-25T18:18:39.045+0000,"[ENG-SDN / dnac-auto / 15e7b6a7b0c - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/15e7b6a7b0cc553e794bb5e1c629d9c79cc63a32]

Hulk branch



[ENG-SDN / dnac-auto / d5f22613456 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d5f22613456825070e108b0eb966729bb9d7b300]

Ghost branch



Issue was that code was added that made restricted configuring ssid when it already exists. However thats exactly what this usecase is trying to do, so added a parameter to the configure_wireless_ssid function to allow for configuring already existing ssid when the param is enabled (disabled default)","['Auton', 'Ghost', 'Hulk', 'auton']",Andrew Chen,Resolved,SAINATH CHATHARASI
SEEN-1730,https://miggbo.atlassian.net/browse/SEEN-1730,[Auton] - Verification of SDAVC app on enabled device fails,"*Regression:* Solution Regression Multisite

*Branch used:* private/Halleck-ms/api-auto, private/Hulk-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Issue Faced:* 

When integrating the Security Sensor Port scan feature, we see verification of SDAVC App on enabled device fails. It looks like there was a change in the output for “show avc sd-service info summary” on the device. The existing code regex does not match the current output on the device, hence the failure.

*Below is the actual verification snippet:*

SJ-FE-9300-1#show avc sd-service info summary
Status: CONNECTED

Device ID: [SJ-FE-9300-1.cisco.com|http://SJ-FE-9300-1.cisco.com]
Device segment name: AppRecognition
*Device address: 204.1.2.3*
Device OS version: 17.10.01
Device type: C9300-48UN

Active controller:
   Type    : Primary
   Address : 50.45.45.6
   Status  : Connected
   Version        : 4.4.0
   Last connection: 06:58:55.000 UTC Fri May 26 2023

Active SDAVC import files:
    Protocol pack:           Not loaded
    Secondary protocol pack: PPDK_AppRecognition_00ae9952d99d4b1efba2d58f7a21e9.pack
    Rules pack:              pp_update_AppRecognition_a_v2_7e79d3a7bd67.pack

SJ-FE-9300-1#

SJ-FE-9300-1#show run | sec avc              
platform wdavc serviceability
avc sd-service
 segment AppRecognition
 controller
  address 50.45.45.6
  destination-ports sensor-exporter 21730
  dscp 16
  source-interface Loopback0
  transport application-updates https url-prefix sdavc
 !
SJ-FE-9300-1#
SJ-FE-9300-1#

SJ-FE-9300-1#sho install summary
[ Switch 1 ] Installed Package(s) Information:
State (St): I - Inactive, U - Activated & Uncommitted,
            C - Activated & Committed, D - Deactivated & Uncommitted

Type  St   Filename/Version

IMG   C    17.10.01.0.1444

Auto abort timer: inactive

SJ-FE-9300-1#
SJ-FE-9300-1#    
SJ-FE-9300-1#

*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/Security+Sensor%3A+Port+Scan+-+Credential+Scan|https://wiki.cisco.com/display/EDPEIXOT/Security+Sensor%3A+Port+Scan+-+Credential+Scan]

*Failed log on Halleck* - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May25_07:57:18.149432.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May25_07:57:18.149432.zip&atstype=ATS] → Refer TC108",2023-05-26T07:12:15.534+0000,"[~accountid:62d2fec15d6f5fd2c3db8f9f] the change appears to be straight. “IP” has been replaced with “Address”.

Below is the PR for the required change:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5900/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5900/overview]

Please execute the failing use-case using “SEEN-1730_Hulk” branch content. Once it goes through, pls. approve and merge above PR.  [~accountid:62ab7a399cd13c0068b18fe0]  : Issue is no more observed after the fix is added. 

Can we have this fix added to Guardian, Ghost, Groot, Ghost & Halleck code bases as well? 

This is a Frey feature related TC.
Reference feature wiki - [https://wiki.cisco.com/display/EDPEIXOT/Security+Sensor%3A+Port+Scan+-+Credential+Scan|https://wiki.cisco.com/display/EDPEIXOT/Security+Sensor%3A+Port+Scan+-+Credential+Scan]

*Pass log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac_dr.2023Jun25_23:24:47.182909.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac_dr.2023Jun25_23:24:47.182909.zip&atstype=ATS] ->Refer TC108.19

*Branch used:* private/Hulk-ms/api-auto

*Uber ISO tested:* Hulk Uber - 2.1.710.70344

*Polaris version used:* 17.12.1prd2 FC1 raised PR for:
Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6101/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6101/overview]; got approved and merged; cherry-picked to Groot. Guardian: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6105/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6105/overview]

Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6106/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6106/overview]","['Auton', 'Halleck', 'Hulk', 'Integration', 'MSTB1', 'Multisite']",Amardeep Kumar,Closed,SANDEEP SHIVARAMAREDDY
SEEN-1731,https://miggbo.atlassian.net/browse/SEEN-1731,[Auton] - Addition open port list fails and then followed by port scan and credentials scan trustscore verification failures ,"*Regression:* Solution Regression Multisite

*Branch used:* private/Halleck-ms/api-auto, private/Hulk-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Issue Faced:* 

When integrating the feature corresponding to Security Sensor Port Scan and Credential scan, we see that 

a) Addition of Open port list to the DNAC has failed. 

b) Its then followed by port scan and credential scan trust score validation fails for the corresponding endpoint  - 00:50:56:A3:90:C6 (host name - SR-MB1-CL-2)

On trying to check from SDAVC section and  AI-Endpoint-analytics section from DNAC GUI, it appears fine. The Open Port list seems to have got added properly.

Also not sure if there is some issue from DNAC side or do we need perform any actions to cause the trustcore to change. Please refer the below attached recording taken from DNAC UI under SDAVC section and AI-Endpoint-analytics section for more details:

!SDAVC_Security_Port_Credential_Scan_Overview_Testbed.mp4|width=1912,height=960!



*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/Security+Sensor%3A+Port+Scan+-+Credential+Scan|https://wiki.cisco.com/display/EDPEIXOT/Security+Sensor%3A+Port+Scan+-+Credential+Scan]

*Failed log on Halleck* - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May25_07:57:18.149432.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May25_07:57:18.149432.zip&atstype=ATS] → Refer TC109 & TC110",2023-05-26T10:41:04.771+0000,"[~accountid:62d2fec15d6f5fd2c3db8f9f] , can you share the testbed in issue state? It seems like a bug that didnot defect the {{unauthorized port}} and Credential Vulnerability trustscore.

Handled for #a: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/4dff2ceae2bca8880d6ff54c6a1cdf97728b2f38|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/4dff2ceae2bca8880d6ff54c6a1cdf97728b2f38] [~accountid:62d2fec15d6f5fd2c3db8f9f] , Please response to the below request comment.
Since you haven’t responded and shared the testbed after 2 months. I put this ticket to cancelled state. 

Please respond to the comment and provide the testbed if you see the issue again.","['Auton', 'Halleck', 'Hulk', 'Integration', 'MSTB1', 'Multisite']",Tran Lam,Cancelled,SANDEEP SHIVARAMAREDDY
SEEN-1734,https://miggbo.atlassian.net/browse/SEEN-1734,[Auton] - Multiple issues seen under remediate Network Setting Compliance Issues TC,"*Regression:* Solution Regression Multisite

*Branch used:* private/Halleck-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Issue Faced:* 

When integrating the feature corresponding to remediate Network Setting Compliance Issues, we have observed below issues:

a) The proc used to perform remediation check is added incorrectly in Second sub TC - test2_verify_remediation_and_verify_remdiation

*perform_remdiation_check* is mentioned instead of *perform_remediation_check.* Rectified this locally to continue with execution.

b) Abrupt script failure in middle of execution with related to TypeError

14297:  Traceback (most recent call last):

14298:    File ""/auto/dna-sol/ws/sr-mb1_Halleck_SEEN-1485/services/dnaserv/lib/api_groups/Compilance/group.py"", line 2876, in perform_remediation_check

14299:      f""Compliance Issue for {issues[ewlc]} on dev {dev} not found!!""

14300:  TypeError: string indices must be integers

c)  Restore configs operation is performed twice in the failure scenario. This needs to be handled properly. In case the Restore operation is already done once, it should not be repeated as it will add extra overhead time during the execution. Only Compliance check can be made later. 

d) Logging info messages has to be enhanced at some places, so that it will be easier for the user to understand the steps in case of Successful or Failures scenarios. Below are the details:

→ Instead of just printing Device name, It can be changed to “Check for the Compliance Issues generated for Device - Device hostname

13539:  FW-5520-2
13540:  Library group ""inventory"" method ""get_network_device_info"" returned in 0:00:00.000013
13541:  Library group ""inventory"" method ""find_uuid_of_device"" returned in 0:00:00.000268
13542:
13543:
13544:   api_switch_call called:
13545:  {'params': {'diffList': 'true', 'complianceType': 'NETWORK_SETTINGS'}}
13546:  Resource path full url: [https://10.195.243.109/api/v2/data/compliance/1df593f9-20ee-46fa-8276-a7be9be15d3c/detail|https://10.195.243.109/api/v2/data/compliance/1df593f9-20ee-46fa-8276-a7be9be15d3c/detail]
13547:  Calculating If the required result found  in the Response!!
13548:  Result Generated is ['Wireless Service Assurance Configuration']

→ Here, NON_COMPLIANT state for device can be changed to, Compliance status of device - Device hostname is <Compliance state>

13548:  Result Generated is ['Wireless Service Assurance Configuration']
13549:  Following Issues is not found {'DNS Server'} on device FW-5520-2. EXPECTED ISSUES {'Wireless Service Assurance Configuration', 'DNS Server'}
13550:  Following Issue has been found: {'NetworkAssuranceConfig': {'Network Assurance Status': {'intended': 'Enable', 'actual': 'Disable'}}}
13551:  NON_COMPLIANT

→ After getting “Intended Value has been fixed!!!” message or Intended value in case is not fixed, after applying remediation, message should be printed if remediation is successful for that particular device - Device name or remediation failed for the device - Device name.

*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/Network-Settings+Compliance|https://wiki.cisco.com/display/EDPEIXOT/Network-Settings+Compliance]

*Failed log on Halleck* - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May29_00:18:17.094543.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May29_00:18:17.094543.zip&atstype=ATS] → Refer TC253",2023-05-29T15:53:01.798+0000,"[~accountid:62d2fec15d6f5fd2c3db8f9f] , the fix has been already merged [~accountid:62d2fed26eba7198372366ca] : Thanks a lot for the syncup to discuss and clarfication on the issues!

As discussed, Please find the below comments for each of the each issue above.

*1)* For issue a), its a minor change and we will be making the changes

*2)* For issue b), below were the fix commits added by you

→ Commit on Halleck - [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/2db08e013ffb1c20bb767bf75db6f62199a5901f|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/2db08e013ffb1c20bb767bf75db6f62199a5901f] 

We will try to pull latest code and verify for this.

→ Commit on Ghost -  [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a10a3721a6a61468c21841d18d21d2c0707e395c|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a10a3721a6a61468c21841d18d21d2c0707e395c] 

We will try to pull latest code and verify for this.

→ Commit on Hulk will be merged by [~accountid:62ab7a399cd13c0068b18fe0]  as per you discussed with him.



*3)* For issues c) and d) , agreed to be addressed. [~accountid:62d2fed26eba7198372366ca] :  I have re-executed the TC on Halleck (private/Halleck-ms/api-auto) latest code base and we see *issue b)* is no more observed. However we observed new script failures during

i) Validation of cli for DNS on Polaris based Wireless controllers. Could be a defect again, analysis is in Progress.

ii) Compliance Remediation workflow failure on ECA 9300 device. Reported defect for this - [https://cdetsng.cisco.com/summary/#/defect/CSCwf48529|https://cdetsng.cisco.com/summary/#/defect/CSCwf48529]

iii) This has to be fixed from script side. Its showing 9800-1 and 9400 devices twice in the failed message.

14800: Failed to Apply Compliance Issues on the device: NY-ECA-9300!!
14801: Remediation failed on ['FW-5520-2', 'FW-5520-1', 'FW-9800-1', 'FW-9800-1', 'NY-FE-9400', 'NY-FE-9400']
 [~accountid:62d2fec15d6f5fd2c3db8f9f] , I see you have marked this as Blocked.

Is it because of the bug? If yes, the status of the bug states as “Unreproducible”. So, please give a re-try.

[~accountid:62d2fed26eba7198372366ca] , do we have any ETA for the fix against suggestion on the script code from Sandeep? Hi [~accountid:62ab7a399cd13c0068b18fe0] ,

[~accountid:62d2fec15d6f5fd2c3db8f9f] has requested a few enhancements in logging!!. Apart from this, there is no script issues and I would pick these enhancements by next week.  [~accountid:62d2fed26eba7198372366ca] : Thanks for syncup over the webex to discuss on the issues related to this TC.

Below are the list of points we discussed and which has to be addressed:

1) Issue *a)* as mentioned in JIRA description is resolved. We have added that cosmetic change needed.

2) Issue *b)* as mentioned in JIRA description has been resolved. The issue is no more observed.

3) Issue *c) & d)* as mentioned in JIRA description is yet to be addressed.

4) Incorrect cli show command check has been passed to check for presence of ip name-server configs and network assurance summary checks on ECA/9800 EWLC devices, post Compliance remediation. This has to be fixed. Due to this the script is failing despite the actual configs existing. 

*Error snips:*

{{14139: Compliance Issue for ip name-server 204.192.3.40 on dev FW-9800-1 not found!!}}

{{14181: Compliance Issue for Network-Assurance\s+:\s+True|Disabled on dev FW-9800-1 not found!!}}

{{14483: Compliance Issue for ip name-server 204.192.3.40 on dev NY-FE-9400 not found!!}}

{{14525: Compliance Issue for Network-Assurance\s+:\s+True|Disabled on dev NY-FE-9400 not found!!}}

{{14855: Compliance Issue for ip name-server 204.192.3.40 on dev NY-ECA-9300 not found!!}}

{{14897: Compliance Issue for Network-Assurance\s+:\s+True|Disabled on dev NY-ECA-9300 not found!!}}

5) In case of remediation failure on devices, the final message with device list has device name repeated twice. This has to be fixed to reflect only once.

*Error snip:*

{{14898: Remediation failed on ['FW-9800-1', 'FW-9800-1', 'NY-FE-9400', 'NY-FE-9400', 'NY-ECA-9300', 'NY-ECA-9300']}}

*Latest Failed log on Ghost P2* - [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2635494&size=598230&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun28_06:14:51.961419.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2635494&size=598230&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun28_06:14:51.961419.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]  As per discussion with [~accountid:62d2fed26eba7198372366ca] , Tentative ETA to address issues as in 3), 4) & 5) mentioned below is by 21st July 2023. [~accountid:62d2fed26eba7198372366ca] had a look into the issue and applied required fixes. 

He has provided a local pass log with fixes to be reviewed before merging - [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1524588&size=303063&archive=%2Froot%2Fpyats%2Fusers%2Froot%2Farchive%2F23-07%2Fsanity_TB1.2023Jul24_09:25:03.265840.zip&ats=%2Froot%2Fpyats&submitter=root&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1524588&size=303063&archive=%2Froot%2Fpyats%2Fusers%2Froot%2Farchive%2F23-07%2Fsanity_TB1.2023Jul24_09:25:03.265840.zip&ats=%2Froot%2Fpyats&submitter=root&from=trade&view=all&atstype=pyATS]

Need to review on this and update. [~accountid:62d2fed26eba7198372366ca] : 

*A)* As per the issues posted in comment - [https://miggbo.atlassian.net/browse/SEEN-1734?focusedCommentId=79125|https://miggbo.atlassian.net/browse/SEEN-1734?focusedCommentId=79125], with respect to the shared local pass log with fix added , *Issue 4)* and *Issue 5)* has been resolved and looks fine. The corresponding fix changes can be merged.
But *Issue 3)* is yet to be addressed (enhancing the logging).

*B)* Also another issue as we discussed to be fixed is - false pass during Compliance remediation job failure. Despite the “NCSP11205: Compliance remediation” failure error for Remediation task, we see corresponding check has been marked passed. This has to be fixed.

*False Pass log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_mb_multi_sites_mdnac.2023Jul21_11:45:29.076034.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_mb_multi_sites_mdnac.2023Jul21_11:45:29.076034.zip&atstype=ATS] -> Refer subTC 253.2 [~accountid:62d2fec15d6f5fd2c3db8f9f] , 

Also another issue as we discussed to be fixed is - false pass during Compliance remediation job failure. Despite the “NCSP11205: Compliance remediation” failure error for Remediation task, we see corresponding check has been marked passed. This has to be fixed.



This is has been taken care Already and the Logging device name has been Included 



[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/services/dnaserv/lib/api_groups/Compilance/group.py?until=5cd8a4d7a469576c7ff345f1e617decd213f294d&untilPath=services%2Fdnaserv%2Flib%2Fapi_groups%2FCompilance%2Fgroup.py&at=refs%2Fheads%2FSEEN-1734|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/services/dnaserv/lib/api_groups/Compilance/group.py?until=5cd8a4d7a469576c7ff345f1e617decd213f294d&untilPath=services%2Fdnaserv%2Flib%2Fapi_groups%2FCompilance%2Fgroup.py&at=refs%2Fheads%2FSEEN-1734] [~accountid:62d2fed26eba7198372366ca] : Thanks for adding the intended fixes! Could you please merge all the code changes to Ghost - private/Ghost-ms/api-auto , Halleck - private/Halleck-ms/api-auto and Hulk - private/Hulk-ms/api-auto  code bases? Hi [~accountid:62d2fed26eba7198372366ca] ,
After cherry picked the changes in  private/Ghost-ms/sanity-api-auto, again we are facing same issue 

Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1433341&size=11269&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug10_22:52:26.252565.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1433341&size=11269&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug10_22:52:26.252565.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Hi [~accountid:63f50bf84c355259db9ccc59] 



Looks in the Some the proc name has been changed. Would be address the commit and reverting the issue

!image-20230811-064611.png|width=1024,height=523!





Recent commit 



!image-20230811-064744.png|width=1321,height=539! Hi [~accountid:62d2fed26eba7198372366ca] ,

Got a pass log after changes done by Vinay using below branch 
Branch:[SEEN-1734-Ghost|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits?until=refs%2Fheads%2FSEEN-1734-Ghost]
Pass log :

[Test_TC213_remediation_network_settings|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1298738&size=231726&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug11_03:57:24.552779.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [~accountid:62d2fed26eba7198372366ca] : Thanks for adding the fixes for various mentioned issues. As discussed below are the PRs raised for each of the releases. 

Hulk - [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6559/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6559/overview]
Halleck - [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6571/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6571/overview]
Ghost - [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6570/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6570/overview]

Have added a comment for one of the log info errors to be corrected. Could you please have a look ? Hi [~accountid:62d2fed26eba7198372366ca] ,

After cherry picking your commint to sanity branch also on AWS Sanity TC Failed with compliance reasons.
Compliance is failing for ECA and EWLC devices
Failed log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11316343&size=258409&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug11_05:38:51.606843.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11316343&size=258409&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug11_05:38:51.606843.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Merged the PRs below.
[~accountid:62d2fec15d6f5fd2c3db8f9f] , [~accountid:61efa8c457b25b006877eda3] , please reopen if still see the issue. [~accountid:62d2fe9f8afb5805e5d5af49], [~accountid:61efa8c457b25b006877eda3]  has already shared the execution log stating the fix is not helping.

I’m reopening this Auton to get it concluded from reporter end. [~accountid:62d2fed26eba7198372366ca] Can you recheck it? [~accountid:5f3c6ae932360700388f7b4b] , The Issues has been addressed and the below log is shared from my [~accountid:63f50bf84c355259db9ccc59] 



[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1298738&size=231726&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug11_03:57:24.552779.zip&ats=%2Fauto%2Fdna-|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1298738&size=231726&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug11_03:57:24.552779.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Hi  [~accountid:62d2fed26eba7198372366ca]  ,
Could you please commit this feature to the optimized code 
Ghost & Hulk  Branch :
[lansanity_usecases_maps.yaml|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/lansanity/lansanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FGhost-ms%2Fapi-auto]
*nonlansanitysuite*  /sanity_usecases_maps.yaml

[dnac-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse?at=refs%2Fheads%2Fprivate%2FGhost-ms%2Fapi-auto]/[testcases|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases?at=refs%2Fheads%2Fprivate%2FGhost-ms%2Fapi-auto]/*sanityusecases*/ [~accountid:62d2fed26eba7198372366ca], after adding the required Optimized script both Ghost and Hulk, I had executed the Compliance test-case with TB7 Ghost P2 RC2 where [test2_verify_remediation_and_verify_remdiation|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-compliance_for_network_settings.py-281-complianceForNetworkSettings&begin=238473&size=32408&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug17_11:45:28.048323.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] has failed.

Please check and let us know if you require testbed to conclude on it. Hi [~accountid:62ab7a399cd13c0068b18fe0] ,

looks like you are hitting below issue

[https://cdetsng.cisco.com/summary/#/defect/CSCwf48529|https://cdetsng.cisco.com/summary/#/defect/CSCwf48529]



[https://miggbo.atlassian.net/browse/SEEN-1741|https://miggbo.atlassian.net/browse/SEEN-1741|smart-link]  Hi [~accountid:62d2fed26eba7198372366ca]  / [~accountid:62ab7a399cd13c0068b18fe0]  / [~accountid:62d2fec15d6f5fd2c3db8f9f] ,

We got a pass log for AWS Sanity with Ghost P2 RC2 ,hence closing the jira
[https://ngdevx.cisco.com/services/taas/results/1a7ac2df-bb91-4f13-b07d-b577681bf4e2/run-results|https://ngdevx.cisco.com/services/taas/results/1a7ac2df-bb91-4f13-b07d-b577681bf4e2/run-results]
We observed still the ask for  below request:
”After getting “Intended Value has been fixed!!!” message or Intended value in case is not fixed, after applying remediation, message should be printed if remediation is successful for that particular device - Device name or remediation failed for the device - Device name.” 
not yet fixed

17702: 2023-08-17T22:37:32:  Library group ""Compilance"" method ""perform_remediation_check"" returned in 0:07:32.416830

{noformat}17703: 2023-08-17T22:37:32:  Test returned in 0:07:32.417977{noformat}

{noformat}17704: 2023-08-17T22:37:32:  Passed reason: Remediation Verification was a Success{noformat}

{noformat}17705: 2023-08-17T22:37:32:  The result of section test2_verify_remediation_and_verify_remediation is => PASSED{noformat}



Thanks,
Anusha John","['Auton', 'Ghost', 'Halleck', 'Hulk', 'Integration', 'IntegrationIssue', 'MSTB1', 'Multisite', 'Sanity']",Vinay Raj V ,Closed,SANDEEP SHIVARAMAREDDY
SEEN-1735,https://miggbo.atlassian.net/browse/SEEN-1735,MSTB2(Non-DR) : Test_TC190_Monitor_And_Debug_BGP_Session_Status_Issues_Between_Border_And_Control_Plane/ test2_collect_data_from_border_to_control_plane_on_provision_page is,"Hi [~accountid:63f50bcafb3ac4003fa2c6dd],

As discussed Subtestcase is failing due to API change on subtestcase_1 and subtestcase_5

At subtest 1: {{test1_get_data_from_border_to_control_plane_on_assurance_page}}there is a change API on Halleck, so the subtest gets the wrong responses. That is why subtest 3 compare gets fail.



At subtest 5: {{test5_get_data_transitsda_on_assurance_page}}is a change API on Halleck, so the subtest gets the wrong responses. That is why subtest 7 compare gets fail.

Fail log :  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1667599&size=116305&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May26_07:44:50.820614.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1667599&size=116305&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May26_07:44:50.820614.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

Branch : private/Halleck-ms/api-auto
Script : solution_test_3sites_sjc_nyc_sf",2023-05-30T03:59:10.500+0000,"Hi [~accountid:63f50bf0e8216251ae4d59ca] [~accountid:62ab7a399cd13c0068b18fe0], I just update API to get data from Border to Control Plane on assurance page and fix function compare. Could you please help me review this PR:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5849/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5849/overview]

*Branch Used:* private/Halleck-ms/api-auto

*Testbed:* mstb2

*Job file:* testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py

*Trade log link:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-05/sr_mb2_three_sites.2023May29_23:55:02.615671.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-05/sr_mb2_three_sites.2023May29_23:55:02.615671.zip&atstype=ATS] Issue got fixed after pulling the latest code
Passlog : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1675451&size=146547&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May30_02:38:12.847732.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1675451&size=146547&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May30_02:38:12.847732.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",['Auton'],NhanHuu Nguyen,Closed,Divakar Kumar Yadav
SEEN-1739,https://miggbo.atlassian.net/browse/SEEN-1739,[GHOST]  Test_TC183_advanced_wlan_configs,"DNAC Release_Version Tested: Ghost RC5 Uber ISO - 2.1.610.70190, Non-FIPS

Device Image Used: 17.10.1

Testbed: AWS-Multisite

Branch Used: private/Ghost-ms/sanity_api_auto

Script Name: solution_test_3sites_sjc_nyc_sf.py

Solution Input File : dnac-auto/configs/config_48hr_test/solution_test_input.json

Testcases Impacted:   Test_TC183_advanced_wlan_configs 

Failed Trade Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1748895&size=41326151&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fauto_MS_job.2023Feb22_05:19:30.716600.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1748895&size=41326151&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-02%2Fauto_MS_job.2023Feb22_05:19:30.716600.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-05-30T14:26:56.155+0000,"Hi I am getting the below error while executing the script failed to create ssids 12158: 
 Provisioning of device-set (10 devices) is done successfuly

{noformat}12159: 
 Library group ""provision"" method ""provision_devices"" returned in 0:01:03.194137{noformat}

{noformat}12160: 
 WLC provisioned correctly{noformat}

{noformat}12161: 
 Library group ""advanced_wlan_configs"" method ""add_advanced_wlan_configs_ssids"" returned in 0:01:40.955515{noformat}

{noformat}12162: 
 Test returned in 0:01:40.956602{noformat}

{noformat}12163: 
 Failed reason: Failed to create ssid, add to nw profile or provision{noformat}

{noformat}12164: 
 The result of section test2_add_new_ssids is => FAILED{noformat} Hi Executed the script with latest change file still cases are failing Failed log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1768801&size=1197221&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fauto_MS_job.2023Jun12_21:20:01.633128.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1768801&size=1197221&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fauto_MS_job.2023Jun12_21:20:01.633128.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

{noformat}11173: 
 Library group ""advanced_wlan_configs"" method ""add_advanced_wlan_configs_ssids"" returned in 0:01:47.739807{noformat}

{noformat}11174: 
 Test returned in 0:01:47.741049{noformat}

{noformat}11175: 
 Failed reason: Failed to create ssid, add to nw profile or provision{noformat}

{noformat}11176: 
 The result of section test2_add_new_ssids is => FAILED{noformat} Some new code has been added, please give it a retry. [[SEEN-1862] 8/[Auton] [Ghost] - Advanced wlan feature where newly created SSIDs are missed with segment onboarding - Jira (atlassian.net)|https://miggbo.atlassian.net/browse/SEEN-1862] here’s the ticket that I resolved regarding tihs.",['Auton'],Andrew Chen,Resolved,Balaji Raju
SEEN-1740,https://miggbo.atlassian.net/browse/SEEN-1740, Test_TC219_configure_device_into_specific_NDG  /   test1_assign_tag_to_device ,"*Reporter Analysis:* 
We observed in Sanity Ghost run tc is failing for tagging(eWLC) the device.
+*Description:*+  
7111: 
 Error Code: 400 for

{noformat}7112: 
 URL:https://10.30.0.100/api/v1/ncp-node/graphql Data:{'timeout': 60, 'data': '{""operationName"": null, ""variables"": {}, ""query"": ""{\\n  allDeviceStore(limit: 100, sortBy: {field: \\""deviceTags.name\\"", order: \\""asc\\""}, offset: 1) {\\n    totalCount\\n    items {\\n      deviceTags {\\n        name\\n        __typename\\n      }\\n      deviceTags {\\n        id\\n        __typename\\n      }\\n      deviceSupportLevel\\n      hostname\\n      family\\n      softwareType\\n      managementIpAddress\\n      blueBeaconStoreStatus {\\n        beaconLedStatus\\n        metadata {\\n          equipmentName\\n          beaconLedStatus\\n          __typename\\n        }\\n        __typename\\n      }\\n      wirelessInfo {\\n        netconfEnabled\\n        isHA\\n        haSyncStatus\\n        __typename\\n      }\\n      ftdHAPairStatus {\\n        status\\n        __typename\\n      }\\n      ftdHAPairStatus {\\n        pairedManagementIpAddress\\n        __typename\\n      }\\n      ftdHAPairStatus {\\n        pairedHostName\\n        __typename\\n      }\\n      ftdHAPairStatus {\\n        pairedStatus\\n        __typename\\n      }\\n      ftdHAPairStatus {\\n        pairedDeviceId\\n        __typename\\n      }\\n      managementIpAddress\\n      family\\n      description\\n      macAddress\\n      id\\n      __typename\\n    }\\n    __typename\\n  }\\n}\\n""}'}{noformat}


*Branch Name:  private/Halleck-ms/sanity_api_auto*

*Script file/Usecase:* 
[Test_TC219_configure_device_into_specific_NDG|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1345858&size=55868&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May30_01:00:42.484428.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  
*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

!image-20230530-163805.png|width=1463,height=636!

*Fail Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1346422&size=50765&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May30_01:00:42.484428.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1346422&size=50765&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May30_01:00:42.484428.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

could you please check ..",2023-05-30T16:34:23.654+0000,"* PR links: 
** Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5869/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5869/overview]
** Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5868/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5868/overview]
* Test Cases Summary: change new API to get all device in inventory
* Trade log link: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-05/sanity_TB1.2023May31_00:45:08.172583.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-05/sanity_TB1.2023May31_00:45:08.172583.zip&atstype=ATS] Hi [~accountid:63f50bcf4e86f362d39acde5] ,
Manually able to tag  ""SN-FOC2311Y129"" ,but TC is failing. Could you please check

+*Failed log 1st Run :*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1345860&size=643732&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May31_05:22:40.409911.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1345860&size=643732&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May31_05:22:40.409911.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

2nd  attempt ,  we observed 219.1 tc is passed( [test1_assign_tag_to_device|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1346433&size=636117&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May31_05:49:32.331974.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS])==Passed   . [test2_check_device_on_ISE|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1982550&size=4343&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May31_05:49:32.331974.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  ==> failed 




!image-20230531-124616.png|width=1190,height=118!



  I think it likes testbed failed. i ran and it’s work fine
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-06/sanity_TB1.2023Jun20_02:02:35.227171.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-06/sanity_TB1.2023Jun20_02:02:35.227171.zip&atstype=ATS]

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-06/sanity-intg2.2023Jun20_00:47:10.325505.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-06/sanity-intg2.2023Jun20_00:47:10.325505.zip&atstype=ATS]

by the way, i have some code enhencement. PR-link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5512/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5512/overview] Hi [~accountid:63f50bcf4e86f362d39acde5]  ,
we observed tc is failing  with  below error , could  you please check,
Build :Hulk 2.1.710.70344 
*Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1338295&size=293171&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun20_05:06:55.612381.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1338295&size=293171&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun20_05:06:55.612381.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Error snip :* 


{code:python}7127:  Traceback (most recent call last):
7128:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
7129:      result = testfunc(func_self, **kwargs)
7130:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 15340, in test1_assign_tag_to_device
7131:      result, self.device, self.tag_name = dnac_handle.assign_tag_to_device(""NDG:Location#All Locations#CHN"")
7132:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
7133:      result = method(*args, **kwargs)
7134:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/api_groups/inventory/group.py"", line 2253, in assign_tag_to_device
7135:      if all_device[""response""] is not None:
7136:  TypeError: list indices must be integers or slices, not str
7137:  Test returned in 0:00:00.884037
7138:  Errored reason: list indices must be integers or slices, not str{code}

 

!image-20230620-122759.png|width=605,height=325! i’ve added some code enhencement in last PR. If you use private/Halleck-ms/api-auto, you will catch error. Can you use *private/Halleck-ms/SEEN-1479* to trigger testcase?

!image-20230621-034855.png|width=632,height=188! Hi [~accountid:63f50bcf4e86f362d39acde5]  ,
We don’t have a cluster with Halleck build, could you please fix Hulk Branch.
we can use below cluster for 1 hour,it’s  running  with Hulk 70344
Once that is done, please let me know,
Testbed (10.30.0.100-admin/Maglev123) 
 Hulk works fine. Please give it a try

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-06/sanity-intg2.2023Jun21_00:36:05.892783.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-06/sanity-intg2.2023Jun21_00:36:05.892783.zip&atstype=ATS] Hi [~accountid:63f50bcf4e86f362d39acde5]  ,
Issue is not resolved after merging PRs     ([https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6048/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6048/overview]) ,
Manullya able to tag :
{{SN-FDO2515JDSL.}} but tc is failing  with  below error : 
7186: 
  api_switch_call called:

{code:python}7186:   api_switch_call called:
7187:  {}
7188:  Resource path full url: https://10.30.0.100/api/v1/task/3e12ed8c-b294-422e-82f8-029587643a6c
7189:  {'endTime': 1687334033506, 'version': 1687334033507, 'startTime': 1687334033392, 'errorCode': 'UnknownGroupingError', 'progress': 'Tag membership update failed', 'operationIdList': ['25df9d3a-e794-4a98-8804-667e7937d1bd'], 'serviceType': 'Grouping Service', 'failureReason': 'One or more tag ids does not exist', 'isError': True, 'instanceTenantId': '64875b633b187c27377dfc44', 'id': '3e12ed8c-b294-422e-82f8-029587643a6c'}
7190:  Library group ""task"" method ""wait_for_task_complete"" returned in 0:00:02.107629
7191:  Something wrong happend when assign tag to device SN-FDO2515JDSL.cisco.com. Check it out
7192:  Library group ""inventory"" method ""apply_tag_to_device"" returned in 0:00:02.393273
7193:  Library group ""inventory"" method ""assign_tag_to_device"" returned in 0:00:02.986060
7194:  Test returned in 0:00:02.987258
7195:  Failed reason: Something wrong happend when assign tag to device{code}


+*Failed Log:*+
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun21_00:46:18.188758.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun21_00:46:18.188758.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]

!image-20230621-083531.png|width=1502,height=422! [~accountid:63f50bcf4e86f362d39acde5] since the issue is not yet resolved, I am moving this Auton back to “In Progress” state.

Please work with Omkar to get it resolved and validated on one of the Sanity Testbed. Hulk 2370-70402
Pass Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1348440&size=691507&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul11_00:42:48.895284.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1348440&size=691507&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul11_00:42:48.895284.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Hi [~accountid:63f50bcf4e86f362d39acde5]  ,

In the recent Hulk RC4, the test case is failing again with the following error. However, it is working fine manually. Could you please check this?

Failed Log:


[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=39459902&size=4623&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug09_10:26:43.039712.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=39459902&size=4623&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug09_10:26:43.039712.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



!image-20230811-150238.png|width=1920,height=812!




!image-20230811-150252.png|width=1910,height=863! hi Omkar. Which branch did you use? Hi [~accountid:63f50bcf4e86f362d39acde5]  ,

Branch=>
private/Hulk-ms/sanity_api_auto

 i got no error in my testbed: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-08/sanity-intg2.2023Aug16_23:36:14.017473.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-08/sanity-intg2.2023Aug16_23:36:14.017473.zip&atstype=ATS]

can you share your testbed for me to test it? since no update from reporter, i marked it as “close”. Please reopen it whenever you hit error again Hi [~accountid:63f50bcf4e86f362d39acde5]  ,


The recent Hulk execution run is blocked due to this[CSCwh53512|https://cdetsng.cisco.com/webui/#view=CSCwh53512]  defect. I'll check in the optimization run, and if the test case fails, I'll reopen it.""

Thanks,
Omkar  Hi [~accountid:63f50bcf4e86f362d39acde5] ,

On recent hulk testing we are seeing TC was failed
Failed log:
[Test_TC217_configure_device_into_specific_NDG|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2488181&size=223464&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep13_03:55:20.105951.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] Hi  [~accountid:63f50bcf4e86f362d39acde5]  ,
Could  you  please  check .. Hi Omkar, Tulasi

it said: “{{Error Code: 403: message"":""Role does not have valid permissions to access the API""}} it mean something happend in other side, like using not authorize user, etc

i suggest you verify it with difference user, or just try to exec the testcase again Since No update from reporter, i will close this auton to reduce number of auton. Please re-open it if you see something wrong Hi  [~accountid:63f50bcf4e86f362d39acde5]  ,
In Recent   Hulk Patch-1 RC3 2.1.713.70319 *optimization* run   TC failed with  below error:
could could you please  check 


Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_tag_config_in_ISE_NDG.py-179-ISENDG_tagDeviceInISENDG&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_10:06:54.734792.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_tag_config_in_ISE_NDG.py-179-ISENDG_tagDeviceInISENDG&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_10:06:54.734792.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]

{noformat}92:  Traceback (most recent call last):
93:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper
94:      result = testfunc(func_self, **kwargs)
95:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/testcases/sanityusecases/ISENDG/tagDeviceInISENDG/verify_tag_config_in_ISE_NDG.py"", line 95, in test1_assign_tag_to_device
96:      result, self.device, self.tag_name = dnac_handle.assign_tag_to_device(""NDG:Location#All Locations#CHN"")
97:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/decorators.py"", line 32, in wrapper
98:      result = method(*args, **kwargs)
99:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/inventory/group.py"", line 2525, in assign_tag_to_device
100:      return self.apply_tag_to_device(device[""hostname""], list_device_tag_id), tag_name
101:  KeyError: 'hostname'
103:  Errored reason: hostname{noformat} hi [~accountid:620b8357878c2f00729881c8], which branch did you using? Hi [~accountid:63f50bcf4e86f362d39acde5]  ,
I am  using  below  branch with updated  code ;

private/HulkPatch-ms/sanity_api_auto

PR:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ee13750bc0d8d34c223c6b4cc0b91ce04eb9b730|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ee13750bc0d8d34c223c6b4cc0b91ce04eb9b730]   Hi [~accountid:620b8357878c2f00729881c8]

PR for branch *private/Hulk-ms/api-auto* is already merged. Can you please chery-pick to you branch? 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5868/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5868/overview] since no update from reporter and the failed is not related to this Jira ticket, i will close this auton. please raised another auton if needed","['AWS-Santiy', 'Auton', 'Ghost', 'Halleck', 'Integration', 'Optimized', 'Sanity']",QuangVinh Nguyen,Closed,Omkar Sharad Wagh
SEEN-1754,https://miggbo.atlassian.net/browse/SEEN-1754,Test_TC65_create_wired_sensors_test_suite related use-case is missing decorator calls inside few scripts,Test_TC65_create_wired_sensors_test_suite related use-case is missing decorator calls inside few scripts.,2023-05-31T06:44:21.976+0000,Created a PR where the decorator functions have been placed before {{test1_create_wired_sensors_test_suite}}  [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5867/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5867/overview],['Auton'],Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1760,https://miggbo.atlassian.net/browse/SEEN-1760,[Auton]:Task-sda_fabric_roles_deploy.py-101-SDADevicesAndFabricRoles/Test_TC1_DNAC_assigne_dhcp_role_deploy_device_in_fabric/test1_verify_assign_roles_and_deploy_devices_on_fabric1_site_san_jose/," we observed sanity Hulk run, In non lan sanity optimized code where sda_fabric_roles_deploy tc failed , its failing to assign site in device fabric role in SVL device.
*Uber ISO Version tested:* Hulk 2.3.7.0-70301
*Script Name:* Optimized code-nonlansanitysuite
Failed log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-sda_fabric_roles_deploy.py-101-SDADevicesAndFabricRoles&begin=4977&size=82900&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May30_19:15:53.910906.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-sda_fabric_roles_deploy.py-101-SDADevicesAndFabricRoles&begin=4977&size=82900&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May30_19:15:53.910906.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Error snip:

{noformat}174: 
 Traceback (most recent call last):{noformat}

{noformat}175: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper{noformat}

{noformat}176: 
     result = testfunc(func_self, **kwargs){noformat}

{noformat}177: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/testcases/sanityusecases/SDADevicesAndFabricRoles/sda_fabric_roles_deploy.py"", line 105, in test1_verify_assign_roles_and_deploy_devices_on_fabric1_site_san_jose{noformat}

{noformat}178: 
     if (dnac_handle.assign_roles_deploy_fabric(""Global/USA/SAN_JOSE"", transitvn=""iptransit"",{noformat}

{noformat}179: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/fabric_wired/group.py"", line 315, in assign_roles_deploy_fabric{noformat}

{noformat}180: 
     entry[""transitNetworks""].append({""idRef"":transit_domain_id}){noformat}

{noformat}181: 
 UnboundLocalError: local variable 'transit_domain_id' referenced before assignment{noformat}

{noformat}182: 
 Test returned in 0:00:01.045332{noformat}

{noformat}183: 
 Errored reason: local variable 'transit_domain_id' referenced before assignment{noformat}",2023-05-31T16:45:45.158+0000,"@ tran : could you please check on priority
Execution was blocked due to this issue Fix: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/af5062089901fc5dbbeb9db0c706f17e446479ad|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/af5062089901fc5dbbeb9db0c706f17e446479ad] [~accountid:62d2fe9f8afb5805e5d5af49] 
We are seeing same issue even after cherry picking the commit to private/Hulk-ms/sanity_api_auto
ss+commit id:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/69e5797495729c060d8e4c421f4f31fd3d9646ea|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/69e5797495729c060d8e4c421f4f31fd3d9646ea]

failed log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=467428&size=75224&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun01_10:12:53.807667.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=467428&size=75224&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun01_10:12:53.807667.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Hi Elton,

 

Currently, your cluster was missing ip transit , you need to run the ‘verify_creating_ip_transit’ test that was added in the fix.

 

Thanks,

Tran [~accountid:62d2fe9f8afb5805e5d5af49] Thanks, I have verified it now
it's working after fix.","['Auton', 'Hulk', 'Optimized', 'Sanity']",Tran Lam,Resolved,Elton GoldChristopher
SEEN-1761,https://miggbo.atlassian.net/browse/SEEN-1761,Auton-Ghost:Test_TC206_wireless_client_data_troubleshoot  /   test1_setup_parameter,"*Reporter Analysis:* We have observed that testcase is getting errored due to below error:

{noformat}8900: 
     interface = self.get_interface_with_selected_device(head_name){noformat}

{noformat}8901: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/DMZ-Ghost-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper{noformat}

{noformat}8902: 
     result = method(*args, **kwargs){noformat}

{noformat}8903: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/DMZ-Ghost-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 9655, in get_interface_with_selected_device{noformat}

{noformat}8904: 
     for interface in interface_result.split(""\n""):{noformat}

{noformat}8905: 
 AttributeError: 'dict' object has no attribute 'split'{noformat}

{noformat}8907: 
 Errored reason: 'dict' object has no attribute 'split'{noformat}

And rest of the sub-tc is blocking due to this


 

*Description*:  

*Branch Name:* Ghost -ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:

**Fail Log:**
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1564610&size=42676&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun01_00:40:50.896185.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1564610&size=42676&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun01_00:40:50.896185.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-06-01T09:31:06.218+0000,"PR-link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5911/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5911/overview] Hi [~accountid:63f50bcf4e86f362d39acde5] 

I gave a try with the branch you gave:
private/Ghost-ms/SEEN-1761

But execution is now directly blocking with following error:
8657: 
 This run does not meet requirement

{noformat}8658: 
 Can not get the correct WLCs and instanceUuid{noformat}

It couldn’t even fetch ewlc on the cluster 

Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1563146&size=41866&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun05_05:45:07.575274.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1563146&size=41866&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun05_05:45:07.575274.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Issue was not seen  in recent runs with Ghost P1 RC6 so closing the jira:
Pass log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4845835&size=295299&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul29_08:01:27.211742.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4845835&size=295299&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul29_08:01:27.211742.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Integration', 'Sanity']",QuangVinh Nguyen,Closed,Anusha John
SEEN-1763,https://miggbo.atlassian.net/browse/SEEN-1763,[Auton] - Multiple issues seen under Assurance stale session scenario Validation TC,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/Halleck-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Issues Faced:* 

When integrating the feature corresponding to Assurance Stale session scenario, we have observed below issues:

a) Under  subTC -  [test6_change_an_external_node_to_an_internal_border|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2574114&size=1524350&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May31_10:44:28.322513.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ Config preview for Fabric domain update fails with error - *_“14012:  Config Preview Activity failed with reason: Layer 3 Handoff VLAN cannot be changed. Remove the Layer 3 Handoff and re-add to update the VLAN”_* while device is being added back to Fabric. Not sure if the removal and add back of the border device is being done by making sure the right attributes are updated for the device before the “removal and re-add” operation. Because of this failure, Regular Fabric update operation also eventually fails.

→ There is no proper logging message at the beginning of the testcase as to which device is being removed and at the end of once the device gets removed successfully. Also logging message is needed once the device is added back and successful. 

→ There is no proper logging message for what Fabric role is being changed for which device. Also there is no logging message for to know what was the earlier role and what role was it changed later before removal of device. This info is important and needed.

→ Logging message *_“Failed reason: Result: Failed to read border device to fabric”_* has to be changed to *_“failed to re-add the Border device - device name to the fabric - Fabric  name”_*

b) Under subTC - [test9_configure_loopback_bgp_on_border|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6580856&size=37952&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May31_10:44:28.322513.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ It fails with undefined script error as below:

*21855:  Library method ""reconnect_clients"" returned in 0:00:01.512586*
*21856:  Traceback (most recent call last):*
*21857:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/services/commonlibs/test_wrapper.py"", line 301, in wrapper*
*21858:      result = testfunc(func_self, **kwargs)*
*21859:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py"", line 31811, in test9_configure_loopback_bgp_on_border*
*21860:      cd_name = get_cd_name_from_device(dnac_handle, dev)*
*21861:  NameError: name 'get_cd_name_from_device' is not defined*
*21862:  Test returned in 0:00:01.515486*
*21863:  Errored reason: name 'get_cd_name_from_device' is not defined*

c) Under subTC - [test7_check_the_device_360_after_change_to_an_internal_border|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4098464&size=643295&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May31_10:44:28.322513.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ Logging message is not there for which device the device 360 page is being queried for. It has to be added.

→ Connect score variable from the response for which value is being checked has to be printed along with the score obtained in device 360. Currently not sure which variable in the response has to be checked for the score as there are multiple score variables in the response obtained in log.

→ The print message for  “Value of connect score must be less than 1 since this is Internal Border!” has to be changed to also print the intended device name for which role was changed 

d) Under subTC - [test1_get_node_and_check_name_before_change|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2395963&size=39796&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May31_10:44:28.322513.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ Logging message has to be added with details of Dual Border devices present in which Fabric site and out of that which Device is chosen and with what Fabric role it has currently.

→ Unable to understand the log info - “Device not in map, updating map, retrying” message. Not sure what it means.

e) Under subTC - [test2_change_the_node_name|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2435759&size=73120&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May31_10:44:28.322513.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ Logging info of “Changing the hostname for device - device_name” has to be included.

f) Under subTC - [test3_confirm_with_the_new_name|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2508879&size=4493&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May31_10:44:28.322513.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ The confirmation of new device hostname does not have any device side logs or any assurance side  response indicating new device hostname after changing.

g) Under sub TC - [test4_change_back_the_original_name|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2513372&size=58371&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May31_10:44:28.322513.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ Logging info of “Changing the hostname for device - device_name back to earlier hostname ” has to be included.

 h) Under sub TC - [test5_confirm_with_the_original_name|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2571743&size=2371&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May31_10:44:28.322513.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ The confirmation of reverting the device hostname does not have any device side logs or any assurance side response indicating device hostname after changing back to original.

i) Under sub TC - [test8_change_back_to_an_external_node|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4741759&size=1839097&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May31_10:44:28.322513.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ There is no proper logging message for what Fabric role is being changed for which device. Also there is no logging message for to know what was the current role and what role its being changed after re-add of the device. This info is important and needed.

j) Under sub TC - [test10_check_the_device_360_after_change_back_to_an_external_border|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6618808&size=108343&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May31_10:44:28.322513.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ Logging message is not there for which device the device 360 page is being queried for. It has to be added.

→ Connect score variable from the response for which value is being checked has to be printed along with the score obtained in device 360. Currently not sure which variable in the response has to be checked for the score as there are multiple score variables in the response obtained in log.

→ No print message at the end as to which device connect score is being checked and what is expected and actual values. It only has large response for the url which has many values. Not sure which values we need to check specifically for.

*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/A+network+admin+should+be+able+to+see+correct+report+and+health+score+when+stale+session+scenario+happens|https://wiki.cisco.com/display/EDPEIXOT/A+network+admin+should+be+able+to+see+correct+report+and+health+score+when+stale+session+scenario+happens]

*Failed log on Halleck* - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May31_10:44:28.322513.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May31_10:44:28.322513.zip&atstype=ATS] → Refer TC266",2023-06-01T12:46:30.969+0000,"PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6092/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6092/overview]

Trade log link:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-07/multi_sites-intg2.2023Jul31_00:51:29.421430.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-07/multi_sites-intg2.2023Jul31_00:51:29.421430.zip&atstype=ATS]
 Hi [~accountid:62d2fec15d6f5fd2c3db8f9f], Could you please help me review this PR Auton? [~accountid:63f50bcafb3ac4003fa2c6dd] : I have added few review comments. Could you please have a look once? Hi [~accountid:62d2fec15d6f5fd2c3db8f9f], I already addressed all the comments on PR. [~accountid:63f50bcafb3ac4003fa2c6dd] : Thanks for addressing the comments! I had a look into them and have approved the same. Could you please cross check once again and merge the changes? [~accountid:62d2fec15d6f5fd2c3db8f9f], mention PR has been merged to Halleck Branch.

Same has been cherry-picked to Hulk branch. Please check on the same and confirm for the Closure. [~accountid:63f50bcafb3ac4003fa2c6dd] : On trying to test for the usecase using the Hulk branch on Hulk RC2 - 2.1.710.70462 we are still observing TC failures during the execution.

Below are the details:

1) [test6_change_an_external_node_to_an_internal_border|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2655300&size=756068&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb_multi_sites_mdnac.2023Aug01_12:47:58.169746.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
-> Failed due to config preview failure. From the reason of failure, it looks like the Border devices are not being updated with right device roles when changing the device roles.

*Error snip:*

15643:  Config Preview Activity failed with reason: Can not add device [SJ-FB-9500.cisco.com|http://SJ-FB-9500.cisco.com] as rendezvous point for multicast configuration as it does not belong to the same fabric site Global/USA/SAN_JOSE.
15644:
15645:  Activity: e752f07a-e78c-487f-a380-8334d2fd278c Trigger job: {'id': '8df578b7-17a5-4fc6-9daa-59315c7c2e27', 'triggeredJobTaskId': '6166e088-4ce4-443c-8f43-7bef99fa675c', 'triggeredTime': 1690920177843, 'status': 'FAILED', 'failureReason': 'Can not add device [SJ-FB-9500.cisco.com|http://SJ-FB-9500.cisco.com] as rendezvous point for multicast configuration as it does not belong to the same fabric site Global/USA/SAN_JOSE.', 'triggeredJobId': '8df578b7-17a5-4fc6-9daa-59315c7c2e27'}

2) [test7_check_the_device_360_after_change_to_an_internal_border|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3411368&size=649885&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb_multi_sites_mdnac.2023Aug01_12:47:58.169746.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
-> Failed as a consequence failure as in 1) above

3) [test8_change_back_to_an_external_node|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4061253&size=751755&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb_multi_sites_mdnac.2023Aug01_12:47:58.169746.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
-> Same config preview error as in 1) above.

*Failed log:* [Test_TC266_Check_correct_issue_report_and_health_score_when_stale_session_scenario_happens|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2434398&size=3942027&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb_multi_sites_mdnac.2023Aug01_12:47:58.169746.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] [~accountid:62d2fec15d6f5fd2c3db8f9f], Which testbed do you use to run? I ran the script on INTG2 Testbed and it work fine. [~accountid:63f50bcafb3ac4003fa2c6dd] : We are using DR+MDNAC multisite testbed for execution. 

Script used for exection: dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py [~accountid:63f50bcafb3ac4003fa2c6dd], do we have any progress update on this Auton?  [~accountid:62ab7a399cd13c0068b18fe0], I am finding a solution to fix the error of test6_change_an_external_node_to_an_internal_border on the DR+MDNAC multisite testbed. [~accountid:63f50bcafb3ac4003fa2c6dd] : Any update on this issue? Were you able to address this issue and add the fix on Hulk and Hulk Patch branches? [~accountid:63f50bcafb3ac4003fa2c6dd] : Were u able to looking into this issue?  Yes, I’m going to take a look into this issue.","['Auton', 'Halleck', 'Hulk', 'Integration', 'MSTB1', 'Multisite']",NhanHuu Nguyen,Open,SANDEEP SHIVARAMAREDDY
SEEN-1767,https://miggbo.atlassian.net/browse/SEEN-1767,"Creation of SSID via BAPI payload need to be changed for radiopolicy with ""2.4GHz Only"", ""5GHz Only"", ""6GHz Only"".","Creation of enterprise SSID via BAPI is failing due to payload validation is added in Hulk.

Branch : private/Hulk-ms/sanity_api_auto

Failed Logs: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-base_wireless_ssid_profile_guest.py-53-designWireless&begin=106342&size=78615&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May24_11:15:49.676522.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-base_wireless_ssid_profile_guest.py-53-designWireless&begin=106342&size=78615&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May24_11:15:49.676522.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Working Payload:

{
	""name"": ""testtb13"",
	""passphrase"": """",
	""enableFastLane"": ""True"",
	""enableMACFiltering"": ""False"",
	""trafficType"": ""voicedata"",
	""radioPolicy"": ""2.4GHz Only"",
	""fastTransition"": ""Disable"",
	""securityLevel"": ""OPEN"",
	""enableSessionTimeOut"": ""True"",
	""sessionTimeOut"": 1800,
	""enableClientExclusion"": ""True"",
	""enableBasicServiceSetMaxIdle"": ""True"",
	""basicServiceSetClientIdleTimeout"": 300,
	""enableDirectedMulticastService"": ""True"",
	""enableNeighborList"": ""True"",
	""mfpClientProtection"": ""Optional""
}",2023-06-01T22:43:18.571+0000,"This issue fixed in this PR as well: [[PR Link|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6112/diff#services/dnaserv/lib/ext_api_groups/ext_enterprise_ssid/group.py]]

Solution is: remove radio_policy key in payload and let them work with 2.4/5/6Ghz as default.

Please share your idea if there is new/update behavior on this. [~accountid:62d2fef2bd54f8d3ffb7d1f7] 

Regards","['Auton', 'Hulk']",ThanhTan Nguyen,Resolved,SANTHOSH MOUNASWAMY
SEEN-1768,https://miggbo.atlassian.net/browse/SEEN-1768,[Auton] - Validate client related scores verifications fails despite having right values,"*Regression:* Solution Regression Multisite

*Branch used:* private/Halleck-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Issue Faced:*

When integrating the feature corresponding to Client 360 with Endpoint Analytics, we have seeing failures for Client scores verification for the clients generated. We see that for few of the client endpoints, even though operating system data is correctly populated in Client 360 page as well from EA page for the endpoints, we see failure errors during execution.

Below are the verification snips:

*Snip from EA page:*

!image-20230602-033123.png|width=1277,height=642!



*Snip from client 360 page:*

!image-20230602-033042.png|width=1280,height=647!

*Feature wiki:* [Client 360 with Endpoint Analytics|https://wiki.cisco.com/display/EDPEIXOT/Client+360+with+Endpoint+Analytics]

*Failed log on Halleck* - [Test_TC230_trustscore_and_alert_customization|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2112002&size=2388421&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac_dr.2023May31_06:10:25.116888.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-06-02T03:32:16.107+0000,"[~accountid:63f50bfce8216251ae4d59d5] : Could you please add the corresponding commit link for this issue addressed along with branch for the code base on which it was committed? [~accountid:62d2fec15d6f5fd2c3db8f9f] , I see below PR related to this Auton:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6054/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6054/overview]



Please confirm if issues reported via this Auton is no more observed during execution and if you are able to proceed with [https://miggbo.atlassian.net/browse/SEEN-1514|https://miggbo.atlassian.net/browse/SEEN-1514|smart-link] and [https://miggbo.atlassian.net/browse/SEEN-1502|https://miggbo.atlassian.net/browse/SEEN-1502|smart-link]? [~accountid:62ab7a399cd13c0068b18fe0] : Thanks for adding the PR details. Will be verifying on this during our ongoing Hulk testing on DR testbed. Tentative ETA - 20th July 2023 Tried giving an on *Hulk RC1 - 2.1.710.70446*. We see client trust score and device form was not updated for the intended client. Looks like the corresponding Wired client had some issue. Could not debug further as we had to move on to  Hulk RC2 testing.

But the script failure wise, the TC should not get errored with string operation error in case of failure. It should fail with error message along with error reason. 

*Error snip:* 
{{TypeError: 'in <string>' requires string as left operand, not NoneType}}

Can we have this issue fixed meanwhile? 
Will try to debug the Wired client related issue during Hulk RC2 testing. [~accountid:62d2fec15d6f5fd2c3db8f9f] 

where are the logs for recent run?  I need logs for recent run Tried testing the same TC on  *Hulk RC2 - 2.1.710.70462*. We see same pattern of issue here.  This time made sure the wired client which had issue was not being included for testing this TC. But we still see same issue. Ideally the TC should perform client trust score validation only for Wired client used for concurrent mac. But looks like TC is validating for all the wired clients available End point query list. This needs to be fixed. Also we see same string operation error in case of this failure, which also needs to be addressed.


*Failed log:* [Test_TC230_trustscore_and_alert_customization|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2136339&size=1281427&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb_multi_sites_mdnac.2023Aug01_08:34:21.893894.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] [~accountid:63f50bfce8216251ae4d59d5] : Thanks a lot for syncup! As discussed, this could be a product issue on EA side not reflecting right trust score in case of disconnected clients. Will be following with DE team on this issue to get confirmation on this.

However this Jira ticket can be used to fix the Failure error handling to fail with appropriate error log message info and not to show Type errors.  Extra validation for data type: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6691/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6691/overview]","['Auton', 'Halleck', 'Hulk', 'Integration', 'MSTB1', 'Multisite']",Moe Saeed,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-1769,https://miggbo.atlassian.net/browse/SEEN-1769,[Auton]:Hulk- Task-assign_devices_to_site_and_provision.py-72-provisioning  /   Test_TC1_DNAC_Device_Provisioning  /   test5_verify_provision_the_devices_fabric1,"+*DNAC Release:*+
Hulk  ISO: 2.3.7.0-70272
Device -Cisco Catalyst 9400 SVL Switch    ==>204.1.2.2
Polarisis:17.12  
+*DE Analysis (*+[+*ajbanerj@cisco.com*+|mailto:ajbanerj@cisco.com]+*)*+
This issue happens if automation suite is using site assignment API /member without adding a flag. The change needed is add DC-Push: false as a header when using group site assignment API
#######################################################
+*Description:*+ 

During Optimized sanity execution,BORDER-CP-9400-SVL.device(border) observing   provision failed for Install of Swim Certificate FAILED

+*Error Message :*+
##########################
Install of Swim Certificate     FAILED

com.cisco.grapevine.api.exception.ResourceNotFoundException: ConfigPreview Not Found for : deviceId = 7f735c1a-52ac-41e5-936f-accd1df34a6b, provisioningTaskId = 3d9671d3-b609-4817-a26d-8a64ba06350c, activityId = null, taskId = null, applicationHandle = swim.certificate.update, appName = null

+*Failed Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assign_devices_to_site_and_provision.py-72-provisioning&begin=350800&size=1751546&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May10_08:43:31.196760.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assign_devices_to_site_and_provision.py-72-provisioning&begin=350800&size=1751546&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May10_08:43:31.196760.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

+*Branch*+  : private/Hulk-ms/sanity_api_auto
 Script  file : \testcases\sanityusecases\provisioning\assign_devices_to_site_and_provision.py

*CDETS :* [https://cdetsng.cisco.com/webui/#view=CSCwf31139|https://cdetsng.cisco.com/webui/#view=CSCwf31139]
",2023-06-02T07:28:42.458+0000,"Hi Omkar, who is the DE here? I’d like to confirm with them on the flag changes Hi  [~accountid:63f50bcece6f37e5ed93c87e]  ,
DE details: [ajbanerj@cisco.com|mailto:ajbanerj@cisco.com] Webex space here: webexteams://im?space=aedc0150-114a-11ee-ba88-c1d69a405e0c where we are discussing [~accountid:620b8357878c2f00729881c8] you said latest run you are not seeing this issue right? Then can this be closed? Issue  not seen latest run, hence moving to close state ","['AWS-Santiy', 'Auton', 'Hulk', 'Optimized', 'Sanity', 'Uplift']",Andrew Chen,Closed,Omkar Sharad Wagh
SEEN-1790,https://miggbo.atlassian.net/browse/SEEN-1790,CSCwf48431: check_system_health_with_validation_tool() from system_health/group.py needs update,"As per [CSCwf48431|https://cdetsng.cisco.com/webui/#view=CSCwf48431]: {{check_system_health_with_validation_tool()}} from [system_health/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/services/dnaserv/lib/api_groups/system_health/group.py?at=private/Hulk-ms/api-auto] needs update to consider the case where if “message” key is not present, consider “messageDetails” key to get the required info and expect the “remedy” key is also not there.

This change is applicable to On-Prem as well as ESXi VM based DNAC.",2023-06-05T06:09:55.726+0000,"Required PR for Hulk branch has been raised:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6056/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6056/overview] Mentioned PR has been merged to Hulk Branch.

Marking this as “Closed”.","['Auton', 'Hulk']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1791,https://miggbo.atlassian.net/browse/SEEN-1791,Auton[MSTB2-Non-DR] : Test_TC205_check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE ,"Hi [~accountid:63f50bcf4e86f362d39acde5],

We are seeing script is trying to create a discovery Job, but it fails we already have job present. Same discovery Job schedular name is present  on Discovery page.



Failed:

15563:  Resource path full url: [https://172.23.241.114/api/v1/task/69b6289a-9140-4e15-979c-46971ac88e59/tree|https://172.23.241.114/api/v1/task/69b6289a-9140-4e15-979c-46971ac88e59/tree]
15564:  Failed Task's data: {'response': [{'version': 1685431915063, 'progress': 'Failed to create discovery', 'startTime': 1685431915043, 'endTime': 1685431915067, 'errorCode': 'NCDS12001', 'isError': True, 'serviceType': 'NCDS', 'rootId': '69b6289a-9140-4e15-979c-46971ac88e59', 'failureReason': 'NCDS12001: Discovery already exists with the same name', 'lastUpdate': 1685431915063, 'instanceTenantId': '646f79caff19c822e5b4c1b0', 'id': '69b6289a-9140-4e15-979c-46971ac88e59'}], 'version': '1.0'}

Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4409645&size=90566&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May29_23:57:32.843309.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4409645&size=90566&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May29_23:57:32.843309.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

For MSTB2 we already have Schedular job on TC10
Test_TC10_DNAC_Verify_adding_range_discovery_ssh_global_credentials
test1_verify_adding_range_discovery_ssh_global_credentials				
test1_verify_discovery_netconf_status_scheduled_task	
test1_verify_adding_range_discovery_through_task_schedular	

Script : solution_test_3sites_sjc_nyc_sf
Branch : private/Halleck-ms/api-auto",2023-06-05T09:11:34.001+0000,"!1849_649_1.5.png|width=925,height=325!

TC205 is assigned to two different testcases. Even this needs to be fixed which branch did you use? also script? any relate information? [~accountid:63f50bf0e8216251ae4d59ca]  Hi [~accountid:63f50bcf4e86f362d39acde5], 

Branch and script related detail is already added in Description section.

Script : solution_test_3sites_sjc_nyc_sf.py
Branch : private/Halleck-ms/api-auto


Thanks,

Divakar * PR-link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5951/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5951/overview]
* Summary: change order number of testcase. and delete exist discovery
* passlog: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-06/sanity_TB16.2023Jun11_23:09:42.415127.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-06/sanity_TB16.2023Jun11_23:09:42.415127.zip&atstype=ATS]","['Auton', 'Integration', 'MSTB2']",QuangVinh Nguyen,Resolved,Divakar Kumar Yadav
SEEN-1792,https://miggbo.atlassian.net/browse/SEEN-1792,"[Auton] - Script execution failing at initial point due to ""Failed to connect to API clients!"" error","*Regression:* Solution Regression Multisite

*Branch used:* private/Halleck-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
 

*Description :* 

Initial script execution is failing due to “Failed to connect to API clients!” error. 

The code base had worked fine during last week Friday. I can confirm that until commit - [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9c1205bc4ae89a6575241e3eba53254c6692fb69|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9c1205bc4ae89a6575241e3eba53254c6692fb69] we did not face any issue. Looks like some commit post this commit has caused some breakage. I tried checking from the code side, but unable to trace back to the code that is causing the issue.

*Error snip:*

1023:  Indexed api group ""scalablegroup""
1024:  Role recovery is not in service
1025:  Library method ""reconnect_clients"" returned in 0:00:00.487672
1026:  Library method ""reconnect_clients"" returned in 0:00:04.527245
1027:  Failed to connect to API clients!
1028:  Failed reason: Failed to instantiate DnaServices

*Failed log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac_dr.2023Jun04_23:11:59.926119.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac_dr.2023Jun04_23:11:59.926119.zip&atstype=ATS]

*Corresponding Jenkins job Console log:*   [https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/view/MSTB1/job/MSTB1 MDNAC DR/21/consoleFull|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/view/MSTB1/job/MSTB1%20MDNAC%20DR/21/consoleFull]",2023-06-05T10:59:37.965+0000,"It worked fine for me with latest Halleck code. Please try again and let me know if you still see the issue and share the testbed in issue state.

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/tranlam-sjc/pyats6/users/tranlam/archive/23-06/sr_mb_multi_sites_mdnac_dr.2023Jun12_13:46:13.489716.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/tranlam-sjc/pyats6/users/tranlam/archive/23-06/sr_mb_multi_sites_mdnac_dr.2023Jun12_13:46:13.489716.zip&atstype=ATS]
 Issue is no more observed during Hulk Testing.

Uber ISO tested - *Hulk RC2 - 2.1.710.70462*","['Auton', 'Halleck', 'Integration', 'MSTB1', 'Multisite']",Tran Lam,Closed,SANDEEP SHIVARAMAREDDY
SEEN-1793,https://miggbo.atlassian.net/browse/SEEN-1793,[Auton] - Failures in TC related to ISE NDG for DNAC map to deleted device Network Group ,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/Halleck-ms/api-auto

*Uber ISO tested:* Halleck Respin - 2.1.660.70351

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

When integrating the feature corresponding to ISE NDG for DNAC map to deleted device Network Group, we have observed below issues:

a) Under subTCs - [test6_verify_adding_range_discovery_ssh_global_credentials|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3637501&size=41183&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac_dr.2023Jun05_06:49:09.343045.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] & [test8_verify_adding_range_discovery_through_task_schedular|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3690545&size=91097&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac_dr.2023Jun05_06:49:09.343045.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ Here the discovery jobs tasks are getting configured with same name as those used earlier at the time of initial device discovery. Hence all these discovery jobs are failing  with errors with *""Failed to create discovery "" and ""NCDS12001: Discovery already exists with the same name"".* Due to this device discovery for deleted device is missed out.  But the testcase is not marked as failure, instead its false Passed. These issues needs to be fixed.

*Below are the relevant screenshots:*

!Screen_shot_1-20230605-164623.png|width=1260,height=536!

!Screen_shot_2-20230605-165003.png|width=1272,height=619!

*Already used Discovery Jobs:*
 

!Screen_shot_3-20230605-164653.png|width=1275,height=608!



b) Under subTC - [test9_verify_device_resync|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3781642&size=445179&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac_dr.2023Jun05_06:49:09.343045.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ This TC is failure is consequence of subTCs problem as mentioned in above point *a)* . Device discovery for deleted device did not happen due to this issue.

c) Under subTC - [test10_provision_device|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4226821&size=11467&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac_dr.2023Jun05_06:49:09.343045.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

 → Incorrect api call name is used for provisioning of devices. *dnac_handle.provision_device* is used instead of *dnac_handle.provision_devices*



*Error snip:* 

18231: Traceback (most recent call last):

18232: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/services/commonlibs/test_wrapper.py"", line 301, in wrapper

18233: result = testfunc(func_self, **kwargs)

18234: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py"", line 31636, in test10_provision_device

18235: if dnac_handle.provision_device(self.edge_name):

18236: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/services/dnaserv/dnaservices.py"", line 325, in __getattr__

18237: raise AttributeError(err_msg)

18238: AttributeError: 'DnaServices' object has no attribute 'provision_device'

18239: Test returned in 0:00:00.399219

18240: Errored reason: 'DnaServices' object has no attribute 'provision_device'



d) Under subTC -  [test11_re_add_edge_to_fabric|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4238288&size=709492&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac_dr.2023Jun05_06:49:09.343045.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ Observing script errors during fabric update operation.

*Error Snip:*

19576: Traceback (most recent call last):

19577: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/services/commonlibs/test_wrapper.py"", line 301, in wrapper

19578: result = testfunc(func_self, **kwargs)

19579: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py"", line 31649, in test11_re_add_edge_to_fabric

19580: sdatransit=""TRANSITSDA"",transitvn=""iptransit"")):

19581: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/services/dnaserv/lib/api_groups/fabric_wired/group.py"", line 708, in assign_roles_deploy_fabric

19582: l3handoff_start_vlan=l3handoff_start_vlan)

19583: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/services/dnaserv/lib/api_groups/fabric_wired/group.py"", line 609, in assign_roles_deploy_fabric

19584: ans.append(x.format(entry['name'],border['role'],vn_name))

19585: UnboundLocalError: local variable 'border' referenced before assignment

19586: Test returned in 0:00:45.055197

19587: Errored reason: local variable 'border' referenced before assignment

 

e) Under subTC -  [test12_verify_deleted_device_appear_in_deleted_NDG_in_ise|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4947780&size=16238&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac_dr.2023Jun05_06:49:09.343045.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ Failed as a consequence problem encountered in point *a)*. Here the Failed log message does not convey the reason of failure. Need to print why the TC failed, what are the attribute values checked, what is expected and what is actual output etc.

*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/ISE+NDG%3A+Verify+if+Deleted+device+in+DNAC+map+to+Deleted+Network+Device+Group+on+ISE|https://wiki.cisco.com/display/EDPEIXOT/ISE+NDG%3A+Verify+if+Deleted+device+in+DNAC+map+to+Deleted+Network+Device+Group+on+ISE]

 

*Failed log on Halleck* - [Test_TC265_check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2383406&size=2580807&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac_dr.2023Jun05_06:49:09.343045.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-06-05T16:55:01.658+0000,"* PR-link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6026/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6026/overview]
* TradeLog: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-06/sanity-intg2.2023Jun19_01:45:55.604801.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-06/sanity-intg2.2023Jun19_01:45:55.604801.zip&atstype=ATS] [~accountid:63f50bcf4e86f362d39acde5] , below PR is currently having Merge Conflict. Please update back once you have resolved the conflict. Thank you [~accountid:63f50bcf4e86f362d39acde5] . [~accountid:62d2fec15d6f5fd2c3db8f9f] I have approved and merged the change to Halleck branch.

During your testing cycles for Hulk, once you are done with your cycle, pls. use Halleck branch code to evaluate the fix.

Once you have the pass log for the use-case in complain, we can cherry-pick this to Hulk as well. [~accountid:62ab7a399cd13c0068b18fe0] : Will be verifying on this during our ongoing Hulk testing on DR testbed. Tentative ETA - 20th July 2023. Still observing same error pattern as in c) d) and e) as mentioned in Jira description.

*Failed log:* [Test_TC265_check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2565644&size=3355309&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_mb_multi_sites_mdnac.2023Jul24_20:29:07.107045.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Uber ISO tested - *Hulk RC1 - 2.1.710.70446* Hi Sandeep. I triggered testscript in *private/Halleck-ms/SEEN-1793* and it work fine. can you check it again? [~accountid:63f50bcf4e86f362d39acde5] : Thanks for looking into the issue. Actually Halleck DNAC testing has stopped as it did not make to Customer release instead Hulk is the front-facing customer release. Could you please cherry pick the changes and commit on Hulk branch?

Currently we have ongoing Hulk RC3 Regression testing. Once its completed, will try to validate again with the changes. In middle of Hulk RC3 testing we had got priority request to Take up Ghost Patch2 RC2. So post Hulk RC3 testing we had to move to quickly to Ghost Patch2 RC2. Currently we have testbed in same Cadence. One we have next Hulk / Hulk Patch1, we will take this usecase and re-validate and confirm. since no update from reporter, i change the Jira status to “close”. you can reopen it whenever you hit the error again Test scenario was tried during Recent Hulk Patch1 - 2.1.713.70147 testing. Same issue is observed.

Branch used - *private/HulkPatch-ms/api-auto* 

*Failed log -* [Test_TC266_check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9541001&size=2245986&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac.2023Sep06_08:05:18.884114.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] Effect by PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6925/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6925/overview] which change the behavior of discovery section, can you please verify it with the lastest code?  [~accountid:63f50bcf4e86f362d39acde5] : We are currently testing on Hulk Patch1 builds. I see this below commit corresponds to Hulk branch. Is this issue fixed on Hulk Patch1 code base? Could you please confirm? [~accountid:62d2fec15d6f5fd2c3db8f9f] yes. i confirm the issue was fixed on Hulk Patch 1 [~accountid:63f50bcf4e86f362d39acde5] : Thanks for confirmation! We will be re-executing the TC and updating the results. Issue is no more observed using the latest code pulled from private/HulkPatch-ms/api-auto. 

*Pass log on Hulk Patch1 RC3 - 2.1.713.70319*: [https://ngdevx.cisco.com/services/taas/results/108375a1-96fd-4aee-94b4-46f19c0f5e8f/run-results|https://ngdevx.cisco.com/services/taas/results/108375a1-96fd-4aee-94b4-46f19c0f5e8f/run-results]","['Auton', 'Halleck', 'Hulk', 'Integration', 'MSTB1', 'Multisite']",QuangVinh Nguyen,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-1795,https://miggbo.atlassian.net/browse/SEEN-1795,[Auton] Guardian - Wireless Solution Sanity - TC85 test2_cleanup_wireless_ssid not deleting few ssids,"* *Regression:* SDA Solution Sanity (SSR)
* *DNAC Release_Version Tested:* Guardian_P4_2.1.518.72328
* *Device Image Used:* 17.9.4
* *Branch Used:* rcdn/Guardian-ms/api-auto (Latest sync to Main Branch done before reg run start - Main Branch Used: Private/Guardian-ms/api-auto)
* *Script Name:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py
* *Test Case:* TC85 test2_cleanup_wireless_ssid not deleting few ssids. It impacts TC97 blocking all other TCs
* *Issue Faced:* Deletion Failed for ssid: Guest_webauthinternal, Guest_passthrough_int
* *Taas Log:* [https://ngdevx.cisco.com/services/taas/results/adb8773b-5825-48b6-b06f-b63a1600c7b8|https://ngdevx.cisco.com/services/taas/results/adb8773b-5825-48b6-b06f-b63a1600c7b8]
* *DE Pushpalatha Comments:* Due to the below SiteProfile API call failure, the SSID Guest_webauthinternal and Guest_passthrough_int have been only removed from CommonSetting but have not been removed from the corresponding network profile profile-SSIDOpenIndia. And this is the reason for the provision failure as informed to you in the past.
So please fix the script to verify the SSID getting deleted both from CommonSetting and SiteProfile, only then proceed with the controller provision.
406 Not Acceptable client error response code for ""Not Acceptable"". This is the expected HTTP Error code. DNAC error response is as below
:{""response"":{""errorCode"":""NCND00002"",""message"":""NCND00002: The Network Profile request has validation errors"",""detail"":""[\""NCND01025: Duplicate keys cannot have the same value: NamedCapabilities\""]""},""version"":""1.0""}
Thanks
Pushpa",2023-06-06T12:23:50.278+0000,Triage with DE with earlier Pass Logs. If anything fails from existing regression it needs to be triaged with Product Issues and Need a confirmation email from SE for any Behaviour change. And Detail of the behavior change.  PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6897/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6897/overview],"['Auton', 'Guardian']",Moe Saeed,Resolved,Yuvarani Iyamperumal
SEEN-1796,https://miggbo.atlassian.net/browse/SEEN-1796,[Auton] Hulk - Wireless Solution Sanity - Script Uplift Required to overcome Aireos WLC Provision Failure due to Hulk Feature (F133388) commit,"*Regression:* Solution Sanity (SSR)

*DNAC Release_Version Used:* FIPS Enabled Hulk_Intg 2.1.710.70333.iso

*Branch:* rcdn/Hulk-ms/api-auto synced to main branch private/Hulk-ms/api-auto - today before regression run start

*Script Used:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

*Cluster:* 10.88.187.195 - admin/Maglev123maglev for UI and maglev/Maglev123 for SSH

*Issue Faced:* Script Uplift Required to overcome Aireos WLC Provision Failure due to Hulk Feature (F133388) commit via CSCwe79550 and CSCwf48664

[https://wiki.cisco.com/display/DNACW/DNAC+WLAN+validation|https://wiki.cisco.com/display/DNACW/DNAC+WLAN+validation]

*Test Case:* TC28_DNAC_Device_Provisioning  /   test2_verify_provision_the_devices_fabric1

*Other TCs Impacted due to it:* TC29, TC30, TC32, TC33 Blocking all other TCs

*Trade Log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/23-06/sanitycombine.2023Jun06_11:24:28.878232.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/23-06/sanitycombine.2023Jun06_11:24:28.878232.zip&atstype=ATS]

{{23407: Config Preview Activity failed with reason: NCWL10135: For Cisco AireOS Wireless Controllers, when Fast Transition is enabled, you must enable at least one FT AKM. Device Version: 8.10.184.78 SSID: Single5KBand.}}",2023-06-06T18:20:03.172+0000,"Committed change for SSIDDUAL BAND , Single5KBand.
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/8608520052b1e566ed5d4309d70755aba7e3428b|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/8608520052b1e566ed5d4309d70755aba7e3428b] Also observed provision preview failure with sensor ssid 'CiscoSensorProvisioning'. However it should be a bug since failed with manually creation in UI too. Need to file defect for it. PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5944/diff#configs/config_48hr_test/solution_test_input.json|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5944/diff#configs/config_48hr_test/solution_test_input.json] Filed CSCwf57853 for the 'CiscoSensorProvisioning' issue. Update for IBSTE solution input

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9c2c905087b98ede571e65a4b212816c982c1190|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9c2c905087b98ede571e65a4b212816c982c1190] I updated the common solution inputs. If you’re using different solution input json, please update same yours.
configs/config_48hr_test/solution_test_input.json
configs/ibste/solution_input/solution_test_input.json","['Auton', 'Hulk']",Moe Saeed,Resolved,Yuvarani Iyamperumal
SEEN-1801,https://miggbo.atlassian.net/browse/SEEN-1801,[Auton]Upgrade:Test_TC53_SWIM_UPGRADE_ECA_DEVICE  /   test1_verify_upgrading_os_image,"*Reporter Analysis:* 
We have observed that swim testcase is failing on Hulk branch with key error:
  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Upgrade-Verify-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper

{noformat}16118: 
     result = method(*args, **kwargs){noformat}

{noformat}16119: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Upgrade-Verify-common-Multi-job/services/dnaserv/lib/api_groups/swim/group.py"", line 796, in swim_updgrade_os_image{noformat}

{noformat}16120: 
     custom_checks_list = self.services.input_data['SWIM_CUSTOM_CHECKS']{noformat}

{noformat}16121: 
 KeyError: 'SWIM_CUSTOM_CHECKS'{noformat}

{noformat}16123: 
 Errored reason: SWIM_CUSTOM_CHECKS{noformat}

{noformat}16124: {noformat}

*Branch Name:* Hulk-ms/sanity_api_auto

**Script* *file:** after_upgrade_verify.py

*Source Team:*  Upgrade-Sanity

Issue Seen first time or day0 issue:

**Fail Log:**
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8446622&size=24139&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun02_11:33:48.204692.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8446622&size=24139&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun02_11:33:48.204692.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-06-07T08:36:52.655+0000,"Added fix [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a2a549b951334280d3f5c13b16109c2f191bc30f|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a2a549b951334280d3f5c13b16109c2f191bc30f] Tried swim Testcase with the fix:
Testcases passed:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=377726&size=9040615&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun12_20:17:00.491795.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=377726&size=9040615&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun12_20:17:00.491795.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Hulk', 'Upgrade', 'Upgrade-Sanity']",Tran Lam,Closed,Anusha John
SEEN-1802,https://miggbo.atlassian.net/browse/SEEN-1802,[Auton]Test_TC14_DNAC_configure_multicast_primary_border_as_rp/test1_configure_multicast_on_site_sjc/test1_configure_multicast_on_site_nyc,"*Reporter Analysis:* 
We have on Upgrade Sanity after SDA2.0 VCS changes below TC are failing from Andrew’s Branch which should have fix:
""private/Hulk-ms/api-auto-suffix-issues""


[Test_TC14_DNAC_configure_multicast_primary_border_as_rp|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=263211&size=553640&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun07_00:20:45.073986.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/[test1_configure_multicast_on_site_sjc|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=489945&size=12469&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun07_00:20:45.073986.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/[test1_configure_multicast_on_site_nyc|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=502414&size=10297&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun07_00:20:45.073986.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Due to the above Testcase below testcase is affected:
[Test_TC15_verify_pim_igmp_config_after_multicast_update|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=816851&size=712129&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun07_00:20:45.073986.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/[test1_verify_pim_igmp_config_on_fabric_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=817595&size=711203&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun07_00:20:45.073986.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Same Testcases Passed from Sanity Branch:
""private/Hulk-ms/sanity_api_auto""
Passed Log from sanity branch:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun05_22:07:16.456718.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun05_22:07:16.456718.zip&atstype=ATS]
Failed Log from Andrews Branch:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun05_21:45:36.487276.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun05_21:45:36.487276.zip&atstype=ATS]

*Branch Name:* Hulk-ms/sanity_api_auto

**Script* *file:** after_upgrade_verify.py

*Source Team:*  Upgrade-Sanity

Issue Seen first time or day0 issue:

*Error from Log:*
2640: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/3N-Cluster-Hulk-Upgrade-Verify-common-Multi-job/services/dnaserv/lib/api_groups/fabric_multicast/group.py"", line 114, in _multicast_config_ip_pool_vn_mapping

{noformat}2641: 
     connectivity_domain = self.services.get_cd_info(name=fabric_name){noformat}

{noformat}2642: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/3N-Cluster-Hulk-Upgrade-Verify-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper{noformat}

{noformat}2643: 
     result = method(*args, **kwargs){noformat}

{noformat}2644: 
 TypeError: get_cd_info() got an unexpected keyword argument 'name'{noformat}

{noformat}2646: 
 Errored reason: get_cd_info() got an unexpected keyword argument 'name{noformat}",2023-06-07T08:46:20.448+0000,"Closing Jira as with the fix of Andrew:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5923/diff#services/dnaserv/lib/api_groups/connectivity_domain/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5923/diff#services/dnaserv/lib/api_groups/connectivity_domain/group.py]

Passed Log:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun11_20:45:52.269324.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun11_20:45:52.269324.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]","['Auton', 'Hulk', 'Sanity', 'Upgrade']",Andrew Chen,Closed,Anusha John
SEEN-1817,https://miggbo.atlassian.net/browse/SEEN-1817,CSCwf55276 - Auton-[Hulk] - TC30_DNAC_Connectivity_domain_creation_in_dnac/test4_verify_adding_vns_to_fabric_sites,"Hi [~accountid:62d2fe9f8afb5805e5d5af49]  Please check on this priority, script execution blocked.

*Branch:* private/Hulk-ms/api-auto

*Script file:* solution_test_sanityecamb.py

*input file:* solution_test_input.json

*ova file:* assembly_release_dnac_hulk-intg_converged_07-3.710.75364.ova

*Failed log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2348329&size=1231197&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun12_04:50:54.984290.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2348329&size=1231197&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun12_04:50:54.984290.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*DE Analsysis :- [*[*sipate@cisco.com*|mailto:sipate@cisco.com]*]*

 [https://wiki.cisco.com/display/APICEMUCI/SDA+2.0+GA+with+VCR+API+changes|https://wiki.cisco.com/display/APICEMUCI/SDA+2.0+GA+with+VCR+API+changes]

 Scripts may need to be brought up to date. The relevant change is number 1 from the table here

""fabricAuthenticationProfile"" needs to contain at least ""name"" and ""type"" fields when fabric sites are created.
 For example:
          ""fabricAuthenticationProfile"": {
              ""name"": ""Closed Authentication"",
              ""deploymentMode"": ""CLOSED"",
              ""dot1xToMabFallbackTimeout"": ""43"",
              ""hostMode"": ""multi_auth"",
              ""order"": ""dot1x"",
              ""priority"": ""dot1x mab"",
              ""type"": ""WIRED_DOT1X"",
              ""bpduGuardEnabled"": false,
              ""wakeOnLan"": true
          },
          ""fabricAuthenticationProfile"": {
              ""name"": ""No Authentication"",
              ""type"": ""WIRED_NOAUTH"",
              ""wakeOnLan"": false
          },",2023-06-12T17:40:43.034+0000,"Hi [~accountid:63f50bcece6f37e5ed93c87e]  pls check on this priority, script execution blocked. [Pull Request #5982: Private/Hulk ms/api auto auth profile - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5982/overview]","['Auton', 'Hulk', 'hulk-vm-sanity', 'sanity']",Andrew Chen,Resolved,Raghavendrachar Baraguru Mallesha Char
SEEN-1818,https://miggbo.atlassian.net/browse/SEEN-1818,"Hulk - Wireless Solution Sanity - Script Uplift Required to overcome Config preview activity failure as per offline discussion with stefan, api change was committed via CSCwf01090","*Regression:* Solution Sanity (SSR)

*DNAC Release_Version Used:* 2.1.710.8010452.iso

*Branch:* bgl/Hulk-ms/api-auto synced to main branch private/Hulk-ms/api-auto - today before regression run start.

*Issue Faced:* Script Uplift Required to overcome Config preview activity failure as per offline discussion with stefan, api change was committed via CSCwf01090

[+https://wiki.cisco.com/display/APICEMUCI/SDA+2.0+GA+with+VCR+API+changes+|https://wiki.cisco.com/display/APICEMUCI/SDA+2.0+GA+with+VCR+API+changes]

*Other TCs Impacted due to it:* TC30, TC32, TC33 Blocking all other TCs

*Failed Trade Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12985682&size=1251932&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-06%2Fsanitycombine.2023Jun10_22:10:25.570189.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12985682&size=1251932&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-06%2Fsanitycombine.2023Jun10_22:10:25.570189.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

*Complete Log :* 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-06%2Fsanitycombine.2023Jun10_22:10:25.570189.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-06%2Fsanitycombine.2023Jun10_22:10:25.570189.zip&atstype=ATS]

{{ Config Preview Activity failed with reason: NCSP11001: User intent validation failed while processing the 'provision' request. Additional info for support: taskId: 'afe35df4-05aa-4482-a643-3e7c504e91b9'.}}

{{Activity: 06cfcf6e-48ab-4aa9-9348-08c555600446 Trigger job: {'id': 'e7143008-e4b2-4714-a52b-aaf6c77fd672', 'triggeredJobTaskId': 'afe35df4-05aa-4482-a643-3e7c504e91b9', 'triggeredTime': 1686420545600, 'status': 'FAILED', 'failureReason': ""NCSP11001: User intent validation failed while processing the 'provision' request. Additional info for support: taskId: 'afe35df4-05aa-4482-a643-3e7c504e91b9'."", 'triggeredJobId': 'e7143008-e4b2-4714-a52b-aaf6c77fd672'}}}",2023-06-12T17:58:02.601+0000,"Hit same failure with below Non-VA regression as well
*Regression:* Solution Sanity (SSR)
*DNAC Release_Version Used:* FIPS Enabled Hulk_Intg 2.1.710.70344.iso
*Branch:* rcdn/Hulk-ms/api-auto synced to main branch private/Hulk-ms/api-auto - today before regression run start
*Script Used:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py
*Cluster:* 10.88.187.195 - admin/Maglev123maglev for UI and maglev/Maglev123 for SSH
*Taas Log:* [https://ngdevx.cisco.com/services/taas/results/a9c7a535-071a-4aef-95a6-54bded3b54c4|https://ngdevx.cisco.com/services/taas/results/a9c7a535-071a-4aef-95a6-54bded3b54c4] Hit same failure with below VA regression as well
*Regression:* Solution Sanity (SSR)
*DNAC Release_Version Used:* retained_forever/assembly_release_dnac_hulk-intg_converged_07-3.710.75364.ova
*Branch:* rcdn/Hulk-ms/api-auto synced to main branch private/Hulk-ms/api-auto - today before regression run start
*Script Used:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py
*Cluster:* 10.88.186.166 - admin/Maglev123 for UI and maglev/Maglev123 for SSH
*Taas Log:* [https://ngdevx.cisco.com/services/taas/results/c1f63720-8813-43a2-9d52-01a04276ad84|https://ngdevx.cisco.com/services/taas/results/c1f63720-8813-43a2-9d52-01a04276ad84] Fix was committed ehre: [Pull Request #5982: Private/Hulk ms/api auto auth profile - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/5982/overview]","['Auton', 'Hulk']",Andrew Chen,Resolved,SAINATH CHATHARASI
SEEN-1819,https://miggbo.atlassian.net/browse/SEEN-1819,CSCwf56854-[Hulk][Auton]-Provision failure - NCSO10011: Error in generating CFS due to internal error,"Hi   [~accountid:62d2fe9f8afb5805e5d5af49] Please check on this priority,

*Branch Name:*  private/Hulk-ms/sanity_api_auto

*Script file: usecasemaps\lansanity\lansanity_usecases_maps.yaml*

*input file:* solution_test_input.json

DNAC Release:
Hulk  ISO:  2.3.7.0-70326

Polarisis:17.9.4 

#######################################################
Description: 

Observed with Hulk:  2.3.7.0-70326

* Provision EWLC device:
Observed: NCSO10011: Error in generating CFS due to internal error. CFS Generation Failed for task id 5f72ac9f-14ad-4664-b274-c16c063000be
* Provision EDGE device:
NCSO10011: Error in generating CFS due to internal error. CFS Generation Failed for task id 5f72ac9f-14ad-4664-b274-c16c063000be

*Failed log:*
[https://ngdevx.cisco.com/services/taas/results/ef29f151-22c7-47cc-a1a7-91ff3dcb40ca/run-results|https://ngdevx.cisco.com/services/taas/results/ef29f151-22c7-47cc-a1a7-91ff3dcb40ca/run-results]

+*DE Analysis:  [*+[+*aatluri@cisco.com*+|mailto:aatluri@cisco.com]+*]*+

|The script seems to be doing this unsupported workflow. please check.
i. A config-preview of  a PUT on DeviceInfo is done 
ii. Devices are added the the fabric with roles
iii. Deploy of the preview operation is done only after the devices are added to the fabric.|


",2023-06-13T01:35:27.189+0000,"Updated then to run sequently instead. 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e4f5549b7029d726e73c55ec716fd39a4921c4ae|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e4f5549b7029d726e73c55ec716fd39a4921c4ae] Hi [~accountid:62d2fe9f8afb5805e5d5af49]  ,

We have observed that after updating the optimized mapping up to 12 , in UC 10, UC11, does not run  sub uc  in UC10 UC11 because  of audit log sub tc is failed.

if we run TC 28 from a legacy script (provisionReprovisionConfigValidation }, The following subtc is failing due to the presence of an existing model configuration, DNA-C.
[Test_TC28_DNAC_Device_Provisioning|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=413254&size=512345&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun14_03:45:59.628118.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  /  [test5_verify_provision_the_devices_fabric1|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=574433&size=70709&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun14_03:45:59.628118.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

could you please check & fix the mapping issue? 
*UC-1-10  log:*
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-06/env_optimized_auto_job.2023Jun14_00:03:16.308719.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-06/env_optimized_auto_job.2023Jun14_00:03:16.308719.zip&atstype=ATS]
*UC11 -log:*
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-06/env_optimized_auto_job.2023Jun14_02:55:47.420265.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-06/env_optimized_auto_job.2023Jun14_02:55:47.420265.zip&atstype=ATS]

 *UCG 10 in  miss   in execution:* 
*{*
    *""2"": [""provisionReprovisionConfigValidation""],*
    *""blocker_uc"": []*
*}*

*same  for  UC 11:*
*{*
    *""2"": [""SDAmulticast"",""ISEVtpChange""],*
    *""blocker_uc"": []*
*}*

For sub-TC, we tried the below job, but it didn't work
*Jenkins Job:1*
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/150/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/150/]
 
*Jenkins job2:*
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/151/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/151/] {{SDADevicesAndFabricRoles}} and {{FEWembededWirelessECA}} are important TCs. We added them as blocker_uc so that we can check why it failed before continuing other tests.

Audit log were changed after VCR changes in Hulk so I disabled audit log for now until the audit log update is done .([https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/899b0f6f0501148fbb75f18c45372764c07b6271|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/899b0f6f0501148fbb75f18c45372764c07b6271])","['Auton', 'Hulk', 'Optimized', 'Sanity']",Tran Lam,Resolved,Omkar Sharad Wagh
SEEN-1822,https://miggbo.atlassian.net/browse/SEEN-1822,Auton: [Hulk] - Test_TC60_Verify_DHCP_server_change_on_segments  /   test3_verify_change_sharedsecret,"Hi [~accountid:63f50bd34c355259db9ccc4d]   Please check on the below Auton

*Branch:* private/Hulk-ms/api-auto

*Script file:* solution_test_sanityecamb.py

*input file:* solution_test_input.json

*ova file:* assembly_release_dnac_hulk-intg_converged_07-3.710.75364.ova

*From day one issue  on Hulk OVA builds .*

*Failed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15147532&size=58111&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun13_18:16:22.407806.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15147532&size=58111&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun13_18:16:22.407806.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Observations:*

TC 60.3  Test_TC60_Verify_DHCP_server_change_on_segments  /   test3_verify_change_sharedsecret

In iteration of execution subTC failed , if rerun TC it’s passed successfully, could be a timing issue
Re-run pass log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=649093&size=54567&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun14_00:45:28.908084.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=649093&size=54567&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun14_00:45:28.908084.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


*Verified radius key password on device it looks  “newcisco“ showing correctly*

TB18-NY-FIAB#sh run | i key
 client 85.1.1.3 server-key 7 1217000011021F0725
 client 10.22.45.220 server-key 7 030A5E1C0506324F41
 rsakeypair TP-self-signed-2846014338
 rsakeypair sdn-network-infra-iwan
  etr map-server 204.1.2.3 key 7 045A0E000E7619160B4A554E175F5B5D73 domain-id 1312922983
  etr map-server 204.1.2.4 key 7 1444110A5D07287C732E666D7447504F05 domain-id 1366989278
  etr map-server 204.1.2.3 key 7 045A0E000E7619160B4A554E175F5B5D73 domain-id 1312922983
  etr map-server 204.1.2.4 key 7 014005050A080458764A1B514F51404B0D domain-id 1366989278
  etr map-server 204.1.2.3 key 7 104F0C1F044047530E577A72217C646C7B
  authentication-key 7 104F0C1F044047530E577A72217C646C7B
 pac key 7 06080A364F471A1A0A  



Regards,

Raghavendra B. M",2023-06-14T08:02:23.457+0000,"Hello [~accountid:63f50bd34c355259db9ccc4d] 

We see this issue in Guardian as well

Failed Log:[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=40801707&size=53981&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun20_10:07:15.150727.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=40801707&size=53981&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun20_10:07:15.150727.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Yes- Observed similar issue on Ghost P2 Runs 
where there is need of delay just before fetching the device shared secret data.  Hi,
This ticket has been fixed in those PRs:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6058/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6058/overview]
* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6059/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6059/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6060/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6060/overview]
* Groot: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6061/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6061/overview]
* Guardian: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6062/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6062/overview] Approved and merged all below PRs.

[~accountid:63f50bd68ab3d6a635ecc29b]  / [~accountid:63f50bf9e8216251ae4d59d4] , pls. monitor the execution of TC60. If you see an issue, pls. raise a new Auton using this one as reference. Sure [~accountid:62ab7a399cd13c0068b18fe0]  we will monitor.","['Auton', 'Guardian', 'Hulk', 'hulk-vm-sanity', 'sanity']",ThangQuoc Tran,Resolved,Raghavendrachar Baraguru Mallesha Char
SEEN-1823,https://miggbo.atlassian.net/browse/SEEN-1823, [Auton] : Hulk- Task-multicast_enable_in_fabric.py-111-SDAmulticast  /   Test_TC3_DNAC_configure_multicast_primary_border_as_rp  /   test1_configure_multicast,"*Reporter Analysis:*
we  observed  in the latest  hulk  build  in SDAmulicast some sub-TC’s  failed with the below  error:
20839: 
     vn_id = self.services.get_virtual_network_info_list(f'{vn_name}-{fabric_name}')[0]['id']

{noformat}20840: 
 IndexError: list index out of range{noformat}

  
*Hulk  Version* : 2.1.710.70344
*Script Name* :  lansanity_usecases_maps.yaml
*script  file :* services\dnaserv\lib\api_groups\fabric_multicast\group.py

*Testcases Impacted :* 

[Test_TC3_DNAC_configure_multicast_primary_border_as_rp|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-111-SDAmulticast&begin=5183340&size=104709&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun14_11:24:57.016476.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 
[test1_configure_multicast|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-111-SDAmulticast&begin=5184280&size=18125&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun14_11:24:57.016476.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] / test1_configure_multicast_on_site_nyc

+*Fail Log :*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-111-SDAmulticast&begin=5183340&size=104709&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun14_11:24:57.016476.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-111-SDAmulticast&begin=5183340&size=104709&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun14_11:24:57.016476.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
 
 +*Snip from Log :*+

{noformat}20885:   api_switch_call called:
20886:  {'params': {'name': 'WiredVNFBLayer2-Global/USA/SAN_JOSE_US_SJ_Fabric1'}}
20887:  Resource path full url: https://10.30.0.100/api/v2/data/customer-facing-service/VirtualNetwork
20888:  Queried VN name pattern ""WiredVNFBLayer2-Global/USA/SAN_JOSE_US_SJ_Fabric1"":
20889:  {'response': [], 'version': '1.0'}
20890:  Library group ""virtual_network"" method ""get_virtual_network_info_list"" returned in 0:00:00.039580
20891:  Traceback (most recent call last):
20892:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper
20893:      result = testfunc(func_self, **kwargs)
20894:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/testcases/sanityusecases/SDAmulticast/multicast_enable_in_fabric.py"", line 198, in test1_configure_multicast_on_site_nyc
20895:      if not dnac_handle.multicast_config_ssm_ip_pool_vn_mapping(site=""New_York""):
20896:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/decorators.py"", line 32, in wrapper
20897:      result = method(*args, **kwargs)
20898:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/fabric_multicast/group.py"", line 15, in multicast_config_ssm_ip_pool_vn_mapping
20899:      return self._multicast_config_ip_pool_vn_mapping('SSM', ssm_range_list=ssm_range_list,
20900:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/decorators.py"", line 32, in wrapper
20901:      result = method(*args, **kwargs)
20902:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/fabric_multicast/group.py"", line 105, in _multicast_config_ip_pool_vn_mapping
20903:      vn_id = self.services.get_virtual_network_info_list(f'{vn_name}-{fabric_name}')[0]['id']
20904:  IndexError: list index out of range{noformat}",2023-06-14T19:06:20.296+0000,"Based on the log, you didnot have the latest code after api changes. You shouldnot have this '{{Global/USA/SAN_JOSE_US_SJ_Fabric1}}' anymore but should be '{{Global/USA/SAN_JOSE}}'



Which branch are you using? Can you use the latest code from Hulk main line?","['Auton', 'Hulk', 'Issue', 'Optimized', 'Sanity']",Tran Lam,Closed,Omkar Sharad Wagh
SEEN-1824,https://miggbo.atlassian.net/browse/SEEN-1824,[Auton] [Hulk] - Multiple execution Failures around Fabric Addition and Related TCs on recently merged VCR based changes,"*Regression:* Solution Regression Multisite - DR+MDNAC

*Branch:* private/Hulk-ms/api-auto

*Uber ISO tested:* Hulk - 2.1.710.70344

*Script Used:* testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Issue Faced:* We are observing multiple failures in TCs corresponding to Connectivity domain creation, VN to IP pool mapping & Addition of device to Fabric and its Deployment.

# Under  [Test_TC29_DNAC_Connectivity_domain_creation_in_dnac|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=929760&size=1757120&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun15_21:08:57.588748.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

->  Sub TC - [test1_verify_virtual_network_fabric_sync_for_reader|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1050269&size=533309&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun15_21:08:57.588748.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] has failed due to VN sync to Reader node is not happening properly, so some VNs are missing on Reader node. This looks genuine issue and we have reported [CSCwf67438|https://cdetsng.cisco.com/webui/#view=CSCwf67438] to track this issue further.

-> Sub TC - [test1_verify_adding_vns_to_fabric_sites|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2215743&size=69449&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun15_21:08:57.588748.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] has failed even though we have VN list available on SJ fabric site. This TC seems to be added newly when compared to older previous pass log.

-> Sub TC - [test1_verify_adding_vns_to_fabric_sites_for_reader|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2285192&size=19455&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun15_21:08:57.588748.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] has failed due to unable to find fabric name. This TC also seems to be added newly when compared to older previous pass log.

-> Sub TCs - [test1_verify_creating_ip_transit|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2304647&size=88973&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun15_21:08:57.588748.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]  &  [test1_verify_creating_ip_transit_for_reader|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2393620&size=47654&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun15_21:08:57.588748.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] also seems to be added newly. In that Reader node related TC has failed below config preview errors -

{color:#bf2600}12031: Config Preview Activity failed with reason: Provisioning failed due to invalid parameter. Device should have BGP AS Number protocol notation.{color}

{color:#bf2600}12101: activity_id is False. Config preview task failed for description Scheduling task for Adding IPTransit iptransit for fabric ['iptransit'] at time 1686889909.4078097 - Configuration Preview{color}

Not sure if these are genuine errors or induced by new script changes.

# Under [Test_TC31_DNAC_configure_virtual_network_ip_pool_with_traffic_types|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2702876&size=6138791&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun15_21:08:57.588748.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

-> All the subTCs under this has failed possibly as a consequence of TC29 failures.



# Under [Test_TC32_DNAC_assigne_dhcp_role_deploy_device_in_fabric|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8841667&size=2592937&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun15_21:08:57.588748.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

-> Sub TC - [test1_verify_assign_roles_and_deploy_devices_on_fabric1_for_reader|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11393526&size=40892&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun15_21:08:57.588748.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] has failed erroring Fabric not found even though fabric is added.

# Under [Test_TC33_enable_fabric_wireless_eca|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11434604&size=1407187&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun15_21:08:57.588748.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

-> Sub TC - [test2_enable_fabric_wireless_eca_for_reader|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12749841&size=91783&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun15_21:08:57.588748.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] has failed with device not found in Connectivity domain added even though we have device under MIPITAS->BLD23.

# Under [Test_TC35_DNAC_verifying_configuration_lisp_on_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=18746233&size=5272592&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun15_21:08:57.588748.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

-> Sub TC - [test1_verify_configuration_fabric1|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=18746993&size=5043015&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun15_21:08:57.588748.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] has failed at configuration verification on device as a consequence of failures in TC29,TC31.

*Failed logs:*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun15_21:08:57.588748.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun15_21:08:57.588748.zip&atstype=ATS] → Refer TCs - 29, 31, 32, 33, 34 & 35

*Previous Pass log on Hulk - 2.1.710.70260* - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May04_06:51:54.611526.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites_mdnac.2023May04_06:51:54.611526.zip&atstype=ATS] → Refer TCs - 29, 31, 32, 33, 34 & 35",2023-06-16T13:22:03.669+0000,"As per email thread, please update your json file.

Hi Sandeep,

 

There is dup of ‘WirelessVNFB’ in your solution input so it did not match the count.

Please remove the dup in your input.

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sr_mb/solution_test_input.json?until=fa33b77800765c7fc2b17334a52678d65335bf5f&untilPath=configs%2Fsr_mb%2Fsolution_test_input.json&at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#3865|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sr_mb/solution_test_input.json?until=fa33b77800765c7fc2b17334a52678d65335bf5f&untilPath=configs%2Fsr_mb%2Fsolution_test_input.json&at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#3865]

 

Thanks,

Tran

Hi Sandeep,

 

About iptransit, your solution input for mdnac were missing remoteAsNotation for iptransit.

 

If you’re using diff solution input, you can ref our common solution input to update the same.

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/config_48hr_test/solution_test_input_mdnac2.json?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#286|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/config_48hr_test/solution_test_input_mdnac2.json?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#286]

 

Thanks,

Tran

Hi Sandeep,

 

Its recommended to use our common solution input.

If you are using your own solution input files, please regularly sync your file with our common files.

 

For the rest of the issue of readers, its because your solution input file is not up to date with the changes in our common input files.

 

Thanks,

Tran Issue has been resolved after incorporating below fixes on branch - private/Hulk-ms/api-auto for MSTB1 specific files

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0f2190f8d030693a3935cfb47d04093065a3de02|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0f2190f8d030693a3935cfb47d04093065a3de02]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/fa9337afca0454ac184e09c18c57f9f8f1c8ae87|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/fa9337afca0454ac184e09c18c57f9f8f1c8ae87]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ed04b67de887ad7fbddcf0317343fb320d9a0a36|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ed04b67de887ad7fbddcf0317343fb320d9a0a36]


*Pass logs:* 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun19_23:37:14.484190.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun19_23:37:14.484190.zip&atstype=ATS] → Refer TC29

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun20_00:38:02.648133.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun20_00:38:02.648133.zip&atstype=ATS] → Refer TC31

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun20_01:00:57.834994.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun20_01:00:57.834994.zip&atstype=ATS] → Refer TC32-TC33

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun20_01:35:50.743843.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun20_01:35:50.743843.zip&atstype=ATS] → Refer TC34 - TC35","['Auton', 'Execution', 'Hulk', 'MSTB1', 'Multisite']",Tran Lam,Closed,SANDEEP SHIVARAMAREDDY
SEEN-1829,https://miggbo.atlassian.net/browse/SEEN-1829,  [Auton]:Hulk Test_TC176_ITSM_ticket_generation_test  /   test2_create_sgt_with_itsm_flag ,"*Reporter Analysis:*
we  observed  in the latest  hulk  build  in create_sgt_with_itsm_flag  failed with the below  error:
5917: 
 Library group ""task"" method ""wait_for_task_complete"" returned in 0:00:00.029494

{code:python}5919:  Scheduling for ""Adding sgt 16 to test ITSM 1687243867_936292"" failed for reason ""NCSS10065: Failed to validate schedule ITSM App ServiceNow is enabled, please use Schedule Later option instead of Now.""{code}

*Hulk  Version* : 2.1.710.70344
*Script Name* :\solution_test_sanityecamb_lan.py
*script  file :* \services\dnaserv\lib\api_groups\itsm\group.py
+*Branch :*+ private/Hulk-ms/sanity_api_auto

*Testcases Impacted :* 
  [Test_TC176_ITSM_ticket_generation_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1093046&size=78724&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun19_23:42:42.093373.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test2_create_sgt_with_itsm_flag

+*Fail Log :*+

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1110830&size=49665&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun19_23:42:42.093373.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1110830&size=49665&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun19_23:42:42.093373.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

+*Error snip from failed  log :*+ 

{code:python}5913:   api_switch_call called:
5914:  {}
5915:  Resource path full url: https://10.30.0.100/api/v1/task/c595a4e8-fa1a-4ec5-9294-df69bab84eee
5916:  {'endTime': 1687237873914, 'version': 1687237873915, 'startTime': 1687237873882, 'errorCode': 'NCSS10065', 'progress': 'NCSS10068: Creating task schedules failed', 'serviceType': 'NCSS', 'username': 'admin', 'failureReason': 'NCSS10065: Failed to validate schedule ITSM App ServiceNow is enabled, please use Schedule Later option instead of Now.', 'isError': True, 'instanceTenantId': '64875b633b187c27377dfc44', 'id': 'c595a4e8-fa1a-4ec5-9294-df69bab84eee'}
5917:  Library group ""task"" method ""wait_for_task_complete"" returned in 0:00:00.032903
5918:  Task Status: {'endTime': 1687237873914, 'version': 1687237873915, 'startTime': 1687237873882, 'errorCode': 'NCSS10065', 'progress': 'NCSS10068: Creating task schedules failed', 'serviceType': 'NCSS', 'username': 'admin', 'failureReason': 'NCSS10065: Failed to validate schedule ITSM App ServiceNow is enabled, please use Schedule Later option instead of Now.', 'isError': True, 'instanceTenantId': '64875b633b187c27377dfc44', 'id': 'c595a4e8-fa1a-4ec5-9294-df69bab84eee'}
5919:  Scheduling for ""Adding sgt 16 to test ITSM 1687237873_8063233"" failed for reason ""NCSS10065: Failed to validate schedule ITSM App ServiceNow is enabled, please use Schedule Later option instead of Now.""
5920:  {'response': {'taskId': 'c595a4e8-fa1a-4ec5-9294-df69bab84eee', 'url': '/api/v1/task/c595a4e8-fa1a-4ec5-9294-df69bab84eee'}, 'version': '1.0'}
5921:  Library group ""schedule-job"" method ""schedule_a_job_externalSchedule"" returned in 0:00:00.121540
5922:  SGT creation requested unsuccessfully{code}",2023-06-20T09:32:37.583+0000,"Is this a regression? Are there previous pass logs? If so this may be a defect. Please let me know. Hi  [~accountid:63f50bcece6f37e5ed93c87e]  ,
we  don’t have Hulk full pass log ,
TB 6 -Hulk 70381 
log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1064921&size=218539&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun24_05:13:38.258858.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1064921&size=218539&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun24_05:13:38.258858.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Error snip :

{code:python}6124:  Traceback (most recent call last):
6125:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
6126:      result = testfunc(func_self, **kwargs)
6127:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 12368, in test1_enable_configure_ITSM_bundle
6128:      if dnac_handle.configure_itsm_bundle():
6129:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
6130:      result = method(*args, **kwargs)
6131:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/api_groups/itsm/group.py"", line 121, in configure_itsm_bundle
6132:      if ""same name"" not in failed[""error""][""name""]:
6133:  KeyError: 'name'{code}

 
build
*Optimized  Log*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun27_05:07:06.214261.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun27_05:07:06.214261.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]
 hulk -2370 =-70389 
Faild  Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=76974&size=18908&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul02_06:11:33.461321.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=76974&size=18908&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul02_06:11:33.461321.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

error  snip:


{code:python}366:  Instance failed: {'details': {'name': 'itsm_DRE', 'description': '', 'ITSMType': 'ServiceNowConnection', 'data': {'endpoint': 'REST API Endpoint', 'DestinationUri': '/api/now/import/x_caci_cisco_dna_create_workflow_for_eve'}, 'dypId': '1194-498b-42a8-b4cc', 'id': '1bb7-2bd4-4f28-8dc8'}, 'error': {'data.ITSMType': 'Invalid value.'}}
367:  Failure in configuring ITSM bundle
368:  {'succeeded': [{'_id': '64a1e1a9fd4dc8bf61a32944', 'id': '7099-b838-412a-a1a0', 'createdBy': '', 'data': {'ConnectionSettings': {'Url': 'https://ven04533.service-now.com/', 'Auth_UserName': 'tranlam', 'Auth_Password': ''}}, 'description': '', 'dypId': 'f4ab-5be5-42fa-9ee1', 'dypMajorVersion': 1, 'dypName': 'ServiceNowConnection', 'name': 'itsm_SAS', 'schemaVersion': 0, 'softwareVersionLog': [], 'tenantId': '649bc0848f04e507c7e97db8', 'uniqueKey': 'ServiceNowConnection-itsm_SAS-1', 'updatedBy': 'admin', 'updatedDate': 1688330665658}], 'failed': [{'details': {'name': 'itsm_DRE', 'description': '', 'ITSMType': 'ServiceNowConnection', 'data': {'endpoint': 'REST API Endpoint', 'DestinationUri': '/api/now/import/x_caci_cisco_dna_create_workflow_for_eve'}, 'dypId': '1194-498b-42a8-b4cc', 'id': '1bb7-2bd4-4f28-8dc8'}, 'error': {'data.ITSMType': 'Invalid value.'}}]}{code} Hi,

Seeing same issue on Hulk-ESXI execution 

ESXI Build#3.710.75456

Script Name : solution_test_3sites_sjc_nyc_sf

Branch : private/Hulk-ms/api-auto

Failed log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17461804&size=17348&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul15_04:21:05.558740.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17461804&size=17348&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul15_04:21:05.558740.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [ENG-SDN / dnac-auto / b3c5296dd73 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/b3c5296dd739e74f6005b38a01fb2864a27bbe3c]

Added a fix in mainline, tried locally and it works, please give it a try. [~accountid:63f50bf0e8216251ae4d59ca] , pls. follow up with Krishna on the resolution of [https://jira-eng-sjc14.cisco.com/jira/browse/TEST-211|https://jira-eng-sjc14.cisco.com/jira/browse/TEST-211] to proceed further. Hi [~accountid:62ab7a399cd13c0068b18fe0],
Sure I’ll follow up with Krishna Hi [~accountid:63f50bcece6f37e5ed93c87e]  ,
Latest   hulk Run, We  observed   reject SGT request  failed  with the below error  &   sub TC  are blocked 
could you please  check 




+*Failed  Log:*+
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul24_08:38:04.414709.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul24_08:38:04.414709.zip&atstype=ATS]




{noformat}6042:  Method: GET

6043:  {'params': {'eventName': 'Group Based Policy', 'timeFilter': 1690213170557}}
6044:  Resource path full url: https://10.30.0.100/dna/intent/api/v1/integration-event-details
6045:  Library method ""ext_api_call_handle"" returned in 0:00:00.060333
6046:  {'eventName': 'Group Based Policy', 'headers': [{'title': 'Event Id', 'key': 'DNAEventID', 'link': 'viewHistory'}, {'title': 'Source', 'key': 'source'}, {'title': 'Destination', 'key': 'destination'}, {'title': 'ITSM Workflow', 'key': 'ITSMWorkflow'}, {'title': 'DNA Event Status', 'key': 'DNAEventStatus'}, {'title': 'ITSM Status', 'key': 'ITSMStatus'}, {'title': 'ITSM Id', 'key': 'ITSMEntityId'}, {'title': 'ITSM Link', 'key': 'ITSMLink', 'link': 'self'}, {'title': 'ITSM Ticket Assigned To', 'key': 'assignedTo'}, {'title': 'ITSM Last UpdatedTime', 'key': 'ITSMLastUpdatedTime', 'format': 'MMMM Do YYYY, h:mm:ss a'}, {'title': 'ITSM Entity Severity/Priority', 'key': 'ITSMEntitySeverity'}, {'title': 'DNA Event Severity', 'key': 'DNAEventPriority'}, {'title': 'ITSM Message', 'key': 'ITSMMessage'}], 'rows': [{'DNAEventID': '666d644e-c6c0-461e-85a8-baa0ca289cfc', 'DNAEvent': 'Group Based Policy', 'ITSMWorkflow': 'RFC', 'ITSMLink': 'https://ven04533.service-now.com/nav_to.do?uri=change_request.do?sys_id=460dada0dbdc71505a95ebcad39619a1', 'source': 'ServiceNow', 'destination': 'Cisco DNA Center', 'ITSMLastUpdatedTime': 1690213659000, 'ITSMEntitySeverity': '4 - Low', 'assignedTo': 'Unassigned', 'DNAEventStatus': 'NA', 'DNAEventPriority': 2, 'ITSMStatus': 'Review', 'ITSMEntityId': 'CHG0031424', 'ITSMMessage': 'Success'}, {'DNAEventID': '0148f616-df2e-4561-b810-a1db3ae5b257', 'DNAEvent': 'Group Based Policy', 'ITSMWorkflow': 'RFC', 'ITSMLink': 'https://ven04533.service-now.com/nav_to.do?uri=change_request.do?sys_id=8cbda9e0dbdc71505a95ebcad396199a', 'source': 'ServiceNow', 'destination': 'Cisco DNA Center', 'ITSMLastUpdatedTime': 1690213749000, 'ITSMEntitySeverity': '4 - Low', 'assignedTo': 'Unassigned', 'DNAEventStatus': 'NA', 'DNAEventPriority': 2, 'ITSMStatus': 'Scheduled', 'ITSMEntityId': 'CHG0031425', 'ITSMMessage': 'Success'}], 'groupedColumns': [{'title': 'Last In-Event Flow', 'keys': ['source', 'destination']}], 'sortBy': 'ITSMLastUpdatedTime', 'sortOrder': 'desc'}
6047:  Library group ""itsm"" method ""get_integration_events"" returned in 0:00:00.060771
6048:  Event with id 8cbda9e0dbdc71505a95ebcad396199a found with status Scheduled
6049:  Library group ""itsm"" method ""get_integration_event_status"" returned in 0:00:00.060991
6050:  Request state not canceled, instead: Sc
heduled{noformat}


[Test_TC176_ITSM_ticket_generation_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1065403&size=416055&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul24_08:38:04.414709.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test15_cleanup===>

failed for 6246: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/api_groups/itsm/group.py"", line 741, in readd_itsm_deleted_dev

{noformat}6247: 
     if ""New York"" in self.deleted_dev_site:{noformat}

{noformat}6248: 
 AttributeError: 'Group' object has no attribute 'deleted_dev_site'{noformat} Hi [~accountid:620b8357878c2f00729881c8] , the first error that you showed seems to be a defect, please raise it. Ping me on webex any clarifications you need.

Second error in cleanup should be resolved in my latest PR awaiting to be approved.","['Auton', 'Hulk', 'MSTB2', 'Sanity']",Andrew Chen,Resolved,Omkar Sharad Wagh
SEEN-1830,https://miggbo.atlassian.net/browse/SEEN-1830,[Auton] [NFW]- TC corresponding setting up proxy  fails with 404 Client Error,"*Regression:*  Solution Regression NFW

*Branch:* private/Ghost-ms/api-auto-nfw

*Uber ISO tested:* Ghost - 2.3.5.3-70194

*Script Used:* testcases/forty_eight_hour/solution_dnac_wireless_hardening.py

*Issue Faced:* We are observing some API issues to get response from server

# Under - [Test_TC7_DNAC_Prime_Migration_DNAC_Server_Settings_NTP_DHCP_AAA_DNS_setting|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=776460&size=405235&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_15:07:12.937070.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS]
# Sub TC- [test29_set_proxy|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1165438&size=16051&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_15:07:12.937070.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] 

TC getting errored for proxy settings. Looks to be script issue 

{{Error Code: 400 URL:https://10.4.23.12/api/system/v1/maglev/proxy Data:{'timeout': 60, 'data': '{""proxy_server"": ""http://proxy-wsa.esl.cisco.com"", ""proxy_port"": ""80"", ""proxy_username"": """", ""proxy_password"": """", ""proxy_settings_validation"": true}'} Headers:{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS}}

{{Reason: 400 Client Error: Bad Request for url: }}[https://10.4.23.12/api/system/v1/maglev/proxy|https://10.4.23.12/api/system/v1/maglev/proxy]

*Failed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=817426&size=15977&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fsr_nfw_job.2023Jun07_20:53:59.171456.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=817426&size=15977&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fsr_nfw_job.2023Jun07_20:53:59.171456.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] → Refer TC 7.29",2023-06-20T10:16:02.307+0000,"!10.4.23.12_proof.png|width=1163,height=620!

Hi [~accountid:712020:06823340-9d28-4722-9cd7-15ef3ea14cc6] ,  I have access DNAC cluster {{10.4.23.12}} and encounter the same issue on the UI, so I suggest it’s the problem with the DNAC itself, not the codes.

Because on our cluster 10.195.247.214, we use the same {{proxy_server}} and the same {{proxy_port}} thus the same payload for POST request, but we have no problem. ","['Auton', 'Execution', 'Ghost', 'NFW']",PhatHong Pham,Resolved,JagadeshKumar Enapanuri
SEEN-1831,https://miggbo.atlassian.net/browse/SEEN-1831,[Auton]:Test_TC203_AP_Zone/cleanup_AP_zone,"*Reporter Analysis:* 

From the logs and DB details, we can infer that there are following AP's which are part of the customZone [Zone1_enterprise].

00:0a:bc:00:1d:00
00:0a:bc:00:18:00
00:0a:bc:00:1f:00
00:0a:bc:00:16:00
00:0a:bc:00:1b:00
c8:84:a1:68:df:a0
00:0a:bc:00:07:00
00:0a:bc:00:09:a00
00:0a:bc:00:03:00
00:0a:bc:00:05:00
00:0a:bc:00:0b:00
00:0a:bc:00:1e:00
00:0a:bc:00:19:00
14:16:9d:7e:f5:00
00:0a:bc:00:17:00
78:bc:1a:88:cf:40
04:5f:b9:35:39:00
00:0a:bc:00:1c:00
00:0a:bc:00:1a:00
10:f9:20:fd:c5:80
00:0a:bc:00:08:00
00:0a:bc:00:06:00
00:0a:bc:00:04:00
00:0a:bc:00:02:00
00:0a:bc:00:0a:00

Reprovision AP to defaultZone  then trigger WLC reprovisoning .Also, pls update this new validation in the script runs and move the APs to defaultZone before removing the siteProfile or deleting the Zones.

Hence, the validation failure [introduced from 2337] where-in if there is an AP part of the customApZone, we will not be able to delete it from the NetworkProfile. I guess, in the scripts, sites are unassigned and moved to new NetworkProfile, where Zones are not present [hence UI siteProfile zone validation is bypassed]



*Description:  The error from log or more info* 
{{The Schedduled Job failed: with reason {'id': '3c2fbad0-353e-41c6-80db-a27386924992', 'triggeredJobTaskId': '07868c1c-c1b9-482a-9f27-ded515b54195', 'triggeredTime': 1686913183744, 'status': 'FAILED', 'failureReason': 'NCWL10219: One or more Access Points are provisioned in below Zones. Please re-provision them to either without Zone or same Zone part of new NetworkProfile. APZones: ', 'triggeredJobId': '3c2fbad0-353e-41c6-80db-a27386924992'}}}

*Branch Name:*  private/Ghost-ms/sanity_api_auto

*Script file/Usecase:* testcases/forty_eight_hour/solution_test_sanityecamb_lan.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:* 

*Fail Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1238373&size=2057999&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun16_03:48:59.799351.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1238373&size=2057999&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun16_03:48:59.799351.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]",2023-06-20T14:10:49.411+0000,"Hi [~accountid:63f50bf5e8216251ae4d59cf] ,
ISO :Ghost2.3.5.4-70759
The same issue  observed, During the Ghost cleanup script , Execution blocked  with the same  error : 

{code:python} The Schedduled Job failed: with reason {'id': '015a5d3f-6310-4161-8793-b4ce82cb8db3', 'triggeredJobTaskId': '4cf30de0-ae87-4d6a-ba73-cfefaf57ebef', 'triggeredTime': 1687780256059, 'status': 'FAILED', 'failureReason': 'NCWL10219: One or more Access Points are provisioned in below Zones. Please re-provision them to either without Zone or same Zone part of new NetworkProfile. APZones: [Zone1_Enterprise]', 'triggeredJobId': '015a5d3f-6310-4161-8793-b4ce82cb8db3'}{code}


+*Failed Log:*+
[https://ngdevx.cisco.com/services/taas/results/971193b7-c90d-48d5-9ac3-11547611cfb6|https://ngdevx.cisco.com/services/taas/results/971193b7-c90d-48d5-9ac3-11547611cfb6] Hi [~accountid:63f50bf5e8216251ae4d59cf] ,

In recent runs we saw different issue:
[2.In|http://2.In] recent runs, we saw different issue:
Test_TC1_AP_Zone  /   test5_provision_aps

5753: Provisioning AP of device failed for reason:NCSP11104: Error occurred while processing the 'modify' request. Additional info for support: taskId: 'c2309c9d-4fe9-498b-ad5b-46d3efd04dfe'. Internal error while attempting to transform the object for further processing.

*Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_zones.py-1311-apZones&begin=1430835&size=176515&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul04_22:05:52.156959.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_zones.py-1311-apZones&begin=1430835&size=176515&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul04_22:05:52.156959.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
RCA:
[http://172.21.236.183/sanity_rca/maglev-10.195.227.92-rca-2023-07-05_07-30-41_UTC.tar.gz|http://172.21.236.183/sanity_rca/maglev-10.195.227.92-rca-2023-07-05_07-30-41_UTC.tar.gz]

Deepak's analysis:

-07-05 05:18:18,388 |   INFO | qtp1076641925-23          |  | c.c.a.c.s.f.ServiceProvisionEngineRequestsFilter | Time taken for 'PUT' request '/data/customer-facing-service/ApWirelessConfiguration' with query 'null' is: 9ms |
2023-07-05 05:18:18,389 |   INFO | service-manager-request-1 |  | c.c.a.c.s.h.ServiceModifyRequestMessageHandler | ServiceModifyRequestMessageHandler :
time:Wed Jul 05 05:18:18 GMT 2023;
task id:c2309c9d-4fe9-498b-ad5b-46d3efd04dfe;
service request:
[{rfProfile=TYPICAL, siteId=c0d12be0-ea65-4a21-8b86-caf44f3a6c94, apMacAddrList=[14:16:9d:7e:f5:00, 78:bc:1a:88:cf:40, 04:5f:b9:35:39:00, c8:84:a1:68:df:a0, 10:f9:20:fd:c5:80, 00:0a:bc:00:02:00, 00:0a:bc:00:0b:00, 00:0a:bc:00:03:00, 00:0a:bc:00:16:00, 00:0a:bc:00:17:00, 00:0a:bc:00:18:00, 00:0a:bc:00:19:00, 00:0a:bc:00:1a:00, 00:0a:bc:00:1b:00, 00:0a:bc:00:1c:00, 00:0a:bc:00:1d:00, 00:0a:bc:00:1e:00, 00:0a:bc:00:04:00, 00:0a:bc:00:1f:00, 00:0a:bc:00:05:00, 00:0a:bc:00:06:00, 00:0a:bc:00:07:00, 00:0a:bc:00:08:00, 00:0a:bc:00:09:00, 00:0a:bc:00:0a:00], apzoneName=Zone1_Enterprise}];
clientType:null;
userName:admin
|
2023-07-05 05:18:18,393 |   INFO | service-manager-request-1 |  | c.c.a.c.s.lock.impl.LockManagerImpl | The owner c2309c9d-4fe9-498b-ad5b-46d3efd04dfe does not own any lock requested. locks requested [CFSLock] |
2023-07-05 05:18:18,410 |   INFO | service-manager-request-1 |  | c.c.a.c.s.lock.impl.LockManagerImpl | SPF Perf Monitoring: Time taken to acquire locks [CFSLock] for owner c2309c9d-4fe9-498b-ad5b-46d3efd04dfe is 0 secs. retVal is ACQUIRED. |
2023-07-05 05:18:18,410 |   INFO | service-manager-request-1 |  | c.c.a.c.s.h.ServiceRequestMessageHandler | Received json request: {""apMacAddrList"":[""14:16:9d:7e:f5:00"",""78:bc:1a:88:cf:40"",""04:5f:b9:35:39:00"",""c8:84:a1:68:df:a0"",""10:f9:20:fd:c5:80"",""00:0a:bc:00:02:00"",""00:0a:bc:00:0b:00"",""00:0a:bc:00:03:00"",""00:0a:bc:00:16:00"",""00:0a:bc:00:17:00"",""00:0a:bc:00:18:00"",""00:0a:bc:00:19:00"",""00:0a:bc:00:1a:00"",""00:0a:bc:00:1b:00"",""00:0a:bc:00:1c:00"",""00:0a:bc:00:1d:00"",""00:0a:bc:00:1e:00"",""00:0a:bc:00:04:00"",""00:0a:bc:00:1f:00"",""00:0a:bc:00:05:00"",""00:0a:bc:00:06:00"",""00:0a:bc:00:07:00"",""00:0a:bc:00:08:00"",""00:0a:bc:00:09:00"",""00:0a:bc:00:0a:00""],""apzoneName"":""Zone1_Enterprise"",""siteId"":""c0d12be0-ea65-4a21-8b86-caf44f3a6c94"",""rfProfile"":""TYPICAL""}
|
2023-07-05 05:18:18,418 |   INFO | service-manager-request-1 |  | c.c.a.c.s.h.ServiceRequestMessageHandler | SPF Perf Monitoring: Time taken to deserialize json for type ApWirelessConfiguration and qualifier null is 8 ms |
2023-07-05 05:18:18,418 |   INFO | service-manager-request-1 |  | c.c.x.m.f.impl.XMPSnapshotCacheImpl | Snapshot Cache is empty! |
2023-07-05 05:18:18,418 |   INFO | service-manager-request-1 |  | c.c.c.m.p.PersistentMetricManager | increment called with name spf.cfs.entity.APWirelessConfiguration, key NA |
2023-07-05 05:18:18,426 |  ERROR | service-manager-request-1 |  | c.c.a.c.s.h.ServiceModifyRequestMessageHandler | Modify Service Failed during CFS Mapping :NCSP11104: Error occurred while processing the 'modify' request. Additional info for support: taskId: 'c2309c9d-4fe9-498b-ad5b-46d3efd04dfe'. Internal error while attempting to transform the object for further processing. |
com.cisco.apic.controller.spf.api.exception.ServiceProvisioningException: NCSP11104: Error occurred while processing the 'modify' request. Additional info for support: taskId: 'c2309c9d-4fe9-498b-ad5b-46d3efd04dfe'. Internal error while attempting to transform the object for further processing.
at com.cisco.apic.controller.spf.handler.ServiceModifyRequestMessageHandler.handleRequest(ServiceModifyRequestMessageHandler.java:142) ~[service-provision-engine-fw-7.1.613.61093.jar:7.1.613.61093]
at com.cisco.grapevine.amqp.impl.GrapevineMessageListener.invokeHandler_aroundBody0(GrapevineMessageListener.java:436) ~[message-queue-sdk-7.0.613.61093.jar:7.0.613.61093]
at com.cisco.grapevine.amqp.impl.GrapevineMessageListener$AjcClosure1.run(GrapevineMessageListener.java:1) ~[na:7.0.613.61093]
at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[aspectjtools-1.8.13.jar:na]
at com.cisco.enc.i18n.localization.aop.EnableI18nOnRequestHandler.getEmptyResponse(EnableI18nOnRequestHandler.java:26) ~[i18n-7.1.613.61093.jar:7.1.613.61093]
at com.cisco.grapevine.amqp.impl.GrapevineMessageListener.invokeHandler(GrapevineMessageListener.java:398) ~[message-queue-sdk-7.0.613.61093.jar:7.0.613.61093]

*I see a PUT call for the APG without the name or namespace. Guess this is triggering the CFS Mapping Internal error exception.* Pls check the same..

yes. looks like the same. If it's a PUT call, then it needs to have the name, namespace and instanceid which is persisted.

Thanks,
Anusha John [~accountid:61efa8c457b25b006877eda3]  PFA image, It’s the PUT call payload for AP provisioning and it has the parameters(highlighted) mentioned in the below issue. Is this an intermittent issue ? Please share the testbed if the hit the error again, as I don’t observe it on automation testbed.

!Screenshot 2023-07-11 at 6.35.36 PM.png|width=1570,height=144! Fix to AP zone clean up workflow [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/7532e345499944e51862a9da03e9fccb41647120|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/7532e345499944e51862a9da03e9fccb41647120] Hi [~accountid:63f50bf5e8216251ae4d59cf] ,

Will share testbed if issue is seen again.

Thanks,
Anusha John Closing the Jira as we got passed log for the testcase where i saw the script issue after the fix:
Pass log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_zones.py-131-apZones&begin=2267&size=2191214&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul16_22:47:30.753874.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_zones.py-131-apZones&begin=2267&size=2191214&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul16_22:47:30.753874.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['AWS-Santiy', 'Auton', 'Ghost', 'Halleck', 'Integration', 'Optimized', 'Sanity']",Raji Mukkamala,Closed,Anusha John
SEEN-1832,https://miggbo.atlassian.net/browse/SEEN-1832,[Auton] [NFW]- Need to add sub testcase under TC28_DNAC_Device_Provisioning,"*Regression:* Solution Regression NFW

*Branch:* private/Ghost-ms/api-auto-nfw

*Uber ISO tested:* Ghost - 2.3.5.3-70194

*Script Used:* testcases/forty_eight_hour/solution_dnac_wireless_hardening.py

we need a separate sub TC for devices aggregate under TC28_DNAC_Device_Provisioning , after provisioning the devices if open the DNAC GUI and check there was a issue with cli authentication failure for this, need to add sub testcase for whether all the devices are in managed state or not.

Log : [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fsr_nfw_job.2023Jun08_00:55:16.882762.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fsr_nfw_job.2023Jun08_00:55:16.882762.zip&atstype=ATS]",2023-06-20T14:29:20.011+0000,"[~accountid:63f50be9e76fc61320f4eab3] , Assign to you for now since its NFW. You can check and reassign to the team if needed. Hi [~accountid:712020:06823340-9d28-4722-9cd7-15ef3ea14cc6] ,

I want to clarify about adding 1 sub-testcase for checking if have issues, based on what can we check it? Is it based on Manageability on the Inventory UI?

Where we can check the issue with cli authentication failure?

Thanks.

!image-20230822-032317.png|width=122,height=386!","['Auton', 'Execution', 'Ghost', 'NFW']",DatChi Pham,Backlog,JagadeshKumar Enapanuri
SEEN-1833,https://miggbo.atlassian.net/browse/SEEN-1833,[Auton] [NFW]- TC130_Verify_SNR_RSSI_Value -Connect wireless clients to ssid failed due to wireless clients not added to the testbed,"*Regression:* Solution Regression NFW

*Branch:* private/Ghost-ms/api-auto-nfw

*Uber ISO tested:* Ghost - 2.3.5.3-70194

*Script Used:* testcases/forty_eight_hour/solution_dnac_wireless_hardening.py

*Issue Faced:* During our NFW testing on Ghost patch1  Promoted Uber ISO - 2.3.5.3-70194, we have observed while trying to retrieve the wireless client could not able fetching details

# Under - [Test_TC130_Verify_SNR_RSSI_Value|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15063317&size=2896&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_17:47:07.205108.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] 

Even though we do not have any wireless clients part of testbed, the TC is being executed. Also the error message is not clear in case of Failure. Could you please check once on this?

Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15063887&size=2164&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_17:47:07.205108.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15063887&size=2164&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_17:47:07.205108.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS]-->Refer TC130.1",2023-06-20T14:37:06.982+0000,"[~accountid:63f50be9e76fc61320f4eab3] , Assign to you for now since its NFW. You can check and reassign to the team if needed. Hi [~accountid:712020:06823340-9d28-4722-9cd7-15ef3ea14cc6] ,

Fix log notification in a clear: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6843/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6843/overview]

Please reviewed and merged,

Thanks","['Auton', 'Execution', 'Ghost', 'NFW']",DatChi Pham,Resolved,JagadeshKumar Enapanuri
SEEN-1834,https://miggbo.atlassian.net/browse/SEEN-1834,[Auton] [NFW]-TC154_verify_wlc_sensor_temperature -Need to add Aireos wlc devices to the testbed ,"*Regression:* Solution Regression NFW

*Branch:* private/Ghost-ms/api-auto-nfw

*Uber ISO tested:* Ghost - 2.3.5.3-70194

*Script Used:* testcases/forty_eight_hour/solution_dnac_wireless_hardening.py

*Issue Faced:*  During our NFW testing on Ghost Promoted Uber ISO - 2.3.5.3-70194, we have observed inside the testcase there was no verification check 



 1. Under [Test_TC154_verify_wlc_sensor_temperature|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24940961&size=5213&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fsr_nfw_job.2023Jun13_20:52:28.091350.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=admin&atstype=PYATS]  /   test1_verify_wlc_sensor_temp

inside the testcase there was no verification check and passed the testcase, it’s a false pass. Could you please check on this?

2. Under -[+Test_TC155_verify_wlc_interface_stats+|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19700608&size=4870&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_17:47:07.205108.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] 

Test case specific Aireos WLC device. We do not have Aireos WLC device in the testbed. So related checks are not performed but still testcase passed. could you please check once this?



Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24940961&size=5213&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fsr_nfw_job.2023Jun13_20:52:28.091350.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24940961&size=5213&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fsr_nfw_job.2023Jun13_20:52:28.091350.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=admin&from=trade&view=all&atstype=pyATS] -->Refer TC154

Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24946174&size=4870&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fsr_nfw_job.2023Jun13_20:52:28.091350.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24946174&size=4870&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fsr_nfw_job.2023Jun13_20:52:28.091350.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=admin&from=trade&view=all&atstype=pyATS]-->Refer TC155",2023-06-20T14:42:59.014+0000,"[~accountid:63f50be9e76fc61320f4eab3] , Assign to you for now since its NFW. You can check and reassign to the team if needed.","['Auton', 'Execution', 'Ghost', 'NFW']",JagadeshKumar Enapanuri,Cancelled,JagadeshKumar Enapanuri
SEEN-1835,https://miggbo.atlassian.net/browse/SEEN-1835,[Auton] [NFW]-TC189_generate_device_link_AP_flap_issues-Device console prompt not recovered after Device reload,"*Regression:* Solution Regression NFW

*Branch:* private/Ghost-ms/api-auto-nfw

*Uber ISO tested:* Ghost - 2.3.5.3-70194

*Script Used:* testcases/forty_eight_hour/solution_dnac_wireless_hardening.py

*Issue Faced:* We are observing some of the API calls to not getting response from server

1.Under- [Test_TC189_generate_device_link_AP_flap_issues|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=26262511&size=737862&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fsr_nfw_job.2023Jun13_20:52:28.091350.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=admin&from=trade&view=all&atstype=pyATS]

[test2_check_power_supply_failure_issue_and_is_auto_resolved|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=26316649&size=683547&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fsr_nfw_job.2023Jun13_20:52:28.091350.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=admin&from=trade&view=all&atstype=pyATS]

We are unable to login the console prompt after reloading the device, have checked on edge devices there was one power supplier active , could you please check and help us get it resolve 

{{NFW-AA-2 con0 is now available Press RETURN to get started. }}

{noformat}134759: 
 Sending prompt recovery command: b'\x1e'{noformat}

{noformat}134760: 
 Traceback (most recent call last):{noformat}

{noformat}134761: 
   File ""/auto/dna-sol/pyats-ws/pyats-phannguy-seen-53/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 725, in call_service{noformat}



Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24146812&size=682303&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_17:47:07.205108.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24146812&size=682303&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_17:47:07.205108.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS]",2023-06-20T14:54:07.507+0000,"[~accountid:63f50be9e76fc61320f4eab3] , Assign to you for now since its NFW. You can check and reassign to the team if needed.","['Auton', 'Execution', 'Ghost', 'NFW']",Phan Nguyen,Cancelled,JagadeshKumar Enapanuri
SEEN-1836,https://miggbo.atlassian.net/browse/SEEN-1836,[Auton]:Hulk- Bapi  - Test_TC11_DNAC_Verify_adding_range_discovery_ssh_global_credentials  / Test_TC19_DNAC_Wireless_SSID_creation_open_enterprise  / Test_TC20_DNAC_addition_of_wireless_nw_profiles,"*Reporter Analysis:*
*1)* we observed in the latest Hulk build in BAPI   script execution  Some  TC failed with the below error  , due to this  issue  discovery failing for ewlc , Transit
2317: 
 Exception: 'hostname'
Error snip:

{code:python}2317:  Exception: 'hostname'
2318:  Traceback (most recent call last):
2319:    File ""/data/jenkins/ws_sjc2_ds/workspace/Express-Sanity/Hulk/Hulk-Legacy-Deployment_and_Express_Sanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper
2320:      result = testfunc(func_self, **kwargs)
2321:    File ""/data/jenkins/ws_sjc2_ds/workspace/Express-Sanity/Hulk/Hulk-Legacy-Deployment_and_Express_Sanity/testcases/ext_api_test/solution_test_sanityext_api.py"", line 569, in test1_verify_adding_range_discovery_ssh_global_credentials
2322:      if dnac_handle.ext_discover_devices_via_multirange(""DNAC-Discovery"", device_list=dnac_handle.dnaconfig.devlist,
2323:    File ""/data/jenkins/ws_sjc2_ds/workspace/Express-Sanity/Hulk/Hulk-Legacy-Deployment_and_Express_Sanity/services/dnaserv/lib/decorators.py"", line 32, in wrapper
2324:      result = method(*args, **kwargs)
2325:    File ""/data/jenkins/ws_sjc2_ds/workspace/Express-Sanity/Hulk/Hulk-Legacy-Deployment_and_Express_Sanity/services/dnaserv/lib/ext_api_groups/ext_discover/group.py"", line 189, in ext_discover_devices_via_multirange
2326:      self.log.info(f""Device: {device['hostname']} with id: {device['id']}"")
2327:  KeyError: 'hostname'{code}


+*Failed log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=326144&size=78310&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fenv_auto_job.2023Jun20_06:02:12.327387.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=326144&size=78310&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fenv_auto_job.2023Jun20_06:02:12.327387.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*2)*  [*Test_TC19_DNAC_Wireless_SSID_creation_open_enterprise*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=326398&size=61028&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fenv_auto_job.2023Jun20_06:50:09.217101.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]  */   test2_ssid_creation_open_enterprise  (Failed)*
Error  snip

{code:python}1896:   External API handler called:
1897: 
1898:  Resource path: /v1/dnacaap/management/execution-status/6ade60f0-237a-45a8-a478-46de62bb3a1f
1899:  Method: GET

1900:  {}
1901:  Resource path full url: https://10.22.40.52/dna/intent/api/v1/dnacaap/management/execution-status/6ade60f0-237a-45a8-a478-46de62bb3a1f
1902:  Library method ""ext_api_call_handle"" returned in 0:00:00.028107
1903:  Task Failed!!! {'bapiKey': '8a96-fb95-4d09-a349', 'bapiName': 'Create Enterprise SSID', 'bapiExecutionId': '6ade60f0-237a-45a8-a478-46de62bb3a1f', 'startTime': 'Tue Jun 20 13:56:15 UTC 2023', 'startTimeEpoch': 1687269375223, 'endTime': 'Tue Jun 20 13:56:15 UTC 2023', 'endTimeEpoch': 1687269375256, 'timeDuration': 33, 'status': 'FAILURE', 'bapiError': 'Execution Failed with error: INVALID_RADIO_POLICY', 'runtimeInstanceId': 'DNACP_Runtime_c17a96d0-6946-401e-8ea4-5aee7efc0fc7'}
1904:  Wireless SSID OPENtb13 configuration failed!!{code}


+*Failed Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=328118&size=59128&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fenv_auto_job.2023Jun20_06:50:09.217101.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=328118&size=59128&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fenv_auto_job.2023Jun20_06:50:09.217101.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*3)*[*Test_TC20_DNAC_addition_of_wireless_nw_profiles*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=387426&size=74193&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fenv_auto_job.2023Jun20_06:50:09.217101.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]  */   test1_addition_of_wireless_nw_profiles* 
Error  snip

{code:python}2235: 
2236:   External API handler called:
2237: 
2238:  Resource path: /v1/dnacaap/management/execution-status/0d8df16a-3536-43fa-b34b-d37089a09218
2239:  Method: GET

2240:  {}
2241:  Resource path full url: https://10.22.40.52/dna/intent/api/v1/dnacaap/management/execution-status/0d8df16a-3536-43fa-b34b-d37089a09218
2242:  Library method ""ext_api_call_handle"" returned in 0:00:00.028569
2243:  Task Failed!!! {'bapiKey': 'daa0-bb75-4e2a-8da6', 'bapiName': 'Create Update Dynamic interface', 'bapiExecutionId': '0d8df16a-3536-43fa-b34b-d37089a09218', 'startTime': 'Tue Jun 20 14:04:20 UTC 2023', 'startTimeEpoch': 1687269860592, 'endTime': 'Tue Jun 20 14:04:20 UTC 2023', 'endTimeEpoch': 1687269860617, 'timeDuration': 25, 'status': 'FAILURE', 'bapiError': 'Validation Error: VLANID should be an integer. Please pass correct PayLoad. ', 'runtimeInstanceId': 'DNACP_Runtime_c17a96d0-6946-401e-8ea4-5aee7efc0fc7'}
2244:  False
2245:  Wireless interface configure error
2246: {code}

+*Failed Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=387984&size=60075&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fenv_auto_job.2023Jun20_06:50:09.217101.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=387984&size=60075&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fenv_auto_job.2023Jun20_06:50:09.217101.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Build:* 2.1.710.70362
*Branch Name:*  private/Hulk-ms/sanity_api_auto
*Script file:*  solution_test_sanityext_api.py

*input file:* solution_test_input.json

*Testbed details*  :[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-13|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-13]",2023-06-20T16:32:24.980+0000,"Issues 2 and 3 are known issues while running regression and raised PR:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6112/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6112/overview]
Issue 1 can’t be found in my latest run ([see pass log|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery.py-5.1-toolsDiscovery&begin=76166&size=102777&archive=%2Fws%2Fsjc-it%2Fthanhtn2%2Fpyats%2Fusers%2Fthanhtn2%2Farchive%2F23-06%2Fextapisanity.2023Jun18_23:50:50.038632.zip&ats=%2Fws%2Fsjc-it%2Fthanhtn2%2Fpyats&submitter=thanhtn2&from=trade&view=all&atstype=pyATS]) [~accountid:620b8357878c2f00729881c8]The PR has been merged. Kindly pull the latest version and re-run your task. If issue 1 persists, please inform me.","['Auton', 'BAPI', 'Hulk', 'Sanity']",ThanhTan Nguyen,Closed,Omkar Sharad Wagh
SEEN-1840,https://miggbo.atlassian.net/browse/SEEN-1840,[Auton]:Hulk:Test_TC213_template_conflicts_in_template_hub,"*Branch Name:*  private/Hulk-ms/sanity_api_auto

*Script file: solution_test_sanityecamb_lan.py*

*input file:* solution_test_input.json

DNAC Release:
Hulk ISO: 2.1.710.70344

Polarisis:17.12,1

#######################################################
Description:

Observed with Hulk: 2.1.710.70344
ssid creation failed with the below error :

{{Message:{""response"":{""errorCode"":""NCND00001"",""message"":""NCND00001: The Common Settings request has validation errors"",""detail"":""[\""NCND03105: Empty WPA2/WPA3 Encryption is not allowed for Enterprise, Personal and Open Secured. VCR_test\""]""},""version"":""1.0""}}}

[https://cdetsng.cisco.com/webui/#view=CSCwf14001|https://cdetsng.cisco.com/webui/#view=CSCwf14001] A similar defect has been moved to auton

*Failed log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6072259&size=750966&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun16_05:49:22.645477.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6072259&size=750966&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun16_05:49:22.645477.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

+*Log 2*+
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-06/env_auto_job.2023Jun20_22:15:06.981507.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-06/env_auto_job.2023Jun20_22:15:06.981507.zip&atstype=ATS]",2023-06-21T05:59:46.994+0000,"[ENG-SDN / dnac-auto / d98f6eb458f - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d98f6eb458f085d0eaf2b8e9387c0167523e4242]

FIx here added to hulk main line Hi [~accountid:63f50bcece6f37e5ed93c87e]  ,
We observed two sub-TCs failed with key errors after pulling the latest code.
Could you please check  

Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1995723&size=412121&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun23_08:09:01.144715.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1995723&size=412121&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun23_08:09:01.144715.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

  [Test_TC213_template_conflicts_in_template_hub|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1995723&size=412121&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun23_08:09:01.144715.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_add_ssid_for_VCR_wireless_template
+*Error snip:*+

{noformat}10940:  Traceback (most recent call last):
10941:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
10942:      result = testfunc(func_self, **kwargs)
10943:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 14454, in test1_add_ssid_for_VCR_wireless_template
10944:      if dnac_handle.add_VCR_ssid_and_associate_profile_provision():
10945:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
10946:      result = method(*args, **kwargs)
10947:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/api_groups/vcr/group.py"", line 583, in add_VCR_ssid_and_associate_profile_provision
10948:      VCR_ssid_config = self.services.input_data[""VCR_SSID""]
10949:  KeyError: 'VCR_SSID'{noformat}


[Test_TC213_template_conflicts_in_template_hub|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1995723&size=412121&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun23_08:09:01.144715.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test9_verify_provision_templates_inventory_provision_VCR

+*Error Snip:*+


{noformat}11768:  Traceback (most recent call last):
11769:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
11770:      result = testfunc(func_self, **kwargs)
11771:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 14543, in test9_verify_provision_templates_inventory_provision_VCR
11772:      if dnac_handle.provision_VCR_template_ewlc_delete_ssid():
11773:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
11774:      result = method(*args, **kwargs)
11775:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/api_groups/vcr/group.py"", line 624, in provision_VCR_template_ewlc_delete_ssid
11776:      if self.services.provision_devices(device_list=dev_list, dev_to_config={dev_list[0]: config_to_check}):
11777:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
11778:      result = method(*args, **kwargs)
11779:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/api_groups/provision/group.py"", line 176, in provision_devices
11780:      if self.services.dnaconfig.testbed.devices[dev].role.find(""WLC"") != -1 :
11781:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/topology/bases.py"", line 101, in __getitem__
11782:      return super().__getitem__(key)
11783:  KeyError: None{noformat}

Branch: private/Hulk-ms/sanity_api_auto
Build  :Hulk 2.3.7.0-70372 Hi Omkar, initially the auton was for VCR ssid api payload, which means it was at least passing this part where you are getting the “VCR_SSID” key error…


I have not made any changes that would get the key to disappear… I tested locally and VCR ssid went through.


Did you change something with choosing the solution input? I checked the sanity api auto branch and it looks like the VCR_SSID is there…

[Source of solution_test_input.json - dnac-auto - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/config_48hr_test/solution_test_input.json?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto]



[VCR Script Backend changes and Template Hub TC enhancements - EDPEIXOT Solution Engineering Team Wiki - IT Wiki (cisco.com)|https://wiki.cisco.com/pages/viewpage.action?pageId=1543351431]

Make sure whichever solution input you are using has the VCR_SSID input there as per the wiki.



But anyhow, your second issue shows you do not have ewlc in your testbed, so anyhow the test should skip, I added a commit which you seem to be missing: [ENG-SDN / dnac-auto / 3abc6c52bcc - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3abc6c52bcce4ce054a2ee54c7072fded80738c6#testcases/forty_eight_hour/solution_test_sanityecamb_lan.py]

This commit was 2 days ago, but you have reverted it because it was causing issue. However I have fixed the issue part, but your branch still does not have the rest of the working code since you reverted the whole commit. Please get the code from that commit back into the branch.



[ENG-SDN / dnac-auto / 51008ceeca4 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/51008ceeca4dedfdc2f050c10cc4d33dc11728ce]

This is the commit which reverts only the error part. So once you get the previous commit, you can use the changes in this commit to revert the error part.
 Thanks [~accountid:63f50bcece6f37e5ed93c87e], I'll check the latest Hulk run and update you. Please address the comment too, as I said there are some changes which are in the main branch that your branch needs to get. Simply pulling may not be enough, please make sure you get the commits in some other way if thats the case. And check your solution input file for VCR_SSID case. Build :Hulk 2370-70389
Pass Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14186596&size=1023810&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul03_00:36:17.018546.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14186596&size=1023810&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul03_00:36:17.018546.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Hulk', 'Integration', 'Optimized', 'Sanity']",Andrew Chen,Closed,Omkar Sharad Wagh
SEEN-1841,https://miggbo.atlassian.net/browse/SEEN-1841,"Auton [On-Prem Sanity] VCR: SDA Automation, Control and Rollback Visibility","*Requirement:* check on the availability of WLC or eWLC and accordingly skip or execute the test-case.

*Reporter Analysis:* 

I have executed this TC on Sanity TB3 - where we have only WLC and not ewlc(9800). 
As the feature pre-requisite we should have one ewlc on the setup. The testcase should check for ewlc on the testbed and then decide to skip the execution of next set of testcases as part of pre-check for this testcase. 

In current run - there is no pre-check and even if we don’t have ewlc device the next set of sub-tests are being executed. Please find the log here

Could you please add pre-check tc and decide on to skip or run the testcase

*Description:  The error from log or more info* 

{noformat}Library group ""wireless"" method ""get_ewlc_dev"" returned in 0:00:00.000452
10023:  Filtered dev list for ewlc: [None]
  
  return super().__getitem__(key)
  10069: KeyError: None{noformat}

*Branch Name:*  private/Halleck-ms/sanity_api_auto

*Script file/Usecase:* testcases/forty_eight_hour/solution_test_sanityecamb_lan.py

Test_TC213_template_conflicts_in_template_hub / test1_add_ssid_for_VCR_wireless_template / test9_verify_provision_templates_inventory_provision_VCR

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0*

*Fail Log:* [Test_TC213_template_conflicts_in_template_hub|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2352049&size=450113&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun05_12:05:18.683447.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[test1_add_ssid_for_VCR_wireless_template|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2352613&size=74349&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun05_12:05:18.683447.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[test9_verify_provision_templates_inventory_provision_VCR|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2718846&size=13483&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun05_12:05:18.683447.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-06-21T07:53:19.558+0000,,"['Auton', 'Halleck', 'Hulk', 'Integration']",Andrew Chen,Cancelled,Ashwini R Jadhav
SEEN-1842,https://miggbo.atlassian.net/browse/SEEN-1842,"[Auton]:Need a requirement check before starting this TC VCR: SDA Automation, Control and Rollback Visibility","*Requirement:* check on the availability of WLC or eWLC and accordingly skip or execute the test-case.

*Reporter Analysis:* 

I have executed this TC on Sanity TB3 - where we have only WLC and not ewlc(9800). 
As the feature pre-requisite we should have one ewlc on the setup. The testcase should check for ewlc on the testbed and then decide to skip the execution of next set of testcases as part of pre-check for this testcase. 

In current run - there is no pre-check and even if we don’t have ewlc device the next set of sub-tests are being executed. Please find the log here

Could you please add pre-check tc and decide on to skip or run the testcase

*Description:  The error from log or more info* 

{noformat}Library group ""wireless"" method ""get_ewlc_dev"" returned in 0:00:00.000452
10023:  Filtered dev list for ewlc: [None]
  
  return super().__getitem__(key)
  10069: KeyError: None{noformat}



*Branch Name:*  private/Halleck-ms/sanity_api_auto

*Script file/Usecase:* testcases/forty_eight_hour/solution_test_sanityecamb_lan.py

Test_TC213_template_conflicts_in_template_hub / test1_add_ssid_for_VCR_wireless_template / test9_verify_provision_templates_inventory_provision_VCR

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0*

*Fail Log:* [Test_TC213_template_conflicts_in_template_hub|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2352049&size=450113&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun05_12:05:18.683447.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[test1_add_ssid_for_VCR_wireless_template|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2352613&size=74349&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun05_12:05:18.683447.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[test9_verify_provision_templates_inventory_provision_VCR|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2718846&size=13483&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun05_12:05:18.683447.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-06-21T08:07:51.300+0000,"Hi Ashwini, I have pushed the changes to latest halleck and hulk. [ENG-SDN / dnac-auto / c7374925af1 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c7374925af163270b0d837a196cd7e3bdcf95ebc] Thanks [~accountid:63f50bcece6f37e5ed93c87e] ","['Auton', 'Hulk', 'Integration', 'Optimized', 'Sanity']",Andrew Chen,Resolved,Ashwini R Jadhav
SEEN-1843,https://miggbo.atlassian.net/browse/SEEN-1843,[Auton][MSTB2] : Test_TC26_DNAC_adding_fabric_devices_to_site  /   test1_verify_adding_devices_to_the_site,"Hi Andrew,

Branch : private/Hulk-ms/api-auto

Script Name : solution_test_3sites_sjc_nyc_sf.py

After latest commit on group.py file in provision we are seeing adding device to site is failing with below error.
File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/utils.py"", line 1024, in check_header_validity
    if not pat.match(value):
TypeError: expected string or bytes-like object

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/dnaserv/lib/api_groups/provision/group.py"", line 49, in add_devices_to_site
    response = self.services.api_switch_call(method=""POST"", resource_path= url, data= parameters, headers={""DC-Push"":False})
  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/dnaserv/lib/api_groups/utils/group.py"", line 51, in api_switch_call
    response = self.services.base.NB_API.call_api(method, resource_path, **kwargs)
  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/dnaserv/client_manager.py"", line 584, in call_api
    response = super(ApicemClientManager, self).call_api(
  File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/dnaserv/client_manager.py"", line 308, in call_api
    response = requests.request(method, url, headers=headers, verify=verify, **kwargs)
2023-06-21T07:04:54: %API-GROUP-PROVISION-ERROR: Adding 9ec11d65-6ff0-4d3a-99b2-b34d547e00a1 device to site failed

Trade log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7342143&size=73778&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fauto_MS_job.2023Jun21_05:50:31.116356.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7342143&size=73778&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fauto_MS_job.2023Jun21_05:50:31.116356.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Jenkins log : [https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/MS-TB2/job/MSTB2_Solution_Regression_Git/353/consoleFull|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/MS-TB2/job/MSTB2_Solution_Regression_Git/353/consoleFull]

Latest PR causing failure : [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3abc6c52bcce4ce054a2ee54c7072fded80738c6#services/dnaserv/lib/api_groups/provision/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3abc6c52bcce4ce054a2ee54c7072fded80738c6#services/dnaserv/lib/api_groups/provision/group.py]",2023-06-21T14:59:04.343+0000,Reverted the change,"['Auton', 'Hulk', 'MSTB2']",Andrew Chen,Resolved,Divakar Kumar Yadav
SEEN-1844,https://miggbo.atlassian.net/browse/SEEN-1844,[Auton]:Hulk  mapping of UCs are not executing properly.,"Hi [~accountid:62d2fe9f8afb5805e5d5af49]   ,
*Branch Name:*  private/Hulk-ms/sanity_api_auto

*Script file: usecasemaps\lansanity\lansanity_usecases_maps.yaml*
*Script file: usecasemaps\non-lansanity\lansanity_usecases_maps.yaml*


*input file:* solution_test_input.json


we  observed that the new  optimized mapping yaml, creating issues we unable to execute a single uc ,

Please  find  below Jenkins jobs :   
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/174/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/174/]
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/175/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/175/]
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/176/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/176/]",2023-06-21T16:58:07.702+0000,"There were some new changes/fix in job utils and mapping yaml  last week.
[~accountid:620b8357878c2f00729881c8]  please pull latest and try again. Sure  [~accountid:62d2fe9f8afb5805e5d5af49] ","['Auton', 'Hulk', 'Optimized', 'Sanity']",Pawan Singh,Closed,Omkar Sharad Wagh
SEEN-1845,https://miggbo.atlassian.net/browse/SEEN-1845,[Auton] Test_TC74_backup_space_test_and_add_server_details / test2_attempt_backup - There is a AssertionError Even after Backup taken is successfully,"*Description:*  The testcase is to take backup of the cluster and the backup is taken successfully.  But the script is failing with Assertion Error

!image-20230622-063216.png|width=1529,height=345!



*ISO:* Ghost P2 

*Script:* testcases/forty_eight_hour/solution_test_sanityecamb_lan.py
*Branch:* private/Ghost-ms/sanity_api_auto

*Impacted Testcase* : Test_TC74_backup_space_test_and_add_server_details / test2_attempt_backup

*Usecase*: systemBackupRestoreconfigs

*Error Snip:* 

{noformat}: Exception:
2023-06-21T23:07:16: %AETEST-ERROR: Traceback (most recent call last):
2023-06-21T23:07:16: %AETEST-ERROR:   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-sanity-common-Multi-job@2/services/commonlibs/test_wrapper.py"", line 301, in wrapper
2023-06-21T23:07:16: %AETEST-ERROR:     result = testfunc(func_self, **kwargs)
2023-06-21T23:07:16: %AETEST-ERROR:   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-sanity-common-Multi-job@2/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 5660, in test2_attempt_backup
2023-06-21T23:07:16: %AETEST-ERROR:     assert result is False
2023-06-21T23:07:16: %AETEST-ERROR: AssertionError{noformat}



*Failed log*: [*Test_TC74_backup_space_test_and_add_server_details*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2591646&size=554737&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun21_22:49:41.752625.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

*Testbed wiki:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2]",2023-06-22T06:15:22.645+0000,"This backup is not supposed to be successful, this because the backup server is not setup correctly. Please follow the steps in the wiki to limit the directory size of the backup dir,  so the backup should fail due to out of space error. [https://wiki.cisco.com/display/EDPEIXOT/System+Health+Notifications|https://wiki.cisco.com/display/EDPEIXOT/System+Health+Notifications] “/data/backup/sanity/backup-test-tempdir” dir is expected to have limited space.

Same has been fixed and now TC is working as expected:

{noformat}root@AS-U16-NFS-WS:/data/backup/sanity/backup-test-tempdir# cd ..
root@AS-U16-NFS-WS:/data/backup/sanity# df -h backup-test-tempdir/
Filesystem      Size  Used Avail Use% Mounted on
/dev/sdb1       5.0T  3.7T  1.1T  77% /data
root@AS-U16-NFS-WS:/data/backup/sanity# mount -t tmpfs -o size=1k tmpfs backup-test-tempdir/
root@AS-U16-NFS-WS:/data/backup/sanity# df -h backup-test-tempdir/
Filesystem      Size  Used Avail Use% Mounted on
tmpfs           4.0K     0  4.0K   0% /data/backup/sanity/backup-test-tempdir {noformat}","['Auton', 'Ghost', 'Halleck', 'Optimized', 'Sanity', 'hulk']",Andrew Chen,Closed,Ashwini R Jadhav
SEEN-1848,https://miggbo.atlassian.net/browse/SEEN-1848,[Auton] [Hulk] - Wireless Fabric enabled ECA device had network assurance related configs not pushed due to incorrect value passed in Payload ,"*Regression:* Solution Regression Multisite - DR+MDNAC

*Branch:* private/Hulk-ms/api-auto

*Uber ISO tested:* Hulk - 2.1.710.70344

*Script Used:* testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Issue Faced:* 

During our ongoing Solution testing on Hulk Promoted Uber ISO - 2.1.710.70344, we observed an issue related to network assurance summary not having right output and also network assurance configs not pushed despite enabling fabric wireless for ECA device on Reader node cluster.

On Trying to debug further along with Vigneswar from DE team, it was observed that the payload being passed while enabling fabric wireless based on new VCR changed implemented has *fabric_device_update* value. But as per Vigneswar it has to be instead *fabric_device_operation* value to be used. And its due to this different value used we have observed the above issue.

Reference wiki shared by Vigneswar - [https://wiki.cisco.com/display/APICEMUCI/OperationalInfo+property+for+SDA+2.0+UI+operations|https://wiki.cisco.com/display/APICEMUCI/OperationalInfo+property+for+SDA+2.0+UI+operations]

*Current Code snip causing problem:*

data['operationalInfo'] = {
""operations"": [""fabric_device_update""],
""change"": ""UPDATE""
}

*Current Reference pass log which still caused issue* - [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6134647&size=483602&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun20_01:00:57.834994.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6134647&size=483602&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun20_01:00:57.834994.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-06-22T18:22:16.665+0000,"[~accountid:63f50bcece6f37e5ed93c87e] : Thanks for adding the fix for this issue!

[*Commit link :* |https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/60e2c8ffbad9d2d5cd9526e07d7554834db1853b][https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/60e2c8ffbad9d2d5cd9526e07d7554834db1853b|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/60e2c8ffbad9d2d5cd9526e07d7554834db1853b]
We will be validating during our next Hulk Regression testing and update our observations

 During last week Regression testing on Promoted Hulk - 2.1.710.70389, We have observed issues during fabric deployment workflow and still the issue has not be resolved. Once this is resolved, will be verifying for the fix. Issue is no more observed during Hulk Testing.

Uber ISO tested - *Hulk RC2 - 2.1.710.70462*

*Pass log:* [Test_TC33_enable_fabric_wireless_eca|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10663690&size=2579929&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_mb_multi_sites_mdnac.2023Jul26_07:19:54.868292.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Execution', 'Hulk', 'MSTB1', 'Multisite']",Andrew Chen,Closed,SANDEEP SHIVARAMAREDDY
SEEN-1855,https://miggbo.atlassian.net/browse/SEEN-1855,Hulk - Failed to push any of the service policy config on 9KB1 device,"Regression -  Solution sanity

DNAC Build - Hulk Devtest -2.1.710.8010454
Device image : V1712_THROTTLE_LATEST_20230530_143221_V17_12_0_24

RCA:/ws/schathar-bgl/rca/maglev-204.192.1.51-rca-2023-06-14_03-50-24_UTC.tar.gz

Failed Log :[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=49926941&size=1110277&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May27_00:00:52.655127.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=49926941&size=1110277&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-05%2Fsanitycombine.2023May27_00:00:52.655127.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

*Branch Used*: bgl/Hulk-ms/api-auto

*Test Suite*: [+https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=refs%2Fheads%2Fbgl%2FHulk-ms%2Fapi-auto+|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=refs%2Fheads%2Fbgl%2FHulk-ms%2Fapi-auto]

*Tran’s comment* - Need to exclude the ‘App’ Interfaces from this test validation.",2023-06-26T00:57:02.747+0000,"* PR has been raised: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6234/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6234/overview]
* passlog: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-07/sanity-intg2.2023Jul10_01:23:41.570075.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-07/sanity-intg2.2023Jul10_01:23:41.570075.zip&atstype=ATS] [~accountid:642a816bd774ab7297295df0],  mentioned PR has been approved and merged to Hulk Branch. Marking this ticket as “Resolved”.

Please validate the fix and confirm back on this ticket to get it “Closed”. [~accountid:62ab7a399cd13c0068b18fe0] Current Ghost run is also facing the same issue can merge the fix in ghost branch as well please!

Branch Used: [bgl/Ghost-ms/api-auto_New|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits?until=refs%2Fheads%2Fbgl%2FGhost-ms%2Fapi-auto_New]

will validate the fix for Hulk in the upcoming run. [+bgl/Ghost-ms/api-auto_New+|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits?until=refs%2Fheads%2Fbgl%2FGhost-ms%2Fapi-auto_New] was not showing in the cherry pick list, so I have explicitly added to it.

Please validate and confirm back. [~accountid:62ab7a399cd13c0068b18fe0] Testcase is passing in Latest Hulk Run. Fix Has been verified! thank you Sainath and Amardeep Added the fix to Ghost branch as well:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/08baf18ed39a688cc5627206eca741b73a5d631f|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/08baf18ed39a688cc5627206eca741b73a5d631f] thank you Amardeep. Next time you dont need to do it yourself. just tell me to do that","['Auton', 'Hulk']",QuangVinh Nguyen,Resolved,SAINATH CHATHARASI
SEEN-1856,https://miggbo.atlassian.net/browse/SEEN-1856,[Auton] Hulk VA - Wireless Solution Sanity - Script Fix Required for JSONDecodeError faced with connect_rbac,"*Regression:* Solution Sanity (SSR) on DNAC-VM

*DNAC Release_Version Used:* retained_forever/assembly_release_dnac_hulk-intg_converged_07-3.710.75408.ova

*Branch:* rcdn/Hulk-ms/api-auto synced to main branch private/Hulk-ms/api-auto - today before regression run start

*Script Used:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

*Trade Log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/23-06/sanitycombine.2023Jun26_03:05:45.313909.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/23-06/sanitycombine.2023Jun26_03:05:45.313909.zip&atstype=ATS]

*Cluster:* 10.88.186.166 – admin/Maglev123 for UI and maglev/Maglev123 for SSH

*Impacted TCs:* TC7 (test24) , TC8, TC9, TC10 - Blocks many TCs

*Error Snapshot:*

5887:  Method {{send_cmd}} returned successfully.
5888:  Encountered unhandled error in Internal API Call
5889:  Flagging result as FAIL!
5890:  	Reason: <class 'json.decoder.JSONDecodeError'>
5891:  Kwargs:
5892:  {}
5893:  Traceback (most recent call last):
5894:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/commonlibs/test_wrapper.py"", line 301, in wrapper
5895:      result = testfunc(func_self, **kwargs)
5896:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 858, in test24_DNAC_Server_Settings_NTP_DHCP_AAA_DNS_settings
5897:      dnac_handle.connect_rbac(role=""design"")
5898:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/dnaservices.py"", line 325, in *getattr*
5899:      return getattr(getattr(self.api_group_manager, group_name), name)
5900:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/lib/managers/group_manager.py"", line 85, in *getattr*
5901:      self.instantiate_group(group_name)
5902:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/lib/managers/group_manager.py"", line 104, in instantiate_group
5903:      group_obj = group_class(self.services, group_name)
5904:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/lib/api_groups/rbac/group.py"", line 11, in *init*
5905:      self.tenant_id = self.get_tenant_id()
5906:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/lib/decorators.py"", line 32, in wrapper
5907:      result = method(*args, **kwargs)
5908:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/lib/api_groups/rbac/group.py"", line 35, in get_tenant_id
5909:      response = self.services.api_switch_call(method=""GET"", resource_path=""/idm/api/v1/im/tenants"")
5910:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/lib/api_groups/utils/group.py"", line 51, in api_switch_call
5911:      response = self.services.base.NB_API.call_api(method, resource_path, **kwargs)
5912:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/client_manager.py"", line 589, in call_api
5913:      **kwargs
5914:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/client_manager.py"", line 296, in call_api
5915:      'x-auth-token': self.get_internal_token()
5916:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/client_manager.py"", line 353, in get_internal_token
5917:      seed_tenant_id_dict = json.loads(seed_tenant_id.split(""\n"")[-1])
5918:    File ""/opt/ats/ats_python36/lib/python3.6/json/*init*.py"", line 354, in loads
5919:      return _default_decoder.decode(s)
5920:    File ""/opt/ats/ats_python36/lib/python3.6/json/decoder.py"", line 339, in decode
5921:      obj, end = self.raw_decode(s, idx=_w(s, 0).end())
5922:    File ""/opt/ats/ats_python36/lib/python3.6/json/decoder.py"", line 357, in raw_decode
5923:      raise JSONDecodeError(""Expecting value"", s, err.value) from None
5924:  json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
5925:  Test returned in 0:00:05.178149
5926:  Errored reason: Expecting value: line 1 column 1 (char 0)
5927:
5928:  Exception:
5929:  Traceback (most recent call last):
5930:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/commonlibs/test_wrapper.py"", line 301, in wrapper
5931:      result = testfunc(func_self, **kwargs)
5932:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 858, in test24_DNAC_Server_Settings_NTP_DHCP_AAA_DNS_settings
5933:      dnac_handle.connect_rbac(role=""design"")
5934:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/dnaservices.py"", line 325, in *getattr*
5935:      return getattr(getattr(self.api_group_manager, group_name), name)
5936:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/lib/managers/group_manager.py"", line 85, in *getattr*
5937:      self.instantiate_group(group_name)
5938:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/lib/managers/group_manager.py"", line 104, in instantiate_group
5939:      group_obj = group_class(self.services, group_name)
5940:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/lib/api_groups/rbac/group.py"", line 11, in *init*
5941:      self.tenant_id = self.get_tenant_id()
5942:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/lib/decorators.py"", line 32, in wrapper
5943:      result = method(*args, **kwargs)
5944:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/lib/api_groups/rbac/group.py"", line 35, in get_tenant_id
5945:      response = self.services.api_switch_call(method=""GET"", resource_path=""/idm/api/v1/im/tenants"")
5946:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/lib/api_groups/utils/group.py"", line 51, in api_switch_call
5947:      response = self.services.base.NB_API.call_api(method, resource_path, **kwargs)
5948:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/client_manager.py"", line 589, in call_api
5949:      **kwargs
5950:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/client_manager.py"", line 296, in call_api
5951:      'x-auth-token': self.get_internal_token()
5952:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod3/services/dnaserv/client_manager.py"", line 353, in get_internal_token
5953:      seed_tenant_id_dict = json.loads(seed_tenant_id.split(""\n"")[-1])
5954:    File ""/opt/ats/ats_python36/lib/python3.6/json/*init*.py"", line 354, in loads
5955:      return _default_decoder.decode(s)
5956:    File ""/opt/ats/ats_python36/lib/python3.6/json/decoder.py"", line 339, in decode
5957:      obj, end = self.raw_decode(s, idx=_w(s, 0).end())
5958:    File ""/opt/ats/ats_python36/lib/python3.6/json/decoder.py"", line 357, in raw_decode
5959:      raise JSONDecodeError(""Expecting value"", s, err.value) from None
5960:  json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
5961:  Session closed.
5962:  The result of section test24_DNAC_Server_Settings_NTP_DHCP_AAA_DNS_settings is => ERRORED",2023-06-26T11:02:51.208+0000," [~accountid:62ab7a399cd13c0068b18fe0] made the last changes in this area on April 19.  Why didnot anyone catch the issue until now? 
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/32c5a9f981f4d3336348bdd5d8462863114a96af#services/dnaserv/client_manager.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/32c5a9f981f4d3336348bdd5d8462863114a96af#services/dnaserv/client_manager.py]


[~accountid:63f50bd640328c12e4ec5b00] , [~accountid:62ab7a399cd13c0068b18fe0] , Was the failure because of script issue or api changes?

[~accountid:63f50bd640328c12e4ec5b00] , since you had a fix for it, please raise PR to main branch?  [~accountid:62d2fe9f8afb5805e5d5af49] there’s a change in the format of the API output from 3.710.75408 OVA which was released on last Friday.
Hence the change was required. PR raised for Hulk branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6103/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6103/overview]

Approved and Merged.

[~accountid:636ce33a6bbefce0aca3df70] , pls. pull the latest content as it has more change that I required to get pass log. [~accountid:62ab7a399cd13c0068b18fe0] thanks for helping confirmation and raised PR to resolve this ticket.  Concluding this ticket with another generalized method to process the cURL output.
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6120/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6120/overview]","['Auton', 'Hulk', 'VM']",ThanhTan Nguyen,Closed,Yuvarani Iyamperumal
SEEN-1857,https://miggbo.atlassian.net/browse/SEEN-1857,[Auton] [Hulk] - Device config push verification failing due to incorrect interface type verification ,"*Regression:* Solution Regression Multisite - DR+MDNAC

*Branch:* private/Hulk-ms/api-auto

*Uber ISO tested:* Hulk - 2.1.710.70344

*Script Used:* testcases/mega_topo/solution_test_apppolicy.py

*Issue Faced:* 

During our Solution testing on Hulk Promoted Uber ISO - 2.1.710.70344, during Application Policy test execution, post Wired Policy app policy deployment the config push verifications on device is failing for {{AppGigabitEthernet}} interface on all the intended devices. It looks like this verification implementation is newly added. This was not part of verification earlier. Also there is no config pushed on this interface type even during earlier DNAC releases testing. This new implementation is causing false failure on this TC. 

*Failed log on Hulk:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites.2023Jun23_05:49:23.372288.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites.2023Jun23_05:49:23.372288.zip&atstype=ATS] ->Refer TC3.5

*Latest Pass log on Halleck :* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites.2023May08_10:38:02.000646.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fsr_mb_multi_sites.2023May08_10:38:02.000646.zip&atstype=ATS] → Refer TC3.5 Task-2",2023-06-26T17:39:03.748+0000,"* PR has been raised: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6234/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6234/overview]
* passlog: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-07/sanity-intg2.2023Jul10_01:23:41.570075.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-07/sanity-intg2.2023Jul10_01:23:41.570075.zip&atstype=ATS] [~accountid:62d2fec15d6f5fd2c3db8f9f], mentioned PR has been approved and merged to Hulk Branch. Marking this ticket as “Resolved”.

Please validate the fix and confirm back on this ticket to get it “Closed”. Moving  To a Closed state 
Build =>Hulk RC2 :
+*Before  merging PR*+ 
*Failed log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul26_10:02:37.377505.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul26_10:02:37.377505.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

+*After  Merging PR*+ 
*Pass Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=854132&size=535710&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug01_07:29:55.755355.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=854132&size=535710&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug01_07:29:55.755355.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


 [~accountid:620b8357878c2f00729881c8] : As discussed, Thanks a lot for checking on this issue and verifying the fix and updating the latest observations.","['Auton', 'Execution', 'Hulk', 'MSTB1', 'Multsite']",QuangVinh Nguyen,Closed,SANDEEP SHIVARAMAREDDY
SEEN-1859,https://miggbo.atlassian.net/browse/SEEN-1859,[Auton] [Hulk] - Polaris version check on devices failing due to Regex mismatch,"*Regression:* Solution Regression Multisite - DR+MDNAC

*Branch:* private/Hulk-ms/api-auto

*Uber ISO tested:* Hulk - 2.1.710.70344

*Scripts Used:* 

testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

testcases/mega_topo/solution_assurance_test.py

*Issue Faced:* 

During our Solution testing on Hulk Promoted Uber ISO - 2.1.710.70344, we have observed some TCs are Errored during Polaris version check on devices failing due to Regex mismatch. Looks like prd versions were missed to consider during the implementation.

Below is example of a device with prd version:

[^Polaris_version_prd_9300_device.txt]


*Failed logs:* 

a) [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24855245&size=90597&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun20_23:48:12.772444.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24855245&size=90597&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites_mdnac.2023Jun20_23:48:12.772444.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] → TC81.14 on mainscript

b) [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13507861&size=10274&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites.2023Jun23_07:09:52.379305.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13507861&size=10274&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites.2023Jun23_07:09:52.379305.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] → TC52.2 on assurance script",2023-06-27T02:21:27.884+0000,[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6309/diff#services/dnaserv/lib/api_groups/assurance_fabric/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6309/diff#services/dnaserv/lib/api_groups/assurance_fabric/group.py] Mentioned PR has been approved and merged to Hulk Branch.,"['Auton', 'Execution', 'Hulk', 'MSTB1', 'Multisite']",Moe Saeed,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-1860,https://miggbo.atlassian.net/browse/SEEN-1860,TC90_verify_frabric_site_data_in_assurance,"!image-20230627-053520.png|width=341,height=203!

!image-20230627-053535.png|width=172,height=145!

!image-20230627-053547.png|width=569,height=126!

*Sub TC Failed:*

test1_verify_frabric_site_data_in_assurance



TC90 is failing due to this, But in both Fabric site and network hierarchy i can see Bangalore
Log:[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=85412998&size=14057&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-06%2Fsanitycombine.2023Jun19_15:23:55.979656.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=85412998&size=14057&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-06%2Fsanitycombine.2023Jun19_15:23:55.979656.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

*Branch Used:*

bgl/Hulk-ms/api-auto

*Test Suite:*

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto]",2023-06-27T05:37:40.964+0000,Commit: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0211293ff35ca397522788c15f392482bd72e916|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0211293ff35ca397522788c15f392482bd72e916],"['Auton', 'Hulk']",Tran Lam,Resolved,SAINATH CHATHARASI
SEEN-1861,https://miggbo.atlassian.net/browse/SEEN-1861,[Auton] : [Ghost] : Task-ise_cleanup_profile_nw_device.py-21-ISECleanupGoldenConfig / Test_TC0_dnac_initial_ise_cleanup_making_tb_ready /  test2_verify_ISE_CSRF_before_integration,"we observed on sanity from Ghost and above  runs  in REG and LAN sanity optimized code failing to verify CSRF :

Description:

{noformat}    316:  Traceback (most recent call last):
317:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper
318:      result = testfunc(func_self, **kwargs)
319:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/testcases/sanityusecases/ISECleanupGoldenConfig/ise_cleanup_profile_nw_device.py"", line 115, in test2_verify_ISE_CSRF_before_integration
320:      if dnac_handle.test2_enabling_CSRF_on_ISE():
321:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/services/dnaserv/dnaservices.py"", line 310, in __getattr__
322:      raise AttributeError(err_msg)
323:  AttributeError: 'DnaServices' object has no attribute 'test2_enabling_CSRF_on_ISE'{noformat}


*Branch Name:* Ghost -ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb.py AND solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

**Fail Log:**Ghost 2.1.614.70759+17.12.02+wlc(17.12.02)+3.1 P3
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_cleanup_profile_nw_device.py-21-ISECleanupGoldenConfig&begin=35639&size=36698&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun26_10:05:13.691049.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_cleanup_profile_nw_device.py-21-ISECleanupGoldenConfig&begin=35639&size=36698&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun26_10:05:13.691049.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_cleanup_profile_nw_device.py-21-ISECleanupGoldenConfig&begin=20807&size=24279&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_optimized_auto_job.2022Nov13_03:11:08.345784.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_cleanup_profile_nw_device.py-21-ISECleanupGoldenConfig&begin=20807&size=24279&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F22-11%2Fenv_optimized_auto_job.2022Nov13_03:11:08.345784.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-06-27T09:45:30.606+0000,"PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6218/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6218/overview] Passlog:
https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_cleanup_profile_nw_device.py-21-ISECleanupGoldenConfig&begin=15860&size=45430&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul10_11:45:20.425132.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS

Issue resolved ","['Auton', 'Ghost', 'Hulk', 'Sanity']",Moe Saeed,Resolved,Elton GoldChristopher
SEEN-1862,https://miggbo.atlassian.net/browse/SEEN-1862,[Auton] [Ghost] - Advanced wlan  feature where newly created SSIDs are missed with segment onboarding,"* *DNAC Release_Version Tested:* Ghost P2 Uber ISO - 2.1.610.70759
* *Device Image Used:* 17.12.1
* *Testbed:* Sanity (REG script)
* *Branch Used:* private/Ghost-ms/Sanity_api-auto
* *Script Name:*  testcases/forty_eight_hour/solution_test_sanityecamb_lan.py
* *Solution Input File :* dnac-auto/configs/config_48hr_test/solution_test_input.json
* *Testcases Impacted:* Test_TC196_advanced_wlan_configs / test2_add_new_ssids,test3_verify_device_configs
* *Failed Trade Log:* [+Test_TC196_advanced_wlan_configs+|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=82140154&size=1720778&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun21_22:49:41.752625.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 
* *Issue details/analysis:* 

where the sub-tc [test2_add_new_ssids|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1160456&size=2162629&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun18_23:45:11.004112.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] failed for reason :

{{Requested SSID Adv_wlan_configs_2 ID is not found in :{'response': [], 'version': '1.0'} with name space 2679d1ca-4401-4784-883b-bcc108ef912a and in ['9f202c27-c53d-42ee-ba2c-a7a024d20a35', 'cc772e7c-f358-4878-82ea-4fca7516200e', '4121ccd5-099a-4d66-b91d-f92f3d9b73da', '26788f57-89d5-4f32-8adb-e5213f1f6304', '3b168420-6349-4055-8458-c3fbe5f4cbd8', 'de539aa1-7742-4929-8c54-ddd060781f58', '0ac03605-592a-421b-83cb-688dbacf58e8', 'e2ba7988-6368-420d-aa4a-6461b85b7f26', 'ce251f57-dbc8-464e-9c70-09d4c17fa6eb'] Library group ""wlan_segment_onboarding"" method ""onboard_wireless_segment"" returned in 0:00:10.731437 Error in onboarding wireless segments Some issue in adding segment for Adv_wlan_configs_2}}

The ssids are added Adv_wlan_configs_1 & Adv_wlan_configs_2, But segment WClients_sub-WirelessVNFB onboarding has failed. Due to which SSIDs were DOWN

* *Failed Log Snapshot :* 

* *Solution Input File Changes:* NA",2023-06-27T10:18:34.328+0000,"[ENG-SDN / dnac-auto / 217a3a50a6a - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/217a3a50a6a6f7ade1f00a08e9d4f491fb9c40a2]

Added this change in ghost halleck and hulk. I believe this should fix the issue. Please let me know if it doesnt work. Thanks [~accountid:63f50bcece6f37e5ed93c87e]  I cherry picked the fix to our sanity branch 
I executed the test Log: [Test_TC196_advanced_wlan_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1198264&size=2218394&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_20:33:39.870051.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

Where the provisioning of ECA device has failed with below reason: 
{{Config Preview Activity failed with reason: NCWL10219: One or more Access Points are provisioned in below Zones. Please re-provision them to either without Zone or same Zone part of new NetworkProfile. APZones: [Zone1_Enterprise]}} 

Looks like for this issue we have a jira: [https://miggbo.atlassian.net/browse/SEEN-1831|https://miggbo.atlassian.net/browse/SEEN-1831|smart-link]   Please check this TC alone as I have tested it locally and it’s fine: [+Test_TC196_advanced_wlan_configs+|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1257710&size=1993062&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_15:45:39.716279.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] Executed on TB2-with Ghost P2 70759 
Got Pass Log: [Test_TC196_advanced_wlan_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1198321&size=3500853&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_22:36:19.187852.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ","['AWS-Santiy', 'Auton', 'Ghost', 'Halleck', 'Optimized']",Andrew Chen,Closed,Ashwini R Jadhav
SEEN-1863,https://miggbo.atlassian.net/browse/SEEN-1863,Auton-MSTB2(Non-DR) : ITSM testcase needs latest hulk changes,"Hi [~accountid:63f50bcece6f37e5ed93c87e],

As per latest Hulk changes there is new field required for ITSM and it needs to be handled in script.

[{""name"":""itsm"",""description"":"""",""data"":{""ConnectionSettings"":{""Url"":"""",""Auth_UserName"":"""",""Auth_Password"":""""}},""placeholders"":{""ConnectionSettings"":{""Url"":""<https://<servicenow-host-name>"",""Auth_UserName"":""<username-for-servicenow-host>"",""Auth_Password"":""<password-to-connect-to-servicenow-host>""}},""dypId"":""f4ab-5be5-42fa-9ee1""},{""name"":""itsm_DRE"",""description"":"""",""data"":{""endpoint"":""REST> API Endpoint"",""DestinationUri"":""/api/now/import/x_caci_cisco_dna_create_workflow_for_eve"",""ITSMType"":""ServiceNowConnection""},""placeholders"":{""endpoint"":"""",""DestinationUri"":"""",""ITSMType"":""ServiceNowConnection""},""dypId"":""1194-498b-42a8-b4cc""}]

Payload for the call: DRE has new field ITSMType

Branch : private/Hulk-ms/api-auto
Script : solution_test_3sites_sjc_nyc_sf.py",2023-06-27T16:38:02.523+0000,"[ENG-SDN / dnac-auto / 6b909734b7b - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6b909734b7b8bedcdffab9d6787b032909c58b6d]

Fixed here for hulk","['Auton', 'Hulk', 'MSTB2']",Andrew Chen,Resolved,Divakar Kumar Yadav
SEEN-1864,https://miggbo.atlassian.net/browse/SEEN-1864,Auton - [Hulk] - Test_TC52_DNAC_verify_SSID_lan_on_ECA_device / test1_DNAC_verify_SSID_lan_on_ECA_device,"*Reporter Analysis:*
we observed in the latest Hulk build in  script execution  Some  TC failed with the below error  :  

 On TB18-SJ-eCA-BORDER-CP device looking for SSID “{{Guest_passthrough_intTB18}}“ which not present on the device hence script got failed

present SSID is “{{Guest_webpassthroughTB18}}“   “int” is missing {{on the present ssid.}}

*Branch:* private/Hulk-ms/api-auto

*Script file:* solution_test_sanityecamb.py and solution_test_sanityecamb_lan.py

*input file:* solution_test_input.json

*ova file:* assembly_release_dnac_hulk-intg_converged_07-3.710.75408.ova

*Note*: Issue seen onprem cluster as well

Failed log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9520530&size=148398&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_07:01:32.510414.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9520530&size=148398&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_07:01:32.510414.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]





Failed log from On-prem sanity:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_segment_onboarding_wireless.py-121-FEWssidsegmentonboarding&begin=1849304&size=182900&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun27_00:44:13.367862.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_segment_onboarding_wireless.py-121-FEWssidsegmentonboarding&begin=1849304&size=182900&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun27_00:44:13.367862.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Previous build pass log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9473772&size=62234&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun15_07:42:25.224718.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9473772&size=62234&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun15_07:42:25.224718.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



",2023-06-28T03:23:31.070+0000,"[~accountid:63f50bd68ab3d6a635ecc29b] , Have you checked why this is not showed on the device. Looks like all ssids are verified and only this one did not. I can see that this SSID got removed afterward, so this ssid is expected to be on the device. [~accountid:63f50bfce8216251ae4d59d5] Yes checked, on the device all SSID present but observed SSID naming mismatch.

On device present SSID is   “{{Guest_webpassthroughTB18}}“ but comparing with the “{{Guest_passthrough_intTB18}}“ hence it’s got failed Defect: [https://cdetsng.cisco.com/summary/#/defect/CSCwh00797|https://cdetsng.cisco.com/summary/#/defect/CSCwh00797]","['Auton', 'Hulk', 'Optimized', 'hulk-vm-sanity', 'sanity']",Moe Saeed,Closed,Raghavendrachar Baraguru Mallesha Char
SEEN-1865,https://miggbo.atlassian.net/browse/SEEN-1865,[Auton] MSTB2(Non-DR) : Icap changes for ESXI-Vm execution,"Hi [~accountid:62ab7a399cd13c0068b18fe0],

Script needs upliftment as per latest API changes for icap and assurance related data.

We are seeing API related to icap is erroring with 400 client error while enabling Spectrum Analysis through script.

Manually we are able to enable icap for both RF statastics and spectrum but script throws below error

2023-06-26T22:11:59: %CLIENTMANAGER-ERROR: URL:[https://10.195.243.37/api/ndp/v1/data/store/app/assurance_ap_stats_v1/query|https://10.195.243.37/api/ndp/v1/data/store/app/assurance_ap_stats_v1/query] Data:{'timeout': 60, 'data': '{""query"": ""SELECT timestamp, radioUtilTx, radioUtilRxInBSS, radioUtilRxOtherBSS, radioUtilNonWiFi, deviceName, band FROM radio_stats WHERE timestamp > 1687840319825000000 AND timestamp <= 1687842719862000000 AND apMac = \'a4:88:73:ce:9a:f4\' AND deviceName = \'wifi0\' AND band = \'2.4\' AND band IS NOT NULL""}'} Headers:{'Content-Type': 'application/json', 'X-Auth-Token': 'eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJDRE5BIiwiYXV0aFNvdXJjZSI6ImxlZ2FjeSIsImNsaWVudElkIjoiYWRtaW4iLCJlbWFpbCI6ImFkbWluQGxvY2FsdXNlci5jb20iLCJleHAiOjE2ODc4NDYzMTksImlhdCI6MTY4Nzg0MjcxOSwiaXNzIjoiZG5hYyIsInJvbGVzIjpbIlNVUEVSLUFETUlOIl0sInNlc3Npb25JZCI6ImVmODI2MjQzLTljOGItNWIyOC1hMmI5LTFkZmZkYjA5YjQyMCIsInN1YiI6ImFkbWluIiwidGVuYW50SWQiOiI2NDkwODdhYjIyNDVjZTAwMTMzNzkzMmQiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInVzZXJuYW1lIjoiYWRtaW4ifQ.ekLaneXW5eKc7EcZSaeosCe7nhHRVHusJVxj7Rw418CkNyBIfA2QujFD_O68V2FLb7ih3qUIUuqPFPAjX97H2Q'} Message:Request with Unknonwn CallerId disallowed.

Below is the comment added by Venkateshan about the Changes
From: Gokul Kannan Coimbatore Kannan (gcoimbat) [gcoimbat@cisco.com|mailto:gcoimbat@cisco.com]
Date: Monday, January 30, 2023 at 9:05 PM
To: dnac-sit-sol-leads(mailer list) [dnac-sit-sol-leads@cisco.com|mailto:dnac-sit-sol-leads@cisco.com], group.loile(mailer list) [group.loile@cisco.com|mailto:group.loile@cisco.com]
Cc: Honey Maharana (hmaharan) [hmaharan@cisco.com|mailto:hmaharan@cisco.com], Jerry David (jerdavid) [jerdavid@cisco.com|mailto:jerdavid@cisco.com], Venkatesan Selvaraj (venkselv) [venkselv@cisco.com|mailto:venkselv@cisco.com], Archana Gupta (archana) [archana@cisco.com|mailto:archana@cisco.com]
Subject: Changes to NDP API invocation in PnC3.0/VM
Hello Team(s)

Wanted to highlight two changes that needs to be made to test automation scripts in PnC3.0/VM, if not done already.

1: All NDP API requests (starting with /api/ndp) needs to include the below mentioned new HTTP header.

Key: X-DNA-SVC-INSTANCE-NAME
Value: <any string value, indicative of the client/application making the API invocation>

2: DSL queries are no longer supported. DSL queries need to be replaced with the corresponding Gremlin queries.

Eg:
DSL:  /api/ndp/v1/data/graph?query=sites()
Gremlin: /api/ndp/v1/data/graph?query=g.V().hasLabel('Site').toList()

The above changes are not mandated by Assurance application, but by the underlying data platform. The API interfaces (/api/assurance) to assurance applications, however,  have no changes.

Let me know if any questions.

-Thanks

For more details we have created below webex group : webexteams://im?space=3017d680-1511-11ee-b044-d5f6a99cf6d6",2023-06-28T04:57:55.433+0000,"Same issue is also observed on On-Prem testing as well. Below are the details:

*Regression testbed:* Solution Regression Multisite - DR+MDNAC (MSTB1)

*Branch:* private/Hulk-ms/api-auto

*Uber ISO tested:* Hulk - 2.1.710.70344 

*Scripts Used:* 

testcases/mega_topo/solution_assurance_test.py

*Issue Faced:* 

1) We see the ICAP enablement for spectrum analysis for all the AP models are failing with 400 client errors for corresponding API calls. But when we tried to enable Spectrum Analysis option manually from Intelligent Capture Section on AP device 360 page, it worked fine. Due to this multiple subsequent corresponding ICAP related TCs are getting Errored with key error failures. Refer TCs - 25,29,30,31,32,33

*Error snip:*
[{""type"":""ICAP_SPECTRUM"",""command"":null,""state"":""WRONG_PARAMS"",{color:#bf2600}""errorString"":""Session is missing spectrum slot number.""{color},""apInfos"":[{""macAddress"":""C4:B2:39:95:8A:20"",""apName"":""SJ-AP1-3802E"",""otaChannel"":0,""otaBand"":"""",""channelWidth"":20,""otaSlot"":"""",""xorRadio"":0,""otaApMode"":0,""etherMac"":"""",""clntServChannel"":0,""clntServChannelWidth"":20,""clntServBand"":"""",""uuid"":""""}],""startTimeSecs"":0,""configDuration"":0,""actualDuration"":0,""remainingDuration"":0,""sessionId"":0,""owner"":"""",""estimatedTime"":0,""originatedAp"":null,""otaDestination"":""""}]}
24640:  Traceback (most recent call last):
24641:    File ""/auto/dna-sol/ws/sr-mb1_Hulk_temp/services/dnaserv/client_manager.py"", line 326, in call_api
24642:      response.raise_for_status()
24643:    File ""/ws/divayada-sjc/Solution_pyatsenv/lib/python3.6/site-packages/requests/models.py"", line 960, in raise_for_status
24644:      raise HTTPError(http_error_msg, response=self)
24645:  requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: [https://10.195.243.123/api/assurance/v1/airsense/createIcapSession|https://10.195.243.123/api/assurance/v1/airsense/createIcapSession]
24646:  Encountered unhandled HTTPError in Internal API Call
24647:  Flagging result as FAIL!
24648:  	{color:#bf2600}Reason: 400 Client Error: Bad Request for url: {color}[https://10.195.243.123/api/assurance/v1/airsense/createIcapSession|https://10.195.243.123/api/assurance/v1/airsense/createIcapSession]
24649:  Kwargs:
24650:  {'data': {'apInfos': [{'apName': 'SJ-AP1-3802E',
24651:                         'macAddress': 'C4:B2:39:95:8A:20'}],
24652:            'spectrumSlot': 0,
24653:            'type': 'ICAP_SPECTRUM'}}
24654:  Error Caught While Querying the Internal API


2) Even though the TC has failed with 400 client errors for ICAP enablement, we have overall TC marked as Pass, which is a False pass. This also has to be addressed. Refer TC25 in from the below failed logs shared.

*Failed logs:* 

[Test_TC25_enable_icap_on_AP|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9943139&size=72750&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites.2023Jun23_07:09:52.379305.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

[Test_TC29_validate_icap_client_statistics|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12300056&size=34385&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites.2023Jun23_07:09:52.379305.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
[Test_TC30_validate_icap_radio_utilization|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12334441&size=36981&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites.2023Jun23_07:09:52.379305.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
[Test_TC31_validate_icap_rx_tx_bytes|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12371422&size=45338&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites.2023Jun23_07:09:52.379305.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
[Test_TC32_validate_icap_rx_tx_errors|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12416760&size=33310&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites.2023Jun23_07:09:52.379305.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
[Test_TC33_validate_icap_rx_tx_powers|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12450070&size=33605&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_mb_multi_sites.2023Jun23_07:09:52.379305.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] PR raise for ndp/v1/data related API issue: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6131/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6131/overview] Mentioned PR has been approved and merged to Hulk Branch.

Marking this ticket as “Done”. [~accountid:62d2fec15d6f5fd2c3db8f9f] , pls. raise separate Auton for your complain.","['Auton', 'Execution', 'Hulk', 'MSTB1', 'MSTB2', 'Multisite', 'Uplift']",Amardeep Kumar,Closed,Divakar Kumar Yadav
SEEN-1866,https://miggbo.atlassian.net/browse/SEEN-1866,TC129 Fails sometimes due to version check for PRD images fails,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*EXSI OVA build:* assembly_release_dnac_hulk-intg_converged_07-3.710.75408.ova
*Branch*: private/Hulk-ms/api-auto
*Script Name :* solution_test_sanity_ecamb.py
*Testbed :* TB4
*Testcases Impacted :*  “Test_TC124_verify_SDA_fabric_issue”

During Hulk ESXI testing : ""TC124_verify_SDA_fabric_issue"" script error out when the device loaded with PRD images.

Failure Logs: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=32676482&size=123474&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_22:05:13.705332.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=32676482&size=123474&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_22:05:13.705332.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Required API Code Change:

dnac-auto/services/dnaserv/lib/api_groups/assurance_fabric/group.py



*The Below fix will work*

##Compare IOS version 15.10 and 15.3.1
def compare_ios_version(version1, version2):
    version1 = version1.split(':')[0]
   {color:#bf2600} if ""prd"" in version1:{color}
{color:#bf2600}        version1 = version1.replace('prd', ''){color}
    version2 = version2.split(':')[0]
    if Version(version1) >= Version(version2):
        return True
    return False



Pass Log after changes: 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=826312&size=115618&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal%2Fusers%2Fkmukku%2Farchive%2F23-06%2Fsanity_TB4.2023Jun28_17:13:15.329114.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=kmukku&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=826312&size=115618&archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal%2Fusers%2Fkmukku%2Farchive%2F23-06%2Fsanity_TB4.2023Jun28_17:13:15.329114.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=kmukku&from=trade&view=all&atstype=pyATS]",2023-06-29T00:54:12.768+0000,"Raised PR for the required change

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6272/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6272/overview] PR got approved and merged to Hulk-ms/api-auto","['Auton', 'ESxi', 'Hulk']",KRISHNA MUKKU,Resolved,KRISHNA MUKKU
SEEN-1867,https://miggbo.atlassian.net/browse/SEEN-1867,Test_TC96_wired_app_policy fails due  to checking AppGigabitEthernet1/0/1 interface,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*EXSI OVA build:* assembly_release_dnac_hulk-intg_converged_07-3.710.75408.ova
*Branch*: private/Hulk-ms/api-auto
*Script Name :* solution_test_sanity_ecamb.py
*Testbed :* TB4
*Testcases Impacted :*  “Test_TC96_wired_app_policy”

During Hulk ESXI testing : ""TC96_wired_app_policy” script checks the config push on “AppGigabitEthernet1/0/1” interface.

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=841104&size=692400&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun28_19:01:06.590344.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=841104&size=692400&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun28_19:01:06.590344.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[SNIP …]

7651: Service-policy configured on interface AppGigabitEthernet1/0/1 not as expected

7652: Validation of wired application policy config failed for reason :: [""Failed to push any of the service policy config (DNA-dscp#APIC_QOS_Q_OUT, input DNA-MARKING_IN, DNA-APIC_QOS_IN) on interfaces: ['interface AppGigabitEthernet1/0/1'] on TB4-DM-eCA-BORDER"", ""Failed to push any of the service policy config (DNA-dscp#APIC_QOS_Q_OUT, input DNA-MARKING_IN, DNA-APIC_QOS_IN) on interfaces: ['interface AppGigabitEthernet1/0/1'] on TB4-DM-NF-Switch""]

*Required API Code Change:*

*dnac-auto/services/dnaserv/lib/api_groups/assurance_fabric/group.py*

*API:* {{def verify_wired_config(self, role):}}



{{self.log.info(""verifying device interface configuration"")output=self.services.execute_command_on_device(dev,cli=""show running-config | sec interface"")match_group = re.findall(""(face\s+\S+\d+/\d+/\d+[\s|\S]+?interface\s)"",output[""output""])interface_failures = []}}",2023-06-29T02:13:22.109+0000,"Effect by [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6234/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6234/overview], this PR will resolve this auton

will notify when PR has been merged [~accountid:63f50bf9e8216251ae4d59d4],  mentioned PR has been approved and merged to Hulk Branch. Marking this ticket as “Resolved”.

Please validate the fix and confirm back on this ticket to get it “Closed”. Pass Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1224430&size=871027&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul31_09:22:06.135521.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1224430&size=871027&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul31_09:22:06.135521.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] Hi Team,
Build: Ghost P2 RC1

Version: 2.3.5.4-70815

Failed Log:[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13890152&size=146472&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug01_09:40:51.331871.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13890152&size=146472&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug01_09:40:51.331871.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Observing the same issue in Ghost Fqdn Delay testing. hi Deepak. Which branch did you use? Hi [~accountid:6357500be14026a7397f37dd] 

Branch Used: Ghost Delay Branch

private/Ghost-ms/sanity_delay_testing

Script Used: solution_test_sanityecamb_cert_lan.py yeah. can you cherry-pick to your branch? Seeing same issue on Ghost P1 RC6 for AWS Sanity:
*Branch*: private/Ghost-ms/sanity_api_auto
*Script Name :* solution_test_sanity_ecamb_lan.py
Failed Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=839200&size=245291&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug10_12:13:59.772074.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=839200&size=245291&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug10_12:13:59.772074.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  This failure is already addressed in another auton, which is resolved. Check on latest and close it.  Pass Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=38459485&size=43611&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug06_12:19:17.251587.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=38459485&size=43611&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug06_12:19:17.251587.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'ESxi', 'Ghost', 'Hulk']",QuangVinh Nguyen,Resolved,KRISHNA MUKKU
SEEN-1868,https://miggbo.atlassian.net/browse/SEEN-1868,Test_TC138_system_health_assurance_checks Error out while grep AP before doing shutdown,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*EXSI OVA build:* assembly_release_dnac_hulk-intg_converged_07-3.710.75408.ova
*Branch*: private/Hulk-ms/api-auto
*Script Name :* solution_test_sanity_ecamb.py
*Testbed :* TB4
*Testcases Impacted :*  “TC138_system_health_assurance_checks test 138.1 ""test1_verify_devices_categorized_health_no_health""

During Hulk ESXI testing : ""TC138_system_health_assurance_checks test 138.1 ""test1_verify_devices_categorized_health_no_health"", script grep for AP’s and try to shut the AP, while grepping the AP script error out as key error.

Please find the Fail logs and SNIP attached.

*Fail Log*: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=37529544&size=17460&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_22:05:13.705332.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=37529544&size=17460&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_22:05:13.705332.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



[Error SNIP…]

TB4-DM-eCA-BORDER# 

130223: Found following CDP Entries:
130224: [{'localintf': 'TenGigabitEthernet1/1/1',
130225: 'name': 'TB4-DM-Fusion',
130226: 'remoteIntf': 'TenGigabitEthernet1/0/2'},
130227: {'localintf': 'TwoGigabitEthernet1/0/35',
130228: 'name': 'SN-FOC2350L1BR',
130229: 'remoteIntf': 'GigabitEthernet0/11'},
130230: {'localintf': 'TwoGigabitEthernet1/0/36',
130231: 'name': 'SN-FOC2350L1BR',
130232: 'remoteIntf': 'GigabitEthernet0/12'},
130233: {'localintf': 'TwoGigabitEthernet1/0/11',
130234: 'name': 'AP502f.a857.c9d4',
130235: 'remoteIntf': 'GigabitEthernet0'},
130236: {'localintf': 'TwoGigabitEthernet1/0/13',
130237: 'name': 'TB4-DM-TSIM',
130238: 'remoteIntf': 'GigabitEthernet0/0/3'},
130239: {'localintf': 'TwoGigabitEthernet1/0/14',
130240: 'name': 'TB4-DM-TSIM',
130241: 'remoteIntf': 'GigabitEthernet0/0/1'},
130242: {'localintf': 'TwoGigabitEthernet1/0/3',
130243: 'name': 'SEPD0EC35FF8A1D',
130244: 'remoteIntf': 'GigabitEthernet1'},
130245: {'localintf': 'TwoGigabitEthernet1/0/7',
130246: 'name': 'AP68CA.E474.2080',
130247: 'remoteIntf': 'GigabitEthernet0'},
130248: {'localintf': 'TwoGigabitEthernet1/0/34',
130249: 'name': 'AP3C41.0EFE.219C',
130250: 'remoteIntf': 'GigabitEthernet0'},
130251: {'localintf': 'GigabitEthernet0/0',
130252: 'name': 'MGMT_NW_123',
130253: 'remoteIntf': 'GigabitEthernet1/0/30'},
130254: {'localintf': 'TenGigabitEthernet1/0/48',
130255: 'name': 'APCC9C.3EF1.0410',
130256: 'remoteIntf': 'GigabitEthernet0'},
130257: {'localintf': 'TwoGigabitEthernet1/0/1',
130258: 'name': 'TB4-DM-Transit',
130259: 'remoteIntf': 'GigabitEthernet1/0/1'}]


{color:#ff5630}130260: Traceback (most recent call last):{color}
{color:#ff5630}130261: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper{color}
{color:#ff5630}130262: result = testfunc(func_self, **kwargs){color}
{color:#ff5630}130263: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 9253, in test1_verify_devices_categorized_health_no_health{color}
{color:#ff5630}130264: if dnac_handle.verify_devices_categorized_health_no_health():{color}
{color:#ff5630}130265: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 6869, in verify_devices_categorized_health_no_health{color}
{color:#ff5630}130266: intf_to_shut = aps[ap_address[:6]]{color}
{color:#ff5630}130267: KeyError: 'AP502f' {color}



*API Call:*

{{def verify_devices_categorized_health_no_health}}()",2023-06-29T05:44:46.688+0000,"Required PR has been raised for Hulk Branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6211/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6211/overview] After fix, ESXi VM DNAC TB - Sanity TB18 got pass log for [Test_TC138_system_health_assurance_checks]([https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1034599&size=178017&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul07_11:19:06.416578.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1034599&size=178017&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul07_11:19:06.416578.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS])



Marking this Ticket as “Done”.","['Auton', 'ESxi']",Amardeep Kumar,Closed,KRISHNA MUKKU
SEEN-1869,https://miggbo.atlassian.net/browse/SEEN-1869,[Auton] [NFW]-TC3_generate_dhcp_server_config_on_fusion -ISE cleanup not performing properly,"*Regression:* Solution Regression NFW

*Branch:* private/Ghost-ms/api-auto-nfw

*Uber ISO tested:* Ghost - 2.3.5.3-70194

*Script Used:* testcases/forty_eight_hour/solution_dnac_wireless_hardening.py

*Issue Faced:* There was no action performed under testcase 

# Under - [Test_TC3_generate_dhcp_server_config_on_fusion|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=263371&size=35788&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_15:07:12.937070.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] 

# [test3_ise_cleanup|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=287247&size=2640&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_15:07:12.937070.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS]  

No device list found and passing but no log message.  

# [test5_ise_cleanup_guest|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=290885&size=8099&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_15:07:12.937070.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] 

No actions performed and displayed as Pass but no log message. 

{noformat}Starting section test3_ise_cleanup                      |{noformat}

{noformat}1861: 
 +------------------------------------------------------------------------------+{noformat}

{noformat}1862: 
 Cannot track test: tracking auth info must be set in order to transfer test tracking data{noformat}

{noformat}1863: 
 Executing testcase Test_TC3_generate_dhcp_server_config_on_fusion test 3.3 ""test3_ise_cleanup"".{noformat}

{noformat}1864: 
 Method GET{noformat}

{noformat}1865: 
 URL ers/config/networkdevice{noformat}

{noformat}1866: 
 Data {'params': {'size': 100, 'page': 1}, 'timeout': 600, 'headers': {'Content-Type': 'application/json', 'Accept': 'application/json', 'X-CSRF-TOKEN': 'fetch', 'Authorization': 'Basic YWRtaW46TGFibGFiMTIz'}}{noformat}

{noformat}1867: 
 []{noformat}

{noformat}1868: 
 Current Device list on iSE []{noformat}

{noformat}1869: 
 ###################################################{noformat}

{noformat}1870: 
 #Successfully delete devices []----#{noformat}

{noformat}1871: 
 ###################################################{noformat}

{noformat}1872: 
 ISE is cleaned up{noformat}

{noformat}1873: 
 Test returned in 0:00:00.083601{noformat}

{noformat}1874: 
 Passed reason: Pass{noformat}

{noformat}1875: 
 The result of section test3_ise_cleanup is => PASSED{noformat}



Could you please check once on this?

Log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_15:07:12.937070.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_15:07:12.937070.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=PYATS]",2023-06-29T07:54:01.932+0000,"Hi [~accountid:712020:06823340-9d28-4722-9cd7-15ef3ea14cc6] ,

In [+Test_TC3_generate_dhcp_server_config_on_fusion+|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=263371&size=35788&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_15:07:12.937070.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] :
+ [+test3_ise_cleanup+|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=287247&size=2640&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_15:07:12.937070.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] :  The purpose of this test case is to clean all devices on ISE. In the log, the test case did not find any devices on ISE (it's possible that in the first run, the devices initialized on ISE have not appeared yet), so it returned an empty list and passed.
+ [+test5_ise_cleanup_guest+|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=290885&size=8099&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_15:07:12.937070.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] : No actions performed because the ISE guest was not initialized previously, there was no input data [""WIRELESS_GUEST_PORTAL_LIST""] available.

Please let us know if you need any further clarification.
Thanks.



!image-20230817-065552.png|width=991,height=98!

!image-20230817-065634.png|width=328,height=66!


","['Auton', 'NFW']",DatChi Pham,Closed,JagadeshKumar Enapanuri
SEEN-1870,https://miggbo.atlassian.net/browse/SEEN-1870,[Auton] [NFW]-TC23_DNAC_verify_creating_wireless_guest_portal -Guest profiles are not getting created,"*Regression:* Solution Regression NFW

*Branch:* private/Ghost-ms/api-auto-nfw

*Uber ISO tested:* Ghost - 2.3.5.3-70194

*Script Used:* testcases/forty_eight_hour/solution_dnac_wireless_hardening.py

*Issue Faced:* Guest profiles are not getting created

# Under - [Test_TC23_DNAC_verify_creating_wireless_guest_portal|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3587959&size=8179&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_15:07:12.937070.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] 

# [test1_subtest1_verify_addition_of_wireless_guest_policy_and_portal|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3588529&size=7427&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_15:07:12.937070.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] 

Guest profiles are not getting created. It’s a False pass. 

Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3588529&size=7427&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_15:07:12.937070.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3588529&size=7427&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_15:07:12.937070.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS]----refer TC23",2023-06-29T09:33:55.754+0000,"*Branch:* private/Ghost-ms/api-auto-nfw

*Script Used:* testcases/forty_eight_hour/solution_dnac_wireless_hardening.py

*Testcase* : Test_TC23_DNAC_verify_creating_wireless_guest_portal

*Input File* : configs/config_48hr_test/solution_test_input.json



I have confirm that {{“WIRELESS_GUEST_PORTAL_LIST}}“ list in  “configs/config_48hr_test/solution_test_input.json” is emty. So that althought the TC was executed, it didn’t create any guest portals. 

Here is my Tradelog for TC23 execution with proper  {{“WIRELESS_GUEST_PORTAL_LIST}}“ list.

Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=262325&size=3131012&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phathpha2%2Fusers%2Fphathpha%2Farchive%2F23-08%2Fnfw3_job.2023Aug31_22:57:36.826130.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phathpha2&submitter=phathpha&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=262325&size=3131012&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phathpha2%2Fusers%2Fphathpha%2Farchive%2F23-08%2Fnfw3_job.2023Aug31_22:57:36.826130.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phathpha2&submitter=phathpha&from=trade&view=all&atstype=pyATS]

Do you want to skip the testcase if {{“WIRELESS_GUEST_PORTAL_LIST}} is emty ?  [~accountid:712020:06823340-9d28-4722-9cd7-15ef3ea14cc6] ","['Auton', 'NFW', 'Regression']",PhatHong Pham,Resolved,JagadeshKumar Enapanuri
SEEN-1881,https://miggbo.atlassian.net/browse/SEEN-1881,[Auton]:Hulk: Test_TC1_DNAC_Prime_Migration_DNAC_Server_Settings_NTP_DHCP_AAA_DNS_settings/test3_configure_device_snmp_interval,"*Reporter Analysis:*

# 

we observed in the latest Hulk build in BAPI   script execution  Some  TC failed with the below error  , d{{Group.ext_configure_snmp_global() got an unexpected keyword argument 'timout}}

Error snip:


{code:python}738:  Group ""ext_global_setting"" initialized successfully
739:  Traceback (most recent call last):
740:    File ""/data/jenkins/ws_sjc2_ds/workspace/Express-Sanity/Hulk/Hulk-BAPI-Optimized-Deployment_and_Express_Sanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper
741:      result = testfunc(func_self, **kwargs)
742:    File ""/data/jenkins/ws_sjc2_ds/workspace/Express-Sanity/Hulk/Hulk-BAPI-Optimized-Deployment_and_Express_Sanity/testcases/extapiusecases/ISEIntegration/ise_integration_verification_aca_sync.py"", line 98, in test3_configure_device_snmp_interval
743:      if (dnac_handle.ext_configure_snmp_global(retry=""3"", timout=""15"")):
744:    File ""/data/jenkins/ws_sjc2_ds/workspace/Express-Sanity/Hulk/Hulk-BAPI-Optimized-Deployment_and_Express_Sanity/services/dnaserv/lib/decorators.py"", line 32, in wrapper
745:      result = method(*args, **kwargs)
746:  TypeError: Group.ext_configure_snmp_global() got an unexpected keyword argument 'timout'
747:  Test returned in 0:00:00.001912{code}


+*Failed log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_integration_verification_aca_sync.py-32-ISEIntegration&begin=168875&size=7226&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun28_22:41:59.551807.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_integration_verification_aca_sync.py-32-ISEIntegration&begin=168875&size=7226&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun28_22:41:59.551807.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Build:* 2.1.710.70389
*Branch Name:*  private/Hulk-ms/sanity_api_auto
*Script file:* usecasemaps\extapisanity\extapi_usecases_maps.yaml

*input file:* solution_test_input.json

*Testbed details*  :[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-13|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-13]",2023-06-30T04:53:47.860+0000,"Raised PR included fixing the auton: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6164/diff#testcases/extapiusecases/ISEIntegration/ise_integration_verification_aca_sync.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6164/diff#testcases/extapiusecases/ISEIntegration/ise_integration_verification_aca_sync.py] [~accountid:63f50bd640328c12e4ec5b00] , the PR has been approved and merged to Hulk Branch.","['Auton', 'BAPI', 'Hulk', 'Optimized']",ThanhTan Nguyen,Resolved,Omkar Sharad Wagh
SEEN-1882,https://miggbo.atlassian.net/browse/SEEN-1882,[Auton]:Upgrade:Test_TC9_DNAC_perform_save_on_fabric  /   test2_ipv6_migration,"*Reporter Analysis:*  

In upgrade sanity execution,
During ipv6_migration via script it is failing with error code 

”Config Preview Activity failed with reason: NCSP11051: Error occurred while processing the 'provision' request. A different version of the same user intent already exists in the database.” 

*Description*:

20374:  Fabric was not found with input fabric name Global/USA/SAN_JOSE id None. Response: {'response': [], 'version': '1.0'}
20588:  Config Preview Activity failed with reason: NCSP11051: Error occurred while processing the 'provision' request. A different version of the same user intent already exists in the database. Additional info for support: taskId: '804c31f6-46d6-4d7e-ac03-a35f458515e8'. Name: 'Global/USA/SAN_JOSE_US_SJ_Fabric1'. Incoming resourceVersion: '50'. resourceVersion in the database: '51'.
20688:  Empty response when getting scheduled-job for taskId -1
20689:  {'response': [], 'version': '1.0'}



*Branch Name:* Hulk-ms/sanity_api_auto

**Script* *file:** after_upgrade_verify.py

*Source Team:*  *Upgrade Sanity* 

Issue we seen after VCR changes:

**Fail Log:*
TC9:

[Test_TC9_DNAC_perform_save_on_fabric|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4202027&size=795541&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun30_00:08:27.738001.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Rerun Pass log:*

[Test_TC9_DNAC_perform_save_on_fabric|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=245927&size=856854&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun30_00:34:20.388740.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
",2023-06-30T08:15:04.365+0000,"[ENG-SDN / dnac-auto / 36f14579641 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/36f14579641714b77c6130d65c58298c55e632ba]

Should be fixed after this commit, please try and let me know Hi [~accountid:63f50bcece6f37e5ed93c87e] , 
We got a pass log on Hulk , thanks for fixing the issue 
Pass log : 

[Test_TC9_DNAC_perform_save_on_fabric|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4300992&size=2120305&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct20_02:37:15.775666.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Ghost', 'Upgrade', 'Uplift', 'sanity']",Andrew Chen,Closed,Tulasi Reddy
SEEN-1883,https://miggbo.atlassian.net/browse/SEEN-1883,Auton: Upgrade:  Test_TC36_ext_node_ipphone_onboarding_verifications  /   test1_extnode_ipphone_static_onboarding_verifications,"*Reporter Analysis:*
We are observed after VCR changed TC36 was failed with below error:

{{description: Scheduling task for Interface onboarding GigabitEthernet0/4 on device SN-FOC2311T18E for fabric ['Global/USA/SAN_JOSE_US_SJ_Fabric1'] at time 1688122766.1124468}}


*Description*:
2287:  Config preview Statuses for devices were not expected
2289:
2290:  ############################################################
2291:  Config preview was failed
2292:  ############################################################
2363:  Config preview flow was failed for description: Scheduling task for Interface onboarding GigabitEthernet0/4 on device SN-FOC2311T18E for fabric ['Global/USA/SAN_JOSE_US_SJ_Fabric1'] at time 1688122766.1124468
2366:
2367:  ############################################################
2368:  FAILED to Onboard interface GigabitEthernet0/4 on device SN-FOC2311T18E with segment_name 80net_sub-WiredVNFB1, voice_segment_name 64net_sub-WiredVNFB1, authType No Authentication, deviceType USER_DEVICE
2369:  ############################################################
2901:  Device id 4d829902-2883-4a41-8606-cbf55796e85d not in activity with activity id a76ded17-a79e-4932-b77c-4169e082d25e, mappings are: {}
2903:  Config preview Statuses for devices were not expected
2905:


*Hulk  Version* : 2.1.710.70389
*Branch Name*: Hulk-ms/sanity_api_auto
*Script Name* :  after_upgrade_verify.py

*Testcases Impacted :* 

[Test_TC36_ext_node_ipphone_onboarding_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=330124&size=291918&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun30_03:56:06.968950.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_extnode_ipphone_static_onboarding_verifications

+*Fail Log :*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=330688&size=289237&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun30_03:56:06.968950.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=330688&size=289237&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun30_03:56:06.968950.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
 
 +*Snip from Log :*+

!image-20230630-133815.png|width=1401,height=522!",2023-06-30T13:41:38.666+0000,"Pr raised for this issue
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/7c4bb05fddc74e29b5a16e3728fd1dd25748465d|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/7c4bb05fddc74e29b5a16e3728fd1dd25748465d] TC36 pass log:
[Test_TC36_ext_node_ipphone_onboarding_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1134514&size=261041&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul01_06:46:30.666982.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Hulk', 'Upgrade', 'sanity']",Unassigned,Closed,Tulasi Reddy
SEEN-1884,https://miggbo.atlassian.net/browse/SEEN-1884,TC79_hitless_authentication -  test3_change_authentication_template and test7_update_authtemplate_default,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*EXSI OVA build:* assembly_release_dnac_hulk-intg_converged_07-3.710.75408.ova
*Branch*: private/Hulk-ms/api-auto
*Script Name :* solution_test_sanity_ecamb.py
*Testbed :* TB4

*Test-cases Impacted :*  “[Test_TC79_hitless_authentication|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13359474&size=4583154&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_14:59:45.662299.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test3_change_authentication_template”

and [Test_TC79_hitless_authentication|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13359474&size=4583154&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_14:59:45.662299.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test7_update_authtemplate_default  (Errored)



*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_14:59:45.662299.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_14:59:45.662299.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]

*Sub Testcase:*  test3_change_authentication_template 

*Error:* 

72234: Exception occured updating authentication profile get_current_authentication_profile_on_cd() got an unexpected keyword argument 'config_preview'
72235: Library group ""connectivity_domain"" method ""switch_auth_temp_all_fabric_cd"" returned in 0:00:00.001655
72236: Test returned in 0:00:00.004847
72237: Failed reason: Result: Failed to update authentication template

*API Call*:

{{def get_current_authentication_profile_on_cd(self,cd_name=""Global/USA/SAN_JOSE""):}}

*API Location:* 

dnac-auto/services/dnaserv/lib/api_groups/connectivity_domain/group.py





*Sub Testcase:* test7_update_authtemplate_default

*Error:* 

82600: Action: Updating Authentication template for all CD to required profile
82601: Traceback (most recent call last):
82602: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
82603: result = testfunc(func_self, **kwargs)
82604: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 6694, in test7_update_authtemplate_default
82605: if (dnac_handle.set_site_level_auth_template(default=True)):
82606: TypeError: set_site_level_auth_template() got an unexpected keyword argument 'default'
82607: Test returned in 0:00:00.004755
82608: Errored reason: set_site_level_auth_template() got an unexpected keyword argument 'default'



*API:* 

{{def set_site_level_auth_template(self):}}

*API Location:* 

dnac-auto/services/dnaserv/lib/api_groups/connectivity_domain/group.py",2023-06-30T18:33:12.778+0000,"[~accountid:62d2fe9f8afb5805e5d5af49] , it seems like the complain in this Auton has something to do with your commit:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/2556ce0ae6d4da9907f2bd5d03880072ec271f6e#services/dnaserv/lib/api_groups/connectivity_domain/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/2556ce0ae6d4da9907f2bd5d03880072ec271f6e#services/dnaserv/lib/api_groups/connectivity_domain/group.py]

checking with you, for quick suggestion as what might be wrong here..

two use-cases are affected [TC79_hitless_authentication|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13359474&size=4583154&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_14:59:45.662299.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]:

# [test3_change_authentication_template|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15786651&size=2454&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_14:59:45.662299.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] - Here, _get_current_authentication_profile_on_cd()_ is having extra parameter - “config_preview”.
# [test7_update_authtemplate_default|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17866459&size=4578&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_14:59:45.662299.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] - Here, _set_site_level_auth_template()_ got updated where it does not require any input parameter. But the repo has calls to this method with ""default=True"" argument.

We need to update the call to _set_site_level_auth_template()_ and _get_current_authentication_profile_on_cd()_ without any argument, right? Raised PR for the required change: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6220/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6220/overview] With PR approval and merger to Hulk branch, marking this Auton as “Resolved” and “Closed”.","['Auton', 'ESXi']",Amardeep Kumar,Closed,KRISHNA MUKKU
SEEN-1885,https://miggbo.atlassian.net/browse/SEEN-1885,TC173_overlapping_pools_negative_operations - test18_validate_overlapping_pool_on_other_site,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*EXSI OVA build:* assembly_release_dnac_hulk-intg_converged_07-3.710.75408.ova
*Branch*: private/Hulk-ms/api-auto
*Script Name :* solution_test_sanity_ecamb.py
*Testbed :* TB4

*Test-cases Impacted :*  “TC173_overlapping_pools_negative_operations”

During Hulk ESXi sanity while testing ""TC173_overlapping_pools_negative_operations - test18_validate_overlapping_pool_on_other_site” Seeing issue while validate if overlapping can be used in more than one site.

Please find the code SNIP below.

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17900735&size=319097&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun29_17:14:08.270707.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17900735&size=319097&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun29_17:14:08.270707.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



[SNIP …]

61478: Config Preview Activity failed with reason: Invalid IP Pool of Fabric Site cc4212b2-d259-4cf2-b2de-22f5cd6d713e. Pick an IP Pool which is not inherited and is associated with this Fabric Site

61479:

61480: Activity: 9f658d5a-bffd-4e93-abb5-30206694879c Trigger job: {'id': '0f6ea8ec-2b6d-4d1e-8399-1c3a677d793b', 'triggeredJobTaskId': '4dd0b45b-6027-4fa9-b3ef-65fb7e4de713', 'triggeredTime': 1688088671488, 'status': 'FAILED', 'failureReason': 'Invalid IP Pool of Fabric Site cc4212b2-d259-4cf2-b2de-22f5cd6d713e. Pick an IP Pool which is not inherited and is associated with this Fabric Site', 'triggeredJobId': '0f6ea8ec-2b6d-4d1e-8399-1c3a677d793b'}",2023-06-30T18:49:03.002+0000,Fail Log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug06_18:01:16.279393.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug06_18:01:16.279393.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS] PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6746/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6746/overview],"['Auton', 'ESxi']",Moe Saeed,Resolved,KRISHNA MUKKU
SEEN-1887,https://miggbo.atlassian.net/browse/SEEN-1887,[Auton] [NFW]-TC54_DNAC_all_aps_verification_in_wlc-AP's Missing RMA configuration,"*Regression:* Solution Regression NFW

*Branch:* private/Ghost-ms/api-auto-nfw

*Uber ISO tested:* Ghost - 2.3.5.3-70194

*Script Used:* testcases/forty_eight_hour/solution_dnac_wireless_hardening.py

*Issue Faced:* AP missing RMA configuration 

Please add RMA configuration on AP 

Under - [Test_TC54_DNAC_all_aps_verification_in_wlc|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=368794&size=123708&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_17:47:07.205108.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] 

# [test2_perform_ap_rma|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=474710&size=11343&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_17:47:07.205108.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] & [test3_verify_ap_rma|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=486053&size=6278&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_17:47:07.205108.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] 

AP RMA feature is not being tested as pre-requisites are not satisfied. Yet the TC is passing. It’s a False pass.  Could you please check on this ?

Log:  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=368794&size=123708&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_17:47:07.205108.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=368794&size=123708&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_17:47:07.205108.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS]",2023-07-03T05:45:46.982+0000,"Hi [~accountid:712020:06823340-9d28-4722-9cd7-15ef3ea14cc6] ,

I noticed a warning in the log indicating that the pre-requisites have not been satisfied. If you have any suggestions for marking it as false or skipping it, we can make the necessary changes to the result here

!image-20230817-034012.png|width=848,height=185!

!image-20230817-033706.png|width=1475,height=686!","['Auton', 'NFW', 'Regression']",DatChi Pham,Closed,JagadeshKumar Enapanuri
SEEN-1888,https://miggbo.atlassian.net/browse/SEEN-1888,[Auton] [NFW]- TC110_ssid_edit -No action performed for SSID verification,"*Regression:* Solution Regression NFW

*Branch:* private/Ghost-ms/api-auto-nfw

*Uber ISO tested:* Ghost - 2.3.5.3-70194

*Script Used:* testcases/forty_eight_hour/solution_dnac_wireless_hardening.py

*Issue Faced:* while verification of modified SSID’s there was no actions performed under this TC’s please check on it

# Under - [Test_TC110_ssid_edit|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11502155&size=21828&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_17:47:07.205108.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] 

# [test2_verify_modify_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11513669&size=10164&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_17:47:07.205108.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS] 

No actions performed and displayed as Pass. SSID verification on device is not performed. Could you please check once this?

Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11513669&size=10164&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_17:47:07.205108.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11513669&size=10164&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53%2Fusers%2Fphannguy%2Farchive%2F23-05%2Fsr_nfw_job.2023May02_17:47:07.205108.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=phannguy&from=trade&view=all&atstype=pyATS]  --referTC110",2023-07-03T05:55:07.907+0000,"Hi [~accountid:712020:06823340-9d28-4722-9cd7-15ef3ea14cc6] , I will update code for fix issue.
- Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6816/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6816/overview]
- Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6814/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6814/overview]
Could you please checked and merged again?
Thanks","['Auton', 'NFW', 'Regression']",DatChi Pham,Resolved,JagadeshKumar Enapanuri
SEEN-1890,https://miggbo.atlassian.net/browse/SEEN-1890,[Auton]:Hulk: Test_TC1_DNAC_EXT_NODE_interface_config_verifications/test7_dnac_ext_node_onboarding_enabling_AEN,"*Reporter Analysis:* We have observed that after VCR changes AEN node is not onboarding on Hulk via script.
On Hulk, there is a script issue which is blocking the creation of AEN segment : Provisioning failed due to invalid request. % operation is now deprecated. Meaning VCR 2.0 changes are required to send payload at PUT on CD compared to earlier which was PUT on VN.

Extra Points from DE shashank:


# Fabric has global no auth template + Extended node segment.
(2) Extended node is onboarded. This creates port-channel on edge device.
(3) Extended node is removed from fabric.
(4) Port-channel on edge is removed.
(5) Port connected to extended node on edge (Gig 1/0/10) is shut.
(6) Global closed auth is configured on fabric site.
(7) After this various AEN related tests are getting started where its checking for dot1x on edge port and failing the test.
The root cause of issue is at step (6) where dot1x config is not pushed on Gig1/0/10. Likely reason for this is that the effective config of the port in DNAC is still trunk. After port-channel is removed in step(4) since the global auth is no auth and CISCO_SWITCH_TEMPLATE is present on the device, it will still leave the effective config of the port as trunk. Now step (5) should remove the attached SWITCH template but because of sticky timer configuration on the device it can take up to 30-60 seconds to remove the configuration from the port. During this time port shut event would have generated a port configuration trap which would have been processed by DNAC. The port state in DNAC will be updated only after trap has been processed and inventory synch for port has completed. In the meantime script is performing operation(6) on the device which will trigger provisioning of the device. Depending on the timing, there are at-least three possibilities which can still leave the port in trunk mode when closed auth configuration intent is being processed by SDA code. The three possibilities are:
(1) Trap processing was very fast and started immediately after port shut event. Since template will still be attached to interface for 30-60 seconds at this time, the port state seen by DNAC will still be trunk and it wont be updated.
(2) Trap processing was slow enough that actual port state changed to non trunk (dynamic auto) on device. DNAC started synching but closed auth provisioning operation took precedence and synch was aborted to be tried again later, meaning SDA code still saw the port state as trunk.
(3) Step-6 happened before inventory trap processing and so result was same as 2.
All in all this looks like a timing issue and it may or may not work depending on timing. So failure could be expected based on some scenarios.


Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-122-SDAExtnodeOnboarding&begin=3900&size=5231249&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun26_21:03:11.926454.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-122-SDAExtnodeOnboarding&begin=3900&size=5231249&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun26_21:03:11.926454.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
 

*Description*:  

{code:python} Device 2530b979-7911-4712-99d6-341cbfef1190 Status is Success, should be Not Started, mappings are: {'2530b979-7911-4712-99d6-341cbfef1190': 'Success', '2d21db94-a3a9-49ee-a7a7-887d6e685079': 'Success', '21f69504-7ec9-49b3-af01-c494f12cf754': 'Not Started', '7abee966-fde9-4596-aee6-d0c22423aa84': 'Success', '2e6b4e79-9caa-4dc8-8ca5-9f8ebdc3b7c7': 'Success'}
{code}

*Branch Name:* Hulk-ms/sanity_api_auto

**Script* *file:** extNode_onboarding /solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:",2023-07-04T04:10:11.763+0000,"Hi [~accountid:63f50bfce8216251ae4d59d5] ,

Can you please look into the issue with priority

Thanks,
Anusha John PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6524/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6524/overview] Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6766/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6766/overview]","['Auton', 'Hulk', 'Optimized', 'Sanity']",Moe Saeed,Resolved,Anusha John
SEEN-1891,https://miggbo.atlassian.net/browse/SEEN-1891,TCP MSS related use-case requires uplift w.r.t. SDA2.0 related API changes,Raising this Auton to track the progress on effort spent for TCP MSS related use-case where it requires uplift w.r.t. SDA2.0 related API changes.,2023-07-04T06:29:11.557+0000,"PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6290/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6290/overview] PR is in ‘Needs work’. Need to uplift library with VCR changes in Hulk. [~accountid:63f50bf5e8216251ae4d59cf] , Can you address the comment in the PR so that we can merge into Hulk onward?","['Auton', 'Hulk', 'Integration']",Raji Mukkamala,Pending Code Review,Amardeep Kumar
SEEN-1892,https://miggbo.atlassian.net/browse/SEEN-1892,Auton:Test_TC204_Provision_single_AP_with_custom_rf_profile  /  test2_provision_ap_with_rf_profile,"Hi  Team
We have observed TC's failing on the Polaris controller (eWLC). We tested Sanity TBs on TB7 and TB11 eWLC at the New York site,
is there a limitation on the Platform model used for Wireless Controller?

Seems like Testbeds with AireOS Controller are only able to get pass log.

*Uber ISO Version tested:*  *Ghost -2.1.614.70716*
*Script Name:* Optimized code 
\testcases\sanityusecases\
*Testbed : TB7*  

+*Failed Log :*+ 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-custom_rf_profile.py-1711-customRFProfile&begin=111591&size=49553&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May20_09:42:25.776261.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-custom_rf_profile.py-1711-customRFProfile&begin=111591&size=49553&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May20_09:42:25.776261.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


*Script Name:* Optimized code 
\testcases\sanityusecases\
*Testbed :TB11(AWS)*
+*Failed Log :*+ 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-custom_rf_profile.py-1711-|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-custom_rf_profile.py-1711-customRFProfile&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr20_07:42:56.388654.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Comment from Venkatesh: 
_Please check to see if eWLC was already provisioned with an AI profile, 
if so we have to make sure AI profile is removed first before provisioning custom RF profile, otherwise provisioning will fail for eWLC._",2023-07-04T11:36:04.151+0000,"Hi Team ,

Optimization On-prem   Ghost RC2 P2 
Failed  Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-custom_rf_profile.py-179-customRFProfile&begin=111478&size=54141&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug15_06:28:36.502184.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-custom_rf_profile.py-179-customRFProfile&begin=111478&size=54141&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug15_06:28:36.502184.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] *Comment from Venkatesh:* 
_Please check to see if eWLC was already provisioned with an AI profile, if so we have to make sure AI profile is removed first before provisioning custom RF profile, otherwise provisioning will fail for eWLC._

as per  Venkatesh  commnet   ,
Could you please  add  the  test case, or  condition?  PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6695/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6695/overview]



Note: This need to be tested in a Ghost cluster! ","['Auton', 'Feature', 'Ghost', 'Integration', 'Optimized', 'Sanity']",Moe Saeed,Resolved,Omkar Sharad Wagh
SEEN-1893,https://miggbo.atlassian.net/browse/SEEN-1893,Common Setup,"Common Setup is getting errored causing all the TCs as Blocked .

!image-20230705-161650.png|width=873,height=605!



Trade Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=531&size=10386&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-07%2Fsanitycombine.2023Jul05_21:35:38.460845.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=531&size=10386&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-07%2Fsanitycombine.2023Jul05_21:35:38.460845.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

*Branch Used:*

bgl/Hulk-ms/api-auto

*Test Suite:*

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=bgl/Hulk-ms/api-auto]",2023-07-05T16:17:46.803+0000,,"['Auton', 'Hulk']",Unassigned,Cancelled,SAINATH CHATHARASI
SEEN-1894,https://miggbo.atlassian.net/browse/SEEN-1894,Test_TC126_verify_inventory_insights/test1_verify_speed_duplex_mismatch/test2_verify_VLAN_mismatch,"[~accountid:62d2fe9f8afb5805e5d5af49]  Please check on the below Auton

*Observations:*

TC126 both sub TCs failed via scripts & performing steps manually & rerunning the TC got passed could be  timing issue. Changing interface speed & duplex mode taking time getting updated changes status.

*Branch:* private/Hulk-ms/api-auto

*Script file:* solution_test_sanityecamb.py

*input file:* solution_test_input.json

*ova file:* assembly_release_dnac_hulk-intg_converged_07-3.710.75408.ova



Failed logs:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1229786&size=249390&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul04_00:01:27.232209.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1229786&size=249390&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul04_00:01:27.232209.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



pass logs after performing manually steps.

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=904286&size=356249&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul04_18:54:23.202434.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=904286&size=356249&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul04_18:54:23.202434.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-07-05T16:19:59.086+0000,"Raised PRs:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6938/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6938/overview]
* HulkPatch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6939/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6939/overview]","['Auton', 'Hulk', 'hulk-vm-sanity', 'sanity']",ThangQuoc Tran,Resolved,Raghavendrachar Baraguru Mallesha Char
SEEN-1901,https://miggbo.atlassian.net/browse/SEEN-1901,TC31_SWIM_UPGRADE_ECA_DEVICE- test14_swim_psirt_smu,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*EXSI OVA build:* assembly_release_dnac_hulk-intg_converged_07-3.710.75408.ova
*Branch*: private/Hulk-ms/api-auto
*Script Name :* solution_test_sanity_ecamb.py
*Testbed :* TB4
*Testcases Impacted :* test14_swim_psirt_smu and test15_verify_custom_checks

During Hulk-ESXI sanity while testing TC31_SWIM_UPGRADE_ECA_DEVICE, 



*test14_swim_psirt_smu* fails with below file check.

*Fail Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7246999&size=2662&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul03_11:44:22.829352.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7246999&size=2662&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul03_11:44:22.829352.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


18589: Action: Validate SWIM PSIRT SMU
18590:
18591: ************************************************************
18592: Start uploading base image and PSIRT SMU bundle to DNAC
18593: ************************************************************
18594: Missing psirt_smu_url in fabric input file. Please provide http_url, base_image and psirt_smu in psirt_smu_url and re-execute the testcase
18595: Library group ""swim"" method ""swim_psirt_smu"" returned in 0:00:00.000627
18596: Test returned in 0:00:00.001601



There should be a variable check to test SMU as every images we are not going to test SMU.",2023-07-05T20:36:09.655+0000,This is a miss from fabric input file. The testbed owner need to add the URL link to fabric input file. Not a script issue.,"['Auton', 'ESxi', 'Hulk']",Moe Saeed,Closed,KRISHNA MUKKU
SEEN-1902,https://miggbo.atlassian.net/browse/SEEN-1902,TC31_SWIM_UPGRADE_ECA_DEVICE- test15_verify_custom_checks,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*EXSI OVA build:* assembly_release_dnac_hulk-intg_converged_07-3.710.75408.ova
*Branch*: private/Hulk-ms/api-auto
*Script Name :* solution_test_sanity_ecamb.py
*Testbed :* TB4
*Testcases Impacted :* test15_verify_custom_checks

During Hulk-ESXI sanity while testing TC31_SWIM_UPGRADE_ECA_DEVICE, 

*test15_verify_custom_checks* fails with some old and new diffs after device upgrade done, not sure what diffs are missing from the log

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7249661&size=264266&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul03_11:44:22.829352.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7249661&size=264266&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul03_11:44:22.829352.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-07-05T22:04:53.589+0000,TC is no longer in the script based on the description above. ,"['Auton', 'ESXi', 'Hulk']",Moe Saeed,Closed,KRISHNA MUKKU
SEEN-1903,https://miggbo.atlassian.net/browse/SEEN-1903,no_health_prepairing() method require better coding to get required info,"[Test_TC218_troubleshoot_bad_health_device / test1_prepare_wlc_device|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=174836&size=228607&archive=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats%2Fusers%2Famardkum%2Farchive%2F23-07%2Fenv_auto_job.2023Jul05_16:31:55.472817.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats&submitter=amardkum&atstype=PYATS] has false failure.

Upon checking, _no_health_prepairing()_ method requires better coding to get required info.

Also need to limit on the content of output that is required.

Based on the content, I feel, finding managementIpAddress should be first done and then use the same info to find the interface name:

{{device[""managementIpAddress}}""]

{noformat}B18-eWLC#show ip interface brief | include 204.192.4.2
Vlan567                204.192.4.2     YES NVRAM  up                    up{noformat}",2023-07-06T00:08:36.977+0000,,"['Auton', 'Integration']",QuangVinh Nguyen,Resolved,Amardeep Kumar
SEEN-1904,https://miggbo.atlassian.net/browse/SEEN-1904,[AUTON][NFW]- Test_TC53_DNAC_provision_all_aps -Ap's not getting added to any floor after provisioning successful,"*Regression:* Solution Regression NFW

*Branch:* private/Ghost-ms/api-auto-nfw

*Uber ISO tested:* Ghost - 2.3.5.4-70759

*Script Used:* testcases/forty_eight_hour/solution_dnac_wireless_hardening.py

We observed that while testing TC53-[Test_TC53_DNAC_provision_all_aps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2817675&size=533501&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_nfw_job.2023Jun30_01:59:25.197826.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]  ,  Ap’s joined to wireless controllers, provisioning went successful but Ap’s not getting assigned to any site.But when manually tried,site addition to Ap’s is successful.Could you please check on this issue?

Log : [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_nfw_job.2023Jun30_01:59:25.197826.zip&reqseq=&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-06%2Fsr_nfw_job.2023Jun30_01:59:25.197826.zip&reqseq=&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=PYATS] --refer TC53",2023-07-06T15:05:38.213+0000,"Hi [~accountid:63f50bcdce6f37e5ed93c87d] ,

As we disussed,I have changed the discovery IP to Vlan Ip address removing management IP.Discovery went fine and also Ap’s getting associated with sites.But the problem is after devices getting assigned to sites,the two EWLC’s going to not managed state with “Netconf access denied error” and the respective devices having provisioning failure with rpc denial error”.This issue is not seen before when management Ip is made as discovery Ip.Please find the attached screen shots.

!1636_888_1.5.png|width=818,height=444! Hi [~accountid:63f50bddc1685a24e1314c87] ,

I see that the 'Netconf access denied: NCIM12028' error does not appear to be related to the expected behavior of TC53. It seems to be a configuration issue within the device or testbed. Could you please open a new ticket to address this specific issue? Sure [~accountid:63f50bcdce6f37e5ed93c87d] .We understood that its not related to TC53 ,but problem here is seen when we had changed the discovery IP to Vlan IP.We will check further on that.","['Auton', 'Execution', 'Ghost', 'Hulk', 'NFW']",DatChi Pham,Closed,Neelima Doddipalli
SEEN-1905,https://miggbo.atlassian.net/browse/SEEN-1905,Auton: [Sanity] Test_TC60_Verify_DHCP_server_change_on_segments  / test5_verify_devices_status_after_change_shared_secret,"*Description:*  The *Test_TC60_Verify_DHCP_server_change_on_segments / test5_verify_devices_status_after_change_shared_secret* is to verify the shared secret on the list of devices where it is verifying on Aireos WLC as well. But the command that is being used to verify the shared secret key is “ {{show run| s aaa server radius}}“ which is not supported on Aireos WLC device. 

*Error Snippet:* 
{{Validating shared secret key: newcisco on TB3-DM-WLC }}

{{(Cisco Controller) > }}

{{show run? run-config running-config (Cisco Controller) >show run| s aaa server radius Incorrect usage. Use the '?' or <TAB> key to list commands. (Cisco Controller) > }} 


*Branch:* private/Hulk-ms/sanity_api_auto & private/Ghost-ms/sanity_api_auto

*Script file:* solution_test_sanityecamb_lan.py / *Test_TC60_Verify_DHCP_server_change_on_segments / test5_verify_devices_status_after_change_shared_secret*

*input file:* solution_test_input.json

*Failed log:* [test5_verify_devices_status_after_change_shared_secret|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=52481973&size=496511&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul06_03:33:51.998558.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed*: Sanity Testbed TB3",2023-07-07T13:25:08.135+0000,"Raised PRs:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6252/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6252/overview]
* Halleck: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6253/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6253/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6254/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6254/overview] [~accountid:63f50bd34c355259db9ccc4d] , I have approved and merged all three PRs.

[~accountid:5e1415780242870e996f0b2f] , pls. validate on your testbed and update this ticket with the status. Thanks [~accountid:63f50bd34c355259db9ccc4d] 
Sure [~accountid:62ab7a399cd13c0068b18fe0]  Will validate it and update with the results! Hi [~accountid:63f50bd34c355259db9ccc4d] ,

Can you please raise a PR for Guardian branch as same issue is seen recently on guardian branches:

*Branch used:*
private/Guardian-ms/sanity_api_auto
*Build used*:Guardian P4 RC4-RSPIN(2337)#2.1.518.72328
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=48551503&size=455963&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul11_11:19:28.723286.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=48551503&size=455963&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul11_11:19:28.723286.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Thanks,
Anusha John [~accountid:5e1415780242870e996f0b2f], I have raised the PR for Guardian branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6298/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6298/overview]

Please review the change, approve and merge to Guardian branch.","['Auton', 'Ghost', 'Hulk', 'sanity']",ThangQuoc Tran,Resolved,Ashwini R Jadhav
SEEN-1906,https://miggbo.atlassian.net/browse/SEEN-1906,[Auton]Ghost:Test_TC30_remove_all_interfaces_from_ext_node/test1_clear_extended_device_all_interface,"*Reporter Analysis:*

Issue was seen in upgrade combination:
Ghost P1 RC6<>Ghost P2
Ghost P1 RC6<>Hulk
Guardian P4 RC4<>Hulk
Guardian P4 RC4<>Ghost P2

We have observed that AEN Connected interface is not getting cleared via Upgrade script.We need to remove the AEN & then only interface can be cleaned as AEN is having supplicant based configs.So before TC 30 or during TC 30  we need  AEN cleanup in upgrade_verify script

*Description*:  

{code:python}3886:   !!!!!!! Clearing subtended node device interface FAILED, ReasonConfiguration on port of Extended Node SN-JAE24040C8K.cisco.com connected to upstream Edge Node or Extended Node cannot be overwritten. !!!!!!!{code}



*Branch Name:* Ghost -ms/sanity_api_auto and Hulk-ms/sanity_api_auto

**Script* *file:**after_upgrade_verify.py

*Source Team:*  Upgrade-Sanity

Issue Seen first time or day0 issue:

**Fail Log TC 30:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=434520&size=227769&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul06_20:39:12.167248.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=434520&size=227769&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul06_20:39:12.167248.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*Affected Testcase:*
[Test_TC31_onboard_device_clear_all_interfaces_all_edges|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=388741&size=233886&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul06_21:08:22.588462.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/[test1_onboard_device_clear_all_interfaces_all_edges|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=389493&size=232950&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul06_21:08:22.588462.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


",2023-07-07T17:33:56.715+0000,"Hi [~accountid:63f50bfce8216251ae4d59d5]  / [~accountid:62d2fe9f8afb5805e5d5af49] 

Can we proritise the Jira as two Testcases has been affected

Thanks,
Anusha John [~accountid:62d2fef2bd54f8d3ffb7d1f7] Was it passed before? in which release? Can you provide passed log? [~accountid:62d2fe9f8afb5805e5d5af49] 

Recently only we are using on prem testbed where we have AEN node for Upgrade Sanity

Eralier Upgrade sanity Testbeds didn’t have AEN Node I need a ghost cluster to test the fix. 
Please let me know once the cluster is ideal. [~accountid:61efa8c457b25b006877eda3]  [~accountid:63f50bfce8216251ae4d59d5] Currently we are using ISE 2.7P10 due to which AEN is not onboarded .
Sure I will check and share cluster Looks Similar to [https://miggbo.atlassian.net/browse/SEEN-2003|https://miggbo.atlassian.net/browse/SEEN-2003|smart-link] May be duplicate. Resolved a as part of [https://miggbo.atlassian.net/browse/SEEN-2003|https://miggbo.atlassian.net/browse/SEEN-2003|smart-link]. Hi [~accountid:5f3c6ae932360700388f7b4b]  / [~accountid:63f50bfce8216251ae4d59d5] 


Reopened bug again hitting same issue 

 I am not seeing issue after that fix while cleanup script is triggered  but I am hitting same issue from Upgrade sanity script


*Branch used:*
private/HulkPatch-ms/sanity_api_auto



*Script used*:after_upgrade_verify.py


Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2498217&size=857302&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct04_22:37:40.761860.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2498217&size=857302&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct04_22:37:40.761860.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Can you please look in to the issue as AEN related upgrade sanity Testbed only hitting issue.



Thanks,
Anusha John","['Auton', 'Ghost', 'Hulk', 'Upgrade']",Moe Saeed,Reopened,Anusha John
SEEN-1907,https://miggbo.atlassian.net/browse/SEEN-1907,Optimized  [Auton]:[Hulk]: Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x  /   Test_TC3_DNAC_configure_multicast_on_sites  /   cleanup_existing_multicast ,"*Reporter Analysis:*
recent Sanity Optimized hulk run (2370-70402) i Observed  freshly deployed  on cluster  cleanup_existing_multicast tc is failed config  preview , due to this  sub tc are blocked ,

could you please check and confirm this,


*Error snip  :*  

9144:  ############################################################
9145:  Config preview was failed
9146:  ############################################################
9198: 
9199:  The activity 5d7f2d90-767d-40f5-b5bb-e32d0b134120 is NOT discarded as expected yet.
9200:  Re-checking
9217:  Config preview flow was failed for description: Scheduling task for Clearing multicast rp for Global/USA/SAN_JOSE for fabric ['Global/USA/SAN_JOSE'] at time 1688658208.666229
9220: 
9221:  ############################################################
9222:  Clearing Multicast is failed for site: Global/USA/SAN_JOSE
9223:  ############################################################
9226:  Failed reason: Failed to cleanup multicast configuration!


+*Please find below the failed log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x&begin=1871232&size=614570&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul06_08:22:01.023566.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x&begin=1871232&size=614570&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul06_08:22:01.023566.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Hulk_70276
*Pass_log:*
[+https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x&begin=6596529&size=3207482&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May17_03:09:39.567349.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS+|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-132-trafficdot1x&begin=6596529&size=3207482&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May17_03:09:39.567349.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]




*Branch Name:*  private/Hulk-ms/sanity_api_auto

*Script file/Usecase:* [Filelansanity_usecases_maps.yaml|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/lansanity/lansanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto]

*Source Team:  Sanity(Optimized)*",2023-07-08T08:00:25.413+0000,"Use cases required VCR couldnot run together.
{{Update mapping files ""13"" : Put VCR related usecases to run sequentiallly.}}
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ceae9a47ac011c84543e77a6be9a5e7b3928c856|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ceae9a47ac011c84543e77a6be9a5e7b3928c856]","['Auton', 'Blocker', 'Hulk', 'Optimized', 'Sanity']",Tran Lam,Resolved,Omkar Sharad Wagh
SEEN-1912,https://miggbo.atlassian.net/browse/SEEN-1912,TC3_generate_dhcp_server_config_on_fusion - test2_verify_ISE_CSRF_before_integration,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*Branch*: private/Hulk-ms/api-auto

*Script Name :* solution_test_sanity_ecamb.py

*Testbed :* TB4

*Testcases Impacted :*  [Test_TC3_generate_dhcp_server_config_on_fusion|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=402155&size=1260431&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul10_18:43:42.462564.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test2_verify_ISE_CSRF_before_integration

During Hulk ESXI testing “[Test_TC3_generate_dhcp_server_config_on_fusion|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=402155&size=1260431&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul10_18:43:42.462564.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]”  found error while checking ISE version, please find the error below.

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1141345&size=13553&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul10_18:43:42.462564.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1141345&size=13553&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul10_18:43:42.462564.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



8884: Writing library method name map into file ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/ext_name_map.csv""

8885: Successfully wrote map

8886: Traceback (most recent call last):

8887: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper

8888: result = testfunc(func_self, **kwargs)

8889: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 285, in test2_verify_ISE_CSRF_before_integration

8890: if not dnac_handle.is_ISE_meet_requirement(""3.1""):

8891: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper

8892: result = method(*args, **kwargs)

8893: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/lib/api_groups/ise_integration/group.py"", line 69, in is_ISE_meet_requirement

8894: if not self.services.ISE_version:

8895: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/dnaservices.py"", line 330, in __getattr__

8896: raise AttributeError(err_msg)

8897: AttributeError: 'DnaServices' object has no attribute 'ISE_version'

8898: Test returned in 0:00:00.352974

8899: Errored reason: 'DnaServices' object has no attribute 'ISE_version'",2023-07-11T06:26:22.471+0000,,"['Auton', 'ESxi', 'Hulk']",KRISHNA MUKKU,Closed,KRISHNA MUKKU
SEEN-1913,https://miggbo.atlassian.net/browse/SEEN-1913,[Auton][MSTB2] : Device are going unreachable post Bordernode delete and Re-add scenario,"Ova#3.710.75438
ISE#3.1 P3
Polaris: 17.10.1/17.12.1Th

h1. Description:

After Bordernode device is delete and re-added we are seeing BGP neighbors is showing as down and devices are showing as unreachable.

h1. Steps to Reproduce:

# Add ISE to DNAC
# Add all design
# Discover the device
# Assign device to site and provision it.
# Add all Fabric and assign it
# Onboard all Extended nodes.
# Remove Bordernode and readd it. We are seeing BGP neighbors showing as down.

h1. Error Log :

131184:  +++ SJC-FB-9500 with via 'a' and alias 'a': executing command 'show bgp vrf VN4 neighbor | inc state' +++
show bgp vrf VN4 neighbor | inc state
  BGP state = Active, down for 00:00:50
  Do log neighbor state changes (via global configuration)
SJC-FB-9500#
131187:  Nbrs expected:1 in state Established
131188:  Expected Number of BGP neighbours not found, wait for some more time

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=29502026&size=1456974&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul06_09:43:23.863824.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=29502026&size=1456974&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul06_09:43:23.863824.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Passlog : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17632783&size=4633565&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fauto_MS_job.2023Mar03_01:37:19.966391.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17632783&size=4633565&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fauto_MS_job.2023Mar03_01:37:19.966391.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-07-11T14:06:15.130+0000,"Hi [~accountid:63f50bcf4e86f362d39acde5]

Can you please let me know the latest on the issue? * PR-link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6327/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6327/overview]
* summary: set up vlan for re-add border
* passlog: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-07/sanity-intg2.2023Jul19_00:59:24.777341.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-07/sanity-intg2.2023Jul19_00:59:24.777341.zip&atstype=ATS] Hi [~accountid:63f50bcf4e86f362d39acde5],

We have run the latest code and we could seen IP assigned to vlan9 is overlapping

64691: 
 Config Preview Activity failed with reason: IP address 40.10.1.1 configured on interface Vlan9 of device SJC-FE-9300-1.cisco.com overlaps with the IP Pool, 40.10.1.0/24, added to the Virtual Network. Unconfigure the overlapping IP address on the device, resync, and retry.

{noformat}64692: 
{noformat}

{noformat}64693: 
 Activity: 018ab17a-0f77-76d8-bd95-dd9535e1e7a9 Trigger job: {'id': '0ce76b54-f111-4bfa-ab82-ae0cdac65e19', 'triggeredJobTaskId': '018ab17a-0faf-7ee8-94d8-c3d5402b9a8e', 'triggeredTime': 1695194681268, 'status': 'FAILED', 'failureReason': 'IP address 40.10.1.1 configured on interface Vlan9 of device SJC-FE-9300-1.cisco.com overlaps with the IP Pool, 40.10.1.0/24, added to the Virtual Network. Unconfigure the overlapping IP address on the device, resync, and retry.', 'triggeredJobId': '0ce76b54-f111-4bfa-ab82-ae0cdac65e19'}{noformat}

{noformat}64784: 
 activity_id is False. Config preview task failed for description Scheduling task for Configuring Fabric roles for devices for fabric ['Global/USA/SAN_JOSE'] at time 1695194681.1404238 - Configuration Preview{noformat}

Log :  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11787289&size=2965339&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fauto_MS_job.2023Sep19_23:01:37.657495.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11787289&size=2965339&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fauto_MS_job.2023Sep19_23:01:37.657495.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Script : solution_test_3sites_sjc_nyc_sf.py
Branch : private/HulkPatch-ms/api-auto
Polaris image : 17.12.1 CCO
ISO#3.713.75113 [~accountid:63f50bcf4e86f362d39acde5], pls. prioritize the fix for below complain as this is affecting overall Quality for ESXi Regression. Hi Amardeep, Divakar. after observed issue, i think the failure is not because of {{Test_TC91_Remove_Border_add_back}}, it because of previous test Test_TC90_remove_re_add_segment_to_anchor_vn which create vlan9 but did not clean it up, which cause the failure

and because it’s not related to “remove and re-add border device”, please verify it one again [~accountid:63f50bf0e8216251ae4d59ca] , pls. check on [~accountid:63f50bcf4e86f362d39acde5]  comment and respond back. Hi [~accountid:63f50bcf4e86f362d39acde5],

Currently we are unable to re-check the scenario because we have one more Open Jira(SEEN-2419) for this usecase. We’ll re-verify the testcase again after Code is merged.  as agreement from reporter, we close this auton as we already handle the issue. For other issue, we already have another auton (SEEN-2419) and we will track on that auton","['Auton', 'Hulk', 'MSTB2']",QuangVinh Nguyen,Resolved,Divakar Kumar Yadav
SEEN-1914,https://miggbo.atlassian.net/browse/SEEN-1914,Test_TC214_SDA_Wired_Host_Onboarding_Uplink_Interfaces requires refactoring and some more checks,"# Below set of “flow monitor” config does not work on C9500 Device due to which the “ip flow monitor” related configs are not getting added to it’s Interface.
Hence, need to put a check to skip the test for C9500.
{noformat}Current configuration:
!
flow monitor dnacmonitor
 exporter dnacexporter
 cache timeout inactive 10
 cache timeout active 60
 record dnacrecord
!
!
flow monitor dnacmonitor_v6
 exporter dnacexporter
 cache timeout inactive 10
 cache timeout active 60
 record dnacrecord_v6
!
!
flow monitor dnacmonitor_dns
 exporter dnacexporter
 cache timeout inactive 10
 cache timeout active 60
 record dnacrecord_dns
!
!
flow monitor dnacmonitor_dns_v6
 exporter dnacexporter
 cache timeout inactive 10
 cache timeout active 60
 record dnacrecord_dns_v6{noformat}
# All the use-cases are having call to “{{get_free_uplink_interface_from_device}}()"" method that can be avoided and create a dict that can hold the Identified Edge device and one of it’s free interface that is with “down” status.

# Script should also consider “{{TwentyFiveGigE}}"" Interfaces.",2023-07-11T23:42:55.487+0000,Raised PR for the required change: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6248/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6248/overview] PR got approved and merged to Hulk branch.,"['Auton', 'Integration']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1915,https://miggbo.atlassian.net/browse/SEEN-1915,Auton: [Sanity] Test_TC123_aaa_per_ssid /test4_connect_clients_to_ssids - which is not fetching clients details from yaml file,"*Description:*  The  *Test_TC123_aaa_per_ssid /test4_connect_clients_to_ssids* sub-tc is looking for wireless clients based on the condition where in the yaml file wireless clients are with {{role: ""wireless-client,static""}} . We have these wireless clients with role: ""wireless-client,static"" still the script is not fetching this details from yaml file. where as we do have wireless clients in yaml as well as onboarded on cluster.

*Error Snippet:* 
{{Client list [] }}

{{No wireless client found at site(s) with WLC. Required in order to test this case!}}

*Branch:* private/Ghost-ms/sanity_api_auto

*Script file:* solution_test_sanityecamb_lan.py /*Test_TC123_aaa_per_ssid /test4_connect_clients_to_ssids*

*input file:* solution_test_input.json

*Failed log:*  [Test_TC123_aaa_per_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=796733&size=3843718&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul11_23:31:05.275357.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] / [test4_connect_clients_to_ssids|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3589629&size=4467&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul11_23:31:05.275357.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed*: Sanity Testbed TB3",2023-07-12T13:13:09.377+0000,"Added fix [09299828b91|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/09299828b9169899b82e60c818f8d7ec9afcb546], run the test and please confirm Hey [~accountid:63f50bf5e8216251ae4d59cf]  
The below fix works fine 
Log: [Test_TC123_aaa_per_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=882371&size=19578&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul19_02:27:24.141091.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] / [test4_connect_clients_to_ssids|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=885766&size=8134&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul19_02:27:24.141091.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] has passed Hi [~accountid:63f50bf5e8216251ae4d59cf] 
Could you please add the fix in Hulk as well Here is the latest log on HULK RC2
Failed: [Test_TC3_aaa_per_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-cmx_config_push_and_device_validations.py-157-cmxConfigsAndValidations&begin=2020066&size=4994391&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul26_03:12:19.132159.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
The below fix [09299828b91|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/09299828b9169899b82e60c818f8d7ec9afcb546], is working on Ghost. Similar fix is needed in Hulk Recent logs for this feature Test_TC123_aaa_per_ssid
where the sub-tc test3_onboard_wireless_segment_for_new_ssid failed for AP provisioning:
Provisioning AP of device failed for reason:AP cannot be provisioned to Basic RF profile when AI is enabled.","['Auton', 'Ghost', 'Hulk', 'Integration', 'sanity']",Raji Mukkamala,Resolved,Ashwini R Jadhav
SEEN-1916,https://miggbo.atlassian.net/browse/SEEN-1916,Testbed Metadata collection is failing for ESXi VM DNAC,"Testbed Metadata collection is failing for ESXi VM DNAC. Upon checking, it turns out that use-case  _common_cleanup / collect_testbed_metadata_ is failing to collect DNAC Version for ESXi VM DNAC.

Failed execution log: [collect_testbed_metadata|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1543868&size=28027&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul06_13:38:44.558237.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS].",2023-07-12T22:12:59.308+0000,Raised PR for the required change: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6258/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6258/overview] PR got approved and merged to Hulk Branch.,"['Auton', 'ESXi']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1917,https://miggbo.atlassian.net/browse/SEEN-1917, [Auton] [Hulk] - [16ssid limit] Error ask-advanced_wlan_configs.py-138-advancedWlanConfigs  /   Test_TC1_advanced_wlan_configs ,"* *Report analysis:*  In hulk  Run  We  observed  wlan config  tc is  failing  with below  error 
{{essage:{""response"":{""errorCode"":""NCND00002"",""message"":""NCND00002: The Network Profile request has validation errors"",""detail"":""[\""NCND01175: This Profile cannot be assigned to more than 16 SSIDs\""]""},""version"":""1.0""}}}
*DNAC Release_Version Tested:* Hulk:2.1.710.70425
* *Device Image Used:* 17.12.1
* *Testbed:* *SanityTB*
* *Branch Used:* private/Hulk-ms/sanity_api_auto
* *Testcase  File:*[*lansanity_usecases_maps.yaml*|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/lansanity/lansanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto]
* *Solution Input File :* [solution_test_input.json|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/config_48hr_test/solution_test_input.json?at=private%2FHulk-ms%2Fsanity_api_auto]
* *Testcases Impacted:* [Task-advanced_wlan_configs.py-138-advancedWlanConfigs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul12_03:58:22.913106.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   Test_TC1_advanced_wlan_configs
* *Failed Trade Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=3938&size=820522&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul12_03:58:22.913106.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=3938&size=820522&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul12_03:58:22.913106.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

* *Failed Log Snapshot :* 
{noformat}Message:{""response"":{""errorCode"":""NCND00002"",""message"":""NCND00002: The Network Profile request has validation errors"",""detail"":""[\""NCND01175: This Profile cannot be assigned to more than 16 SSIDs\""]""},""version"":""1.0""}
214:  Traceback (most recent call last):
215:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/client_manager.py"", line 326, in call_api
216:      response.raise_for_status()
217:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status
218:      raise HTTPError(http_error_msg, response=self)
219:  requests.exceptions.HTTPError: 406 Client Error: Not Acceptable for url: https://10.30.0.100/api/v1/siteprofile/f96eb42e-e0c9-4385-82da-9291a38ec818
220:  Encountered unhandled HTTPError in Internal API Call
221:  Flagging result as FAIL!
222:  	Reason: 406 Client Error: Not Acceptable for url: https://10.30.0.100/api/v1/siteprofile/f96eb42e-e0c9-4385-82da-9291a38ec818
223:  Kwargs:
224:  {'data': {'lastUpdatedBy': 'admin',
225:            'lastUpdatedDatetime': 1689162862223,
226:            'name': 'profile-SSIDOpenIndia',
227:            'namespace': 'wlan',
228:            'profileAttributes': [{'attribs': [{'key': 'wireless.authMode',
229:                                                'value': 'central'},
230:                                               {'key': 'wireless.fabric',
231:                                                'value': 'true'},
232:                                               {'key': 'wireless.wlanProfileName',
233:                                                'value': 'Adv_wlan_configs_1_profile'},
234:                                               {'key': 'wireless.trafficSwitchingMode',
235:                                                'value': 'fabric'},
236:                                               {'key': 'wireless.isGuestAnchor',
237:                                                'value': 'false'},
238:                                               {'key': 'wireless.flexConnect',
239:                                                'value': 'false'},
240:                                               {'key': 'wireless.policyProfileName',
241:                                                'value': 'Adv_wlan_configs_1_profile'}],
242:                                   'key': 'wireless.ssid',
243:                                   'value': 'Adv_wlan_configs_1'},
244:                                  {'attribs': [{'attribs': [{'key': 'NamedCapability.tag',
245:                                                             'value': ''},
246:                                                            {'key': 'NamedCapability.description',
247:                                                             'value': 'Rogue '
248:                                                                      'General '
249:                                                                      'config'},
250:                                                            {'key': 'NamedCapability.name',{noformat}

 ",2023-07-13T07:09:35.284+0000,"Hulk P1  (2370-70446) 
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=3919&size=1109165&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_05:28:59.067532.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=3919&size=1109165&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_05:28:59.067532.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] During optimized code runs: The advanced TC is in UC13 - until then on DNAC we have around 17 SSIDs created. Hence the creation of new SSID failed

Cluster Sanity TB3 is available with this issue- which is with Hulk RC1 70446


!image-20230718-025347.png|width=536,height=590! SSID more than 16 limit is not handled in optimized code yet. Will go through them.  The same, issue observed  in  Ghost P2  RC2  run  

Failed Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-1310-advancedWlanConfigs&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug15_01:21:29.931354.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-1310-advancedWlanConfigs&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug15_01:21:29.931354.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] During optimized code run on HULK P1 70125 
The test Test_TC1_advanced_wlan_configs /test2_add_new_ssids has failed while creating SSIDs {{""message"":""NCND00002: The Network Profile request has validation errors"",""detail"":""[\""NCND01175: This Profile cannot be assigned to more than 16 SSIDs\""]""}}}

This test is part of UC 13.8: advancedWlanConfigs 

Log: 
[https://ngdevx.cisco.com/services/taas/results/8601967a-ae5a-4619-9219-e19f62c97fd0/run-results|https://ngdevx.cisco.com/services/taas/results/8601967a-ae5a-4619-9219-e19f62c97fd0/run-results] PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6863/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6863/overview]","['Auton', 'Ghost', 'Hulk', 'Issue', 'Optimized', 'Sanity', 'sanity']",Moe Saeed,Resolved,Omkar Sharad Wagh
SEEN-1918,https://miggbo.atlassian.net/browse/SEEN-1918,"[AUTON][NFW]Facing SSID related issues and Network profile,AP profile data specific to NFW missing in json files","
*Uber ISO Version tested :* Hulk-2.3.7.0-70414

*Script Name :* solution_dnac_wireless_hardening.py

*Testbed :* NFW(Base automation)
*Branch Used*:Private/Hulk-ms/api-auto-nfw
*Input file used:*solution_test_input.json

*Testcases Impacted :*Test_TC20_DNAC_addition_of_wireless_nw_profiles,Test_TC22_DNAC_verify_addition_of_wireless_profiles_to_site_bld_floor,Test_TC23_DNAC_verify_creating_wireless_guest_portal,Test_TC123_aaa_per_ssid,Test_TC191_edit_site_name

Failed Log:
1. TC20,22,23 log:[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_nfw_job.2023Jul10_22:28:39.756421.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_nfw_job.2023Jul10_22:28:39.756421.zip&atstype=ATS]
2.TC123 log:[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14750777&size=1849167&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_nfw_job.2023Jul11_04:07:35.833645.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14750777&size=1849167&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_nfw_job.2023Jul11_04:07:35.833645.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
 Hi Phan/Raji,
While testing Hulk-2.3.7.0-70414 on NFW(Base Automation) test bed,we see many use cases getting failed due to SSID related issues and also observed that defaults network profiles are getting added instead of nfw-np-00,nfw-np-01 and nfw-np-02.Also observed that ap profile data specific to NFW test bed is missing from json files.
Since sr_nfw folder not present in main Hulk branch-”Private/Hulk-ms/api-auto-nfw”,we have synced sr_nfw from Ghost branch and started execution.Please check on it and let us know whether we missing anything here?

Thanks,
Neelima",2023-07-13T15:53:20.420+0000,"Hi [~accountid:63f50bddc1685a24e1314c87] ,

Starting from the Hulk version, we no longer use 'solution_test_input.json' for input file testing. Instead, we only utilize 'nfw_solution_test_input.json' as the input data Hi [~accountid:63f50bcdce6f37e5ed93c87d] ,
Yes we got to know that later and now we are using “nfw_solution_test_input.json” for Hulk.This is not informed to us  before hand and faced issues while executing.I will close this ticket.Thanks.","['Auton', 'Execution', 'Hulk', 'NFW', 'Regression']",DatChi Pham,Closed,Neelima Doddipalli
SEEN-1919,https://miggbo.atlassian.net/browse/SEEN-1919, Optimized  [Auton]:[Hulk]:Task-assurance_maintaince_mode_on_off.py-163-assuranceMaintanenceMode  /Task-ap_reachability_issue.py-162-FEWapReachabilityIssue,"*Reporter Analysis:*
recent Sanity Optimized *hulk run (2370-70402)* i Observed  freshly deployed  on cluster  
*After maintenance UC  execution we observed   New  York sites all AP’s  Went unreachable state,*
*Multiple Tcs failed because of the above issue.*
""After disabling the maintenance mode, the AP and devices should become reachable.""   

could you please check and confirm this,

+*Hulk_70424*+
*Pass_log: UC  [*[*Task-ap_reachability_issue.py-162-FEWapReachabilityIssue*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_reachability_issue.py-162-FEWapReachabilityIssue&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul12_11:15:53.341205.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]*]*
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul12_11:15:53.341205.zip&atstype=ATS#:~:text=verify_anchorvn_withIXIA-,Task-ap_reachability_issue.py-162-FEWapReachabilityIssue,-00%3A46%3A42|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul12_11:15:53.341205.zip&atstype=ATS#:~:text=verify_anchorvn_withIXIA-,Task%2Dap_reachability_issue.py%2D162%2DFEWapReachabilityIssue,-00%3A46%3A42]


*UC [*[*Task-assurance_maintaince_mode_on_off.py-163-assuranceMaintanenceMode*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_maintaince_mode_on_off.py-163-assuranceMaintanenceMode&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul12_11:15:53.341205.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]*]*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_maintaince_mode_on_off.py-163-assuranceMaintanenceMode&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul12_11:15:53.341205.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_maintaince_mode_on_off.py-163-assuranceMaintanenceMode&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul12_11:15:53.341205.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*Source Team:  Sanity(Optimized)*

+*Yaml Snip :*+    


{code:actionscript}  ],
    ""16"": [
        {
        ""parallelrun"": [""SDAFabricAnchorvn"",
                ""FEWapReachabilityIssue"",""assuranceMaintanenceMode""],
        ""blocker_uc"": []
        },
        {
        ""parallelrun"": [""FEWAccessPointOeapMode"", ""BAPI""],
        ""blocker_uc"": []
        },{code}

  

!image-20230713-181359.png|width=1230,height=585!



!image-20230713-182423.png|width=1690,height=722!


+*Testbed wiki  :*+

[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7]",2023-07-13T17:50:49.929+0000,"It could be defect. Please check with DE and raise defect for it. Defect has been filed  for this issue 
[https://cdetsng.cisco.com/webui/#view=CSCwf95037|https://cdetsng.cisco.com/webui/#view=CSCwf95037]","['Auton', 'Blocker', 'Hulk', 'Optimized', 'Sanity']",Omkar Sharad Wagh,Closed,Omkar Sharad Wagh
SEEN-1920,https://miggbo.atlassian.net/browse/SEEN-1920,"Test_TC109_DNAC_maps:: test3_import_Ekahau_file[ekahau_type=ekahau_with_lat_long,expect_failure=False]","*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*Branch*: private/Hulk-ms/api-auto

*Script Name :* solution_test_sanity_ecamb.py

*Testbed :* TB4

*Testcases Impacted :*  [Test_TC109_DNAC_maps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=116113989&size=47171&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May08_22:30:57.365391.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

During Hulk ESXI testing : ""[Test_TC109_DNAC_maps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=116113989&size=47171&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May08_22:30:57.365391.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]"" found “Error importing Ekahau file. 409 Client Error”

Please find the Logs attached

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=116148916&size=12094&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May08_22:30:57.365391.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=116148916&size=12094&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May08_22:30:57.365391.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[Error SNIP …]

406156: Step 3: Start Ekahu file import

406157: *************************************************
406158: ------------Import Ekahau file ------------------

406159: *************************************************

406160:

406161:

406162: api_switch_call called:

406163: {'response_dict': False}

406164: Resource path full url: [https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async|https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async]

406165: Error Code: 409 for

406166: URL:https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async Data:{'timeout': 60} Headers:{'Content-Type': 'application/json', 'X-Auth-Token': 'eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJDRE5BIiwiYXV0aFNvdXJjZSI6ImxlZ2FjeSIsImNsaWVudElkIjoiYWRtaW4iLCJlbWFpbCI6ImFkbWluQGxvY2FsdXNlci5jb20iLCJleHAiOjE2ODM2NjEwMzEsImZpcnN0TmFtZSI6ImFkbWluIiwiaWF0IjoxNjgzNjU3NDMxLCJpc3MiOiJkbmFjIiwicm9sZXMiOlsiU1VQRVItQURNSU4iXSwic2Vzc2lvbklkIjoiMzNiZWNlOTAtOTY3MS01M2FkLThjMTItOWQ1ODZjMjQ1NDYyIiwic3ViIjoiYWRtaW4iLCJ0ZW5hbnRJZCI6IjY0NTM3ZDc4ZmQzZjVkMDAxMzc1MjI0MCIsInRlbmFudE5hbWUiOiJUTlQwIiwidXNlcm5hbWUiOiJhZG1pbiJ9.cQOMVvTuWVbrq9qL8R2P2okwIcB04bjMEIi_JAc3JOO4JL0Gbm9WHYgL1Eq8twfWEESCBk7Dl-0pJwGA2CMRDw'} Message:{""response"":{""errorCode"":""Conflict"",""message"":""Error: Context is in invalid state: VALIDATING"",""detail"":""maps.import-export.context.state.invalid:[VALIDATING]"",""href"":""""},""version"":""1.0""}

406167: Traceback (most recent call last):

406168: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/VM-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 326, in call_api

406169: response.raise_for_status()

406170: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status

406171: raise HTTPError(http_error_msg, response=self)

406172: requests.exceptions.HTTPError: 409 Client Error: Conflict for url: [https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async|https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async]

406173: Encountered unhandled HTTPError in Internal API Call

406174: Flagging result as FAIL!

406175: Reason: 409 Client Error: Conflict for url: [https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async|https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async]

406176: Kwargs:

406177: {'response_dict': False}

406178: Error Caught While Querying the Internal API

406179: !!!!!!!! Error importing Ekahau file. 409 Client Error: Conflict for url: [https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async|https://10.22.45.61/api/v1/dna-maps-service/archives/import/f7b42630-3bdf-4e1d-ae44-1fca5bfabcb7/start-async] !!!!!!!!

406180: Test returned in 0:00:00.203841

406181: Failed reason: Failed to import Ekahau file",2023-07-13T20:24:09.895+0000,"Raised PR for the required change
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6274/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6274/overview] Raised PR for code review
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6274/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6274/overview] PR Approved and merged to Hulk-ms/api-auto","['Auton', 'ESxi', 'Hulk']",KRISHNA MUKKU,Resolved,KRISHNA MUKKU
SEEN-1921,https://miggbo.atlassian.net/browse/SEEN-1921,verify_PEN_interface_config() has a typo,*verify_PEN_interface_config()* has a typo - causing Script error for [Test_TC101_DNAC_Policy_Extended_node|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=45201969&size=431524&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun28_06:31:28.059646.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS].,2023-07-13T20:26:10.357+0000,"Raised PR for required change: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6273/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6273/overview] Another PR was required to avoid the retry loop:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6275/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6275/overview] PRs have been merged to Hulk branch.",['Auton'],Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1922,https://miggbo.atlassian.net/browse/SEEN-1922,"Ghost P2 - Creation of SSID via BAPI payload need to be changed for radiopolicy with ""2.4GHz Only"", ""5GHz Only"", ""6GHz Only"".","Creation of enterprise SSID via BAPI is failing due to payload validation is added in Hulk.

Branch : private/Ghost-ms/sanity_api_auto

Failed Logs: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-base_wireless_ssid_profile_guest.py-53-designWireless&begin=106342&size=78615&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May24_11:15:49.676522.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-base_wireless_ssid_profile_guest.py-53-designWireless&begin=106342&size=78615&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_optimized_auto_job.2023May24_11:15:49.676522.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Working Payload:

{
""name"": ""testtb13"",
""passphrase"": """",
""enableFastLane"": ""True"",
""enableMACFiltering"": ""False"",
""trafficType"": ""voicedata"",
""radioPolicy"": ""2.4GHz Only"",
""fastTransition"": ""Disable"",
""securityLevel"": ""OPEN"",
""enableSessionTimeOut"": ""True"",
""sessionTimeOut"": 1800,
""enableClientExclusion"": ""True"",
""enableBasicServiceSetMaxIdle"": ""True"",
""basicServiceSetClientIdleTimeout"": 300,
""enableDirectedMulticastService"": ""True"",
""enableNeighborList"": ""True"",
""mfpClientProtection"": ""Optional""
}",2023-07-14T06:32:17.575+0000,"I’ve raised PR to fix this issue: [[PR Link|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6281/overview]]

Solution is: remove radio_policy key in payload and let them work with 2.4/5/6Ghz as default.

Please share your idea if there is new/update behavior on this. [~accountid:62d2fef2bd54f8d3ffb7d1f7] 

Regards","['Auton', 'GhostP2']",ThanhTan Nguyen,Resolved,SANTHOSH MOUNASWAMY
SEEN-1923,https://miggbo.atlassian.net/browse/SEEN-1923,Optimized  [Auton]:[Hulk]:Task-Test_TC1_Monitor_And_Debug_BGP_Session_Status_Issues_Between_Border_And_Control_Plane  /   test3_verify_BGP_session_status_issues_between_border_and_local_control_plane,"Hi [~accountid:63f50bcafb3ac4003fa2c6dd],


recent Sanity Optimized *hulk run (2370-70402)  with  SDA 1.0* i Observed  freshly deployed  on cluster  
{{Data of BGP session status/issues between border and control plane are incorrect.}}
issue is  not generated  ,

API

""After disabling the maintenance mode, the AP and devices should become reachable.""   

{code:python}irst: [{'fabricSiteName': 'New York', 'fabricSiteId': '8224ee19-0e8b-47d7-a37d-21515e6fe7cb', 'totalCount': 0, 'goodCount': 0, 'noHealthCount': 0}, {'fabricSiteName': 'SAN JOSE', 'fabricSiteId': '3d1c1252-9b09-4a77-b9b4-c94681fab577', 'totalCount': 0, 'goodCount': 0, 'noHealthCount': 0}]
264:  Second: [{'fabricSiteName': 'New York', 'fabricSiteId': '8224ee19-0e8b-47d7-a37d-21515e6fe7cb', 'totalCount': '1', 'goodCount': '0', 'noHealthCount': '1'}, {'fabricSiteName': 'SAN JOSE', 'fabricSiteId': '3d1c1252-9b09-4a77-b9b4-c94681fab577', 'totalCount': '1', 'goodCount': '0', 'noHealthCount': '1'}, {'fabricSiteName': 'Bangalore', 'fabricSiteId': '88970772-5b3c-46e5-b6d1-951d8f02b914', 'totalCount': '0', 'goodCount': '0', 'noHealthCount': '0'}, {'fabricSiteName': 'BayAreaGuest', 'fabricSiteId': '4c48eddc-6c0f-4954-8820-22f300b103fa', 'totalCount': '0', 'goodCount': '0', 'noHealthCount': '0'}, {'fabricSiteName': 'SAN-FRANCISCO', 'fabricSiteId': '60ee0fe3-fc95-4466-ade5-ae59b33ecd90', 'totalCount': '0', 'goodCount': '0', 'noHealthCount': '0'}]
265:  Library group ""assurance"" method ""verify_between_two_list_dictionary"" returned in 0:00:00.000286{code}



Fail log :  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-monitor_bgp_between_border_and_control_plane.py-131-monitorBGPBetweenBorderAndControlPlane&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul12_07:56:18.543443.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-monitor_bgp_between_border_and_control_plane.py-131-monitorBGPBetweenBorderAndControlPlane&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul12_07:56:18.543443.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS][sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1667599&size=116305&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fauto_MS_job.2023May26_07:44:50.820614.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

+*Branch :*+ private/Hulk-ms/san_auto
+*Script*+ : [lansanity_usecases_maps.yaml|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/lansanity/lansanity_usecases_maps.yaml?at=private%2FHulk-ms%2Fsanity_api_auto]



+*Testbed wiki  :*+

[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7]",2023-07-14T14:54:49.553+0000,"Hulk Pacth1 (2.1.710.70446) with  SDA 2. 0  +ISE 3.3 cco +Devcies image  17.12. FCV 

Failed log  : 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-monitor_bgp_between_border_and_control_plane.py-1311-monitorBGPBetweenBorderAndControlPlane&begin=38925&size=4513&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_05:28:59.067532.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-monitor_bgp_between_border_and_control_plane.py-1311-monitorBGPBetweenBorderAndControlPlane&begin=38925&size=4513&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_05:28:59.067532.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
 PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6366/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6366/overview]

Testbed: {{configs/sanity_tb7/SanityTB7.yaml}}

Script path: {{testcases/forty_eight_hour/solution_test_sanityecamb.py}}

Trade log link: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-07/sanity_TB7.2023Jul24_00:57:26.877240.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-07/sanity_TB7.2023Jul24_00:57:26.877240.zip&atstype=ATS]

Branch: private/Hulk-ms/api-auto Hi [~accountid:620b8357878c2f00729881c8], Could you please help me review this PR Auton? [~accountid:63f50bcafb3ac4003fa2c6dd] , I have approved and merged the PR to Hulk Branch.

[~accountid:620b8357878c2f00729881c8] , pls. pick the latest and validate the same.","['Auton', 'Hulk', 'Optimized', 'Sanity']",NhanHuu Nguyen,Resolved,Omkar Sharad Wagh
SEEN-1924,https://miggbo.atlassian.net/browse/SEEN-1924,Test_TC113_enable_application_telmetry - Fails,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*Branch*: private/Hulk-ms/api-auto

*Script Name :* solution_test_sanity_ecamb.py

*Testbed :* TB4

*Testcases Impacted :*  [test1_enable_application_telmetry|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=51965869&size=16793&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul13_10:43:47.780811.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

During Hulk ESXI testing : ""[test1_enable_application_telmetry|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=51965869&size=16793&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul13_10:43:47.780811.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]"" enable telmetry is disabled by default

{{response1 = self.services.api_switch_call(    method=""POST"",    resource_path=""/v1/deploysettings/ENABLE_TELEMETRY"",    data=[device_id])}}

{color:#ff991f}*===> response1 PASS as there is no validation*{color}

*But the next step*

{{time.sleep(60)}}

{{params = {    ""deviceId"": device_id,    ""sortBy"": ""lastUpdateTime"",    ""order"": ""des""}}}

{{work_response = self.services.api_switch_call(    method=""GET"",    resource_path=""/v2/data/device-config-status/workflow"",    params=params)}}

{{if not work_response['response']:    }}

{{    self.log.error(""Received empty response against Workflow API Call."")    }}

{{    return False}}

{color:#ff991f}*===> work_response FAILS as there was no response for the deviceId (We would not see any response because there is no response happened on the device)*{color}

*Please find the Fail Logs attached*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=51965869&size=16793&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul13_10:43:47.780811.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=51965869&size=16793&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul13_10:43:47.780811.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



{color:#ff991f}*My suggestion is validate using* {color}[{color:#ff991f}*https://10.22.45.61/api/v2/data/device-config-status*{color}|https://10.22.45.61/api/v2/data/device-config-status]{color:#ff991f} *and check for “Device Controllability and Telemetry” is present in response or not.*{color}

*[SAMPLE RESPONSE …]*


{'params': {'isLatest': True, 'deviceId': 'f43e7178-8de0-4f5a-b0b7-88f11906ccee'}}

Resource path full url: [https://10.22.45.61/api/v2/data/device-config-status|https://10.22.45.61/api/v2/data/device-config-status]

{'response': [{'id': '0cb04bad-93a2-499b-ab8a-f4141d5cb87e', 'instanceId': 699941, 'instanceVersion': 0, 'activityid': 'd98bb1bd-fa26-469b-a750-6fbfd3f1db64', 'cfsVersion': 6, 'createTime': 1689286772062, 'deviceId': 'f43e7178-8de0-4f5a-b0b7-88f11906ccee', 'endTime': 1689286772076, 'family': 'Switches and Hubs', 'groupId': '8c47d55d-928e-4144-93f9-7891c4535846', 'groupNameHierarchy': 'Global/USA/SAN JOSE/BLD23', 'hostname': '[TB4-DM-eCA-BORDER.cisco.com|http://TB4-DM-eCA-BORDER.cisco.com]', 'isLatest': True, 'isMonitoring': False, 'isTempFailure': False, 'lastSuccessfulVersion': 0, 'lastUpdateTime': 1689286772062, 'latestEquivalentVersion': 0, 'managementIpAddress': '204.1.2.1', 'namespace': 'dtls.cipher', 'outOfNamespace': False, 'provisionedCapabilities': [], 'provisioningData': {'featureName': 'Device Controllability and Telemetry'},",2023-07-14T19:20:33.318+0000,"PR Link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6501/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6501/overview] [~accountid:63f50bf9e8216251ae4d59d4], once you are done with Hulk RC3, pls. validate this PR.

After that, we can decide if the PR has right fix. What I see from your description, provided fix is very simple. Hi Quang,

The pass log which you have shared, I might have enabled telemetry on force from device side, can you please try your code on existing Hulk-RC4.

Fail Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=818092&size=19514&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug15_10:19:32.604696.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=818092&size=19514&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug15_10:19:32.604696.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Thanks,

Krishna Mukku  i exec code and got passlog: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-08/sanity_TB4.2023Aug16_00:27:37.687000.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-08/sanity_TB4.2023Aug16_00:27:37.687000.zip&atstype=ATS]

please make sure you try in the correct branch: private/quangvin-hulk/SEEN-1924","['Auton', 'ESxi', 'Hulk']",QuangVinh Nguyen,Resolved,KRISHNA MUKKU
SEEN-1925,https://miggbo.atlassian.net/browse/SEEN-1925,Test_TC118_vnid_override - test4_check_all_the_device_status,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*EXSI OVA build:* assembly_release_dnac_hulk-intg_converged_07-3.710.75408.ova
*Branch*: private/Hulk-ms/api-auto
*Script Name :* solution_test_sanity_ecamb.py
*Testbed :* TB4
*Testcases Impacted :*  “Test_TC118_vnid_override - test4_check_all_the_device_status”

During Hulk ESXI testing : ""Test_TC118_vnid_override - test4_check_all_the_device_status"" test failed due to {color:#bf2600}NCSP11051: Error occurred while processing the 'modify' request. A different version of the same user intent already exists in the database. Additional info for support: taskId: '10856939-55b5-4f01-8001-f2750ee38762'. Name: 'TB4-DM-WLC'. Incoming resourceVersion: '88'. resourceVersion in the database: '90'.""{color}

Fail Log: 


[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=825090&size=281911&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul15_09:39:41.967475.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=825090&size=281911&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul15_09:39:41.967475.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-07-15T20:17:39.209+0000,"The error was resulted of the previous operation. Please share the previous run long for the other tests that run before this test? ({{test1_override_vnid_BLD23}}, {{test2_override_vnid_BLDNYC}}…..)



----

{{------------------}}

{{'error': {'id': 'a362c8ec-e589-4ea3-ae4b-049cc985d38e', 'data': {}, 'key': 'NCSP11051', 'message': ""NCSP11051: Error occurred while processing the 'modify' request. A different version of the same user intent already exists in the database. Additional info for support: taskId: '10856939-55b5-4f01-8001-f2750ee38762'. Name: 'TB4-DM-WLC'. Incoming resourceVersion: '88'. resourceVersion in the database: '90'.""}}} Can you find those task id in any of previous run log? please share the logs.
{{10856939-55b5-4f01-8001-f2750ee38762}}

 There is no previous task_id running, if we run this single sub-testcase also it is failing with same failure error Is this error still being seen? Please re-open with latest info. Going to close for now since seems like hasnt been seen since July but feel free to open if it comes again.","['Auton', 'ESxi', 'Hulk']",Andrew Chen,Cancelled,KRISHNA MUKKU
SEEN-1926,https://miggbo.atlassian.net/browse/SEEN-1926,TC126_verify_inventory_insights - test1_verify_speed_duplex_mismatch,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*Branch*: private/Hulk-ms/api-auto

*Script Name :* solution_test_sanity_ecamb.py

*Testbed :* TB4

*Testcases Impacted :*  Test_TC126_verify_inventory_insights - test1_verify_speed_duplex_mismatch

During Hulk ESXI testing “Test_TC126_verify_inventory_insights - test1_verify_speed_duplex_mismatch”  fails due to unable to capture the insight issues.

Manually verified and captured the steps.

*Fail Log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=61759552&size=662846&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul13_10:43:47.780811.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=61759552&size=662846&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul13_10:43:47.780811.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-07-16T08:47:19.391+0000,"!image-20230716-085157.png|width=1532,height=212! Raised PRs:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6938/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6938/overview]
* HulkPatch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6939/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6939/overview]","['Auton', 'ESxi', 'Hulk']",ThangQuoc Tran,Resolved,KRISHNA MUKKU
SEEN-1927,https://miggbo.atlassian.net/browse/SEEN-1927,TC126_verify_inventory_insights - test2_verify_VLAN_mismatch,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*Branch*: private/Hulk-ms/api-auto

*Script Name :* solution_test_sanity_ecamb.py

*Testbed :* TB4

*Testcases Impacted :*  Test_TC126_verify_inventory_insights - test1_verify_speed_duplex_mismatch

During Hulk ESXI testing “Test_TC126_verify_inventory_insights - test1_verify_speed_duplex_mismatch”  fails due to unable to capture the insight issues.

Manually verified and captured the steps.

*Fail Log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=61759552&size=662846&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul13_10:43:47.780811.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=61759552&size=662846&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul13_10:43:47.780811.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-07-16T09:29:30.827+0000,"!image-20230716-093059.png|width=1534,height=254! Raised PRs:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6938/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6938/overview]
* HulkPatch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6939/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6939/overview]","['Auton', 'ESxi', 'Hulk', 'Sanity']",ThangQuoc Tran,Resolved,KRISHNA MUKKU
SEEN-1928,https://miggbo.atlassian.net/browse/SEEN-1928,TC129_Disconnect_Delete_Reason - test2_verify_Disconnect_Delete_Reason,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*Branch*: private/Hulk-ms/api-auto

*Script Name :* solution_test_sanity_ecamb.py

*Testbed :* TB4

*Testcases Impacted :*  TC129_Disconnect_Delete_Reason - test2_verify_Disconnect_Delete_Reason

During Hulk ESXI testing “TC129_Disconnect_Delete_Reason - test2_verify_Disconnect_Delete_Reason”  fails due to unable to join SSID - ‘SSIDDot1XIndiatb4’

*Fail Log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=63727275&size=19087&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul13_10:43:47.780811.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=63727275&size=19087&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul13_10:43:47.780811.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-07-16T20:05:41.376+0000,"This test required clients with rf box and with role ‘{{'wireless-client,static'}} in yaml.
Seem like your testbed don’t have any. Please add them.","['Auton', 'ESxi', 'Hulk']",KRISHNA MUKKU,Resolved,KRISHNA MUKKU
SEEN-1929,https://miggbo.atlassian.net/browse/SEEN-1929,TC136_enable_ICMP_ping_check_AP_reachability - test2_deploy_AP_specific_configs_to_controller,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*Branch*: private/Hulk-ms/api-auto

*Script Name :* solution_test_sanity_ecamb.py

*Testbed :* TB4

*Testcases Impacted :*  TC136_enable_ICMP_ping_check_AP_reachability  - test2_deploy_AP_specific_configs_to_controller

During Hulk ESXI testing “TC136_enable_ICMP_ping_check_AP_reachability  - test2_deploy_AP_specific_configs_to_controller”  fails due to unable to join SSID - ‘SSIDDot1XIndiatb4’

*Fail Log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2178758&size=1573226&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul14_00:29:07.945971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2178758&size=1573226&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul14_00:29:07.945971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=863796&size=60908&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun15_23:59:12.196482.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=863796&size=60908&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun15_23:59:12.196482.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-07-16T20:28:36.912+0000,"Did it work before? Please share passed log whenever you raised a defect that worked before.
From log, seem like bug, please check with DE and raise defect.
10801: 
 The Schedduled Job failed: with reason {'id': '78400567-bca0-4961-a7ff-21d83d0438ee', 'triggeredJobTaskId': '5d8243e8-9d13-41bf-90be-dae882a431bb', 'triggeredTime': 1689321274612, 'status': 'FAILED', 'failureReason': 'Unable to push to device 204.1.2.1 using protocol ssh2 the CLI do ap name AP502f.a857.c9d4 dot11 24ghz slot 0 radio role auto', 'triggeredJobId': '78400567-bca0-4961-a7ff-21d83d0438ee'}

{noformat}10802: {noformat} [test1_enable_icmp_verify_ap_reachability|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10696616&size=1396&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul31_22:37:41.610225.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] requires AireOS, need to implement for 9800 WLC.

|Test_TC136_enable_ICMP_ping_check_AP_reachability| |Failed|
|[test1_enable_icmp_verify_ap_reachability|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10696616&size=1396&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul31_22:37:41.610225.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]| |Skipped|
|[test2_deploy_AP_specific_configs_to_controller|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10698012&size=859024&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul31_22:37:41.610225.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]| |Failed| Hi [~accountid:63f50bf9e8216251ae4d59d4], The pass log in 'Description' is from another use case. Could you check it again? The root cause:

The if statement {{if response['primaryControllerName']!=None:}} is wrong due to {{bool('' != None)}} still True. In case the value is an empty string, it will run {{execute_command_on_device}} and get the failed.

!image-20230920-034843.png|width=1013,height=469! # PR Hulk-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7079/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7079/overview]
# Test Case:  TC_enable_ICMP_ping_check_AP_reachability/test2_deploy_AP_specific_configs_to_controller.
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Testbed: {{SanityTB1}}
# Trade log link Hulk-ms/api_auto:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-09/sanity_TB1.2023Sep19_02:44:21.268564.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-09/sanity_TB1.2023Sep19_02:44:21.268564.zip&atstype=ATS] [~accountid:63f50bf9e8216251ae4d59d4]: Could you cherry-pick this PR and check again if the TC run passes or fails? [~accountid:63f50bf9e8216251ae4d59d4]; This PR has merged into branch: *private/Hulk-ms/api-auto*","['Auton', 'ESxi', 'Hulk']",NhanHuu Nguyen,Resolved,KRISHNA MUKKU
SEEN-1930,https://miggbo.atlassian.net/browse/SEEN-1930,TC139_Enhance_RCA_AAA_Issue - test2_Enhance_RCA_AAA_Issue,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*Branch*: private/Hulk-ms/api-auto

*Script Name :* solution_test_sanity_ecamb.py

*Testbed :* TB4

*Testcases Impacted :*  TC139_Enhance_RCA_AAA_Issue - test2_Enhance_RCA_AAA_Issue

During Hulk ESXI testing “TC139_Enhance_RCA_AAA_Issue - test2_Enhance_RCA_AAA_Issue”  fails due to unable to see wireless-client but we have wireless-clients in assurance page.

*Fail Log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4224194&size=7917&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul14_00:29:07.945971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4224194&size=7917&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul14_00:29:07.945971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=889274&size=20437&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun29_06:20:15.871824.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=889274&size=20437&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun29_06:20:15.871824.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Attached screen shots of RCA",2023-07-16T21:13:22.811+0000,"!image-20230716-211947.png|width=1536,height=340! !image-20230716-212507.png|width=1536,height=336! Was it passed before? Can you share passed log? This test required wireless clients with rf box and role ‘{{wireless-client,static}}' in yaml.
Seem like you don’t have those. Please add those and re-run the usecase. I have wireless clients with role ‘wireless-client’

Please find the Pass logs
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=889274&size=20437&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun29_06:20:15.871824.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=889274&size=20437&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun29_06:20:15.871824.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] As mentioned below, This test required wireless clients with rf box and role ‘{{wireless-client,static}}' in yaml. ('static' meaning in rf box with specific ap connection in yaml topology)
Seem like you don’t have those. Please add those and re-run the usecase.
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb4/SanityTB4.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#711|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb4/SanityTB4.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#711]","['Auton', 'ESxi', 'Hulk']",KRISHNA MUKKU,Resolved,KRISHNA MUKKU
SEEN-1931,https://miggbo.atlassian.net/browse/SEEN-1931,TC130_Verify_SNR_RSSI_Value - test1_connect_clients_to_ssids,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*Branch*: private/Hulk-ms/api-auto

*Script Name :* solution_test_sanity_ecamb.py

*Testbed :* TB4

*Testcases Impacted :*  TC130_Verify_SNR_RSSI_Value - test1_connect_clients_to_ssids

During Hulk ESXI testing “TC130_Verify_SNR_RSSI_Value - test1_connect_clients_to_ssids”  fails due to unable to see wireless-client but we have wireless-clients in assurance page.

*Fail Log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=63746938&size=2180&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul13_10:43:47.780811.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=63746938&size=2180&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul13_10:43:47.780811.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=35018968&size=49330&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_22:05:13.705332.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=35018968&size=49330&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_22:05:13.705332.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Attached screen shots of RCA",2023-07-16T22:19:15.840+0000,"!image-20230716-222732.png|width=1536,height=388!

!image-20230716-222812.png|width=1501,height=746! Was it passed before? Can you share passed log? This test required wireless clients with rf box and role ‘{{wireless-client,static}}' in yaml.
Seem like you don’t have those. Please add those and re-run the usecase. Attached pass log  As mentioned below, This test required wireless clients with rf box and role ‘{{wireless-client,static}}' in yaml.
Seem like you don’t have those. Please add those and re-run the usecase.
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb4/SanityTB4.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#711|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb4/SanityTB4.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#711] Done modifications to the yaml file and submitted test, please find the PR and Pass log

PR Request

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6418/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6418/overview]

Pass log:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal%2Fusers%2Fkmukku%2Farchive%2F23-07%2Fsanity_crft_tb4.2023Jul26_22:31:34.081220.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal%2Fusers%2Fkmukku%2Farchive%2F23-07%2Fsanity_crft_tb4.2023Jul26_22:31:34.081220.zip&atstype=ATS]","['Auton', 'ESxi', 'Hulk']",KRISHNA MUKKU,Resolved,KRISHNA MUKKU
SEEN-1932,https://miggbo.atlassian.net/browse/SEEN-1932,[HULK][AUTON]-TEST Test_TC0_dnac_initial_cleanup,"DNAC Release_Version Tested:Hulk ber ISO - 2.1.710.70192, Non-FIPS

Device Image Used: 17.12.1

Testbed: AWS-Multisite

Branch Used: private/Hulk-ms/before-api-changes

Script Name: solution_test_3sites_sjc_nyc_sf.py

Solution Input File : dnac-auto/configs/config_48hr_test/solution_test_input.json

Testcases Impacted:   [Test_TC0_dnac_initial_cleanup|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=615901&size=5205248&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul14_01:36:13.225674.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]



Failed Log:

[Test_TC0_dnac_initial_cleanup|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=615901&size=5205248&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul14_01:36:13.225674.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]



Devices cleaned up done and DNAC cleaned up manually, We executed the dnac initail cleanup script is failing in this build

Please look into this and update",2023-07-17T05:39:29.013+0000,"why do you use this branch ? “private/Hulk-ms/before-api-changes”
You should use the Hulk main branch “private/Hulk-ms/api-auto” Hi [~accountid:62d2fe9f8afb5805e5d5af49] , We are having the old hulk cluster 70192 so we used this branch You should use new Hulk version. We don’t support for this old branch anymore. ","['Auton', 'Hulk']",Balaji Raju,Resolved,Balaji Raju
SEEN-1933,https://miggbo.atlassian.net/browse/SEEN-1933,[Auton]:[Ghost]:Task-1 /Test_TC211_ap_profiles / test4_reprovision_wlc / test5_reprovision_aps/test7_verify_ssh_enable_and_country_code/  test8_confirm_ap_power_save_mode_enabled /  test11_confirm_ap_power_save_mode_disabled / ,"Recently we observed during  Ghost sanity executions , In non lan sanity optimized code where [ap_profiles|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1322636&size=693251&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul17_00:20:12.949047.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] tc failing 

*Reporter analysis:*

All provision of devices were success on dnac but script was throwing ” none type error.”
*Uber ISO Version tested:* Ghost P2 #2.1.614.70780
*Script Name:* Optimized code-nonlansanitysuite

*Branch used:* private/Ghost-ms/sanity_api_auto

*Script used:* solution_test_sanityecamb.py

*snip from failed log:*

{noformat} Traceback (most recent call last):{noformat}

{noformat}7958: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper{noformat}

{noformat}7959: 
     result = testfunc(func_self, **kwargs){noformat}

{noformat}7960: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 15042, in test4_reprovision_wlc{noformat}

{noformat}7961: 
     associated_wlc = wlc['associated_wlc']{noformat}

{noformat}7962: 
 TypeError: 'NoneType' object is not subscriptable{noformat}

{noformat}7964: 
 Errored reason: 'NoneType' object is not subscriptable{noformat}

 
Failed log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1322636&size=693251&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul17_00:20:12.949047.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1322636&size=693251&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul17_00:20:12.949047.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Earlier pass log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_profile.py-149-apProfile&begin=3539&size=2615884&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul05_02:55:34.420745.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_profile.py-149-apProfile&begin=3539&size=2615884&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul05_02:55:34.420745.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-07-17T09:23:58.171+0000,"[~accountid:63f50be71223974bc04b0534] , seeing the same issue in my current run Ghost p2 rc1 #2.1.614.70815
Failed Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_profile.py-149-apProfile&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug01_11:46:11.629102.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_profile.py-149-apProfile&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug01_11:46:11.629102.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] PR: [+https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6933/diff#testcases/forty_eight_hour/solution_test_sanityecamb_cert.py+|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6933/diff#testcases/forty_eight_hour/solution_test_sanityecamb_cert.py]","['Auton', 'Feature', 'Ghost', 'Integration', 'Optimized', 'Sanity']",Majlona 'Luna' Aliaj,Resolved,Elton GoldChristopher
SEEN-1934,https://miggbo.atlassian.net/browse/SEEN-1934,[Auton]:[Ghost]: Task-discovery_brownfield.py-51-toolsDiscoveryBrownfield / Test_TC2_DNAC_Verify_adding_range_discovery_ssh_global_credentials /  test1_verify_adding_range_discovery_ssh_global_credentials,"Recently we observed during Ghost sanity executions , In non lan sanity optimized code where [verify_adding_range_discovery_ssh_global_credentials|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery_brownfield.py-51-toolsDiscoveryBrownfield&begin=125780&size=9752846&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_04:32:50.083565.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] tc failing

*Reporter analysis:*

All devices discovery was success via lldp and ip address
On dnac also we are seeing even discovery is success but lldp of fiab device is stuck in -progresss forever.
Testcase is passing in rerun


*Uber ISO Version tested:* Ghost P2 #2.1.614.70794


*Script Name:* Optimized code-nonlansanitysuite

*Branch used:* private/Ghost-ms/sanity_api_auto and  private/Hulk-ms/sanity_api_auto

*Script used:* solution_test_sanityecamb.py

*snip from failed log:*

{{AssertionError: Task {'version': 1689603277135, 'progress': '79', 'data': '79', 'startTime': 1689603276965, 'serviceType': 'Discovery Service', 'rootId': '21bf77ad-0014-46f1-a845-b6df7d3c375f', 'lastUpdate': 1689603277135, 'isError': False, 'instanceTenantId': '64b508668d239b0c37c78ace', 'id': '21bf77ad-0014-46f1-a845-b6df7d3c375f'} didn't complete within 1800 seconds}}



*Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery_brownfield.py-51-toolsDiscoveryBrownfield&begin=125780&size=9752846&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_04:32:50.083565.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery_brownfield.py-51-toolsDiscoveryBrownfield&begin=125780&size=9752846&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_04:32:50.083565.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Re-run  Passlog:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=382948&size=139844&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul17_08:57:47.005085.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=382948&size=139844&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul17_08:57:47.005085.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-07-17T15:30:51.687+0000,"[~accountid:63f50bf5e8216251ae4d59cf] 
seeing same issue in the hulk run RC2 #2.1.710.70462
Failed Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery_brownfield.py-51-toolsDiscoveryBrownfield&begin=126024&size=9751230&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul24_06:10:11.847849.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery_brownfield.py-51-toolsDiscoveryBrownfield&begin=126024&size=9751230&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul24_06:10:11.847849.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Issue observed on HULK P1 runs as well
*Major issue here is its run time is 02:57 hours*
please prioritize this and try to fix this ASAP

LOG: [Test_TC2_DNAC_Verify_adding_range_discovery_ssh_global_credentials|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery_brownfield.py-51-toolsDiscoveryBrownfield&begin=124701&size=7686054&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug08_05:49:05.160552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  [~accountid:5e1415780242870e996f0b2f] / [~accountid:63a2a522082abdd71bb36e09]  Can you please check with dev team for changes from DNAC side, this looks more like a product issue as DNAC is taking longer time to complete the discovery process, please raise auton once confirmed by DE. Thank you [~accountid:63f50bf5e8216251ae4d59cf]  for the fix

Addressed the issue in this commit : [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/610cabaeb46e24eb83d7a4d273020ef79140e6e0|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/610cabaeb46e24eb83d7a4d273020ef79140e6e0]

Cherry picked to all Hulk Patch branches 
Will try to validate the fix in upcoming runs","['Auton', 'Blocked', 'Ghost', 'Halleck', 'Hulk', 'Optimized', 'Sanity']",Raji Mukkamala,Resolved,Elton GoldChristopher
SEEN-1935,https://miggbo.atlassian.net/browse/SEEN-1935,[Auton][MSTB2] : Enabling Kairos CS is failing with 404 client error,"Ova#3.710.75456
IOS: 17.12.1
Script : solution_test_3sites_sjc_nyc_sf.py

Branch : private/Hulk-ms/api-auto

Description : 

While enabling Endpoint analytics through script we are seeing 404 client error. Manually when we tried enabling it it works fine.

Error:

========

 api_switch_call called:
69396:  {}
69397:  Resource path full url: [https://10.195.243.37/api/system/v1/maglev/nodes/config|https://10.195.243.37/api/system/v1/maglev/nodes/config]
69398:  Error Code: 404 for
69399:  URL:[https://10.195.243.37/api/system/v1/maglev/nodes/config|https://10.195.243.37/api/system/v1/maglev/nodes/config] Data:{'timeout': 60} Headers:{'Content-Type': 'application/json', 'X-Auth-Token': 'eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJDRE5BIiwiYXV0aFNvdXJjZSI6ImxlZ2FjeSIsImNsaWVudElkIjoiYWRtaW4iLCJlbWFpbCI6ImFkbWluQGxvY2FsdXNlci5jb20iLCJleHAiOjE2ODkzMjUzNzksImZpcnN0TmFtZSI6ImFkbWluIiwiaWF0IjoxNjg5MzIxNzc5LCJpc3MiOiJkbmFjIiwicm9sZXMiOlsiU1VQRVItQURNSU4iXSwic2Vzc2lvbklkIjoiZmI1YmM2YmItMGUwOS01NzJmLWFmNDEtZGI1ZDFlMjIzMjMwIiwic3ViIjoiYWRtaW4iLCJ0ZW5hbnRJZCI6IjY0YTlhZjAzMDFmNGFjMDAxMzRhY2Q5MyIsInRlbmFudE5hbWUiOiJUTlQwIiwidXNlcm5hbWUiOiJhZG1pbiJ9.F11QwHoWGe5oD3iZxKbhWIJyhlPS7XbKIhkPchvvynbcqZ0_xRQQdJP9dAEC81aDssHTQ09cdNWZ9it0Lqycqg'} Message:404: Page Not Found
69400:  Traceback (most recent call last):
69401:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/dnaserv/client_manager.py"", line 326, in call_api
69402:      response.raise_for_status()
69403:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status
69404:      raise HTTPError(http_error_msg, response=self)
69405:  requests.exceptions.HTTPError: 404 Client Error: Not Found for url: [https://10.195.243.37/api/system/v1/maglev/nodes/config|https://10.195.243.37/api/system/v1/maglev/nodes/config]
69406:  Encountered unhandled HTTPError in Internal API Call
69407:  Flagging result as FAIL!
69408:  	Reason: 404 Client Error: Not Found for url: [https://10.195.243.37/api/system/v1/maglev/nodes/config|https://10.195.243.37/api/system/v1/maglev/nodes/config]
69409:  Kwargs:

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16163332&size=44182&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul13_23:02:37.104064.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16163332&size=44182&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul13_23:02:37.104064.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-07-17T17:48:57.020+0000,"Hi 

We are seeing below integration testcase is failing due to EA not getting enabled by script

[Test_TC208_vlalidate_dashlets|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=41608553&size=3920&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul24_07:47:32.497575.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]

Error:
158020:  AI Analytics output: {'time': 1690239231907, 'aiEnabled': False, 'errorCode': 11017}
158021:  Library group ""dcs_ep"" method ""is_ai_analytics_enabled"" returned in 0:00:00.579254
158022:  AI Analytics is not configured yet!!
158023:  Library group ""dcs_ep"" method ""validate_EA_dashlet"" returned in 0:00:00.580119
158024:  Test returned in 0:00:00.581346
158025:  Failed reason: Failed to validate EA Analytics dahslet on DNAC!!

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=41609129&size=3184&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul24_07:47:32.497575.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=41609129&size=3184&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul24_07:47:32.497575.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 Issue seen only in ESXI and AWS and passing in on-prem [~accountid:63f50bf0e8216251ae4d59ca],  based on the debug, couple of points observed:

# [+system/v1/maglev/nodes/config+|https://10.195.243.37/api/system/v1/maglev/nodes/config] is no more supported for ESXi VM DNAC
# The usage of above API is only to get the serial number of the DNAC, which would get overwritten by the configs stored in the Cluster’s json file.
This means, call to “{{get_serial}}()” method is of no use for ESXi.

Required PR has been raised for Hulk Branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6835/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6835/overview]

[~accountid:63f50bddc1685a24e1314c87],  please share the execution log from AWS Regression to double check if it’s the same case for AWS or there’s different problem. Found an API that can get the “va-member-id” required in case of ESXi VM DNAC.
Hence, raising another PR to make use of it: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6904/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6904/overview] The related PRs have been approved and merged to *private/Hulk-ms/api-auto* branch.

Marking this Auton as “Resolved” and “Closed”.","['Auton', 'Hulk', 'Integration', 'MSTB2']",Amardeep Kumar,Closed,Divakar Kumar Yadav
SEEN-1937,https://miggbo.atlassian.net/browse/SEEN-1937,Test_TC171_assurance_health_validation ,"Testcase Failed: Test_TC171_assurance_health_validation 

Sub Testcase Failed: test1_assurance_health_validation

Please Exclude netflow_essential validation as netflow_essential is removed now in Hulk

Failed Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=164670769&size=149571&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-07%2Fsanitycombine.2023Jul17_12:29:56.570216.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=164670769&size=149571&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-07%2Fsanitycombine.2023Jul17_12:29:56.570216.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

*Branch Used*: bgl/Hulk-ms/api-auto

*Test Suite*: [+https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=refs%2Fheads%2Fbgl%2FHulk-ms%2Fapi-auto+|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=refs%2Fheads%2Fbgl%2FHulk-ms%2Fapi-auto]",2023-07-18T10:04:45.093+0000,"It still showed as critical assurance health in UI so our script caught it since it is visible for customer.

Please file bug for it. If they remove it, they should remove from UI too since it will show up for customer those issues.

!system health.png|width=1079,height=491!","['Auton', 'Hulk']",SAINATH CHATHARASI,Resolved,SAINATH CHATHARASI
SEEN-1938,https://miggbo.atlassian.net/browse/SEEN-1938,[Auton] IXIA connectivity issue on IBSTE testbed ,"In IBSTE testbed,  I am facing ixia connectivity error while running the script, because of that traffic related testcases are blocked


2023-07-14T00:10:35: %IXNETWORK_RESTPY-INFO: Determining the platform and rest_port using the 10.195.247.170 address...

2023-07-14 07:10:37 [ixnetwork_restpy.connection] [WARNING] Unable to connect to [http://10.195.247.170:11009|http://10.195.247.170:11009].

2023-07-14T00:10:37: %IXNETWORK_RESTPY-WARNING: Unable to connect to [http://10.195.247.170:11009|http://10.195.247.170:11009].

2023-07-14 07:10:39 [ixnetwork_restpy.connection] [WARNING] Unable to connect to [https://10.195.247.170:11009|https://10.195.247.170:11009].

2023-07-14T00:10:39: %IXNETWORK_RESTPY-WARNING: Unable to connect to [https://10.195.247.170:11009|https://10.195.247.170:11009].

2023-07-14 07:10:41 [ixnetwork_restpy.connection] [WARNING] Unable to connect to [http://10.195.247.170:443|http://10.195.247.170:443].

2023-07-14T00:10:41: %IXNETWORK_RESTPY-WARNING: Unable to connect to [http://10.195.247.170:443|http://10.195.247.170:443].

2023-07-14 07:10:43 [ixnetwork_restpy.connection] [WARNING] Unable to connect to [https://10.195.247.170:443|https://10.195.247.170:443].

2023-07-14T00:10:43: %IXNETWORK_RESTPY-WARNING: Unable to connect to [https://10.195.247.170:443|https://10.195.247.170:443].

2023-07-14T00:10:43: %SERVICES-ERROR: Traceback (most recent call last):

 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9163976&size=19526&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fsr_ibste.2023Jul12_06:18:47.757076.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9163976&size=19526&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fsr_ibste.2023Jul12_06:18:47.757076.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

+*Workarounds tried :*+

 1) Have tried restarting the ixia client but still issue persists.

2) Ixia client has been started and listening on the respective port.  (""C:\Program Files (x86)\Ixia\IxNetwork\9.10.2007.7\IxNetwork.exe"" -tclPort 8009 -restPort 11009 -restOnAllInterfaces\)

 3)  Able to connect to Restapi port also. Manually I don’t see any issue.


*IXIA windows RDP details* 

ip addr : 10.195.247.170    root/Nbv123!",2023-07-18T14:57:23.396+0000,Did it work before? CAn you share passed log? [~accountid:712020:96c61f39-1766-4002-9406-bfc1c806f040]  Is this issue still observed? Ping ixia with port from the execution server and also check any firewall enabled/blocking the port. Close this ticket as its most likely environment issue and the reporter didnot reply to the questions to provide information.,"['Auton', 'Execution', 'Hulk', 'IBSTE']",Unassigned,Cancelled,Karventhan Velusamy
SEEN-1939,https://miggbo.atlassian.net/browse/SEEN-1939,[Auton]:[Ghost]: Task-reprovisioning_provision_conifg_validation.py-102-provisionReprovisionConfigValidation  /   Test_TC1_DNAC_Device_Provisioning  /   test5_verify_provision_the_devices_fabric1 / test6_aduit_log_verify/ Validate_nw_ping_tester_unitility,"Recently we observed during Ghost sanity executions , In non lan sanity optimized code where DNAC_Device_Provisioning tc failing

*Reporter analysis:*

On DNAC Provisioning sucess but in script failing 



*Uber ISO Version tested:* Ghost P2 #2.1.614.70794



*Script Name:* Optimized code-nonlansanitysuite

*Branch used:* private/Ghost-ms/sanity_api_auto 

*Script used:* solution_test_sanityecamb.py

*snip from failed log:*

{{Failed reason: Result: Device provision failed}}

*Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-reprovisioning_provision_conifg_validation.py-102-provisionReprovisionConfigValidation&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_09:29:07.215235.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-reprovisioning_provision_conifg_validation.py-102-provisionReprovisionConfigValidation&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_09:29:07.215235.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 

Re-run Passlog:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul17_11:35:37.816108.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul17_11:35:37.816108.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]",2023-07-18T16:13:11.854+0000,"[~accountid:63a2a522082abdd71bb36e09] , did you see the issue again in your recent run? [~accountid:62d2fe9f8afb5805e5d5af49] , yes Seeing the same issue in current runs Ghost P2 Rc1 #2.1.614.70815
Failed Log :

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-reprovisioning_provision_conifg_validation.py-102-provisionReprovisionConfigValidation&begin=4660&size=1568469&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul31_08:10:33.688127.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-reprovisioning_provision_conifg_validation.py-102-provisionReprovisionConfigValidation&begin=4660&size=1568469&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul31_08:10:33.688127.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [~accountid:63a2a522082abdd71bb36e09] [~accountid:62d2fef2bd54f8d3ffb7d1f7] [~accountid:63f50bccce6f37e5ed93c87b] [~accountid:62ab7a399cd13c0068b18fe0] [~accountid:63f50bf9e8216251ae4d59d4] I know the runs are tight right now because report turnaround time is quick, so mentioning everyone here: To verify this auton I’ll need a cluster in ghost so please if anyone has a cluster in ghost let me know, I’ll need to test reprovisioning. Prefereably [~accountid:63a2a522082abdd71bb36e09] the same testbed as in this auton. [~accountid:63f50bcece6f37e5ed93c87e] 
Now the cluster is in Hulk , once the runs are completed will deploy ghost and  share with you . Closing until testbed is provided or issue is reproduced. ","['Auton', 'Ghost', 'Optimized', 'Sanity']",Andrew Chen,Cancelled,Elton GoldChristopher
SEEN-1940,https://miggbo.atlassian.net/browse/SEEN-1940,[Auton]:[Ghost]: Task-wireless_client_data_troubleshoot.py-1712-wirelessClientTroubleshoot  /   Test_TC1_wireless_client_data_troubleshoot  /   test1_setup_parameter /  test2_wireless_client_data_troubleshoot /,"Recently we observed during Ghost sanity executions , In non lan sanity optimized code where setup_parameter  tc failing

*Reporter analysis:*

Seeing script issue  in the setup itself due to that remaining Tcs getting blocked

*Uber ISO Version tested:* Ghost P2 #2.1.614.70794



*Script Name:* Optimized code-nonlansanitysuite

*Branch used:* private/Ghost-ms/sanity_api_auto 

*Script used:* solution_test_sanityecamb.py

*snip from failed log:*

{noformat}145: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/assurance/group.py"", line 9653, in get_client_mac_address_in_ewlc{noformat}

{noformat}146: 
     if ""AP"" + result not in results[""output""]:{noformat}

{noformat}147: 
 TypeError: list indices must be integers or slices, not str{noformat}



 

*Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-wireless_client_data_troubleshoot.py-1712-wirelessClientTroubleshoot&begin=5049&size=133039&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_20:45:00.455712.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-wireless_client_data_troubleshoot.py-1712-wirelessClientTroubleshoot&begin=5049&size=133039&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_20:45:00.455712.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 ",2023-07-18T18:01:12.526+0000,"hi [~accountid:63a2a522082abdd71bb36e09] . i can not catch the error you describe above. can you please try it again with lastest code? 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-07/sanity_TB16.2023Jul31_00:40:28.464566.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-07/sanity_TB16.2023Jul31_00:40:28.464566.zip&atstype=ATS] hi [~accountid:63a2a522082abdd71bb36e09] , please tried again in your testbed and ping me if you still get error [~accountid:63f50bcf4e86f362d39acde5] No cluster available in ghost at the moment to validate the testcase , i can re-open the case once i catch the error again .","['Auton', 'Feature', 'Ghost', 'Sanity', 'optimized']",QuangVinh Nguyen,Closed,Elton GoldChristopher
SEEN-1941,https://miggbo.atlassian.net/browse/SEEN-1941,[Auton]:[Ghost]: Task-verify_inventory_insight.py-193-verifyInventoryInsights  /   Test_TC1_verify_inventory_insights  /   test1_verify_speed_duplex_mismatch ,"Recently we observed during Ghost sanity executions , In non lan sanity optimized code where verify_speed_duplex_mismatch tc failing

*Reporter analysis:*

On previous uber #2.1.614.70759 didint see the issue but now the command itself not taking .

*Uber ISO Version tested:* Ghost P2 #2.1.614.70794

 

*Script Name:* Optimized code-nonlansanitysuite

*Branch used:* private/Ghost-ms/sanity_api_auto

*Script used:* solution_test_sanityecamb.py

*snip from failed log:*

{{Enter configuration commands, one per line. End with CNTL/Z. SN-FDO2034U0EP(config)#interface GigabitEthernet1/1 SN-FDO2034U0EP(config-if)#speed 100 speed 100 ^ % Invalid input detected at '^' marker. SN-FDO2034U0EP(config-if)#end SN-FDO2034U0EP# }}

{noformat}327: 
 Traceback (most recent call last):{noformat}

{noformat}328: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Ghost/Ghost-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/Compilance/group.py"", line 1295, in verify_speed_duplex_mismatch{noformat}

{noformat}329: 
     out = self.services.dnaconfig.testbed.devices[src_dev].configure(cmd){noformat}

{noformat}330: 
   File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 259, in __call__{noformat}

{noformat}331: 
     self.call_service(*args, **kwargs){noformat}

{noformat}332: 
   File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 973, in call_service{noformat}

{noformat}333: 
     self.process_dialog_on_handle(handle, dialog, timeout){noformat}

{noformat}334: 
   File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 1020, in process_dialog_on_handle{noformat}

{noformat}335: 
     self.get_service_result(){noformat}

{noformat}336: 
   File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 235, in get_service_result{noformat}

{noformat}337: 
     raise SubCommandFailure({noformat}

{noformat}338: 
 unicon.core.errors.SubCommandFailure: ('sub_command failure, patterns matched in the output:', ['^%\\s*[Ii]nvalid (command|input|number)'], 'service result', ""interface GigabitEthernet1/1 \r\nspeed 100 \r\nspeed 100 \r\n  ^\r\n% Invalid input detected at '^' marker.\r\n\r\n""){noformat}



Failed Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_inventory_insight.py-193-verifyInventoryInsights&begin=4763&size=82033&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_20:45:00.455712.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_inventory_insight.py-193-verifyInventoryInsights&begin=4763&size=82033&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_20:45:00.455712.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Previous pass log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_inventory_insight.py-193-verifyInventoryInsights&begin=4785&size=12839&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul06_03:31:36.276299.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_inventory_insight.py-193-verifyInventoryInsights&begin=4785&size=12839&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul06_03:31:36.276299.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 ",2023-07-18T18:19:03.072+0000,"[~accountid:63f50bd34c355259db9ccc4d] I have tested with latest run Ghost p2 rc1 #2.1.614.70815
Now the Tc is passing 
Passlog:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_inventory_insight.py-193-verifyInventoryInsights&begin=4785&size=12839&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug03_12:17:09.849899.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_inventory_insight.py-193-verifyInventoryInsights&begin=4785&size=12839&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug03_12:17:09.849899.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Ghost', 'Sanity', 'optimized']",ThangQuoc Tran,Closed,Elton GoldChristopher
SEEN-1942,https://miggbo.atlassian.net/browse/SEEN-1942,[Auton][MSTB2] - Policy deletion is failing through script.,"Hi,

While deleting wireless policy in script it is throwing below error

28276:  Resource path full url: [https://10.195.243.37/api/v1/task/616421b2-9560-4dac-bc92-e08274284fcf|https://10.195.243.37/api/v1/task/616421b2-9560-4dac-bc92-e08274284fcf]
28277:  {'version': 1689703353052, 'endTime': 1689703353052, 'progress': 'TASK_INTENT', 'startTime': 1689703353020, 'errorCode': 'NCSP01001', 'data': 'workflow_id=0;cfs_id=0;rollback_status=not_supported;rollback_taskid=0;failure_task=NA;processcfs_complete=false', 'serviceType': 'NCSP', 'isError': True, 'failureReason': ""NCSP01001: Error occurred while processing the 'provision' request. Additional info for support: taskId: '616421b2-9560-4dac-bc92-e08274284fcf'. Empty 'cfs create, update and delete lists' provided in the request."", 'instanceTenantId': '64a9af0301f4ac00134acd93', 'id': '616421b2-9560-4dac-bc92-e08274284fcf'}
28278:  Library group ""task"" method ""wait_for_task_complete"" returned in 0:00:02.041560
28279:  Policy deletion failed:NCSP01001: Error occurred while processing the 'provision' request. Additional info for support: taskId: '616421b2-9560-4dac-bc92-e08274284fcf'. Empty 'cfs create, update and delete lists' provided in the request.
28280:  Library group ""app_policy"" method ""delete_wireless_policy"" returned in 0:00:04.174970
28281:  Test returned in 0:00:04.175964
28282:  Failed reason: Result :: wireless policy at PSK deletion failed

Manually we are able to delete the policy successfully.

Ova#3.710.75456

Script : solution_test_apppolicy.py

Branch : private/Hulk-ms/api-auto

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6062559&size=12225&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul18_10:28:57.432777.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6062559&size=12225&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul18_10:28:57.432777.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-07-19T12:19:39.865+0000,"Hi [~accountid:63f50bf0e8216251ae4d59ca] ,

Do you know if this testcase passed before? Could you please provide the latest Passed log of it?

From my debugging, it’s failed to deploy wireless policy PSK in subtest `test4_deploy_wireless_policy_PSK` due to the api call failed to retrieve {{fileId}} value of device

{noformat}2217:   api_switch_call called:
2218:  {'params': {'resolveScope': 'true', 'scratchPadId': '3e0a78c4-6e04-4277-81d6-299ab274aedf'}}
2219:  Resource path full url: https://10.30.0.100/api/v2/data/customer-facing-service/summary/policy/application
2220:  res1 output {'id': '4da1f2fc-403c-4ce8-8e32-de8c423b69cd', 'response': [{'instanceId': 0, 'instanceVersion': 0, 'count': 0, 'applicationPolicyScopeResolutionDeviceSummary': [{'instanceId': 0, 'instanceVersion': 0, 'deviceId': 'bb84e43e-74a2-419c-82b6-991eb0241e9d', 'deviceName': 'TB7-eWLC.cisco.com', 'deviceRole': 'ACCESS', 'deviceType': 'Cisco Catalyst 9800-40 Wireless Controller', 'excluded': False, 'groupNameHierarchy': 'Global/USA/New York/BLDNYC', 'isExcluded': False, 'isStale': False, 'managementIpAddress': '204.192.4.2', 'sppSettingsCount': 0, 'displayName': '0'}], 'displayName': '0'}], 'version': '1.0'}
2221:  Unable to get the fileid for device TB7-SJ-eCA-BORDER-CP{noformat}

{noformat}2229:  Unable to get the fileid for device TB7-SJ-eCA-BORDER-CP
2230:  Failed to retrieve file ID of device TB7-SJ-eCA-BORDER-CP
2231:  Library group ""app_policy"" method ""deploy_wireless_policy"" returned in 0:08:05.933229
2232:  Test returned in 0:08:27.002770
2233:  Failed reason: Result :: Wireless policy deployment failed{noformat}

In addition, updated code to enhancement subtest `test7_delete_existing_wireless_policy`. Pass log:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-thangqtr%2Fusers%2Fthangqtr%2Farchive%2F23-10%2Fsanity_TB1.2023Oct12_02:50:37.490137.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-thangqtr&submitter=thangqtr&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-thangqtr%2Fusers%2Fthangqtr%2Farchive%2F23-10%2Fsanity_TB1.2023Oct12_02:50:37.490137.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-thangqtr&submitter=thangqtr&from=trade&view=all&atstype=PYATS] Hi [~accountid:63f50bd34c355259db9ccc4d],

Issue seems to be day-one issue on ESXI testbed. But same works fine on On-prem testbed.

Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=608446&size=2069034&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites.2023Oct04_01:35:04.832333.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=608446&size=2069034&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites.2023Oct04_01:35:04.832333.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

MSTB2-vewlc is a non-fabric device.","['Auton', 'Hulk', 'MSTB2']",ThangQuoc Tran,Reopened,Divakar Kumar Yadav
SEEN-1943,https://miggbo.atlassian.net/browse/SEEN-1943,[Auton]:Hulk-AWS: Test_TC2_creating_global_ip_sub_pools/test1_subtest1_verify_creating_global_ip_sub_pools_for_fabric1/test3_aduit_log_verify,"*Reporter Analysis:* We have observed that testcase is picking  wrong subnet mask only on AWS Sanity which is working on-prem:
Till *Ghost* it was taking:
6730:  {'data': {'groupName': 'underlay_sf', 'type': 'lan', 'siteId': '7422314b-6ad2-41f1-abbf-26a39caee32c', 'ipPools': [{'ipPoolName': 'underlay_sf', 'ipPoolCidr': '10.12.96.0/24'.

From *Hulk it is expecting  ip pool in 204.subnet range:*
IP Pool {'groupName': 'underlay_sf_small', 'type': 'lan', 'siteId': '5866517b-c249-449c-bc54-206fb0a88e24', 'ipPools': [{'ipPoolName': 'underlay_sf', 'ipPoolCidr': '204.1.3.0/28', 'shared': True, 'ipPoolOwner': 'DNAC', 'overlapping': False, 'parentUuid': '58ebd73a-0f5a-48cc-8204-eb05b9ee72ae'}]} group configuration for reason:NCIP10213: Failed to create group underlay_sf_small because: NCIP10092: The proposed subpool 204.1.3.0/28 is not contained in the parent's range.


 

*Description*:  

{code:python} IP Pool {'groupName': 'underlay_nyc_small', 'type': 'lan', 'siteId': '5ebf769a-7c9c-46a3-90dc-101682a58b3b', 'ipPools': [{'ipPoolName': 'underlay_nyc_small', 'ipPoolCidr': '204.1.2.0/28', 'shared': True, 'ipPoolOwner': 'DNAC', 'overlapping': False, 'parentUuid': '301a5ad4-ff9b-4566-843f-493aec18cd3d'}]} group configuration for reason:NCIP10213: Failed to create group underlay_nyc_small because: NCIP10092: The proposed subpool 204.1.2.0/28 is not contained in the parent's range.
{code}



*Branch Name:* Hulk-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:

**Pass Log:**Ghost 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ip_pool_subpool_reservation.py-52-designNWSettingsIppools&begin=1098938&size=926115&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr19_01:18:39.817582.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ip_pool_subpool_reservation.py-52-designNWSettingsIppools&begin=1098938&size=926115&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-04%2Fenv_optimized_auto_job.2023Apr19_01:18:39.817582.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Fail Log:* Hulk
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ip_pool_subpool_reservation.py-52-designNWSettingsIppools&begin=1096503&size=943295&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul18_03:54:26.756761.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ip_pool_subpool_reservation.py-52-designNWSettingsIppools&begin=1096503&size=943295&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul18_03:54:26.756761.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Using same input json as on prem:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/config_48hr_test/solution_test_input.json?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/config_48hr_test/solution_test_input.json?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto]

Fabric json: solution_sanityeca_lan_SanityTB11.json
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb11/solution_sanityeca_lan_SanityTB11.json?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb11/solution_sanityeca_lan_SanityTB11.json?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto]
",2023-07-20T04:59:49.603+0000,"Add enhancement to support for pool generation for {{underlay_nyc_small }}
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/22c8118e756953fca9c00ed37da04e831e394f3c#configs/sanity_tb11/solution_sanityeca_lan_SanityTB11.json|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/22c8118e756953fca9c00ed37da04e831e394f3c#configs/sanity_tb11/solution_sanityeca_lan_SanityTB11.json] Hi Tran, I have modified the fabric json file and executed the TC25 its created underlay_nyc_small but ip sub pools are not reserved.[Test_TC25_creating_global_ip_sub_pools|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=818201&size=725991&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul20_01:38:12.467398.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] it took 10.8.0 network, but we need to reserve it on this pools 204.1.0.x

!underlay_nyc_small.png|width=653,height=317! Hi @Tran Lam
I cleaned cluster and started from scratch.
I cherry picked your commits to sanity branch:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3964bfa5a892cc6826a7cd073720a2a714a66376#services/dnaserv/dnaservices.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3964bfa5a892cc6826a7cd073720a2a714a66376#services/dnaserv/dnaservices.py]
Even after that I am seeing same issue:
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ip_pool_subpool_reservation.py-52-designNWSettingsIppools&begin=692275&size=945665&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul20_03:05:37.859341.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ip_pool_subpool_reservation.py-52-designNWSettingsIppools&begin=692275&size=945665&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul20_03:05:37.859341.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
But i observed for newyork site ""underlay_nyc_small"" is created this time
Now error throwing for San francisco:
error code:
7998:  Unable To Find the Values: ['Successfully created the ip pool group underlay_sf_small for the site SAN-FRANCISCO']
8001:  Could not find the expected child response even after retry:: {'underlay_sf_small': ['Received a request to create an ip pool group named underlay_sf_small for the site SAN-FRANCISCO', ['Successfully created the ip pool group underlay_sf_small for the site SAN-FRANCISCO']]}

Thanks,
Anusha John Need to add underlay_sf_small/28 in fabric json same as underlay_nyc_small
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/29f01b8037834de5c9c7b140b74d27f2d92f9ac7|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/29f01b8037834de5c9c7b140b74d27f2d92f9ac7] Hi Tran,
After the latest fix Testcase got passed:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ip_pool_subpool_reservation.py-52-designNWSettingsIppools&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul20_21:10:19.513865.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ip_pool_subpool_reservation.py-52-designNWSettingsIppools&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul20_21:10:19.513865.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
For san francisco site subpool is created
So closing the jira

Thanks,
Anusha John","['AWS', 'AWS-Santiy', 'AWS_MSTB', 'Auton', 'Execution', 'Hulk', 'Multisite', 'Optimized', 'Regression', 'Sanity']",Tran Lam,Closed,Anusha John
SEEN-1945,https://miggbo.atlassian.net/browse/SEEN-1945,Need option to generate rca,Please have an option to generate rca at the end of script run,2023-07-21T04:14:59.274+0000,"[Pull Request #6468: RCA generation - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6468/overview]

Please add “generate_rca”: true in the fabric file to enable it. Also make sure the dnac has secure shell disabled, as there is some error when secure shell is enabled.","['Auton', 'Enhancement']",Andrew Chen,Resolved,Sowmya Ramakrishnan
SEEN-1946,https://miggbo.atlassian.net/browse/SEEN-1946,TC181_generate_Worst_Interferers_report - test2_generate_Worst_Interferers_CSV_report,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*Branch*: private/Hulk-ms/api-auto

*Script Name :* solution_test_sanity_ecamb.py

*Testbed :* TB4

*Testcases Impacted :*  TC181_generate_Worst_Interferers_report - test2_generate_Worst_Interferers_CSV_report

During Hulk ESXI testing “TC181_generate_Worst_Interferers_report - test2_generate_Worst_Interferers_CSV_report”  fails due to unable to see wireless-client but we have wireless-clients in assurance page.

*Fail Log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=39779142&size=158530&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul14_00:29:07.945971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=39779142&size=158530&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul14_00:29:07.945971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=62131763&size=162591&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_22:05:13.705332.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=62131763&size=162591&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun27_22:05:13.705332.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-07-21T06:46:30.955+0000,"PR-Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6436/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6436/overview]

Trade log link: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-07/sanity_TB4.2023Jul27_02:48:34.252805.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-07/sanity_TB4.2023Jul27_02:48:34.252805.zip&atstype=ATS] Mentioned PR has been validated with TB4: [Test_TC181_generate_Worst_Interferers_report|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1135073&size=239642&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug01_10:12:47.877171.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Marking this Jira as “Resolved” and “Closed”.","['Auton', 'ESxi', 'Hulk']",NhanHuu Nguyen,Closed,KRISHNA MUKKU
SEEN-1953,https://miggbo.atlassian.net/browse/SEEN-1953,[Auton]:Hulk: TC2_DNAC_addition_of_wireless_nw_profiles/test1_addition_of_wireless_nw_profiles,"*Reporter Analysis:*

Build : *Hulk Patch 1  :2.1.713.70080*

In DNAC , script is trying to add the wireless network profiles but getting the below error

{{NCND00002: The Network Profile request has validation errors"",""detail"":""[\""NCND01157: Unable to find a namespace validator for the given type of tta\""]""},""version"":""1.0""}}}

Profile:{{{name"": ""profile-tta"", ""namespace"": ""tta"",}}}

*Branch -* private/Hulk-ms/sanity_api_auto

*Script file /Use case -* *solution_test_sanityecamb_lan.py*

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log :* [Test_TC2_DNAC_addition_of_wireless_nw_profiles|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-base_wireless_ssid_profile_guest.py-53-designWireless&begin=585201&size=116482&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul19_22:10:47.689096.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

{noformat}1372:   api_switch_call called:
1373:  {'data': {'name': 'profile-tta', 'namespace': 'tta', 'profileAttributes': [{'key': 'nfv.serviceprovider.list', 'value': '0', 'attribs': []}, {'key': 'nfv.device.name', 'value': 'TTA', 'attribs': [{'key': 'nfv.device.deviceType', 'value': 'Cisco DNA Traffic Telemetry Appliance', 'attribs': []}, {'key': 'nfv.device.displayDeviceFamilyValue', 'value': '286326115', 'attribs': []}, {'key': 'nfv.device.familyName', 'value': 'Network Management', 'attribs': []}, {'key': 'nfv.device.deviceSeries', 'value': 'Cisco DNA Traffic Telemetry Appliances', 'attribs': []}, {'key': 'nfv.device.order', 'value': 'dev1', 'attribs': []}]}], 'sites': []}}
1374:  Resource path full url: https://10.22.40.52/api/v1/siteprofile
1375:  Error Code: 406 for
1376:  URL:https://10.22.40.52/api/v1/siteprofile Data:{'timeout': 60, 'data': '{""name"": ""profile-tta"", ""namespace"": ""tta"", ""profileAttributes"": [{""key"": ""nfv.serviceprovider.list"", ""value"": ""0"", ""attribs"": []}, {""key"": ""nfv.device.name"", ""value"": ""TTA"", ""attribs"": [{""key"": ""nfv.device.deviceType"", ""value"": ""Cisco DNA Traffic Telemetry Appliance"", ""attribs"": []}, {""key"": ""nfv.device.displayDeviceFamilyValue"", ""value"": ""286326115"", ""attribs"": []}, {""key"": ""nfv.device.familyName"", ""value"": ""Network Management"", ""attribs"": []}, {""key"": ""nfv.device.deviceSeries"", ""value"": ""Cisco DNA Traffic Telemetry Appliances"", ""attribs"": []}, {""key"": ""nfv.device.order"", ""value"": ""dev1"", ""attribs"": []}]}], ""sites"": []}'} Headers:{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2NGI4YzYzN2E5ZjVjYzQwOGEyZTU4NDYiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjY0YjhjNjI2YTlmNWNjNDA4YTJlNTJlNiJdLCJ0ZW5hbnRJZCI6IjY0Yjg4N2JhYTlmNWNjNDA4YTJlMzQyZiIsImV4cCI6MTY4OTgzNDg2NiwiaWF0IjoxNjg5ODMxMjY2LCJqdGkiOiI5Mzk5ODBjYi1lMjVhLTRlM2EtYjRlZS0xMGI5OWVjNTI2MGUiLCJ1c2VybmFtZSI6ImRlc2lnbiJ9.Z12ilbz8ozZFhpWH5MwWdnnB55f1ZXPvWZ_gZqWFfzxROeO_vKIXdjt7byMfL-XsEyD5Hckpk8GVIrHdBPGJio2y6efIfWR9rmXb_Kw7tunx7R2FUTUVUKXjeTrMoIcyh9MYqDGS_z7UwVyPvRXWDdESqoBUQIWl0R0qMyiNhjqwyxVwN-6LVikvqYsRyGdccEAMXS-oA_KVe0tF0yNE5zY2dZIRjaUbIcM9vu3nKXZUIXHMxGr89Rhx50Q_A4uRVQa3es4yjfP_0Z3Im9lTxgT8WX63tBgkAilzQbfGAaZGovBP35Q-h7U08bae4DTqn6evZT_bDy8p-zq2zl4T_w;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:{""response"":{""errorCode"":""NCND00002"",""message"":""NCND00002: The Network Profile request has validation errors"",""detail"":""[\""NCND01157: Unable to find a namespace validator for the given type of tta\""]""},""version"":""1.0""}
1377:  Traceback (most recent call last):
1378:    File ""/data/jenkins/ws_sjc2_ds/workspace/Express-Sanity/Hulk/Hulk-Optimized-Deployment_and_Express_Sanity/services/dnaserv/client_manager.py"", line 326, in call_api
1379:      response.raise_for_status()
1380:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status
1381:      raise HTTPError(http_error_msg, response=self)
1382:  requests.exceptions.HTTPError: 406 Client Error: Not Acceptable for url: https://10.22.40.52/api/v1/siteprofile
1383:  Encountered unhandled HTTPError in Internal API Call{noformat}


Pass Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-base_wireless_ssid_profile_guest.py-53-designWireless&begin=555479&size=37897&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun14_03:18:48.751978.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-base_wireless_ssid_profile_guest.py-53-designWireless&begin=555479&size=37897&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_optimized_auto_job.2023Jun14_03:18:48.751978.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

+*Testbed wiki:*+
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-13?src=contextnavpagetreemode|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-13?src=contextnavpagetreemode]",2023-07-21T12:43:17.127+0000,"Please use branch private/HulkPatch-ms/api-auto for Hulk Patch. Hi [~accountid:62d2fe9f8afb5805e5d5af49] 
I have used the branch private/HulkPatch-ms/api-auto to run Hulk P1 on Sanity TB8 
Still there is same failure as mentioned  Hi  [~accountid:62d2fe9f8afb5805e5d5af49]  ,

We are experiencing the same problem after using the Hulk patch  branch,

could  you please check 
  
Failed Log :
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-base_wireless_ssid_profile_guest.py-53-designWireless&begin=477586&size=65496&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug08_05:49:05.160552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-base_wireless_ssid_profile_guest.py-53-designWireless&begin=477586&size=65496&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug08_05:49:05.160552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

10.195.247.188 (admin /Maglev123)
+*Branch*+ 
{{private/HulkPatch-ms/sanity_api_auto}}

wiki  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8?src=contextnavpagetreemode|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8?src=contextnavpagetreemode] looks the code is not pulled latest one, can you pull and try again, make sure in input.json the {{profile-tta}} present or not i am seeing issue with guest portal 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=854702&size=702153&archive=%2Fws%2Fvkuttykr-sjc%2Fpyats_dnac_kk%2Fusers%2Fvkuttykr%2Farchive%2F23-08%2Fsanity_TB23_standalone.2023Aug09_10:32:24.036921.zip&ats=%2Fws%2Fvkuttykr-sjc%2Fpyats_dnac_kk&submitter=vkuttykr&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=854702&size=702153&archive=%2Fws%2Fvkuttykr-sjc%2Fpyats_dnac_kk%2Fusers%2Fvkuttykr%2Farchive%2F23-08%2Fsanity_TB23_standalone.2023Aug09_10:32:24.036921.zip&ats=%2Fws%2Fvkuttykr-sjc%2Fpyats_dnac_kk&submitter=vkuttykr&from=trade&view=all&atstype=pyATS] Please make sure you pull your branch correctly.
I just checked there is still “profile-tta” in your branch '{{private/HulkPatch-ms/sanity_api_auto}}'

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/config_48hr_test/solution_test_input.json?at=refs%2Fheads%2Fprivate%2FHulkPatch-ms%2Fsanity_api_auto#8508|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/config_48hr_test/solution_test_input.json?at=refs%2Fheads%2Fprivate%2FHulkPatch-ms%2Fsanity_api_auto#8508]

!sanity branch.png|width=827,height=488!","['Auton', 'Hulk', 'Optimized', 'Sanity', 'auton', 'expres-sanity']",Tran Lam,Resolved,KARTHIKEYAN KRISHNAMURTHY
SEEN-1955,https://miggbo.atlassian.net/browse/SEEN-1955,[Auton]:Hulk: Test_TC189_edit_site_name/test8_AP_reonboarding_verifications/test12_provision_ap_with_rf_profile/test13_configure_aps_with_work_flow/test14_verify_ap_new_site/test18_configure_aps_with_work_flow,"*Reporter Analysis:*

Observed due to VCR changes one sub tc is failing , rf issue is failed due to changes in the rf libraries.

Description:
EDIT SITE NAME Feature on Hulk RC1 Testbed:
First attempt it failed with unicon errors:
60905:  unicon.core.errors.StateMachineError: Failed while bringing device to ""config"" state
60906:  Result:FAILED shut down APs:{'name': 'AP2C57.4184.2BF4', 'type':
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16786798&size=157862&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul20_03:20:38.142103.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16786798&size=157862&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul20_03:20:38.142103.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
So i retried same usecase again:
Second attempt testcases didn't blocked but it was not able to create a new site bld_23_new.
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1171304&size=5461555&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul20_21:22:02.542335.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1171304&size=5461555&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul20_21:22:02.542335.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
In third attempt it is getting errored:
[https://ngdevx.cisco.com/services/taas/results/350792d5-1569-4578-967f-c27a6019fd8e/run-results|https://ngdevx.cisco.com/services/taas/results/350792d5-1569-4578-967f-c27a6019fd8e/run-results]",2023-07-21T16:35:39.982+0000,"PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6356/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6356/overview]

This is for Hulk and once merged will push the changes into older releases. VCR error will be handled by [~accountid:63f50bcece6f37e5ed93c87e]. Are you working on handling the VCR if no config to push?  [~accountid:63f50bfce8216251ae4d59d5] , The pR [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6356/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6356/overview] is approved.  Can you resolve the conflict then we can merge it? Hi Team,

Tested this Feature on Hulk P1

We have a Pass Log for this Feature
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8808212&size=5478127&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug23_10:35:38.880853.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8808212&size=5478127&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug23_10:35:38.880853.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

As this part of Edit Site Name Feature

Attaching separate Logs of this Feature
[test3_configure_wireless_custom_RF_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9186351&size=47665&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug23_10:35:38.880853.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[test15_provision_ap_with_rf_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12633302&size=737703&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug23_10:35:38.880853.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Hulk', 'Integration', 'Sanity']",Moe Saeed,Resolved,Anusha John
SEEN-1956,https://miggbo.atlassian.net/browse/SEEN-1956,[Auton] [Hulk] - Need for script enhancement to handle failures for SSID to Network profile association ,"*Regression:* Solution Regression Multisite - DR+MDNAC

*Branch:* private/Hulk-ms/api-auto

*Uber ISO tested:* Hulk RC1 - 2.1.710.70446

*Scripts Used:* 

testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Background and Issue Faced:* 

During ongoing Solution testing on Hulk RC1, we see issue related to SSID Addition to Network profile for GUEST and GUEST2 ssids. We see even tough the tasks corresponding to these SSIDs to Network profile addition is successful, on the DNAC we see the mapping is missing for SSID to Network profile.

Due to this, these SSIDs are not showing up under any of the Fabric sites, and because of this mapping for SSIDs to Segment has got Failed fo . Eventually all the SSID states are down on all the wireless devices.

After discussion with Automation team and DE team it was confirmed that its DNAC side issue related API calls. Reported defect  - [CSCwh00957|https://cdetsng.cisco.com/webui/#view=CSCwh00957].

This Jira is being reported to handle such scenarios where tasks are being marked success but actual association did not happen. Below are the scenarios to be added:

1) To check whether SSID is added to Network profile properly. Mark as failed with if it does not meet the criteria. 

2) After network profile association to SSID corresponding task is successful ,confirm if the GET call for the network profile has the SSID really mapped. Marked as Failed with log if it does not meet the criteria. 

Please refer Relevant Team space - [https://eurl.io/#xlJqsjKEN|https://eurl.io/#xlJqsjKEN] for more details

*Current implementation logs and details:*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_mb_multi_sites_mdnac.2023Jul17_21:25:55.023959.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_mb_multi_sites_mdnac.2023Jul17_21:25:55.023959.zip&atstype=ATS] -> Refer TC22.1 

Here the task corresponding to Guest SSIDs - *GUESTMS1* and *GUEST2MS1* mapping to network profile - *profile-SSIDOpenIndia* was successful but actually these SSIDs did not have mapping to network profile when checked from GUI and also from GET API call for network profile after the issue.",2023-07-21T17:10:59.344+0000,"Please implement same verifications for Solution Sanity Script as well.
*Script:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py [~accountid:63f50bfce8216251ae4d59d5] : Could you please help to prioritize this ticket? PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6863/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6863/overview]","['Auton', 'Execution', 'Hulk', 'MSTB1', 'Multisite', 'Regression']",Moe Saeed,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-1957,https://miggbo.atlassian.net/browse/SEEN-1957,[Auton][MSTB2] : Test_TC31_validate_icap_rx_tx_bytes,"Hi

During ESXI execution we are seeing execution got stuck for almost 4 hrs
2023-07-16T09:01:19: %CLIENTMANAGER-INFO: Resource path full url: https://10.195.243.37/api/ndp/v1/data/store/app/assurance_ap_stats_v1/query
2023-07-16T09:01:20: %API-GROUP-ICAP-INFO: AP a4:88:73:cf:1a:70 TX RX PACKETS
2023-07-16T09:01:20: %API-GROUP-ASSURANCE-INFO: Calculating If the required result found  in the Response!!
2023-07-16T09:01:20: %API-GROUP-ASSURANCE-INFO: Result Generated is []
2023-07-16T09:01:20: %API-GROUP-ICAP-INFO: Total Rx Packets Available is []
2023-07-16T09:01:20: %API-GROUP-ASSURANCE-INFO: Calculating If the required result found  in the Response!!
2023-07-16T09:01:20: %API-GROUP-ASSURANCE-INFO: Result Generated is []
2023-07-16T09:01:20: %API-GROUP-ICAP-INFO: Total Tx Management Packets Available is []
2023-07-16T09:01:20: %API-GROUP-ASSURANCE-INFO: Calculating If the required result found  in the Response!!
2023-07-16T09:01:20: %API-GROUP-ASSURANCE-INFO: Result Generated is []
2023-07-16T09:01:20: %API-GROUP-ICAP-INFO: Total Rx Management Packets Available is []
2023-07-16T09:01:20: %API-GROUP-ASSURANCE-INFO: Calculating If the required result found  in the Response!!
2023-07-16T09:01:20: %API-GROUP-ASSURANCE-INFO: Result Generated is []

Failed Log : [https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/MS-TB2/job/MSTB2_Solution_Regression_Git/419/console|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/MS-TB2/job/MSTB2_Solution_Regression_Git/419/console]

Script Name : solution_assurance_test.py

Branch : private/Hulk-ms/api-auto",2023-07-25T06:54:15.262+0000,"Did it work before? Can you share passed log? Hi [~accountid:62d2fe9f8afb5805e5d5af49],

It was working fine earlier

Pass Version#3.660.75424

Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7993118&size=9158&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fauto_MS_job.2023Mar28_09:51:13.192524.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7993118&size=9158&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fauto_MS_job.2023Mar28_09:51:13.192524.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [~accountid:63f50bf0e8216251ae4d59ca]  This seem like a bug. PLease check with DE and file defect for it.
[~accountid:63f50bcafb3ac4003fa2c6dd] Please check why it took too long in case of failure. Please handle to wait for reasonable time in case of failure. PR HulkPatch-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7341/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7341/overview] [~accountid:63f50bf0e8216251ae4d59ca] The PR was merged, could you check it?","['Auton', 'Execution', 'Hulk', 'MSTB2']",NhanHuu Nguyen,Resolved,Divakar Kumar Yadav
SEEN-1958,https://miggbo.atlassian.net/browse/SEEN-1958,[Auton][MSTB2] :  Test_TC57_DEV_STRESS_verify_edge_reload_ap_clients_stability  /   test1_verify_edge_reload_ap_clients_stability,"Hi,

While we are running “[Test_TC57_DEV_STRESS_verify_edge_reload_ap_clients_stability|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=843862&size=1429401&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul23_21:13:51.161848.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_edge_reload_ap_clients_stability” testcase we are seeing border node is getting reloaded instead of Edgenode. Further debugging we could find that Edge1 is defined as device with Border node.

{{edge1 = dnac_handle.dnaconfig.borders[0]['name']}}
We are seeing script is unable to handle login prompt and throws below error

6593:  Traceback (most recent call last):
6594:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/commonlibs/test_wrapper.py"", line 301, in wrapper
6595:      result = testfunc(func_self, **kwargs)
6596:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py"", line 3295, in test1_verify_edge_reload_ap_clients_stability
6597:      if (dnac_handle.dnaconfig.testbed.devices[edge1].reload(prompt_recovery=True)):
6598:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 259, in *call*
6599:      self.call_service(*args, **kwargs)
6600:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/iosxe/service_implementation.py"", line 142, in call_service
6601:      super().call_service(reload_command=reload_command or ""reload"", reply=reply,
6602:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 2142, in call_service
6603:      raise SubCommandFailure(""Reload failed : %s"" % err) from err
6604:  unicon.core.errors.SubCommandFailure: Reload failed : Failed while bringing device to ""any"" state

Failed log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=844614&size=39883&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul23_21:13:51.161848.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=844614&size=39883&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul23_21:13:51.161848.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Script Name : solution_assurance_test.py

Branch : private/Hulk-ms/api-auto",2023-07-25T07:45:03.811+0000,"Did it work before? Can you share passed log? Hi Tran,

Just tried to compare the Testcase with MSTB1 and we could observe there also Border device only getting reloaded and it works fine there. But difference is in MSTB1 Bordernode is standalone device but MSTB2 is having SVL node. After device reload on MSTB2 script is unable to handle the Device console and testcase fails.

Passlog from MSTB1 : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=31485265&size=308505&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb_multi_sites_mdnac.2023Aug05_01:16:20.218349.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=31485265&size=308505&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb_multi_sites_mdnac.2023Aug05_01:16:20.218349.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] Loop over {{dnac_handle.dnaconfig.borders}} + {{dnac_handle.dnaconfig.Edges and look for EDGEROLE in role of the device.}}

{{edge1 = dnac_handle.dnaconfig.borders[0]['name']}}","['Auton', 'Execution', 'Hulk', 'MSTB2']",Moe Saeed,In Progress,Divakar Kumar Yadav
SEEN-1959,https://miggbo.atlassian.net/browse/SEEN-1959,[Auton] Hulk Optimized Run - Test Case not Stopping after Blocker Use Case Failure,"

After mapping changes, we are experiencing an issue in the recent optimized run. TC case is not stopped after blocker uc failed. Do we need to pick this mapping or Let us know how will be processed out .

Please find below the Hulk- log  &  use-case-yaml:

+*lansanity_usecases__yaml:*+
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/lansanity/lansanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/lansanity/lansanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto]





+*Hulk Trade-log:*+
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul04_06:01:09.957654.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul04_06:01:09.957654.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]",2023-07-25T08:34:55.532+0000,"Hi Team,
The same issue  observed in the recent in express sanity  Hulk execution  : 

+*Please  find below  log:*+
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul19_22:10:47.689096.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul19_22:10:47.689096.zip&atstype=ATS]

{{USECASES_MAP_FILE : usecasemaps/express_sanity/lansanity_exp_usecases_maps.yaml}} The looping is fixed to pick right failed case status. 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/713904a5f4154640e5357550603b3a12a23efceb#job/jobutils.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/713904a5f4154640e5357550603b3a12a23efceb#job/jobutils.py]","['Auton', 'Hulk', 'Optimized', 'Sanity', 'Yamlmapping', 'auton', 'expres-sanity']",Pawan Singh,Resolved,Omkar Sharad Wagh
SEEN-1960,https://miggbo.atlassian.net/browse/SEEN-1960,RBAC lib needs update w.r.t. ESXi after the fix came for CSCwf02465 and CSCwe23203,"RBAC lib needs update w.r.t. ESXi after the fix came for [CSCwf02465|https://cdetsng.cisco.com/webui/#view=CSCwf02465] and [CSCwe23203|https://cdetsng.cisco.com/webui/#view=CSCwe23203].

This will unblock execution of {{Test_TC6_DNAC_RBAC_create_users_roles}} test-case for ESXi.",2023-07-25T21:41:34.850+0000,"PR raised for required change: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6401/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6401/overview] Mentioned PR has been approved and merged to Hulk Branch.

Marking this ticket as “Done”.","['Auton', 'ESXi', 'Hulk']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1961,https://miggbo.atlassian.net/browse/SEEN-1961,[Auton] [Hulk] - Addition of gb policy on ISE failing with 400 client error,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/Hulk-ms/api-auto

*Uber ISO tested:* Hulk RC1 - 2.1.710.70446

*ISE version used:* 3.2 P4 non-CCO

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

When integrating the feature corresponding to MDNAC ACA sync, we have observed gb policy creation on ISE has got failed with 400 Client error. Due to this subsequent sub TCs got blocked. Looks like creating of policy url has some problem.

*Error snip:*

16182:  Traceback (most recent call last):
16183:    File ""/auto/dna-sol/ws/sr-mb1_Hulk_July_17/services/iseserv/client_manager.py"", line 278, in call_api
16184:      response.raise_for_status()
16185:    File ""/ws/divayada-sjc/Solution_pyatsenv/lib/python3.6/site-packages/requests/models.py"", line 960, in raise_for_status
16186:      raise HTTPError(http_error_msg, response=self)
{color:#bf2600}16187:  requests.exceptions.HTTPError: 400 Client Error:  for url: {color}[https://10.195.243.236:9060/ers/config/egressmatrixcell|https://10.195.243.236:9060/ers/config/egressmatrixcell]
16188:  Traceback (most recent call last):
16189:    File ""/auto/dna-sol/ws/sr-mb1_Hulk_July_17/services/iseserv/iseapi.py"", line 4914, in create_gbpolicy
16190:      res = self.post_egressmatrixcell(json=d_data)
16191:    File ""/auto/dna-sol/ws/sr-mb1_Hulk_July_17/services/iseserv/iseapi.py"", line 4607, in post_egressmatrixcell
16192:      return self.api_client.call_api(method, resource_path, **kwargs)
16193:    File ""/auto/dna-sol/ws/sr-mb1_Hulk_July_17/services/iseserv/client_manager.py"", line 289, in call_api
16194:      raise e
16195:    File ""/auto/dna-sol/ws/sr-mb1_Hulk_July_17/services/iseserv/client_manager.py"", line 278, in call_api
16196:      response.raise_for_status()
16197:    File ""/ws/divayada-sjc/Solution_pyatsenv/lib/python3.6/site-packages/requests/models.py"", line 960, in raise_for_status
16198:      raise HTTPError(http_error_msg, response=self)


*Failed log on Halleck* - [Test_TC267_Trustsec_synced_from_ise|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2440862&size=6168253&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_mb_multi_sites_mdnac.2023Jul24_07:44:54.799707.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/MDNAC+ACA+Sync|https://wiki.cisco.com/display/EDPEIXOT/MDNAC+ACA+Sync]",2023-07-26T03:45:47.126+0000,"{{Handled for getting all SGs if in multiple pages}}

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/7b1b4f32ffad298c14954894493404c6133d6577|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/7b1b4f32ffad298c14954894493404c6133d6577]","['Auton', 'Halleck', 'Hulk', 'Integration', 'MSTB1', 'Multisite']",Tran Lam,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-1962,https://miggbo.atlassian.net/browse/SEEN-1962,uplift TC102/103 DNAC_External_Authentication related use-cases for ESXi,Uplift use-cases under Test_TC102_DNAC_External_Authentication_Radius and Test_TC103_DNAC_External_Authentication with APIs specific to ESXi VM DNAC.,2023-07-26T18:26:11.526+0000,"PR has been raised for the required change: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6486/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6486/overview]

* Execution log with On-Prem Sanity TB7: [Test_TCxx_DNAC_External_Authentication[_RADIUS]|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats%2Fusers%2Famardkum%2Farchive%2F23-08%2Fenv_auto_job.2023Aug02_00:55:24.425148.zip&atstype=ATS]
* Execution log with ESXi Sanity TB18: [Test_TCxx_DNAC_External_Authentication[_RADIUS]|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats%2Fusers%2Famardkum%2Farchive%2F23-08%2Fenv_auto_job.2023Aug02_00:57:00.853077.zip&atstype=ATS] Mentioned PR has been approved and merged to Hulk branch.

Marking this Auton as “Done”.","['Automation', 'Auton']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1963,https://miggbo.atlassian.net/browse/SEEN-1963,Replace '/v1/ncp-node/graphql' in verify_ncp_node_graph_status in Hulk Patch,"Comments in [https://cdetsng.cisco.com/webui/#view=CSCwd96194|https://cdetsng.cisco.com/webui/#view=CSCwd96194]: 

'The support is not there for on-prem as well. The fact that is works does not mean its supported and can stop working anytime.

The request to the consuming team is to use the direct APIs.'



From Hulk Patch, the '/v1/ncp-node/graphql' did not response with metric data anymore.  We need to replace it with a supported working APIs.
Once this uplift is done, we will enable it back in {{verify_devices_aggregated_status}}.",2023-07-26T21:03:03.689+0000,Raised PR on Hulk branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6474/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6474/overview] ,"['Uplift', 'auton']",ThangQuoc Tran,Resolved,Tran Lam
SEEN-1964,https://miggbo.atlassian.net/browse/SEEN-1964,Hulk RC2 - Creation of SSID via BAPI is failing with KeyError: 'isAuthKey8021xPlusFT'.,"Creation of enterprise SSID via BAPI is failing due to KeyError: 'isAuthKey8021xPlusFT' in Hulk.

Branch : private/Hulk-ms/sanity_api_auto

Failed Logs: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-base_wireless_ssid_profile_guest.py-52-designWireless&begin=92101&size=13140&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul26_12:35:59.148672.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-base_wireless_ssid_profile_guest.py-52-designWireless&begin=92101&size=13140&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul26_12:35:59.148672.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

We see recent commit on the below changes, could you please help on it.

[+https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6112/diff#services/dnaserv/lib/ext_api_groups/ext_enterprise_ssid/group.py+|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6112/diff#services/dnaserv/lib/ext_api_groups/ext_enterprise_ssid/group.py]",2023-07-27T14:53:57.393+0000,"I found the issue and fixed it in PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6429/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6429/overview]

Thanks for raising this issue","['Auton', 'Hulk']",ThanhTan Nguyen,Resolved,SANTHOSH MOUNASWAMY
SEEN-1965,https://miggbo.atlassian.net/browse/SEEN-1965,Redundant TCs across MultiSite and Assurance script,"While reviewing the report for MultiSite Regression, I came across multiple redundant TCs.

solution_test_3sites_sjc_nyc_sf:

||*TC Name*||*Duplicate TC*||*Comment*||
|{{Test_TC59_DNAC_static_onboarding_ixia_scale}}|{{Test_TC110_DNAC_static_onboarding_ixia_scale}}|Duplicate within same script|
|{{Test_TC61_DNAC_verify_pim_igmp_config_on_fabric_devices}}|{{Test_TC109_DNAC_verify_pim_igmp_config_on_fabric_devices}}|Duplicate within same script|
|{{Test_TC62_DEV_STRESS_mcast_traffic_convergence_test_primary_border_reload}}|{{Test_TC111_DEV_STRESS_mcast_traffic_convergence_test_primary_border_reload}}|Duplicate within same script|
|{{Test_TC72_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_reload}}|{{Test_TC76_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_reload}}|Duplicate within same script|
|Test_TC134_disable_oeap_mode_on_site|Test_TC136_disable_oeap_mode_on_site|Duplicate within same script|

||*TC Name*||*solution_test_3sites_sjc_nyc_sf*||*solution_assurance_test*||*Comment*||
|generate_exceutive_summary_report|{{Test_TC82_generate_exceutive_summary_report}}|{{Test_TC8_generate_exceutive_summary_report}}|Double execution; can be limited to single script|
|compare_RF_KPIs_AP|{{Test_TC115_compare_RF_KPIs_AP}}|{{Test_TC56_compare_RF_KPIs_AP}}|Double execution; can be limited to single script|
|poor_RF_AP_Issue|{{Test_TC116_poor_RF_AP_Issue}}|{{Test_TC57_poor_RF_AP_Issue}}|Double execution; can be limited to single script|
|path_trace_betweet_diff_sites|{{Test_TC137_path_trace_betweet_diff_sites}}|{{Test_TC65_path_trace_betweet_diff_sites}}|Double execution; can be limited to single script|
|configure_the_external_sftp|{{Test_TC143_configure_the_external_sftp}}|{{Test_TC66_configure_the_external_sftp}}|Double execution; can be limited to single script|
|cisco_telemetry_broker_as_netflowcollector|{{Test_TC144_cisco_telemetry_broker_as_netflowcollector}}|{{Test_TC67_cisco_telemetry_broker_as_netflowcollector}}|Double execution; can be limited to single script|
|verify_wlc_interface_stats|{{Test_TC146_verify_wlc_interface_stats}}|{{Test_TC64_verify_wlc_interface_stats}}|Double execution; can be limited to single script|

Please review the same and remove the redundant ones.",2023-07-27T19:31:58.201+0000,,"['Auton', 'Hulk']",Tran Lam,Backlog,Amardeep Kumar
SEEN-1970,https://miggbo.atlassian.net/browse/SEEN-1970,[Auton] [Hulk] Task-verify_tag_config_in_ISE_NDG.py-1713-verifyConfigISENDG,"*Reporter Analysis:*
During Solution Sanity on Hulk 2370-70446 execution, we are observing the ISE NDG (Network Device Group) feature to verify if the deleted device in DNAC (DNA Center) maps to the Deleted Network Device Group on ISE after deleting the Edge node.

# During Solution Sanity, we use the Lan_script (eca, eWLC) to discover the devices, while other devices come in DNA-C via PNP (Plug and Play).
# We observed that the script is discovering all devices; however, most devices are appearing in the discovery page with a ""Net config failed"" issue .It’s creating  issue  

*Expected: Behavior*: If you are deleting specific devices from the inventory or fabric, the expectation is to only discover those devices.

{{Please refer the attached  snip for more details.}}


Failed Log:
[Task-check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE.py-1713-verifyConfigISENDG|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE.py-1713-verifyConfigISENDG&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_22:14:21.768251.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

 


Note:
*Could You please confirm, we need to include  or not  insanity RUN*",2023-07-28T18:23:39.888+0000,"to conclusion, you want me to change the testscript, from “discover all device” to “discover deleted device only”, right?  Hi [~accountid:63f50bcf4e86f362d39acde5]  ,

If we are deleting one device (EDGE) from the fabric site, we need to discover only EDGE.

# In the Sanity run, we are using the lan_automation script. Could you please confirm if we can execute it or not in the sanity execution?
# I observed that after UC 17 execution, all devices were discovered, but Netconf was failed. Hi Omkar

# i’m not sure the “lan_automation” script you was mentioned is. You can use testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_dr.py for sure script working fine
# About Netconf failed, maybe it because of testbed/cluster failed. can you please verify it in other testbed?
# and yeah, with your request (discover only edge), i can changes the script. But i dont think it’s the reason for netconf failed. So please make sure that you are correct about netconf

thank you alot Omkar ❤️ Hi Omkar. Since you change the priority to “Highest”, please confirm that it’s needed to do, so that i can raised PR for that Hi [~accountid:6357500be14026a7397f37dd] ,


I have already provided the input. Please check the description in the trail's comments,
 a testbed is been redeployed for the next Sanity request.

Currently, we do not have a testbed in the issuing state, Meanwhile can you please try in your local testbed?

 Hi [~accountid:63f50bcf4e86f362d39acde5]  ,
Could you please share latest update on this jira PR has been raised:

* PR-link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6925/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6925/overview]
* Summary: change behavior of discovery section: Discovery deleted device only instead of discovery all devices
* Tradelog: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-09/sanity-intg2.2023Sep07_01:01:30.366387.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-quangvin/users/quangvin/archive/23-09/sanity-intg2.2023Sep07_01:01:30.366387.zip&atstype=ATS] Hulk  P1  RC3  

We observed the following issues:

# The script is unable to delete from 'inventory'.
# The script failed while discovering the edge node.""

+*Failed Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE.py-1719-ISENDG_deletedDeviceInISENDG&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_10:06:54.734792.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE.py-1719-ISENDG_deletedDeviceInISENDG&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_10:06:54.734792.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]
 hi [~accountid:620b8357878c2f00729881c8] 

which branch did you using? Hi [~accountid:63f50bcf4e86f362d39acde5] ,
I'm using this branch: 

*private/HulkPatch-ms/sanity_api_auto*
 can you please using Main branch to trigger testcase? if you using other branch, please make sure that you pull all the change from main branch to your branch since no update from reporter and the failed is not related to this Jira ticket, i will close this auton. please raised another auton if needed Hi  [~accountid:63f50bcf4e86f362d39acde5]  / [~accountid:62d2fe9f8afb5805e5d5af49] ,

We are still observing issues on Hulk P2, P3 run. Please find the log below. I am wondering if this TC is not part of the main script but is in the optimization script. The trail discussion mentioned the same. If it's not applicable for sanity, could you please remove it from the optimized script?

[solution_test_sanityecamb_lan.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py?at=refs%2Fheads%2Fprivate%2FHulkPatch2-ms%2Fapi-auto]  ===>  test case is not included.
[lansanity_usecases_maps.yaml|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/lansanity/lansanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulkPatch2-ms%2Fapi-auto] ====>  UC17.17
[nonlansanitysuite|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/nonlansanitysuite?at=refs%2Fheads%2Fprivate%2FHulkPatch2-ms%2Fapi-auto]==>UC17.17


Hulk  P2  Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE.py-1717-ISENDG_deletedDeviceInISENDG&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov23_06:17:28.774282.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE.py-1717-ISENDG_deletedDeviceInISENDG&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov23_06:17:28.774282.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]

Hulk  P3  Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE.py-1717-ISENDG_deletedDeviceInISENDG&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov19_21:35:34.683137.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE.py-1717-ISENDG_deletedDeviceInISENDG&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov19_21:35:34.683137.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]


Thanks ,
Omkar  Hi [~accountid:620b8357878c2f00729881c8]. in the failed log you’ve provided, [test1_select_device_to_be_remove|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE.py-1717-ISENDG_deletedDeviceInISENDG&begin=5828&size=2598&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov23_06:17:28.774282.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] return BLOCKED. When a sub-test is blocked, it should skip the testcase. so the issue here is the subtest does not skip the testcase when it failed. it’s not related to the old issue

about the optimized script, it’s was a mistake

so, i will close this ticket and create a new auton for is. I will provide Jira ticket here Closed this auton since i’ve create new auton for new issue
[https://miggbo.atlassian.net/browse/SEEN-3106|https://miggbo.atlassian.net/browse/SEEN-3106|smart-link] ","['Auton', 'Feature', 'Hulk', 'Optimized', 'Sanity', 'Yamlmapping']",QuangVinh Nguyen,Closed,Omkar Sharad Wagh
SEEN-1971,https://miggbo.atlassian.net/browse/SEEN-1971,[Auton][Hulk]-Task-aca_security_groups_policy.py-152-policytAcaValidations-Test_TC2_aca_test-test7_cleanup_bulk_contract,,2023-07-28T18:46:38.345+0000,"This could be some changes in ISE 3.3. Can you share your 3.3 ISE to look into? Currently, Ghost P2 RC3 execution is ongoing. Once the report is closed, I'll share the setup. Hi  [~accountid:62d2fe9f8afb5805e5d5af49]  ,

 Please find below 3.3  ISE details   
ISE : 10.30.0.101(admin/Lablab123)
Cluster details: 10.30.0.100(admin/Maglev123)  Cluster and ISE are available today only. Please check on the priority

Ghost P2 RC3: 2.1.614.70852
Polaris version: 17.11

 From log, ACL was deleted in DNAC but still existed in ISE after 5 mins. It should be bug.

I added some more log. Please try again. If still see the issue, please file defect.","['Hulk', 'Optimized', 'Sanity', 'Uplift', 'auton']",Tran Lam,Resolved,Omkar Sharad Wagh
SEEN-1972,https://miggbo.atlassian.net/browse/SEEN-1972,[Auton]: [ Hulk]: Task-eox_scan.py-1310-configpreview  /   Test_TC1_config_preview_treeview_cliconfig_enabled  /   test3_verify_treeview_cliconfig_enabled ,"{quote}*Reporter Analysis:* The sub-testcases [Task-eox_scan.py-1310-configpreview|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eox_scan.py-1310-configpreview&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul25_22:37:18.810068.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   [Test_TC1_config_preview_treeview_cliconfig_enabled|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eox_scan.py-1310-configpreview&begin=3596&size=43915&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul25_22:37:18.810068.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test3_verify_treeview_cliconfig_enabled  Failed  with below error:

*Description:  The error from log or more info* {quote}

{noformat}93: 
94:   api_switch_call called:
95:  {'params': {'activityId': '68d52374-eaf5-482a-b4fe-129da86b9e42', 'deviceId': 'f1d77289-d88b-4750-8b0a-c4c95fcbfe1a'}}
96:  Resource path full url: https://10.30.0.100/api/v1/dna/configpreview/summary
97:  config preview in json response {'isWirelessDevice': True, 'deviceId': 'f1d77289-d88b-4750-8b0a-c4c95fcbfe1a'}
98:  Traceback (most recent call last):
99:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper
100:      result = testfunc(func_self, **kwargs)
101:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/testcases/sanityusecases/configpreview/eox_scan.py"", line 110, in test3_verify_treeview_cliconfig_enabled
102:      if dnac_handle.verify_treeview_cliconfig_enabled(self.dev_details[0],self.dev_details[1]):
103:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/decorators.py"", line 32, in wrapper
104:      result = method(*args, **kwargs)
105:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/inventory/group.py"", line 2034, in verify_treeview_cliconfig_enabled
106:      if ""native"" in tree_view_json['configData']['root'] and ""CliConfig"" in tree_view_json['configData']:
107:  KeyError: 'configData'{noformat}



*Branch Name:private/Hulk-ms/sanity_api_auto-*

*Script file/Usecase :*[Task-eox_scan.py-1310-configpreview|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eox_scan.py-1310-configpreview&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul25_22:37:18.810068.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 
usecasemaps\lansanity\lansanity_usecases_maps.yaml

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0 issue*

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eox_scan.py-1310-configpreview&begin=39276&size=8027&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul25_22:37:18.810068.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-eox_scan.py-1310-configpreview&begin=39276&size=8027&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul25_22:37:18.810068.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-3|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-3] 


*Code  Snip*

{code:python}  def verify_treeview_cliconfig_enabled(self, task_id, netword_id):
        """"""
        Using this function we are going to check treeview and cliconfig options are availble in device config preview
        To call this function we need to pass config preview schedule task id and netwok device id
        """"""
        #check treeview and cliconfig options are enabled in ""configpreview summary"" api
        url = f""/v1/dna/configpreview/summary""
        params = {'activityId': task_id,'deviceId': netword_id}
        tree_view_json = self.services.api_switch_call(method=""GET"",resource_path=url,params=params)
        if not tree_view_json:
            self.log.error(f""config preview summary api does not have any response"")
            return False
        self.log.info(f""config preview in json response {tree_view_json}"")
        if ""native"" in tree_view_json['configData']['root'] and ""CliConfig"" in tree_view_json['configData']:
            self.log.info(f""config preview is having treeview and cliconfig options"")
            return True
        else:
            self.log.error(f""config preview is NOT having treeview and cliconfig options"")
            return False{code}",2023-07-28T19:11:15.639+0000,"This testcase was created under invalid assumptions, that even without config changes, config preview will be generated and have tree view available. It may have been working in the past due to some defects, but now its not really a valid scenario, as the config preview will not generate any configs unless changes have been made. Need to re-evaluate and probably completely revamp the feature.","['Auton', 'Hulk', 'Optimized', 'Sanity', 'ghost']",Andrew Chen,Cancelled,Omkar Sharad Wagh
SEEN-1973,https://miggbo.atlassian.net/browse/SEEN-1973,[Auton]:Hulk : Task-wireless_client_data_troubleshoot.py-1710-wirelessClientTroubleshoot/test3_get_log_file_content_and_validate,"*Reporter Analysis:*
During solution sanity on  Hulk  testing. We  observed that get_log_file_content_and_validat failed with the below  error 

912: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/dnaservices.py"", line 331, in __getattr__

{noformat}913: 
     raise AttributeError(err_msg){noformat}

{noformat}914: 
 AttributeError: 'DnaServices' object has no attribute 'get_log_file_content'{noformat}

*Found on:*
Uber ISO : Hulk 
Polaris version: 17.12

*Script file/Usecase :* lansanity_usecases_maps.yaml

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0 issue*

*Fail Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-wireless_client_data_troubleshoot.py-1710-wirelessClientTroubleshoot&begin=269700&size=16800&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul26_08:32:30.057971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-wireless_client_data_troubleshoot.py-1710-wirelessClientTroubleshoot&begin=269700&size=16800&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul26_08:32:30.057971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] 
",2023-07-28T20:07:39.568+0000,"PR has been raised: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6458/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6458/overview] Mentions PR has been approved and merged to Hulk Branch. Hi [~accountid:63f50bcf4e86f362d39acde5]  ,
TC failed with below  error  could  you please  check :   
827: 
     result = method(*args, **kwargs)

{noformat}828: 
 TypeError: get_log_troubleshoot_file_content() missing 1 required positional argument: 'troubleshoot_url'{noformat}

Failed Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-wireless_client_data_troubleshoot.py-171-wirelessClientTroubleshoot&begin=258610&size=6771&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug02_05:39:17.906663.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-wireless_client_data_troubleshoot.py-171-wirelessClientTroubleshoot&begin=258610&size=6771&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug02_05:39:17.906663.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ah yeah. Sorry. It’s failed because of my mistake. Addressed it in PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6496/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6496/overview]","['Auton', 'Hulk', 'Optimized', 'Sanity']",QuangVinh Nguyen,Resolved,Omkar Sharad Wagh
SEEN-1974,https://miggbo.atlassian.net/browse/SEEN-1974,[Auton]:Hulk: Task-application_policy.py-183-policyApplicationPolicy  /   Test_TC3_wireless_policy_PSK  /   test6_verify_policy_config ,"*Reporter Analysis:*
During solution sanity on  Hulk. We observed verify_policy_config  failed with below error:
{{7479: AttributeError: 'bool' object has no attribute 'items'}}

*Found on:*
Uber ISO : Hulk 
Polaris version: 17.12

*Branch Name:*  private/Hulk-ms/sanity_api_auto-

*Script file/Usecase :* lansanity_usecases_maps.yaml

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0 issue*

*Fail Log:* 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=2338089&size=57679&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul26_10:02:37.377505.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=2338089&size=57679&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul26_10:02:37.377505.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] 

Snip from LOg

{code:python}7470:  Traceback (most recent call last):
7471:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper
7472:      result = testfunc(func_self, **kwargs)
7473:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/testcases/sanityusecases/policyApplicationPolicy/application_policy.py"", line 275, in test6_verify_policy_config
7474:      if (dnac_handle.verify_wireless_device_config(res_val, types=""PSK"")):
7475:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/decorators.py"", line 32, in wrapper
7476:      result = method(*args, **kwargs)
7477:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/app_policy/group.py"", line 704, in verify_wireless_device_config
7478:      for key, value in dev_config.items():
7479:  AttributeError: 'bool' object has no attribute 'items'
7481:  Errored reason: 'bool' object has no attribute 'items'{code}",2023-07-28T20:22:14.967+0000,"Hi [~accountid:620b8357878c2f00729881c8], the root cause of this error is:

test4 failed so the variable ‘{{dev_config}}' returned '{{False}}’. At test6 when calling {{dev_config.items()}} got an error due to {{dev_config}} is not a {{dictionary}}.

I already add code to catch this error. Please pass test4 if you want to run pass test6. # PR Hulk-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6756/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6756/overview]
# Test Case:  {{TC_wireless_policy_PSK}}
# Testbed: TB7
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link Hulk-ms/api_auto:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-08/sanity_TB7.2023Aug25_01:44:27.418722.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-08/sanity_TB7.2023Aug25_01:44:27.418722.zip&atstype=ATS]","['Auton', 'Hulk', 'Optimized', 'Sanity']",NhanHuu Nguyen,Resolved,Omkar Sharad Wagh
SEEN-1975,https://miggbo.atlassian.net/browse/SEEN-1975,[Auton]:[Optimized ]Hulk : Task-random_mac_enable.py-195-randomMacEnable /Task-radius_profile_enable.py-196-radiusProfileEnable,"*Reporter Analysis:*
""During solution sanity in Hulk Optimization, we observed that 'Random MAC enabled' and 'Radius Profile enabled' are failing in the optimized code. Those test cases are passing when rerun in legacy   script .  We need to fix the YAML mapping on the optimized code.""

*Branch Name:*  private/Hulk-ms/sanity_api_auto

*Script file/Usecase:* lansanity_usecases_maps.yaml

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: 1)*[*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_custom_vlan_id.py-192-verifyCustomVlanID&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul26_11:36:06.772976.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_custom_vlan_id.py-192-verifyCustomVlanID&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul26_11:36:06.772976.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
2)[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-random_mac_enable.py-195-randomMacEnable&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul26_11:36:06.772976.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-random_mac_enable.py-195-randomMacEnable&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul26_11:36:06.772976.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:*[*https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7*|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] 

{{TESTSCRIPT_FILE : testcases/forty_eight_hour/solution_test_sanityecamb_lan.py}}
*Passlog:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9299193&size=758217&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul26_23:41:38.456980.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9299193&size=758217&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul26_23:41:38.456980.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

2)[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8442767&size=856426&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul26_23:41:38.456980.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8442767&size=856426&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul26_23:41:38.456980.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-07-28T20:37:24.970+0000,"The failure was because multiple VCR operations were running at the same time.

This issue should be addressed together with  [https://miggbo.atlassian.net/browse/SEEN-1990|https://miggbo.atlassian.net/browse/SEEN-1990|smart-link] ","['Auton', 'Hulk', 'Optimized', 'Sanity', 'Yamlmapping']",Tran Lam,Resolved,Omkar Sharad Wagh
SEEN-1976,https://miggbo.atlassian.net/browse/SEEN-1976,[Auton]:Hulk: Test_TC218_APs_negative_operations,"+*Normal Script  Issue*+ 

During solution sanity hulk testing in normal script execution    ([Test_TC218_APs_negative_operations|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=36045250&size=134761&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul28_02:13:38.386245.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]) failed with  Below Error , ,  others tc are blocked.

148058: 
  api_switch_call called:

{code:python}148059:  {'params': {'managementIpAddress': '204.1.1.64'}}
148060:  Resource path full url: https://10.30.0.100/api/v1/network-device
148061:  Library group ""inventory"" method ""get_device_hostname_from_IP"" returned in 0:00:00.023153
148062:  Library group ""fabric_wired"" method ""get_all_aps_from_inventory"" returned in 0:00:01.844584
148063:  Library group ""inventory"" method ""_get_site_with_max_aps"" returned in 0:00:01.844932
148064:  Site fetched with max APS: default-location
148065:  ERROR changing the site in the input file!!, please check if the input file has changed!!
148066:  Reason key error: list index out of range
148067:  Library group ""inventory"" method ""set_site_with_max_aps"" returned in 0:00:01.846032
148068:  Test returned in 0:00:04.491173
148069:  Failed reason: Failed setting up use case to choose a site that has APs!{code}



*Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=36045250&size=134761&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul28_02:13:38.386245.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=36045250&size=134761&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul28_02:13:38.386245.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

{{TESTSCRIPT_FILE : testcases/forty_eight_hour/solution_test_sanityecamb_lan.py}}
{{SECURE_FABRIC_FILE : solution_test_input.json}}
[solution_test_input.json|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/config_48hr_test/solution_test_input.json?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto]
*Branch Name:*  private/Hulk-ms/sanity_api_auto

*Script file/Usecase:* solution_test_sanityecamb_lan.py

*Source Team:  Sanity*



+*Optimized code  Issue*+ 

It seems like there might be an issue with the YAML mapping in your code that's causing the use case (UC) with ID 24 and the use case named ""ApsNegativeOperations"" to be skipped after merging a pull request (PR). To help you fix the issue, I would need to see the relevant trade log and Jenkins job details you mentioned.

PR:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/b281028b392302c28d998f2e6d09bb6ebf4efbe0|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/b281028b392302c28d998f2e6d09bb6ebf4efbe0]
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/12f66025a5cf4a6f675b1ed54c0ac3075ecf7079|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/12f66025a5cf4a6f675b1ed54c0ac3075ecf7079]

Trade Log:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-07/env_optimized_auto_job.2023Jul10_23:26:48.738929.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-07/env_optimized_auto_job.2023Jul10_23:26:48.738929.zip&atstype=ATS]

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-07/env_optimized_auto_job.2023Jul10_23:31:14.566849.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-07/env_optimized_auto_job.2023Jul10_23:31:14.566849.zip&atstype=ATS]

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-07/env_optimized_auto_job.2023Jul10_23:48:48.008373.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-07/env_optimized_auto_job.2023Jul10_23:48:48.008373.zip&atstype=ATS]

Jenkins Job:
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/240/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/240/]
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/241/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/241/]
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/242/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/242/]


[*sanity  TB:*|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7]
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7]

+*10.30.0.100(admin/Maglev123)*+
",2023-07-28T20:58:15.852+0000,"PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6503/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6503/overview] Mentioned PR has been approved and merged to Hulk Branch. Hi Team,
I have cherry-picked it for the PR above in the Sanity branch.
 We observed the following issue in Rerun, so we have reopened Auton   

In the scenario below, if we rerun the test case, it should be skipped. Alternatively, if there is any defect, the log collector should be enabled( RCA collection). 
:*Please re-provision them to either without Zone or same Zone part of new NetworkProfile. APZones: [Zone1_Enterprise]*
Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1314466&size=11553609&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug11_01:16:47.219925.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1314466&size=11553609&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug11_01:16:47.219925.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*8792: Config Preview Activity failed with reason: NCWL10219: One or more Access Points are provisioned in below Zones. Please re-provision them to either without Zone or same Zone part of new NetworkProfile. APZones: [Zone1_Enterprise]*
8793:
8794: Activity: 38ae88c9-7ef8-4eb9-b6b1-51a96820e8a5 Trigger job: {'id': '60a4159f-68ee-49ec-bbde-07a6ab198e44', 'triggeredJobTaskId': '82dcda86-7111-47b6-abee-1ecd64cba7b8', 'triggeredTime': 1691742270698, 'status': 'FAILED', 'failureReason': 'NCWL10219: One or more Access Points are provisioned in below Zones. Please re-provision them to either without Zone or same Zone part of new NetworkProfile. APZones: [Zone1_Enterprise]', 'triggeredJobId': '60a4159f-68ee-49ec-bbde-07a6ab198e44'} PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6787/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6787/overview]



A lot of the issues in this ticket has been addressed and new changes in this PR. But please if it is merged. Open a new ticket for new issues.  Hi  [~accountid:63f50bfce8216251ae4d59d5] 
Hulk P1 2.1.713.70263,
We observed that in the optimized code, the script is still not running UC 24. Could you please check? The issue has not been resolved.
*optimization  log:*
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-09/env_optimized_auto_job.2023Sep28_00:32:55.123854.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-09/env_optimized_auto_job.2023Sep28_00:32:55.123854.zip&atstype=ATS] This issue is being handled by Vinay. So I am closing this Auton. ","['Auton', 'Blocked', 'Feature', 'Hulk', 'Integration', 'Optimized', 'Sanity', 'Yamlmapping']",Moe Saeed,Closed,Omkar Sharad Wagh
SEEN-1977,https://miggbo.atlassian.net/browse/SEEN-1977,Test_TC58_Verify_DHCP_server_change_on_segments  /  test6_verify_VLAN_Virtual_ports should exclude VoIP-Null0 and SR0 ports that show up in the API output,"Test_TC58_Verify_DHCP_server_change_on_segments  /  test6_verify_VLAN_Virtual_ports should exclude VoIP-Null0 and SR0 ports that show up in the API output.

UI and Network Device API for Virtual Interface type is listing two additional interfaces which is not there in the Device’s CLI - ‘show interfaces’ output which is causing Error to the current script.
[https://{{dnac}}/api/v1/interface/network-device/c96a0ab0-cf68-47e8-af62-15336379b1a0?interfaceType=Virtual|https://{{dnac}}/api/v1/interface/network-device/c96a0ab0-cf68-47e8-af62-15336379b1a0?interfaceType=Virtual]

# {{""portName"": ""SR0""}}
# {{""portName"": ""VoIP-Null0""}}

*Errored Execution log from:*

# MS TB2: [test6_verify_VLAN_Virtual_ports|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=59817062&size=702717&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul28_07:27:42.476490.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# MS TB3: [test6_verify_VLAN_Virtual_ports|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=41261677&size=537585&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul21_11:29:22.823657.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Device CLI output:*

{noformat}NYC-FB-ASR#show interfaces | include line protocol
GigabitEthernet0/0/0 is down, line protocol is down 
GigabitEthernet0/0/1 is up, line protocol is up 
GigabitEthernet0/0/2 is down, line protocol is down 
GigabitEthernet0/0/3 is down, line protocol is down 
GigabitEthernet0/0/4 is down, line protocol is down 
GigabitEthernet0/0/5 is down, line protocol is down 
GigabitEthernet0/0/6 is down, line protocol is down 
GigabitEthernet0/0/7 is down, line protocol is down 
TenGigabitEthernet0/1/0 is down, line protocol is down 
TenGigabitEthernet0/1/1 is up, line protocol is up 
TenGigabitEthernet0/1/2 is down, line protocol is down 
TenGigabitEthernet0/1/3 is up, line protocol is up 
TenGigabitEthernet0/1/4 is down, line protocol is down 
TenGigabitEthernet0/1/5 is down, line protocol is down 
TenGigabitEthernet0/1/6 is down, line protocol is down 
TenGigabitEthernet0/1/7 is down, line protocol is down 
GigabitEthernet0 is up, line protocol is up 
LISP0 is up, line protocol is up 
LISP0.4097 is up, line protocol is up 
LISP0.4099 is up, line protocol is up 
LISP0.4101 is up, line protocol is up 
LISP0.4104 is up, line protocol is up 
LISP0.4105 is up, line protocol is up 
LISP0.4106 is up, line protocol is up 
LISP0.4107 is up, line protocol is up 
LISP0.4115 is up, line protocol is up 
Loopback0 is up, line protocol is up 
Loopback9 is up, line protocol is up 
Loopback1021 is up, line protocol is up 
Loopback1022 is up, line protocol is up 
Loopback1023 is up, line protocol is up 
Loopback1024 is up, line protocol is up 
Loopback1025 is up, line protocol is up 
Loopback1026 is up, line protocol is up 
Loopback1027 is up, line protocol is up 
Loopback1028 is up, line protocol is up 
Loopback1029 is up, line protocol is up 
Loopback1030 is up, line protocol is up 
Loopback1032 is up, line protocol is up 
Loopback2046 is up, line protocol is up 
Loopback3333 is up, line protocol is up 
Loopback4105 is up, line protocol is up 
Loopback4106 is up, line protocol is up 
Tunnel0 is up, line protocol is up 
Tunnel1 is up, line protocol is down 
Tunnel2 is up, line protocol is up 
Tunnel3 is up, line protocol is up 
Tunnel4 is up, line protocol is up 
Tunnel5 is up, line protocol is up 
Tunnel6 is up, line protocol is up 
Tunnel7 is up, line protocol is up 
Tunnel8 is up, line protocol is up 
Tunnel9 is up, line protocol is up 
Tunnel10 is up, line protocol is up 
Tunnel11 is up, line protocol is up {noformat}",2023-07-28T21:22:04.054+0000,"It turns out that we already have a “skip list” - {{INTERFACE_LIST}} defined inside {{solution_test_input.json}} file where “SR0” would be a new entry. PR raised for the required change: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6450/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6450/overview] Mentioned PR has been approved and merged to Hulk Branch.

Marking this ticket as “Done”.","['Auton', 'Hulk']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1978,https://miggbo.atlassian.net/browse/SEEN-1978,TC43_DNAC_EXT_NODE_interface_config_verifications - test2_dnac_ext_node_onboarding_verifications,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*Branch*: private/Hulk-ms/api-auto

*Script Name :* solution_test_sanity_ecamb.py

*Testbed :* TB4

*Testcases Impacted :*  TC136_enable_ICMP_ping_check_AP_reachability  - test2_deploy_AP_specific_configs_to_controller

During Hulk ESXI testing “TC136_enable_ICMP_ping_check_AP_reachability  - test2_deploy_AP_specific_configs_to_controller”  fails due to unable to join SSID - ‘SSIDDot1XIndiatb4’

*Fail Log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2178758&size=1573226&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul14_00:29:07.945971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2178758&size=1573226&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul14_00:29:07.945971.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=863796&size=60908&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun15_23:59:12.196482.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=863796&size=60908&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun15_23:59:12.196482.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-07-30T17:58:48.848+0000,,"['Auton', 'ESxi', 'Hulk']",KRISHNA MUKKU,Cancelled,KRISHNA MUKKU
SEEN-1980,https://miggbo.atlassian.net/browse/SEEN-1980,Auton:Test_TC204_Provision_single_AP_with_custom_rf_profile  /   test2_provision_ap_with_rf_profile,"*Reporter Analysis:* 

the Custom RF Profile related use-case is only working on AireOS Controllers.

Comment from [~accountid:63f50bf640328c12e4ec5b01]  - 


{noformat}Please check to see if eWLC was already provisioned with an AI profile, 
if so we have to make sure AI profile is removed first before provisioning custom RF profile, otherwise provisioning will fail for eWLC.{noformat}

*Description:*

{noformat}88636: 
 No wireless controller device items found{noformat}

{noformat}88642: 
 Failed reason: Failed to provision AP with custom RF profile{noformat}

*Failed log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=23071214&size=42967&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul27_11:01:10.564204.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=23071214&size=42967&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul27_11:01:10.564204.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Branch Name:* Ghost -ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:",2023-07-31T16:46:29.052+0000,"This usecase was not automated by me , please assign to original scriptor Duplicate of SEEN-1892; hence marking it as “Cancelled”.","['AWS_Sanity', 'Auton', 'Ghost', 'Integration']",Tran Lam,Cancelled,Anusha John
SEEN-1987,https://miggbo.atlassian.net/browse/SEEN-1987,[Auton] Hulk - Wireless Solution Sanity - Script Uplift Required to match the UI payload with minify=true as per Eng-Notes of CSCwh06964,"*Regression:* Solution Sanity (SSR) on On-Prem

*DNAC Release_Version Used:* Hulk_Intg_2.1.710.70462.iso (RC2)

*Branch:* rcdn/Hulk-ms/api-auto synced to main branch private/Hulk-ms/api-auto - before regression run start

*Script Used:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

*Issue Faced:*  Script Uplift Required to match the UI payload with minify=true as per Eng-Notes of CSCwh06964

*Fix Required Testcase:* TC77 ",2023-08-01T14:16:51.481+0000,"Blocked by CSCwf79263 that it was failed to do L3handoff for anchorvn in the automation testbed in RC2 build. 
Wait for a new handoff build with the fix to work on this uplifting. Commit: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/685be431df565db2b644e1444f47b55978d432ab|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/685be431df565db2b644e1444f47b55978d432ab]
Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=563722&size=1159497&archive=%2Fws%2Ftranlam-sjc%2Fpyats6%2Fusers%2Ftranlam%2Farchive%2F23-08%2Fsanity_ams2.2023Aug14_03:20:01.571888.zip&ats=%2Fws%2Ftranlam-sjc%2Fpyats6&submitter=tranlam&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=563722&size=1159497&archive=%2Fws%2Ftranlam-sjc%2Fpyats6%2Fusers%2Ftranlam%2Farchive%2F23-08%2Fsanity_ams2.2023Aug14_03:20:01.571888.zip&ats=%2Fws%2Ftranlam-sjc%2Fpyats6&submitter=tranlam&from=trade&view=all&atstype=pyATS]","['Auton', 'Hulk', 'Uplift', 'hulk-vm-sanity']",Tran Lam,Resolved,Yuvarani Iyamperumal
SEEN-1988,https://miggbo.atlassian.net/browse/SEEN-1988,[Auton] - Unable to retrieve the wireless client details for Assurance Top N client feature ,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/Hulk-ms/api-auto

*Uber ISO tested:* Hulk RC2 - 2.1.710.70462

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

When integrating the feature corresponding to Assurance Top n clients feature related to Hulk, we see first sub TC1 to get the details of wireless client (SR-MB1-WL-3) is failing despite we have intended wireless client available. Due to this is subsequent subTCs are Blocked.

This Wireless client - SR-MB1-WL-3 corresponds to SF site and it joins 9800 EWLC device part of the same SF site.

[FW-9800-1.cisco.com|http://FW-9800-1.cisco.com]>
[FW-9800-1.cisco.com|http://FW-9800-1.cisco.com]> sh wireless client sum | in  74ee.2af4.7769
74ee.2af4.7769 SF-AP31-2802E                                  WLAN 18   Run               11ac     None       Local
[FW-9800-1.cisco.com|http://FW-9800-1.cisco.com]>

[FW-9800-1.cisco.com|http://FW-9800-1.cisco.com]> show wlan summary

Number of WLANs: 8

h4. ID   Profile Name                     SSID                             Status 2.4GHz/5GHz Security                                                                                 6GHz Security

1    CiscoSensorProvisioning          CiscoSensorProvisioning          UP     [WPA2][802.1x][AES]
17   Single5KBand_SAN_PSK             Single5KBandMS1                  UP     [WPA2][FT + PSK][AES],[FT Enabled],MAC Filtering
18   OPEN_profile                     OPENMS1                          UP     [open]
19   Radius_ssid_profile              Radius_ssidMS1                   UP     [WPA2][802.1x][AES][PMF 802.1X]
20   Random_mac_profile               Random_macMS1                    UP     [WPA2][802.1x][AES][PMF 802.1X]
21   posture_profile                  postureMS1                       UP     [WPA2][802.1x][AES][PMF 802.1X]
26   GUEST_profile                    GUESTMS1                         UP     [open],MAC Filtering
31   SSIDDot1XIndia_profile           SSIDDot1XIndiaMS1                UP     [WPA2][802.1x][AES][PMF 802.1X]

[FW-9800-1.cisco.com|http://FW-9800-1.cisco.com]> 

*Snip of Wireless client 360 page from DNAC:*

!image-20230801-144313.png|width=1269,height=545!

!image-20230801-145009.png|width=977,height=602!

*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/Wireless+Assurance+-+Top+n+clients+with+highest+TX+drops|https://wiki.cisco.com/display/EDPEIXOT/Wireless+Assurance+-+Top+n+clients+with+highest+TX+drops]

*Failed log* - [Test_TC270_DNAC_assurance_top_n_clients_with_highest_Tx_drops|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2474112&size=60829&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb_multi_sites_mdnac.2023Aug01_06:20:35.096126.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-01T14:52:13.045+0000,"Hi [~accountid:62d2fec15d6f5fd2c3db8f9f] 

Could you please add your testbed info?

What is the software version of wlc {{FW-9800-1.cisco.com}}? Make sure the image version of wlc should be 17.12.01 or higher.

From Failed log, there was only have {{NY-ECA-9300}} with supported version 17.12.1. So the wireless client `SR-MB1-WL-3` connected to a different wlc which does not install supported image

Snip of failed log:

{noformat}Device NY-ECA-9300 with version 17.12.1prd6 is supported for Tx drops count{noformat} [~accountid:63f50bd34c355259db9ccc4d] : Currently we do not have the testbed in same state, It has been moved out to Ghost Patch2 RC1. The Software version which have used for testing is 17.12.1 prd6 FC1 on all the Polaris devices including 9800 EWLC and ECA devices.

As I have attached the snapshot and device output posted in description, the Wireless client has joined 9800 EWLC Controller not the ECA device. Raised PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6780/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6780/overview] [~accountid:63f50bd34c355259db9ccc4d] Why was it failed in your trade log in the PR? Was it bug or still script issue? Raised PR to update {{test1_preparing_before_test}} to pick up a wireless client for test case. Added {{goto=[""next_tc""]}} in subtest 2 {{test2_get_tx_metrics_from_ap360}} to block the test case in case we cannot get enough response data. It was failed due to the assurance response did not have enough data, so it blocked the test case. [~accountid:63f50bf5e8216251ae4d59cf] / [~accountid:63f50bd34c355259db9ccc4d] : Could you please commit the fixes to Hulk Patch branches as well? Currently most of then testing is being done on Hulk Patch1 and Hulk Patch2.

 Hi [~accountid:62d2fec15d6f5fd2c3db8f9f] 

I’ve raised PRs for Hulk Patch branches as follows. Please check it.

Hulk Patch1: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7117/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7117/overview]

Hulk Patch2: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7118/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7118/overview]","['Auton', 'Hulk', 'Integration', 'MSTB1', 'Multisite', 'Optimized', 'Sanity']",ThangQuoc Tran,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-1989,https://miggbo.atlassian.net/browse/SEEN-1989,[Auton]:Hulk: Task-assurance_client_highest_tx_drops.py-1714-assuranceClientHighestTxDrops ," *Profile:* Solution sanity 

*Branch used:* private/Hulk-ms/sanity_api-auto

*Uber ISO tested:* Hulk RC2 - 2.1.710.70462

*Script Used:* lansanity_usecases_maps.yaml

+*Issues Faced:*+
After adding *'l2_only:* True,' we observed the sanity run script failing to find the wireless client
Could you please find the devices with the role *'wireless-client”*

*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/Wireless+Assurance+-+Top+n+clients+with+highest+TX+drops|https://wiki.cisco.com/display/EDPEIXOT/Wireless+Assurance+-+Top+n+clients+with+highest+TX+drops]

Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_client_highest_tx_drops.py-1714-assuranceClientHighestTxDrops&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_22:14:21.768251.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_client_highest_tx_drops.py-1714-assuranceClientHighestTxDrops&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul17_22:14:21.768251.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-01T17:54:02.957+0000,"Hi [~accountid:620b8357878c2f00729881c8] 

Please add your testbed info details. Make sure the testbed has any wireless client to execute this test case. If it’s no wireless client this testcase would be blocked. Hi [~accountid:63f50bd34c355259db9ccc4d] ,

the wireless client are up  , please find below snapshot,
Cluster  details:10.30.0.100(admin/Maglev123)


!image-20230802-094744.png|width=1913,height=912! Hi [~accountid:63f50bd34c355259db9ccc4d] ,

Cluster Details: 10.105.227.31 (admin/Maglev123)

Wireless Clients are up on our testbed, However, we see the test case be blocked

!image-20230809-070529.png|width=1917,height=663!

Failed Log: [Test_TC222_DNAC_assurance_top_n_clients_with_highest_Tx_drops|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1311543&size=10969&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug08_23:29:21.094520.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Raised PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6780/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6780/overview] [~accountid:63f50bd34c355259db9ccc4d] Why was it failed in your trade log in the PR? Was it bug or still script issue? Raised PR to update {{test1_preparing_before_test}} to pick up a wireless client for test case. Added {{goto=[""next_tc""]}} in subtest 2 {{test2_get_tx_metrics_from_ap360}} to block the test case in case we cannot get enough response data. It was failed due to the assurance response did not have enough data, so it blocked the test case. Hi [~accountid:63f50bd34c355259db9ccc4d] ,
After pulling the latest code, we are getting the same results. Could you please check?

Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-09/env_optimized_auto_job.2023Sep25_00:06:49.747284.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-09/env_optimized_auto_job.2023Sep25_00:06:49.747284.zip&atstype=ATS]


[PR:|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7146]
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7146|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7146]





!image-20230925-071632.png|width=1919,height=907!

!image-20230925-071903.png|width=1920,height=891! Hi [~accountid:620b8357878c2f00729881c8] 

Just checked from the failed log, this test case is added from Hulk. See that it was executed on the cluster with Ghost {{2.3.5.3}}

{noformat}47:  Resource path full url: https://10.30.0.100/api/system/v1/maglev/release/current
48:  DNAC Version: 2.3.5.3
49: 
50: 
51:  Connected to  DNAC{noformat}

Please re-run it in the correct DNAC version.

The wlc is not supported to enable application telemetry, so the script will skip getting wireless client on this device:

{noformat}69:  Device TB7-SJ-eCA-BORDER-CP with PID C9500-48Y4C, C9500-48Y4C is not supported for enabling app telemtry{noformat} Hi [~accountid:63f50bd34c355259db9ccc4d]  ,

 I ran the correct cluster. Could you please check?""

*Jenkins  job  :*
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/542/parameters/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/542/parameters/]

Cluster  details :10.30.0.100(admin/Maglev123) - *Read-only access* 
Cluster  Running  on Hulk  2.1.713.70255:

!image-20230925-084757.png|width=1920,height=705!

API=[https://{{dnac}}/api/system/v1/maglev/release/current|https://{{dnac}}/api/system/v1/maglev/release/current]


{noformat}{
    ""response"": {
        ""name"": ""dnac"",
        ""version"": ""2.3.7.3.70255"",
        ""_limitedAccessPackages"": [],
        ""corePackages"": [
            ""rbac-extensions:2.1.713.1900008"",{noformat} Hi [~accountid:620b8357878c2f00729881c8] 

This testcase was blocked since script could not find any wireless client associated with AP and WLC from the testbed yaml file. From my debugging, there is no wireless client in the client list of devices on either the SJ or NYC site

!image-20230926-034659.png|width=828,height=576!

!image-20230926-034709.png|width=680,height=485!

Please add configurations for the wireless clients in the testbed yaml file, refer to `configs/sanity_tb1/SanityTB1.yaml` as follows:

!image-20230926-041923.png|width=429,height=230!

!image-20230926-041941.png|width=610,height=367! Hi [~accountid:63f50bd34c355259db9ccc4d] ,
Why is the script not able to find the 'wireless-client' role? You should use a regex pattern to find a role in the YAML. Could you please update the wiki , which  config required in YAML? It will be helpful for everyone

[SanityTB7.yaml|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb7/SanityTB7.yaml?at=private%2FHulkPatch-ms%2Fsanity_api_auto] Hi [~accountid:620b8357878c2f00729881c8] 

It failed to find wireless client on Sanity TB7 because the testbed YAML is missing topology linking between the wireless client and the AP which joined in the WLC.

Could you please add the topology config for the wireless client in the YAML file and re-run this testcase. Refer to the topology config in Sanity TB1 as the below snips.

[SanityTB1.yaml|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb1/SanityTB1.yaml?at=refs%2Fheads%2Fprivate%2FHulkPatch-ms%2Fsanity_api_auto] Sure  , [~accountid:63f50bd34c355259db9ccc4d]  
I'll update the YAML and share the results. Hi [~accountid:63f50bd34c355259db9ccc4d] 

Executed TC Test_TC1_DNAC_assurance_top_n_clients_with_highest_Tx_drops using optimized script failed due to unable to fetch the wireless client from Testbed yaml 

Wireless clients are successful showing on the ECA  BORDER device & Assurance client as well

*Note:  This is EXSI VM TB*

*Used Optimized Script.*

*OVA:* 3.713.75159

*Branch: private/HulkPatch-ms/api-auto*

*Here is the Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_client_highest_tx_drops.py-171-assuranceClientHighestTxDrops&begin=5345&size=20693&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct19_05:36:35.863022.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_client_highest_tx_drops.py-171-assuranceClientHighestTxDrops&begin=5345&size=20693&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct19_05:36:35.863022.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Raised PRs to update get wireless client:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7590/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7590/overview]
* HulkP1: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7589/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7589/overview]
* HulkP2: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7588/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7588/overview]","['Auton', 'Blocked', 'Feature', 'Hulk', 'Integration', 'Optimized', 'Sanity']",ThangQuoc Tran,Resolved,Omkar Sharad Wagh
SEEN-1990,https://miggbo.atlassian.net/browse/SEEN-1990,[Legacy_Optimized_Code_mapping] Testcase and Usecase mapping are missed for few tests in HULK,"The intention of this ticket is to track the testcases that are not part of any usecase and are part of the usecases but are missed from mapping yaml files such as: 

*usecasemaps/lansanity/lansanity_usecases_maps.yaml*

Branch referred here is: *private/Hulk-ms/sanity_api_auto*

legacy script referred to: *testcases/forty_eight_hour/solution_test_sanityecamb.py*


*Below are the tests that are NOT part of any usecase :*
1. Test_TC159_cisco_telemetry_broker_as_netflowcollector 

2. Test_TC170_verify_global_level_events

3.Test_TC172_true_trace_between_clients 

4.Test_TC181_generate_Worst_Interferers_report 

5.Test_TC185_manually_disable_clients_on_wlc 

6.Test_TC193_enable_AP_parameters_recurrence_using_workflow 

7.Test_TC209_remediation_network_settings 

8. Test_TC216_configure_device_into_specific_NDG 

9.Test_TC129_Disconnect_Delete_Reason 

10.Test_TC141_verify_port_channel 



*Below are the tests that are part of usecases mentioned in brackets but are missed from usecase mapping files:*

# Test_TC166_wireless_posture_url_filter (UC: wired_wireless_posture)
# Test_TC167_wired_AEN_posture (UC: wired_wireless_posture)
# Test_TC174_syslog_server_event_notification (UC:syslog_server_event_notification)
# Test_TC187_Check_Network_Security_Trust_Settings  (UC: checkNetworkSecurityTrustSettings )
# Test_TC188_netconf_tdl_notification_for_swim  (UC: notificationForSwim )
# Test_TC189_generate_device_link_AP_flap_issues  (UC: generateDeviceLinkAPFlapIssues )
# Test_TC190_generate_Port_Reclaim_report (UC: generatePortReclaimReport )
#  Test_TC191_edit_site_name (UC: sitesModification)
# Test_TC194_dnac_rlan_workflow (UC: enableAPParametersRecurrence )
#  Test_TC195_default_route_verification  (UC:defaultRouteVerification )
# Test_TC197_N_plus_one_wlc (UC: nplus1wlc)
# Test_TC218_troubleshoot_bad_health_device  (UC: networkReasonal)
# Test_TC79_hitless_authentication (UC: MSDnacHitlessAuth / sdaHitlessAuthSwitch )
# Test_TC73_DNAC_verify_aaa_lisp_radius_configuration_on_devices_through_scheduled_job  (UC: MSDnacVerifyLispAfterReload )",2023-08-02T11:14:36.077+0000,"Commit:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/564bfa113c514c12f73c6eb7302c70379faa4558|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/564bfa113c514c12f73c6eb7302c70379faa4558]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a2dc468ec4ea617962cd75cf5448dd8d9de9b76b|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a2dc468ec4ea617962cd75cf5448dd8d9de9b76b] Thanks [~accountid:62d2fe9f8afb5805e5d5af49]  for the fix. 
I will try to include these new-usecases in current runs and will update the results here!","['Auton', 'Blocked', 'Ghost', 'Hulk', 'Optimized', 'sanity']",Tran Lam,Resolved,Ashwini R Jadhav
SEEN-1991,https://miggbo.atlassian.net/browse/SEEN-1991,[Auton][MSTB3] : Talos is failing witha attribute error,"DNAC Release_Version Tested:Hulk RC1 Uber ISO - 2.1.710.70446, Non-FIPS

Device Image Used: 17.12.1

Testbed: AWS-Multisite

Branch Used: private/Hulk-ms/api_auto

Script Name: solution_test_3sites_sjc_nyc_sf.py

Solution Input File : dnac-auto/configs/config_48hr_test/solution_test_input.json

Testcases Impacted: Test_TC166_cloud_Talos_anomaly_detection

Failed Trade Log: Test_TC166_cloud_Talos_anomaly_detection

Issue Analysis: AttributeError: 'DnaServices' object has no attribute 'get_clients_with_talos_validation'",2023-08-02T11:56:28.675+0000,Failed log: [Test_TC166_cloud_Talos_anomaly_detection|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1649288&size=206711&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fauto_MS_job.2023Aug01_03:05:50.540554.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6504/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6504/overview],"['Auton', 'Hulk', 'MSTB3', 'Regression']",Moe Saeed,Resolved,Balaji Raju
SEEN-1992,https://miggbo.atlassian.net/browse/SEEN-1992, [Auton]:Hulk :  Test_TC142_Add_interace_description  /   test1_add_interface_description ,"*Reporter Analysis:*
During solution sanity on  Hulk testing. We  observed that Tc failed for  TB7-SJ-EDGE & TB7-NY-FIAB doesn't  connect any Access Point, But  cluster  all  ap’s connected   +*NY-Fiab*+ 

!image-20230802-131822.png|width=1184,height=520!

  +*EDGE -node  snip:*+


!image-20230802-132131.png|width=1856,height=583!


+*Error Snip:*+

{noformat}5142:  Cannot track test: tracking auth info must be set in order to transfer test tracking data
5207:  The device: TB7-SJ-EDGE doesn't connect any Access Point
5260:  The device: TB7-NY-FIAB doesn't connect any Access Point
5261:  Failed to get the interface or access point on the device TB7-NY-FIAB.
5263:  Failed reason: interface description validation failed{noformat}

*Found on:*
Uber ISO : Hulk 
Polaris version: 17.12

*Script file/Usecase :* lansanity_usecases_maps.yaml

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0 issue*

*Fail Log:*[*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=976670&size=28946&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul31_08:14:04.750450.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=976670&size=28946&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul31_08:14:04.750450.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


+*Optimization script  Failed Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-add_interace_description.py-171-addInteraceDescription&begin=4763&size=53687&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug02_06:25:35.797080.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-add_interace_description.py-171-addInteraceDescription&begin=4763&size=53687&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug02_06:25:35.797080.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] 

+*Devices CDP  details:*+

*TB7-NY-FIAB#sh cdp neighbors*
Capability Codes: R - Router, T - Trans Bridge, B - Source Route Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater, P - Phone,
                  D - Remote, C - CVTA, M - Two-port Mac Relay 

Device ID        Local Intrfce     Holdtme    Capability  Platform  Port ID
APF01D.2D1C.1AA8 Ten 1/0/27        136              R T   C9130AXE- Gig 0
TB7-DM-TSIM      Gig 1/0/11        154               H    AIR-CT550 Gig 0/0/2
TB7-DM-TSIM      Gig 1/0/12        154               H    AIR-CT550 Gig 0/0/4
[TB7-Fusion.cisco.com|http://TB7-Fusion.cisco.com]
                 Ten 2/0/1         165             R S I  WS-C3850- Ten 1/1/2
SEPA4B439731D12  Gig 1/0/24        123             H P M  IP Phone  Port 1
[TB7-Transit.cisco.com|http://TB7-Transit.cisco.com]
                 Ten 2/0/3         165             R S I  C9300-24U Ten 1/1/4
APA00F.379C.3820 Ten 1/0/28        138              R T   C9120AXP- Gig 0
APDC8C.3796.20EC Ten 1/0/35        122              R T   AIR-AP480 Gig 0
AP70F3.5A7A.1470 Ten 1/0/37        118              R T   AIR-AP180 Gig 0
AP5CE1.7629.CEF0 Gig 1/0/14        158              R T   C9120AXP- Gig 0
MGMT_NW_123.x.x.x
                 Gig 0/0           121              S I   UA-C3850- Gig 1/0/9
AP687D.B45C.2054 Gig 1/0/13        129              R T   C9136I-B  Gig 1
AP5CE1.7629.C894 Gig 1/0/15        146              R T   C9120AXP- Gig 0
[SN-JAE242302CZ.cisco.com|http://SN-JAE242302CZ.cisco.com]
                 Ten 1/0/48        154              S I   C9200L-48 Gig 1/0/48
[SN-JAE242302CZ.cisco.com|http://SN-JAE242302CZ.cisco.com]
                 Ten 1/0/47        138              S I   C9200L-48 Gig 1/0/47

Total cdp entries displayed : 15
TB7-NY-FIAB#





*TB7-SJ-EDGE#sh cdp neighbors*
Capability Codes: R - Router, T - Trans Bridge, B - Source Route Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater, P - Phone,
                  D - Remote, C - CVTA, M - Two-port Mac Relay 

Device ID        Local Intrfce     Holdtme    Capability  Platform  Port ID
[SN-FOC2311Y129.cisco.com|http://SN-FOC2311Y129.cisco.com]
                 Ten 1/0/47        143              S I   WS-C3560C Gig 0/12
[SN-FOC2311Y129.cisco.com|http://SN-FOC2311Y129.cisco.com]
                 Ten 1/0/48        176              S I   WS-C3560C Gig 0/11
AP70F3.5A7A.13C8 Two 1/0/24        150              R T   AIR-AP180 Gig 0
AP1416.9D2E.1FD4 Two 1/0/14        120              R T   C9130AXE- Gig 0
TB7-DM-TSIM      Two 1/0/1         136               H    AIR-CT550 Gig 0/0/1
TB7-DM-TSIM      Two 1/0/2         136               H    AIR-CT550 Gig 0/0/3
[TB7-SJ-eCA-BORDER-CP.cisco.com|http://TB7-SJ-eCA-BORDER-CP.cisco.com]
                 For 1/1/1         166             R S I  C9500-48Y Hun 2/0/49
AP3C41.0EFE.20C0 Two 1/0/35        167              R T   C9130AXI- Gig 0
MGMT_NW_123.x.x.x
                 Gig 0/0           145              S I   UA-C3850- Gig 1/0/7
[SN-FDO2515JDSL.cisco.com|http://SN-FDO2515JDSL.cisco.com]
                 Ten 1/0/38        165              S I   IE-9320-2 Gig 1/0/16

Total cdp entries displayed : 10

",2023-08-02T13:14:23.732+0000,"Hi [~accountid:620b8357878c2f00729881c8] and [~accountid:62ab7a399cd13c0068b18fe0], There is no AP appearing on the inventory page with your Testbed.

Could you please check again and onboard the AP? Then, check if the EWLC device and ECA are connected with AP

!image-20230831-101116.png|width=1912,height=949! Root Cause Auton:

The library run the command line {{""show ap summary""}} with {{""EDGENODE""}}, but it can’t be. That is why it failed and show

{{5207: The device: TB7-SJ-EDGE doesn't connect any Access Point }}

{{ 5260: The device: TB7-NY-FIAB doesn't connect any Access Point}}

!image-20230831-101741.png|width=831,height=312! Hi [~accountid:63f50bcafb3ac4003fa2c6dd]  , 


When did you check in the cluster? Currently, the cluster execution is ongoing. How can you see an AP in the cluster without running the AP on-boarding test cases? Hi [~accountid:620b8357878c2f00729881c8], I checked your testbed yesterday and saw the AP has yet to be on-boarding. So, please onboard the AP and I can continue to check this issue. Hi  [~accountid:63f50bcafb3ac4003fa2c6dd]  ,


In the Background  execution, it's currently ongoing. Please use read-only access.
Please find the below snapshot:

*eWLC:* sh AP  summary 

!image-20230901-111819.png|width=1853,height=870!



*eCA:* Sh ap  summary 


!image-20230901-111753.png|width=1874,height=868! Hi [~accountid:620b8357878c2f00729881c8], I already checked your testbed and saw the AP was onboarding. I will be working on it and use read-only access. # PR Hulk-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6851/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6851/overview]
# Test Case:  {{TC_Test_TC142_Add_interace_description}}
# Testbed: TB1
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link Hulk-ms/api_auto:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-09/sanity_TB1.2023Sep01_01:10:00.588548.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-09/sanity_TB1.2023Sep01_01:10:00.588548.zip&atstype=ATS] Hi [~accountid:63f50bcafb3ac4003fa2c6dd]  ,

Even after pulling the latest code, the TC is still failing. Could you please check?

PR:[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7214/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7214/overview]


+Failed Log:+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7243039&size=48753&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_05:04:47.281218.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7243039&size=48753&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_05:04:47.281218.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] 

+*Branch:*+  *private/HulkPatch-ms/sanity_api_auto*
 Hi [~accountid:620b8357878c2f00729881c8], After checking your new log, this is another issue:

{noformat}30204: Failed to grep the interface TwoGigabitEthernet1/0/35 description{noformat}

{noformat}30206: Failed reason: interface description validation failed{noformat}

It is different from the old issue:

{{5207: The device: TB7-SJ-EDGE doesn't connect any Access Point}}

{{5260: The device: TB7-NY-FIAB doesn't connect any Access Point}}

This failed message was raised from the below code. That means the first issue was resolved and run to the next step.

I will check this fail. PR HulkPatch1: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7311/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7311/overview]

Trade log link: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-10/sanity_TB1.2023Oct05_02:44:46.989696.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-10/sanity_TB1.2023Oct05_02:44:46.989696.zip&atstype=ATS]

Testbed: TB1","['Auton', 'Hulk', 'Optimized', 'Sanity']",NhanHuu Nguyen,Resolved,Omkar Sharad Wagh
SEEN-1993,https://miggbo.atlassian.net/browse/SEEN-1993,Enhance Test_TC0_dnac_initial_cleanup  /  test1dnac_installing_packages for ESXi VM DNAC,"Enhance Test_TC0_dnac_initial_cleanup  /  test1dnac_installing_packages for ESXi VM DNAC.

As per communication from Dinesh ([dineshj@cisco.com|mailto:dineshj@cisco.com]), we need to make use of below API to find the list of all packages and use the “status” value to find if it’s “Deployed” or not:

{noformat}https://{{dnac}}/api/v1/system-orchestrator/software-management/releases/installed{noformat}",2023-08-02T23:03:28.518+0000,"Raised required PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6495/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6495/overview] Mentioned PR got approved and merged to Hulk Branch.

Marking this automation Auton as “Done”.","['Automation', 'Auton']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-1994,https://miggbo.atlassian.net/browse/SEEN-1994,[Auton]:Test_TC6_DNAC_Prime_Migration_DNAC_Server_Settings_NTP_DHCP_AAA_DNS_settings/test29_configure_ap_pnp_workflow,"In recent Ghost & Hulk sanity Executions, We are observing that the configuration of Dot1X Ap Profile is set to No-Auth instead of EAP-FAST

*Reporter analysis:*

We are lately observing that the Configure Ap Pnp Workflow testcase under TC6 of Lan script and cert lan script which is to configure the Default AP Profile for Dot1X Ap configuration with EAP-FAST Method as Auth Type is not getting configured and instead stays as No-Auth on DNAC due to which the Dot1X AP onboarding has been failing, The test case is getting false passed

*Uber ISO Version tested:* Ghost P2 RC1 #2.3.5.4-70815

*Script Name:* Lan Script & Cert Lan Script

*Branch used:* private/Ghost-ms/sanity_api_auto, private/Hulk-ms/sanity_api_auto, private/Ghost-ms/sanity_delay_testing & private/Hulk-ms/sanity_delay_testing

*Script used:* solution_test_sanityecamb_cert_lan.py & solution_test_sanityecamb_lan.py

*Snip from the log:* 

16586:   api_switch_call called:
16587:  {'params': {'key': '[pnpdot1x.info|http://pnpdot1x.info]'}, 'data': [{'instanceType': 'pnpdot1x', 'namespace': 'wlan', 'type': 'pnpdot1x.setting', 'key': '[pnpdot1x.info|http://pnpdot1x.info]', 'value': [{'userName': 'apacess', 'password': 'Lablab#123', '*authType': 'EAP-FAST'*}], 'groupUuid': '-1', 'inheritedGroupUuid': '', 'inheritedGroupName': ''}]}

*Log:*[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3289385&size=8977&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul31_20:39:12.592199.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3289385&size=8977&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul31_20:39:12.592199.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-03T06:56:54.296+0000,"Hi [~accountid:62d2fe9f8afb5805e5d5af49],

The same issue observed  on +*Hulk RC3-2.1.710.70479*+
 
+*Failed Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_integration_verification_aca_sync.py-32-ISEIntegration&begin=178719&size=12581&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug03_01:10:38.988684.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_integration_verification_aca_sync.py-32-ISEIntegration&begin=178719&size=12581&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug03_01:10:38.988684.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Why is it auton, the API is success. Raise a DNAC defect on why the Auth type is not changing. Reopen if there is any actual script issue. Hi [~accountid:5f3c6ae932360700388f7b4b] / [~accountid:63f50bfce8216251ae4d59d5] 
I checked with ap_profile DE Vijayandar (vveesam), there is a change in the api payload that is required for updating ap-profile auth type

*API payload being used in script:*
{'params': {'key': '[pnpdot1x.info|http://pnpdot1x.info]'}, 'data': [{'instanceType': 'pnpdot1x', 'namespace': 'wlan', 'type': 'pnpdot1x.setting', 'key': '[pnpdot1x.info|http://pnpdot1x.info]', 'value': [{'userName': 'apacess', 'password': 'Lablab#123', 'authType': 'EAP-FAST'}], 'groupUuid': '-1', 'inheritedGroupUuid': '', 'inheritedGroupName': ''}]}

URL: [https://10.195.247.188/api/v1/commonsetting/wlan/-1|https://10.195.247.188/api/v1/commonsetting/wlan/-1]

*The required payload for reference is here :* 

{
    ""response"": [
        {
            ""instanceType"": ""approfile"",
            ""instanceUuid"": ""14704f67-8e34-40f5-96d8-502442961d30"",
            ""namespace"": ""wlan"",
            ""type"": ""approfile.setting"",
            ""key"": ""approfile.info.default-ap-profile"",
            ""version"": 4,
            ""value"": [
                {
                    ""deviceType"": ""IOS-XE"",
                    ""apProfileName"": ""default-ap-profile"",
                    ""description"": ""Default AP Profile for IOS-XE"",
                    ""remoteWorkerEnable"": false,
                    ""authType"": ""EAP-FAST"",
                    ""dot1xUsername"": ""apacess"",
                    ""dot1xPassword"": ""zjOAONu8k6+agOFY3UJ4Lg=="",
                    ""sshEnabled"": false,
                    ""telnetEnabled"": false,
                    ""mgmtUserName"": null,
                    ""mgmtPassword"": """",
                    ""mgmtEnablePassword"": """",
                    ""cdpState"": true,
                    ""awipsEnabled"": true,
                    ""awipsForensicEnabled"": false,
                    ""rogueDetection"": true,
                    ""meshEnabled"": false,
                    ""meshSetting"": null,
                    ""powerProfileName"": null,
                    ""calendarPowerProfiles"": [],
                    ""countryCode"": ""unconfigured"",
                    ""timeZone"": ""NOT_CONFIGURED"",
                    ""tzOffsetHour"": 0,
                    ""tzOffsetMin"": 0,
                    ""clientLimit"": 0,
                    ""rogueDetectionReportInterval"": 10,
                    ""rogueDetectionTransientInterval"": 0,
                    ""rogueDetectionMinRssi"": -90,
                    ""pmfDenialEnabled"": false,
                    ""brownfield"": false
                }
            ],
            ""groupUuid"": ""-1"",
            ""inheritedGroupUuid"": """",
            ""inheritedGroupName"": """"
        },
        {
            ""instanceType"": ""approfile"",
            ""instanceUuid"": ""9f60fc6b-8c66-4ae8-a85b-2875bb017ced"",
            ""namespace"": ""wlan"",
            ""type"": ""approfile.setting"",
            ""key"": ""approfile.info.Default_AP_Profile_AireOS"",
            ""version"": 2,
            ""value"": [
                {
                    ""deviceType"": ""AireOS"",
                    ""apProfileName"": ""Default_AP_Profile_AireOS"",
                    ""description"": ""Default AP Profile for AireOS"",
                    ""remoteWorkerEnable"": false,
                    ""authType"": ""NONE"",
                    ""dot1xUsername"": null,
                    ""dot1xPassword"": """",
                    ""sshEnabled"": false,
                    ""telnetEnabled"": false,
                    ""mgmtUserName"": null,
                    ""mgmtPassword"": """",
                    ""mgmtEnablePassword"": """",
                    ""cdpState"": true,
                    ""awipsEnabled"": false,
                    ""awipsForensicEnabled"": false,
                    ""rogueDetection"": true,
                    ""meshEnabled"": false,
                    ""meshSetting"": null,
                    ""powerProfileName"": null,
                    ""calendarPowerProfiles"": [],
                    ""countryCode"": ""unconfigured"",
                    ""timeZone"": ""NOT_CONFIGURED"",
                    ""tzOffsetHour"": 0,
                    ""tzOffsetMin"": 0,
                    ""clientLimit"": 0,
                    ""rogueDetectionReportInterval"": 10,
                    ""rogueDetectionTransientInterval"": 0,
                    ""rogueDetectionMinRssi"": -90,
                    ""pmfDenialEnabled"": false,
                    ""brownfield"": false
                }
            ],
            ""groupUuid"": ""-1"",
            ""inheritedGroupUuid"": """",
            ""inheritedGroupName"": """"
        }
    ],
    ""version"": ""1.0""
} PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7210/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7210/overview] Mentioned PR has been approved and merged to *private/HulkPatch-ms/api-auto* branch and cherry-picked to *private/Hulk-ms/api-auto* branch.

[~accountid:61efa8c457b25b006877eda3]  / [~accountid:63f50be24e86f362d39acde8] / [~accountid:620b8357878c2f00729881c8] , pls. validate the latest code and confirm on the status. Sure [~accountid:62ab7a399cd13c0068b18fe0]
We will pull the latest code and attempt this in our current regression cycle and update the results
 Hello Team,

We have pass log from the Latest Hulk run
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2055531&size=16474&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct04_02:08:30.121788.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2055531&size=16474&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct04_02:08:30.121788.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

!image-20231010-162638.png|width=1904,height=917!","['Auton', 'Ghost', 'Hulk', 'HulkPatch', 'Optimized', 'Sanity', 'Uplift']",Moe Saeed,Resolved,DeepakPratap Shinde
SEEN-1995,https://miggbo.atlassian.net/browse/SEEN-1995,[Auton]:Test_TC14_DNAC_configure_multicast_primary_border_as_rp  /   test1_configure_multicast_on_site_sjc,"*Reporter Analysis:*

Recently on AWS Sanity Upgrade Runs we observed that TC 14 is getting failed due to wrong “{{{'params': {'name': 'Global/USA/SAN_JOSE'}}}}""

Same testcase is passing in ON-PREM Solution Upgrade Sanity and it is where it is different  “{{{'params': {'name': 'Global/USA/SAN_JOSE_US_SJ_Fabric1'}}}}""

Description :

{noformat}20696: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Hulk/Hulk-Upgrade-Verify-common-Multi-job/services/dnaserv/lib/api_groups/fabric_multicast/group.py"", line 144, in _multicast_config_ip_pool_vn_mapping{noformat}

{noformat}20697: 
     vn_id = self.services.get_virtual_network_info_list(f'{vn_name}-{fabric_name}')[0]['id']{noformat}

{noformat}20698: 
 IndexError: list index out of range{noformat}


Pass Log from ON-PREM Solution Upgrade Sanity:[*Upgrade was done from* *Guardian P4 RC4-RSPIN# 2.1.518.72328 <>Hulk RC2(2370)#2.3.7.0.70462]*:


[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9779718&size=264858&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul29_08:14:40.371447.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9779718&size=264858&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul29_08:14:40.371447.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Failed log from AWS Solution Sanity Upgrade [*Upgrade done from Ghost P1 RC6(2353)# 2.1.613.70194<>Hulk RC2(2370)#2.3.7.0.70462]:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5925162&size=15973&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug02_10:23:57.593958.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5925162&size=15973&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug02_10:23:57.593958.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-03T10:19:34.172+0000,"[ENG-SDN / dnac-auto / dcaece7d15c - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/dcaece7d15c471fb498b979a64907ed2a62d21fa]

Added fixes for some possible suffix issues, including the one thats causing this auton [~accountid:63f50bcece6f37e5ed93c87e] 

Observed same issue in EXSI VM Hulk latest build 3.710.75530: 

Script is looking for in Sanjose Site “SAN_JOSE_US_SJ_Fabric1“ which is found

18938: Site is: SAN_JOSE

18939: Fabric site is: Global/USA/SAN_JOSE_US_SJ_Fabric1

Failed log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3981551&size=6029&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug24_03:29:35.240719.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3981551&size=6029&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug24_03:29:35.240719.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 



*VM Build used:* 3.710.75530

*Branch Name:*  private/Ghost-ms/sanity_api_auto

*Script file/Usecase:* 
after_upgrade_verify.py

*Source Team:  EXSI Upgrade Sanity* Please check the input file you are using for the correct fabric name WITHOUT the suffix Verified fabric name  on the *solution_test_input.json* file showing expected fabric name no mismatch found.



 {
  ""vn-name"": ""WirelessVNFB"",
  ""fabric-name"": ""Global/USA/SAN_JOSE"",
  ""mcast-pools"": [
    { Still observed same issue latest EXSI VM 2.3.7.3  #3.710.75122 build
Please look into issue as priority 
 Failed log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2785891&size=6029&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep21_23:25:06.877663.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2785891&size=6029&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep21_23:25:06.877663.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] {{./configs/upgrade/ghost_to_hulk_solution_test_input_upgrade.json}} this is the solution input file being used, which does have the suffix. The question is, if its an upgrade cluster, and ghost branch has the suffix, why is the suffix not there in the tb? I suspect that solution input file for ghost doesnt have the suffix, and so its mismatching when it comes. But the solution input file I checked for ghost has the suffix… so wonder if you are using the same one I checked or a different one. [~accountid:63f50bd68ab3d6a635ecc29b] please provide the ghost run logs. (Also, please open a new defect as this is now a different issue than the original auton.","['Auton', 'Hulk', 'Upgrade', 'exsivm', 'hulk-vm-sanity']",Andrew Chen,Resolved,Anusha John
SEEN-1999,https://miggbo.atlassian.net/browse/SEEN-1999,"method to disable ""maglev shell"" on SIT TB is failing while handling ""prompt""","method to disable ""maglev shell"" on SIT TB is failing while handling ""prompt""

Failed execution log: [Test_TC1_DNAC_RBAC_create_users_roles|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-rback_roles_users_configs.py-31-systemUserAndRoleback&begin=4109&size=195944&archive=%2Froot%2F.pyats%2Farchive%2F23-08%2Fsanity_TB1_cert.2023Aug01_10:15:32.399428.zip&ats=%2Fws%2Favdas-sjc%2Fpyats-new&submitter=root&from=trade&view=all&atstype=pyATS]

{noformat}259:  Using persistent connection.
260:  Calling method `send_cmd`.
261:  ('10.195.214.103', 'maglev', 'xxxxxxxx')
262:  Received login response:
263: 
264: 
265:  Welcome to the Maglev Appliance
266: 
267:    System information as of Tue Aug  1 17:45:25 UTC 2023
268: 
269:    System load:                   6.01
270:    Usage of /:                    3.7% of 89.45GB
271:    Memory usage:                  54%
272:    Swap usage:                    0%
273:    Processes:                     1272
274:    Users logged in:               0
275:    IP address for node-local-dns: 169.254.20.10
276:    IP address for management:     17.1.104.103
277:    IP address for enterprise:     10.195.214.103
278:    IP address for cluster:        169.254.6.66
279:    IP address for tunl0:          169.254.38.0
280: 
281:  Maglev Restricted Shell is active
282:  Last login: Tue Aug  1 17:45:24 2023 from 172.29.76.64
283: 
284:  [Tuesday Aug 01 17:45:27 UTC] maglev@169.254.6.66 (maglev-master-169-254-6-66)
285:  Disabling secure shell
286:  Sending line: `_shell -c 'sudo magctl ssh shell bash'`

287:  Expected prompts' list: [<class 'pexpect.exceptions.TIMEOUT'>, <class 'pexpect.exceptions.EOF'>, '\\[PEXPECT\\][\\$\\#] ', '\\[sudo\\] password for \\w+:', ""\\[administration\\] username for \\'[0-9a-zA-Z.:/-]+\\':"", ""\\[administration\\] password for \\'\\w+\\':"", 'Password:', 'Activity within this shell can jeopardize the functioning of the system!', 'Successfully enabled bash for user, will be effective from next login']
288:  Encountered Secure shell prompt for password
289:  Index: 6 Context:

290:  Sending required credential for secure shell prompt: `xxxxxx`
291:  Received WARNING message.
292:  Index: 7 Context:
293:  Warning!
294:  Encountered sudo password prompt with auto-auth
295:  Index: 3 Context:

296:  Sending required sudo credential: `xxxxxx`
297:  Parsed timeout: sending interrupt.
298:  Session closed.
299:  ('10.195.214.103', 'maglev', 'xxxxxxxx')
300:  Encountered error on login. Check login details or try again.  Error details:
301:  Could not establish connection to host
302:  Encountered error on login. Check login details or try again.  Error details:
303:  Could not establish connection to host
304:  Encountered error during excecution of `send_cmd`
305:  Could not establish connection to host
306:  Traceback (most recent call last):
307:    File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/OptimizedSanity/services/maglev_cli/maglevclihandler.py"", line 151, in connect
308:      if not self._send(""_shell -c \'sudo magctl ssh shell bash\'"", timeout=1):
309:    File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/OptimizedSanity/services/maglev_cli/maglevclihandler.py"", line 499, in _send
310:      raise PexpectTimeout('Timeout exceeded.')
311:  pexpect.exceptions.TIMEOUT: Timeout exceeded.
312: 
313:  During handling of the above exception, another exception occurred:
314: 
315:  Traceback (most recent call last):
316:    File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/OptimizedSanity/services/maglev_cli/utils.py"", line 34, in wrapper
317:      self.connect()
318:    File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/OptimizedSanity/services/maglev_cli/maglevclihandler.py"", line 160, in connect
319:      return self.connect(retry=retry - 1)
320:    File ""/home/gvaddadi/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/OptimizedSanity/services/maglev_cli/maglevclihandler.py"", line 141, in connect
321:      if not self.ssh.login(*args, **kwargs):
322:    File ""/ws/avdas-sjc/pyats-new/lib/python3.6/site-packages/pexpect/pxssh.py"", line 424, in login
323:      raise ExceptionPxssh('Could not establish connection to host')
324:  pexpect.pxssh.ExceptionPxssh: Could not establish connection to host
325:  Last ssh response:{noformat}",2023-08-04T01:21:34.986+0000,"Required PR has been raised for Hulk branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6509/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6509/overview] Mentioned PR has been merged into Hulk Branch.

Marking this as “Closed”.","['Auton', 'Enhancement']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-2000,https://miggbo.atlassian.net/browse/SEEN-2000,[Auton][MSTB2] : Prime feature migration on MSTB2,"We are trying Prime-migration testcase on MSTB2, but we are seeing XML pointer error.

Below is the error we are seeing
5067: Sending Credentials Into Prime
5068: ************************************************************
5069: Traceback (most recent call last):
5070: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/commonlibs/test_wrapper.py"", line 301, in wrapper
5071: result = testfunc(func_self, **kwargs)
5072: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py"", line 433, in test3_add_ise_server
5073: if dnac_handle.add_ise_server():
5074: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/dnaserv/lib/api_groups/prime_infra/group.py"", line 1731, in add_ise_server
5075: ui_handle = self.login_setup(url)
5076: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/dnaserv/lib/api_groups/prime_infra/group.py"", line 56, in login_setup
5077: ui_handle.browser.find_element_by_xpath(""//input[@id='label_username']"").send_keys(ui_handle.username)
5078: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py"", line 394, in find_element_by_xpath
5079: return self.find_element(by=By.XPATH, value=xpath)
5080: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py"", line 976, in find_element
5081: return self.execute(Command.FIND_ELEMENT, {
5082: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py"", line 321, in execute
5083: self.error_handler.check_response(response)
5084: File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_response
5085: raise exception_class(message, screen, stacktrace)
5086: selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {""method"":""xpath"",""selector"":""//input[@id='label_username']""}
5087: (Session info: chrome=109.0.5414.120)
5088:
5089: Test returned in 0:00:33.223384
5090: Errored reason: no such element: Unable to locate element: {""method"":""xpath"",""selector"":""//input[@id='label_username']""}
5091: (Session info: chrome=109.0.5414.120)
Manually when we tried accessing Prime infra it works fine.
Branch : private/hulk-prime/api-auto
Script : solution_test_3sites_sjc_nyc_sf.py",2023-08-04T11:56:30.141+0000,"Hi [~accountid:63f50bf0e8216251ae4d59ca] 

Once we close this, please investigate the root cause in order to prevent any similar issues in the future.","['Auton', 'Hulk', 'Integration', 'MSTB2']",Unassigned,Cancelled,Divakar Kumar Yadav
SEEN-2001,https://miggbo.atlassian.net/browse/SEEN-2001,[Auton][MSTB2] :  Test_TC196_template_conflicts_in_template_hub  /   test7_attach_template_to_profile,"Hi [~accountid:63f50bcece6f37e5ed93c87e],

We are seeing Template push is failing with below error
12627:  Error Code: 406 for
12628:  URL:[https://10.195.243.37/api/v1/siteprofile/3fae2c27-2574-47a1-8b25-6512f196270a|https://10.195.243.37/api/v1/siteprofile/3fae2c27-2574-47a1-8b25-6512f196270a] Data:{'timeout': 60, 'data': '{""siteProfileUuid"": ""3fae2c27-2574-47a1-8b25-6512f196270a"", ""version"": 7, ""name"": ""switching_profile"", ""namespace"": ""switching"", ""status"": ""final"", ""lastUpdatedBy"": ""admin"", ""lastUpdatedDatetime"": 1690986526057, ""profileAttributes"": [{""key"": ""cli.templates"", ""attribs"": [{""key"": ""[template.id|http://template.id]"", ""value"": ""4e38f342-f232-498f-8832-8ee043f713a0""}, {""key"": ""[template.id|http://template.id]"", ""value"": ""0cdf837a-e3ce-4e4d-8734-f48b3e53ebd5""}, {""key"": ""[template.id|http://template.id]"", ""value"": ""0cdf837a-e3ce-4e4d-8734-f48b3e53ebd5""}]}], ""sites"": [{""name"": ""FLOOR1_LEVEL1"", ""uuid"": ""043666b4-a2a2-4cce-b3cd-8b65ef6c737e"", ""isInherited"": false}, {""name"": ""FLOOR1_LEVEL2"", ""uuid"": ""4be650f8-f0b7-4348-b92e-d9d66f4fc558"", ""isInherited"": false}, {""name"": ""BLD20_FLOOR2"", ""uuid"": ""4c4d3330-daef-4c16-94aa-9a688c932306"", ""isInherited"": false}, {""name"": ""BLD23"", ""uuid"": ""6f1ee214-82d8-4b00-b400-c1af3d2d0391"", ""isInherited"": false}, {""name"": ""FLOOR1_LEVEL4"", ""uuid"": ""d24df4d5-eec9-40ed-9dc8-e7b179216861"", ""isInherited"": false}, {""name"": ""FLOOR1_LEVEL3"", ""uuid"": ""d5f5fa4e-aef1-46af-852f-3842e415f97a"", ""isInherited"": false}], ""attributesList"": [], ""interfaceList"": [], ""groupTypeList"": []}'} Headers:{'Content-Type': 'application/json', 'X-Auth-Token': 'eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJDRE5BIiwiYXV0aFNvdXJjZSI6ImxlZ2FjeSIsImNsaWVudElkIjoiYWRtaW4iLCJlbWFpbCI6ImFkbWluQGxvY2FsdXNlci5jb20iLCJleHAiOjE2OTA5ODk5NTMsImlhdCI6MTY5MDk4NjM1MywiaXNzIjoiZG5hYyIsInJvbGVzIjpbIlNVUEVSLUFETUlOIl0sInNlc3Npb25JZCI6IjRkNmQ0ZWU1LTg1OTQtNTk2Zi04ZGM5LWZmY2MyZWU3Yjc5OCIsInN1YiI6ImFkbWluIiwidGVuYW50SWQiOiI2NGM0M2ZlNTc2ZjQ4MTAwMTM4YzJhZDIiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInVzZXJuYW1lIjoiYWRtaW4ifQ.Ss8Tzs0I6FQIKOF2chhrAvhslIAL2fLDajnFwxfdBMlnBUkeWYYeSue6WJB5Dh3eJ_l7d2UkitaVSskB-jyKKg'} Message:{""response"":{""errorCode"":""NCND00002"",""message"":""NCND00002: The Network Profile request has validation errors"",""detail"":""[\""NCND01025: Duplicate keys cannot have the same value: template.id\""]""},""vers[ion"":""1.0""}|http://template.id]

Script complains Duplicate keys cannot have same value.

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2345997&size=96937&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fauto_MS_job.2023Aug02_07:11:49.215947.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2345997&size=96937&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fauto_MS_job.2023Aug02_07:11:49.215947.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Passlog : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19503520&size=47047&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul15_04:21:05.558740.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19503520&size=47047&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul15_04:21:05.558740.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

Branch : private/hulk-ms/api-auto
Script : solution_test_3sites_sjc_nyc_sf.py",2023-08-04T12:44:35.639+0000,"[Pull Request #6468: RCA generation - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6468/overview]

Changed some logic around in attaching template to nw profile, should resolve this.","['Auton', 'Hulk', 'MSTB2']",Andrew Chen,Resolved,Divakar Kumar Yadav
SEEN-2002,https://miggbo.atlassian.net/browse/SEEN-2002,[Auton]Test_TC67_DNAC_CriticalVLAN_onboarding_ixia_scale,"*Uber ISO Version tested :* 3.710.75272 - Hulk ESXI

*Branch*: private/Hulk-ms/api-auto

*Script Name :* solution_test_sanity_ecamb.py

*Testbed :* TB4

*Testcases Impacted :*  test66_subtest2_dot1x_auth_ixia

During Hulk ESXI testing : ""test66_subtest2_dot1x_auth_ixia"" configurations of critical_auth_dot1x template are missing on extended node

After checking manual and adding the configurations I can see dot1x clients on Extended-Nodes



Pass Log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-sol%2Fusers%2Fkmukku%2Farchive%2F23-08%2Fsanity_crft_tb4.2023Aug04_13:42:19.588724.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-sol&submitter=kmukku&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-sol%2Fusers%2Fkmukku%2Farchive%2F23-08%2Fsanity_crft_tb4.2023Aug04_13:42:19.588724.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-sol&submitter=kmukku&from=trade&view=all&atstype=PYATS]



Fail Log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul30_11:10:19.316301.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul30_11:10:19.316301.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]



*Extended Node Configuration on New York Site*



SN-FCW2233L0EE#show running-config interface TenGigabitEthernet 1/0/1
Building configuration...

Current configuration : 873 bytes
!
interface TenGigabitEthernet1/0/1
 description Interface type USER_DEVICE onboarded by sol script
 *switchport access vlan 1026*   *<===== MISSING CONFIG*
 switchport mode access
 device-tracking attach-policy IPDT_POLICY
 ip flow monitor dnacmonitor input
 ip flow monitor dnacmonitor_dns input
 ip flow monitor dnacmonitor output
 ip flow monitor dnacmonitor_dns output
 load-interval 30
 ipv6 flow monitor dnacmonitor_v6 input
 ipv6 flow monitor dnacmonitor_dns_v6 input
 ipv6 flow monitor dnacmonitor_v6 output
 ipv6 flow monitor dnacmonitor_dns_v6 output
 access-session inherit disable interface-template-sticky
 access-session inherit disable autoconf
 cts manual
  policy static sgt 30001
  no propagate sgt
 dot1x timeout tx-period 7
 dot1x max-reauth-req 3
 no macro auto processing
 *source template DefaultWiredDot1xClosedAuth   <== MISSING CONFIG*
 spanning-tree portfast
 spanning-tree bpduguard enable
end



*Extended Node Configuration on San Jose Site*

SN-FOC2350L1BR#show running-config interface GigabitEthernet 0/5
Building configuration...

Current configuration : 469 bytes
!
interface GigabitEthernet0/5
 description Interface type USER_DEVICE onboarded by sol script
 *switchport access vlan 1038   <== MISSING CONFIG*
 switchport mode access
 ip device tracking maximum 65535
 access-session inherit disable interface-template-sticky
 access-session inherit disable autoconf
 no macro auto processing
 dot1x timeout tx-period 7
 dot1x max-reauth-req 3
 source template DefaultWiredDot1xClosedAuth
 spanning-tree portfast edge
 spanning-tree bpduguard enable
end",2023-08-04T21:28:08.027+0000,"Is this issue still observed ?This config should be pushed by DNAC when the interface is onboarded  TC 66 [test2_onboard_all_ixia_onboarddot1x|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7362841&size=1188773&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul30_11:10:19.316301.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]. Closing this Auton [~accountid:712020:a269cb40-4972-4cdd-b4be-1a9a0bc46160] Please check on latest HulkPatch1 and update here Not getting same issue on latest releases, looking into device configs

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-139-trafficdot1x&begin=13307302&size=724528&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov28_15:35:28.078439.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-139-trafficdot1x&begin=13307302&size=724528&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov28_15:35:28.078439.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'ESxi', 'Hulk']",Raghavendrachar Baraguru Mallesha Char,In Progress,KRISHNA MUKKU
SEEN-2003,https://miggbo.atlassian.net/browse/SEEN-2003,[Auton]:Hulk:Test_TC5_remove_all_ext_node_from_fabric/test3_clear_subtended_devices_from_fabrics,"*Reporter Analysis:* 

Script is not clearing all interfaces assigned to ports of extended nodes / AEN[PEN] nodes.
When I tried manually clearing interfaces and retried same testcase passed.

*Description:*  

{noformat}1545: 
 Config Preview Activity failed with reason: Extended Node SN-JAE24040C8K.cisco.com cannot be deleted since it has Port Assignment(s). Please delete the Port Assignment(s) and retry.{noformat}

{noformat}1546: 
{noformat}

{noformat}1547: 
 Activity: bfe81ed1-e628-4c23-bcb0-ddfd29ed65cd Trigger job: {'id': '077f07b4-fa12-4ec9-ab32-7d3f726b3b61', 'triggeredJobTaskId': '992a64a7-6548-45d4-a20e-2ab6964d1478', 'triggeredTime': 1691212145397, 'status': 'FAILED', 'failureReason': 'Extended Node SN-JAE24040C8K.cisco.com cannot be deleted since it has Port Assignment(s). Please delete the Port Assignment(s) and retry.', 'triggeredJobId': '077f07b4-fa12-4ec9-ab32-7d3f726b3b61'}{noformat}

{noformat}1641: 
 activity_id is False. Config preview task failed for description Scheduling task for Removing ext nodes from fabric for fabric ['Global/USA/New_York_US_SJ_Fabric1'] at time 1691212145.2276087 - Configuration Preview{noformat}



*Branch Name:  private/Hulk-ms/sanity_api_auto*

*Script file/Usecase:* 

dnac_cleanup_script.py

*Source Team:  Upgrade Sanity*

*Issue Seen first time or day0 issue:*


*Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=477245&size=236498&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug04_21:59:41.458203.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=477245&size=236498&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug04_21:59:41.458203.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*Pass Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=189580&size=205719&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug04_22:54:00.815501.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=189580&size=205719&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug04_22:54:00.815501.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=189739&size=92442&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug04_23:04:33.260117.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=189739&size=92442&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug04_23:04:33.260117.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
",2023-08-05T08:08:22.038+0000,"PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6707/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6707/overview] Hi [~accountid:63f50bfce8216251ae4d59d5] 

Again Cleanup TC also failed with different error:

Snip from error log:
3817: 
     result &= self.services.onboard_device_clear_interface(ext_node['name'], clientAP['intf'].name)

{noformat}3818: 
 AttributeError: 'list' object has no attribute 'name'{noformat}

{noformat}3820: 
 Errored reason: 'list' object has no attribute 'name'{noformat}



Script:
dnac_cleanup_script.py



*Branch used:*
private/HulkPatch-ms/sanity_api_auto and private/Hulk-ms/sanity_api_auto

Failed log from cleanup script:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=464023&size=805182&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct05_06:24:44.358446.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=464023&size=805182&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct05_06:24:44.358446.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

[https://ngdevx.cisco.com/services/taas/results/0acac0aa-af0b-43cd-9bdf-9278a245ba06|https://ngdevx.cisco.com/services/taas/results/0acac0aa-af0b-43cd-9bdf-9278a245ba06]



Due to TC 5 getting errored TC7 is geeting affected , TC 7 log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1965692&size=659325&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct05_06:24:44.358446.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1965692&size=659325&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct05_06:24:44.358446.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Thanks,
Anusha John I added a fix, You can open it if you still see the issue since I do not have an AEN node onboarded. 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e9af32db487753a9788c6aa5c91ed5c6f5841230|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e9af32db487753a9788c6aa5c91ed5c6f5841230]","['Auton', 'Cleanup', 'Hulk', 'Upgrade']",Moe Saeed,Resolved,Anusha John
SEEN-2004,https://miggbo.atlassian.net/browse/SEEN-2004,[Auton]:Hulk: Test_TC14_remove_devices_from_fabric/ test1_remove_devices_from_fabric,"*Reporter Analysis:* 

Script is failing slice error.
Due to which TC 15:Test_TC15_clear_device_onboarding_virtual_segment_mapping  / test_clear_device_onboarding_virtual_segment_mapping  is also failing.

Manually I tried deleting devices from Fabric and retried TC 15 testcase passed.

*Description:* 

{noformat}21587: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/api_groups/fabric_wired/group.py"", line 1370, in remove_devices_from_fabric{noformat}

{noformat}21588: 
     result &= self.remove_devices_from_fabric(fabric_name=cd['name'], device_list=device_list,{noformat}

{noformat}21589: 
 TypeError: list indices must be integers or slices, not str{noformat}

{noformat}21591: 
 Errored reason: list indices must be integers or slices, not str{noformat}



*Error Snippet from TC 15:*
23166: 
 Config Preview Activity failed with reason: NCWL11701: Segment WClients_nyc-WirelessVNFB has Wlan(s) mapped to it, please remove the mapping between the segment and the Wlan(s) and retry the operation.

{noformat}23167: 
{noformat}

{noformat}23168: 
 Activity: f603b380-f28b-4edd-94fb-567c82a8a26f Trigger job: {'id': 'd458e63e-be8c-4a69-b6f1-ac31d7391420', 'triggeredJobTaskId': 'c6589871-07fb-4331-951a-812368d1a8cb', 'triggeredTime': 1691217197018, 'status': 'FAILED', 'failureReason': 'NCWL11701: Segment WClients_nyc-WirelessVNFB has Wlan(s) mapped to it, please remove the mapping between the segment and the Wlan(s) and retry the operation.', 'triggeredJobId': 'd458e63e-be8c-4a69-b6f1-ac31d7391420'}{noformat}

{noformat}23255: 
 activity_id is False. Config preview task failed for description Scheduling task for Removing segments from VN - Configuration Preview{noformat}



*Branch Name:  private/Hulk-ms/sanity_api_auto*

*Script file/Usecase:* 

dnac_cleanup_script.py

*Source Team:  Upgrade Sanity*

*Issue Seen first time or day0 issue:*



*Failed Log TC 14:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6325996&size=8893&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug04_23:04:33.260117.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6325996&size=8893&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug04_23:04:33.260117.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*Failed Log TC15:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6335816&size=490265&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug04_23:04:33.260117.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6335816&size=490265&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug04_23:04:33.260117.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*Pass log of TC 15 after trying TC 14 manually:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=210736&size=736356&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug05_00:29:36.236760.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=210736&size=736356&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug05_00:29:36.236760.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-05T08:15:05.501+0000,"[~accountid:61efa8c457b25b006877eda3] which solution input was this run with? Seems to be a solution input issue. Is this issue seen again? If cd[“name”] is giving a list indices error, its saying that cd is a list. But if the input file is correct, cd should be a dict not a list. So closing this as its mostly a solution input error. If it comes up again, please provide the solution input file as well and reopen the auton. Hi [~accountid:63f50bcece6f37e5ed93c87e] 
Issue was seen in after upgrade on cleanup script
Input used:

""dnac_input"": ""./configs/upgrade/ghost_to_hulk_solution_test_input_upgrade.json""

As we used to create an upgrade json on both Sanity and AWS sanity upgrade as we see ssid issues earlier

Thanks,
Anusha John Hi [~accountid:63f50bcece6f37e5ed93c87e] 

I am reopening auton as i tried with all input and still getting same error for this TC 14:

*Branch Name:  private/HulkPatch-ms/sanity_api_auto*

*Script file/Usecase:* 

dnac_cleanup_script.py





[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2189199&size=9772&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct05_22:47:45.797941.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2189199&size=9772&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct05_22:47:45.797941.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]





[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2189199&size=9772&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct05_22:47:45.797941.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2189199&size=9772&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct05_22:47:45.797941.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=207685&size=24682&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct05_23:12:41.892446.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=207685&size=24682&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct05_23:12:41.892446.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



Due to TC 14 , TC 15 is failing



Thanks,
Anusha John [ENG-SDN / dnac-auto / 6cd1fa1133c - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6cd1fa1133c4b2e8bd617c72fde28898a3a6500f] ← adding support for the legacy solution inputs you are using. ","['Auton', 'Cleanup', 'Hulk', 'Upgrade']",Andrew Chen,Resolved,Anusha John
SEEN-2005,https://miggbo.atlassian.net/browse/SEEN-2005,[Auton]:Hulk: Test_TC25_delete_wireless_ssid  /   test1_delete_wireless_interfacesd,"*Reporter Analysis:* 

Script is not deleting vlan’s on Design---->Network settings--->Wireless
When I tried manually deleting it was deleting but via script it throwed errors

*Description:*  

{noformat}Message:{""response"":{""errorCode"":""NCND00001"",""message"":""NCND00001: The Common Settings request has validation errors"",""detail"":""[\""NCND80400: Interface(s) [VLAN0921] is/are assigned to SSID(s) or part of dynamic interface in Wireless profile(s) or linked with a VLAN Group. They cannot be deleted.\""]""},""version"":""1.0""}{noformat}

{noformat}5028: 
 Traceback (most recent call last):{noformat}

{noformat}5029: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 326, in call_api{noformat}

{noformat}5030: 
     response.raise_for_status(){noformat}

{noformat}5031: 
   File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status{noformat}

{noformat}5032: 
     raise HTTPError(http_error_msg, response=self){noformat}

{noformat}5033: 
 requests.exceptions.HTTPError: 406 Client Error: Not Acceptable for url: https://10.195.227.92/api/v1/commonsetting/global/-1{noformat}

{noformat}5034: 
 Encountered unhandled HTTPError in Internal API Call{noformat}

{noformat}5035: 
 Flagging result as FAIL!{noformat}

{noformat}5036: 
 	Reason: 406 Client Error: Not Acceptable for url: https://10.195.227.92/api/v1/commonsetting/global/-1{noformat}

{noformat}5037: 
 Kwargs:{noformat}

*Branch Name:  private/Hulk-ms/sanity_api_auto*

*Script file/Usecase:* 

dnac_cleanup_script.py

*Source Team:  Upgrade Sanity*

*Issue Seen first time or day0 issue:*



*Failed Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1111060&size=23721&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug05_00:42:49.099959.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1111060&size=23721&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug05_00:42:49.099959.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*After deleting vlan manually and retried Testcase passed:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=227656&size=30136&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug05_01:18:37.458380.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=227656&size=30136&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug05_01:18:37.458380.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-05T08:24:42.016+0000,"We see only 1 SSID is getting deleted and the loop is coming out after that. Check why the loop is not running on all SSIDs to remove from profile and Delete. The Interface failure is expected because the SSID are not deleted. It is giving a false pass.  Same issue is seen on Ghost P2 RC3 cleanup:
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1075246&size=23242&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug23_03:01:10.611202.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1075246&size=23242&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug23_03:01:10.611202.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Script is not deleting vlan from wireless page

!image-20230823-102019.png|width=1498,height=826! Ghost: [8f5b21daab5|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/8f5b21daab55ee12f0ace11469e236c89ac76663]

Hulk: [e426fa5accb|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e426fa5accbc68914a51368348a8e84596b00dd8] Closing issue as got pass log from Hulk Patch1 
Branch used:private/HulkPatch-ms/sanity_api_auto

Pass log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1105555&size=389621&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct06_01:18:02.654014.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1105555&size=389621&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct06_01:18:02.654014.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Cleanup', 'Ghost', 'Hulk', 'Upgrade']",Raji Mukkamala,Closed,Anusha John
SEEN-2006,https://miggbo.atlassian.net/browse/SEEN-2006,[Auton]:Hulk: Test_TC31_cleanup_ise_sgtvnvlan  /   cleanup_ise_sgtvnvlan,"*Reporter Analysis:* 

On ISE 3.3 Script is not able to fetch {{sgtvnvlan}} id.

Tried on ISE side fetching vlan’s but couldn’t find anything

*Description:* 

{noformat}7858: 
 Failed to cleanup for id b4adebdd-6096-4797-bc2b-07455961cd9d{noformat}

{noformat}7941: 
 Failed to cleanup for id f2dd8280-adb8-493f-816e-50ff3ab3cff5{noformat}

{noformat}8024: 
 Failed to cleanup for id 781cc001-a12c-49e4-91a3-5b1a4a90e517{noformat}

{noformat}8107: 
 Failed to cleanup for id c944856b-cefd-4a47-9191-b3e081401d7a{noformat}

{noformat}8120: 
 Failed reason: Failed to cleanup sgt-vlan-vlan mapping{noformat}



*SNIP AFTER RETRYING:*



{noformat}1081: 
 Error Code: 404 URL:https://10.195.227.93:9060/ers/config/sgtvnvlan Data:{'params': {'size': 100, 'page': 1}, 'timeout': 600} Headers:{'Content-Type': 'application/json', 'Accept': 'application/json', 'Authorization': 'Basic YWRtaW46TGFibGFiMTIz'} Message:{noformat}

{noformat}1082: 
 Traceback (most recent call last):{noformat}

{noformat}1083: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/iseserv/client_manager.py"", line 278, in call_api{noformat}

{noformat}1084: 
     response.raise_for_status(){noformat}

{noformat}1085: 
   File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status{noformat}

{noformat}1086: 
     raise HTTPError(http_error_msg, response=self){noformat}

{noformat}1087: 
 requests.exceptions.HTTPError: 404 Client Error:  for url: https://10.195.227.93:9060/ers/config/sgtvnvlan?size=100&page=1{noformat}

{noformat}1088: 
 ###################################################{noformat}

{noformat}1089: 
 #!!!FAILED TO GET ALL sgtvnvlan id list in ISE. ERROR 404 Client Error:  for url: https://10.195.227.93:9060/ers/config/sgtvnvlan?size=100&page=1----#{noformat}

{noformat}1090: 
 ###################################################{noformat}

{noformat}1092: 
 404 Client Error:  for url: https://10.195.227.93:9060/ers/config/sgtvnvlan?size=100&page=1{noformat}

{noformat}1095: 
 Failed reason: Failed to cleanup sgt-vlan-vlan mapping{noformat}

 

*Branch Name:  private/Hulk-ms/sanity_api_auto*

*Script file/Usecase:* 

dnac_cleanup_script.py

*Source Team:  Upgrade Sanity*

*Issue Seen first time or day0 issue:*



*First time Failed Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3096092&size=97208&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug05_00:42:49.099959.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3096092&size=97208&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug05_00:42:49.099959.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Retry Failed Log:*

After trying deleting vlan from DNAC--->Network settings--->wireless saw new failures

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=266647&size=7474&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug05_01:18:37.458380.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=266647&size=7474&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug05_01:18:37.458380.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-05T08:47:09.593+0000,"Please add previous pass log  Raised PRs:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7710/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7710/overview]
* Hulk Patch 1: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7711/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7711/overview]
* Hulk Patch 2: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7712/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7712/overview]","['Auton', 'Cleanup', 'Hulk', 'Upgrade']",ThangQuoc Tran,Resolved,Anusha John
SEEN-2007,https://miggbo.atlassian.net/browse/SEEN-2007,[Auton] [Hulk] - Need for script enhancement to handle model config update during Device -reprovisioning,"*Regression:* Solution Regression Multisite - DR+MDNAC, Solution Regression Multisite - Non-DR, Solution Sanity

*Branch:* private/Hulk-ms/api-auto

*Uber ISO tested:* Hulk RC1 - 2.1.710.70446

*Scripts Used:*

testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py

testcases/forty_eight_hour/solution_test_sanityecamb_lan.py

*Background and Issue Faced:* 

During ongoing Solution testing on Hulk RC1, we observed Device provisioning issue related to model config for all the Wireless devices - (Aireos, 9800 EWLC and ECA devices)as reported in defects - [+CSCwh00148+|https://cdetsng.cisco.com/webui/#view=CSCwh00148] & [+CSCwf99788+|https://cdetsng.cisco.com/webui/#view=CSCwf99788]

On analysis from DE Team it was confirmed that , the provisioning issue observed is due to the script is not refreshing or rebuilding the model config payload, every time the device provisioning is attempted. It was found that if user performs provisioning operations manually on wireless devices with some model configs in place and then followed by provisioning operation via script execution, we see this provisioning failure on devices because the script constructing a new payload for model config but rather using the predefined  or previous older payload. This behavior got exposed as we had done manual device provisioning as a workaround to tackle issues related defects [+CSCwf89431+|https://cdetsng.cisco.com/webui/#view=CSCwf89431] / [CSCwh00797|https://cdetsng.cisco.com/webui/#view=CSCwh00797] during Hulk RC1 testing.

Please refer team spaces [https://eurl.io/#DKzs3L9Cy|https://eurl.io/#DKzs3L9Cy] & [https://eurl.io/#QVrZLfxin|https://eurl.io/#QVrZLfxin] for model details.

Also it was discussed with [khanhvn@cisco.com|mailto:khanhvn@cisco.com] from DE team on this issue and agreed by [andrech3@cisco.com|mailto:andrech3@cisco.com] from Automation team to be addressed from Automation implementation



*Reference Failed logs:*

Multisite Profile - [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2406562&size=6259300&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_mb_multi_sites_mdnac.2023Jul19_06:27:55.462917.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2406562&size=6259300&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_mb_multi_sites_mdnac.2023Jul19_06:27:55.462917.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Sanity profile - [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-clientRateLimiting.py-261-clientRateLimiting&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul18_04:17:06.072942.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-clientRateLimiting.py-261-clientRateLimiting&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul18_04:17:06.072942.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-07T07:53:41.258+0000,"[Pull Request #6468: RCA generation - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6468/overview]

title is a bit misleading but this PR I have changed our script to support model config during provisioning, which was never supported before.","['Auton', 'EXECUTION', 'Execution', 'Hulk', 'MSTB1', 'MSTB2', 'Multisite', 'Sanity']",Andrew Chen,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2008,https://miggbo.atlassian.net/browse/SEEN-2008,"[Auton] [Hulk] - Need for change in netflow configuration check for ""flow exporter"" config cli on devices","*Regression & Sanity:* Solution Regression Multisite - DR+MDNAC, Solution Regression Multisite - Non-DR, Solution Sanity

*Branch:* private/Hulk-ms/api-auto

*Uber ISO tested:* Hulk RC1 - 2.1.710.70446, Hulk RC2 - 2.1.710.70462

*Scripts Used:*

testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py

testcases/forty_eight_hour/solution_test_sanityecamb_lan.py


*Description :* 

We observed configuration verification check for netflow related configs on telemetry enabled devices failed with config missing for ""flow exporter <ip>” cli

On trying to check further with DE Team ([suppurus@cisco.com|mailto:suppurus@cisco.com]), it was confirmed that there was recent changes done from Hulk onwards releases + any Polaris + any device type/series. Need to handle these changes from Automation script with respect to Hulk.

*Config push check validation passed during earlier working scenario:*

{color:#bf2600}flow exporter 50.45.45.6{color}
{color:#bf2600}destination 50.45.45.6{color}
{color:#bf2600}transport udp 6007{color}
{color:#36b37e}flow exporter dnacexporter{color}
{color:#36b37e}destination 50.45.45.6{color}
{color:#36b37e}source Loopback0{color}
{color:#36b37e}transport udp 6007{color}
{color:#36b37e}export-protocol ipfix{color}
{color:#36b37e}option interface-table timeout 300{color}
{color:#36b37e}option vrf-table timeout 300{color}
{color:#36b37e}option sampler-table{color}
{color:#36b37e}option application-table timeout 300{color}
{color:#36b37e}option application-attributes timeout 300{color}

*Config push check failure current failed log:*
{color:#36b37e}flow exporter dnacexporter{color}
{color:#36b37e}destination 50.45.45.6{color}
{color:#36b37e}source Loopback0{color}
{color:#36b37e}transport udp 6007{color}
{color:#36b37e}export-protocol ipfix{color}
{color:#36b37e}option interface-table timeout 300{color}
{color:#36b37e}option vrf-table timeout 300{color}
{color:#36b37e}option sampler-table{color}
{color:#36b37e}option application-table timeout 300{color}
{color:#36b37e}option application-attributes timeout 300{color}

*Failed Log:* 

*Regression Profile -* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=40738611&size=14323755&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_mb_multi_sites_mdnac.2023Jul26_12:11:00.629706.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=40738611&size=14323755&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-07%2Fsr_mb_multi_sites_mdnac.2023Jul26_12:11:00.629706.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Sanity Profile* - [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5753347&size=5635357&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug04_06:13:16.231452.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5753347&size=5635357&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug04_06:13:16.231452.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-07T08:53:08.882+0000,"Hi Team , 

During solution sanity on Hulk testing. We observed that Tc failed for {{flow exporter}},&{{flow monitor dnacmonito}}
Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1810160&size=20502888&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug09_10:26:43.039712.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1810160&size=20502888&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug09_10:26:43.039712.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Raised PRs to solve this issue:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7068/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7068/overview]
* Hulk P1: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7067/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7067/overview]","['Auton', 'Execution', 'Hulk', 'MSTB1', 'MSTB2', 'Multisite', 'Optimized', 'Sanity']",ThangQuoc Tran,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2010,https://miggbo.atlassian.net/browse/SEEN-2010,[Auton][MSTB2] -  Test_TC200_client_limiting_rate / test3_provsion_wlc_after_updating_ssid ,"Hi,

With the latest Hulk ESXI execution. We are seeing client_limit_rate testcase is failing with below error

47031:  Resource path full url: [https://10.195.243.37/api/v1/template-programmer/template/compatible-devices|https://10.195.243.37/api/v1/template-programmer/template/compatible-devices]
47032:  Error Code: 404 for
47033:  URL:[https://10.195.243.37/api/v1/template-programmer/template/compatible-devices|https://10.195.243.37/api/v1/template-programmer/template/compatible-devices] Data:{'timeout': 60, 'data': '{""templateId"": ""cf5a4781-bc21-43a6-b884-9870e74bb41d"", ""deviceTypeAndIds"": {""17d454c2-f89e-4fe8-aff8-52e453ff651e"": ""MANAGED_DEVICE_UUID""}}'} Headers:{'Content-Type': 'application/json', 'X-Auth-Token': 'eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJDRE5BIiwiYXV0aFNvdXJjZSI6ImxlZ2FjeSIsImNsaWVudElkIjoiYWRtaW4iLCJlbWFpbCI6ImFkbWluQGxvY2FsdXNlci5jb20iLCJleHAiOjE2OTE0NzQ4OTAsImlhdCI6MTY5MTQ3MTI5MCwiaXNzIjoiZG5hYyIsInJvbGVzIjpbIlNVUEVSLUFETUlOIl0sInNlc3Npb25JZCI6ImI3NWE2YjFlLTMyMGMtNWE1Yy1hY2ZhLTA1NWRlNmJhZGRmMyIsInN1YiI6ImFkbWluIiwidGVuYW50SWQiOiI2NGNjYTQyNWQ5NmNlMjAwMTNjMzY0MjUiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInVzZXJuYW1lIjoiYWRtaW4ifQ.d4mVOiSSF8SocD-GORmEFSP6Op2OkMgtnBw6qQ5pmvcOGHpvMNk0IeONw4z_0vkSSD-dmGPEI-rYukVkBSCQOA'} Message:{""response"":{""errorCode"":""NOT_FOUND"",""message"":""Resource does not exist"",""detail"":""Resource not found"",""href"":""""},""version"":""1.0""}

Failed log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12187315&size=161313&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fauto_MS_job.2023Aug07_19:44:15.170386.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12187315&size=161313&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fauto_MS_job.2023Aug07_19:44:15.170386.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Testcase affected : [Test_TC200_client_limiting_rate|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12162704&size=358455&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fauto_MS_job.2023Aug07_19:44:15.170386.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test3_provsion_wlc_after_updating_ssid

[Test_TC200_client_limiting_rate|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12162704&size=358455&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fauto_MS_job.2023Aug07_19:44:15.170386.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test6_provsion_wlc_after_updating_ssid

Script used : solution_test_3sites_sjc_nyc_sf
Branch : private/Hulk-ms/api-auto

Passlog : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8815425&size=1356258&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fauto_MS_job.2023Aug01_19:08:27.381059.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8815425&size=1356258&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fauto_MS_job.2023Aug01_19:08:27.381059.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-09T09:43:13.503+0000,"Hi [~accountid:63f50bfce8216251ae4d59d5],

Same issue is seen for AP negative scenario

14475:  Resource path full url: [https://10.195.243.37/api/v1/template-programmer/template/compatible-devices|https://10.195.243.37/api/v1/template-programmer/template/compatible-devices]
14476:  Error Code: 404 for
14477:  URL:[https://10.195.243.37/api/v1/template-programmer/template/compatible-devices|https://10.195.243.37/api/v1/template-programmer/template/compatible-devices] Data:{'timeout': 60, 'data': '{""templateId"": ""cf5a4781-bc21-43a6-b884-9870e74bb41d"", ""deviceTypeAndIds"": {""17d454c2-f89e-4fe8-aff8-52e453ff651e"": ""MANAGED_DEVICE_UUID""}}'} Headers:{'Content-Type': 'application/json', 'X-Auth-Token': 'eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJDRE5BIiwiYXV0aFNvdXJjZSI6ImxlZ2FjeSIsImNsaWVudElkIjoiYWRtaW4iLCJlbWFpbCI6ImFkbWluQGxvY2FsdXNlci5jb20iLCJleHAiOjE2OTE5OTA4ODMsImlhdCI6MTY5MTk4NzI4MywiaXNzIjoiZG5hYyIsInJvbGVzIjpbIlNVUEVSLUFETUlOIl0sInNlc3Npb25JZCI6ImVmYWM0ZThlLTNkZDktNTVhYS1hMzgyLTU5MDEwOTAwMjU0YyIsInN1YiI6ImFkbWluIiwidGVuYW50SWQiOiI2NGNjYTQyNWQ5NmNlMjAwMTNjMzY0MjUiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInVzZXJuYW1lIjoiYWRtaW4ifQ._XdgSXOGctLkeKQPawXdfXa3RQWDCln6IDgiaKt4k3IStSGNNrLUZJ2cOScxeP2LVnvayzulCe-q2hv6S9lnRA'} Message:{""response"":{""errorCode"":""NOT_FOUND"",""message"":""Resource does not exist"",""detail"":""Resource not found"",""href"":""""},""version"":""1.0""}

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2857682&size=181954&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fauto_MS_job.2023Aug13_20:52:41.368825.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2857682&size=181954&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fauto_MS_job.2023Aug13_20:52:41.368825.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Execution', 'Hulk', 'Integration', 'MSTB2']",Andrew Chen,Backlog,Divakar Kumar Yadav
SEEN-2014,https://miggbo.atlassian.net/browse/SEEN-2014,[Auton]:Hulk:Task-assign_devices_to_site_and_provision.py-72-provisioning /Test_TC3_DNAC_Device_Provisioning/test1_verify_provision_the_devices_fabric1 ,"+*Reporter Analysis:*+

Build : Hulk RC3 2.1.710.70479

Getting Provision failed error on eWLC Node:

Error : {{NCWL10663: Security type OPEN and PMF value REQUIRED is not supported for the SSID: OPENtb13 and device: TB13-eWLC.cisco.com/204.192.4.2}}

{{Provision Failed on TB13-eWLC}}

{noformat}Provisioned Failed on Following Devices ['TB13-eWLC']
 Failed reason: Result: Device provision failed{noformat}

*Branch -* private/Hulk-ms/sanity_api_auto

*Script file /Use case -* *solution_test_sanityecamb_lan.py*

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

Fail Log: [Task-assign_devices_to_site_and_provision.py-72-provisioning|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assign_devices_to_site_and_provision.py-72-provisioning&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug09_21:22:18.244895.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

+*Testbed wiki:*+
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-13?src=contextnavpagetreemode|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-13?src=contextnavpagetreemode]",2023-08-10T08:55:21.943+0000,"Analyzed error:

* NCWL10663: Security type OPEN and PMF value REQUIRED is not supported for the SSID: OPENtb13 and device: [TB13-eWLC.cisco.com/204.192.4.2|http://TB13-eWLC.cisco.com/204.192.4.2]

!image-20230810-090610.png|width=1445,height=620!



I’ve investigated the root cause, if our payload doesn’t mention ""protectedManagementFrame"", it will set “Required” as a default. This caused the issue when provisioning OPEN SSID on eWLC/WLC.

!image-20230810-123200.png|width=1018,height=887!



Another issue found is that UI is showing “Disabled” and making us got the misunderstanding

!image-20230810-090522.png|width=1677,height=285!

Refer to this ticket as well: 

# [https://cdetsng.cisco.com/webui/#view=CSCwh16267|https://cdetsng.cisco.com/webui/#view=CSCwh16267]
# [https://cdetsng.cisco.com/webui/#view=CSCwh22448|https://cdetsng.cisco.com/webui/#view=CSCwh22448] Fixed in PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6578/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6578/overview]","['Auton', 'BAPI', 'Hulk', 'Optimized']",ThanhTan Nguyen,Resolved,KARTHIKEYAN KRISHNAMURTHY
SEEN-2015,https://miggbo.atlassian.net/browse/SEEN-2015,[Auton][MSTB2] :  Test_TC207_Check_correct_issue_report_and_health_score_when_stale_session_scenario_happens ,"Hi,

While running [Test_TC207_Check_correct_issue_report_and_health_score_when_stale_session_scenario_happens|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16800612&size=3403963&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fauto_MS_job.2023Aug08_22:29:43.822171.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  testcase on latest hulk-ESXI we are observing 500 client error for below API
{{Resource path full url: }}[https://10.195.243.37/api/assurance/v1/network-device/d2308335-7683-4cc9-9e88-68b07c09bf86/neighbor-topology|https://10.195.243.37/api/assurance/v1/network-device/d2308335-7683-4cc9-9e88-68b07c09bf86/neighbor-topology]

But same works fine in assurance script
38076:  Resource path full url: [https://10.195.243.37/api/assurance/v1/network-device/49c3932c-6b30-4ec0-bcbd-73faf26bdb64/neighbor-topology|https://10.195.243.37/api/assurance/v1/network-device/49c3932c-6b30-4ec0-bcbd-73faf26bdb64/neighbor-topology]
38077:  Neighbor Topology For device 204.1.212.150 and response {'version': '1.0', 'response': {'nodes': [{'role': 'Client', 'name': '2.4GHz Clients', 'id': 'client2.4ghz', 'description': '2.4GHz Clients', 'deviceType': None, 'platformId': None, 'family': 'client2.4ghz', 'ip': None, 'softwareVersion': None, 'userId': None, 'nodeType': None, 'radioFrequency': None, 'clients': 1, 'count': None, 'healthScore': None, 'level': 2, 'fabricGroup': None, 'connectedDevice': None, 

Failed log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16801368&size=24304&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fauto_MS_job.2023Aug08_22:29:43.822171.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=16801368&size=24304&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fauto_MS_job.2023Aug08_22:29:43.822171.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Script Used : solution_test_3sites_sjc_nyc_sf
Branch : private/Hulk-ms/api-auto
Ova# 3.710.75522",2023-08-10T14:02:47.596+0000,"Hi [~accountid:63f50bf0e8216251ae4d59ca], After checking your script and the cluster you use in this run. I found the script pick-up device “{{SJC-FB-9500}}“. But when I check that device on the inventory page, I see “No devices available“. That is why it gets the error “{{Reason: 500 Server Error}}“

So could you check your cluster again?

!image-20230823-090346.png|width=1853,height=489!

!image-20230823-090415.png|width=1916,height=956! Hi [~accountid:63f50bcafb3ac4003fa2c6dd],

We tried re-executing the script and we are seeing same issue. SJC-FB-9500 is already present in Inventory but still it’s failing with same reason.

21900:  SJC-FB-9500
21901:  Library group ""inventory"" method ""get_network_device_info"" returned in 0:00:00.000209
21902:  Library group ""inventory"" method ""find_uuid_of_device"" returned in 0:00:00.000746
21903:
21904:
21905:   api_switch_call called:
21906:  {}
21907:  Resource path full url: [https://10.195.243.37/api/assurance/v1/network-device/6ad7a781-484f-45cc-a511-c80e1b03293f/neighbor-topology|https://10.195.243.37/api/assurance/v1/network-device/6ad7a781-484f-45cc-a511-c80e1b03293f/neighbor-topology]
21908:  Error Code: 500 for
21909:  URL:[https://10.195.243.37/api/assurance/v1/network-device/6ad7a781-484f-45cc-a511-c80e1b03293f/neighbor-topology|https://10.195.243.37/api/assurance/v1/network-device/6ad7a781-484f-45cc-a511-c80e1b03293f/neighbor-topology] Data:{'timeout': 60} Headers:{'Content-Type': 'application/json', 'X-Auth-Token': 'eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJDRE5BIiwiYXV0aFNvdXJjZSI6ImxlZ2FjeSIsImNsaWVudElkIjoiYWRtaW4iLCJlbWFpbCI6ImFkbWluQGxvY2FsdXNlci5jb20iLCJleHAiOjE2OTI5NDM1MTAsImZpcnN0TmFtZSI6ImFkbWluIiwiaWF0IjoxNjkyOTM5OTEwLCJpc3MiOiJkbmFjIiwicm9sZXMiOlsiU1VQRVItQURNSU4iXSwic2Vzc2lvbklkIjoiYmI3YzU1NTAtY2NjYS01YzMwLTk1MzUtNzIzMTcyODRkNjNjIiwic3ViIjoiYWRtaW4iLCJ0ZW5hbnRJZCI6IjY0ZDYwMjJhZDhlMjRkMDAxMTRhNGNkMiIsInRlbmFudE5hbWUiOiJUTlQwIiwidXNlcm5hbWUiOiJhZG1pbiJ9.ySr79Rlf1cytGFOr6yuHoPSPUF7wOt-RPN8SrUcNis3r5X0wm9Z7THJH2yVY59-Ge0BldezcdHvVRzM8xjHnqQ'} Message:{""response"":{""errorCode"":5000,""message"":""An internal has error occurred while processing this request."",""detail"":""An internal has error occurred while processing this request.""}}
21910:  Traceback (most recent call last):
21911:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/dnaserv/client_manager.py"", line 326, in call_api
21912:      response.raise_for_status()
21913:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status
21914:      raise HTTPError(http_error_msg, response=self)
21915:  requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: [https://10.195.243.37/api/assurance/v1/network-device/6ad7a781-484f-45cc-a511-c80e1b03293f/neighbor-topology|https://10.195.243.37/api/assurance/v1/network-device/6ad7a781-484f-45cc-a511-c80e1b03293f/neighbor-topology]
21916:  Encountered unhandled HTTPError in Internal API Call

Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5421737&size=3834441&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fauto_MS_job.2023Aug24_20:55:22.813716.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5421737&size=3834441&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fauto_MS_job.2023Aug24_20:55:22.813716.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] The root cause: There is a minor change for API in Hulk Patch1.

The old API works fine with Hulk but doesn’t work with Hulk Patch 1 # PR HulkPatch-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6813/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6813/overview]
# Test Case:  {{TC_Check_correct_issue_report_and_health_score_when_stale_session_scenario_happens}}
# Testbed: MSTB2
# Script files: testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py
# Trade log link Hulk-ms/api_auto:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-08/sr_mb2_three_sites.2023Aug30_00:41:11.099967.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-08/sr_mb2_three_sites.2023Aug30_00:41:11.099967.zip&atstype=ATS]","['Auton', 'Hulk', 'Integration', 'MSTB2', 'Uplift']",NhanHuu Nguyen,Resolved,Divakar Kumar Yadav
SEEN-2028,https://miggbo.atlassian.net/browse/SEEN-2028,[Auton-Hulk] : Test_TC9_DNAC_Verify_adding_range_discovery_ssh_global_credentials/test1_verify_adding_range_discovery_ssh_global_credentials/test2_add_device_inventory_fqdn,"*Reporter Analysis:* 

We have observed that testbed which is not having FQDN+DELAY Feature is expecting device to be discovered with FQDN which is blocking runs.

Can we have a  check and skip the tc as it is blocking while running discovery itself with non-optimised lan script

*Description*:  

{noformat}1977: 
 Traceback (most recent call last):{noformat}

{noformat}1978: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper{noformat}

{noformat}1979: 
     result = testfunc(func_self, **kwargs){noformat}

{noformat}1980: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 951, in test1_verify_adding_range_discovery_ssh_global_credentials{noformat}

{noformat}1981: 
     if not self.testbed.devices[device].device_fqdn:{noformat}

{noformat}1982: 
 AttributeError: 'Test_TC9_DNAC_Verify_adding_range_discovery_ssh_gl' object has no attribute 'testbed'{noformat}

{noformat}1984: 
 Errored reason: 'Test_TC9_DNAC_Verify_adding_range_discovery_ssh_gl' object has no attribute 'testbed'{noformat}

Initially we tried 2-60 testcases and it was stuck in loop as discovery failed :
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-sanity-common-Multi-job/658/consoleFull|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-sanity-common-Multi-job/658/consoleFull]

Later we aborted and
retried TC 9 alone  after making some commits in fabric json as per wiki again it failed:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/configs/sanity_tb6/solution_sanityeca_lan_SanityTB6.json?until=6861c5f7c3e1ee35760994240b6ea75b542c744a&at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/configs/sanity_tb6/solution_sanityeca_lan_SanityTB6.json?until=6861c5f7c3e1ee35760994240b6ea75b542c744a&at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto]

*Branch Name:* Hulk-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

*Fail Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=396558&size=7817&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug10_03:35:48.121839.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=396558&size=7817&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug10_03:35:48.121839.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Issue Seen first time or day0 issue:

If we are not testing optimsed for TB2 but if we test optimised code ,sub testacsebis missing in Optimised code , Latest code from Optimised Hulk :
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery.py-51-toolsDiscovery&begin=3596&size=234105&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug04_04:48:30.953774.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery.py-51-toolsDiscovery&begin=3596&size=234105&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug04_04:48:30.953774.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-10T18:06:52.799+0000, fix [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0cc3557f713c67066759df026cdcb272f881c0fe|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0cc3557f713c67066759df026cdcb272f881c0fe],"['Auton', 'Hulk', 'Integration', 'Sanity']",Raji Mukkamala,Resolved,Anusha John
SEEN-2047,https://miggbo.atlassian.net/browse/SEEN-2047,[Halleck][AUTON]-Test_TC204_configure_device_into_specific_NDG,"DNAC Release_Version Tested:Hulk ber ISO - 2.1.710.70497, Non-FIPS

Device Image Used: 17.12.1

Testbed: AWS-Multisite

Branch Used: private/Hulk-ms/api-auto

Script Name: solution_test_3sites_sjc_nyc_sf.py

Solution Input File : dnac-auto/configs/config_48hr_test/solution_test_input.json

Testcases Impacted:   Test_TC204_configure_device_into_specific_NDG

not exist', 'isError': True, 'instanceTenantId': '64ca09d59e81e428989c044e', 'id': 'c159c4a8-bc3d-45db-8bda-3ea0dc4662cf'}
11498:  Library group ""task"" method ""wait_for_task_complete"" returned in 0:00:02.112393
11499:  Something wrong happend when assign tag to device FW-5520-1. Check it out
11500:  Library group ""inventory"" method ""apply_tag_to_device"" returned in 0:00:02.553445
11501:  Library group ""inventory"" method ""assign_tag_to_device"" returned in 0:00:03.434049
11502:  Test returned in 0:00:05.249373
11503:  Failed reason: Something wrong happend when assign tag to device
11504:  The result of section test1_assign_tag_to_device is => FAILED

Please find the Failed Log:[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-08/auto_MS_job.2023Aug11_03:48:03.843059.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-08/auto_MS_job.2023Aug11_03:48:03.843059.zip&atstype=ATS]  --Refer TC204
",2023-08-11T11:49:43.935+0000,"Hi Balaji, can you provide failed log for more detail?

and, can you use main branch to chery-pick to you branch? * PR-link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6623/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6623/overview]
* Summary: get the wrong data for api response, changed it back, add retry

please cherry-pick to your branch","['Auton', 'Halleck']",QuangVinh Nguyen,Resolved,Balaji Raju
SEEN-2055,https://miggbo.atlassian.net/browse/SEEN-2055,[Auton]: [Sanity] Test_TC123_aaa_per_ssid /test3_onboard_wireless_segment_for_new_ssid-  where it failed to provision APs when AI PROFILE is enabled,"*Description:*  The  *Test_TC123_aaa_per_ssid /test4_connect_clients_to_ssids* sub-tc [test3_onboard_wireless_segment_for_new_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3183297&size=892950&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug04_01:11:12.566850.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] failed for AP provisioning due to AP being provisioned using Basic RF profile where as on DNAC AI PROFILE is enabled for the APs from the previous tests. 

Hence the failed with reason: *AP cannot be provisioned to Basic RF profile when AI is enabled.*

The tests need to handle this by adding a check to verify if AI profile is enabled or not and  then based on the AI profile enable/disable status we can pick the profile to provision APs.

*Error Snippet:* 


{noformat}Provisioning AP of device failed for reason:AP cannot be provisioned to Basic RF profile when AI is enabled.
 Result: AP Provision failed{noformat}



*Branch:* private/Hulk-ms/sanity_api_auto, private/HulkPatch-ms/sanity_api_auto, 

*Script file:* solution_test_sanityecamb_lan.py /*Test_TC123_aaa_per_ssid / test3_onboard_wireless_segment_for_new_ssid*

*input file:* solution_test_input.json

*Failed log:*  [Test_TC123_aaa_per_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=815837&size=3550794&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep05_12:41:35.708494.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed*: Sanity Testbed TB3",2023-08-15T06:34:04.790+0000,"Fix Ghost : [1aeb3f26f71|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/1aeb3f26f711c7910db95a82c697387c5449ad14]

Hulk: [e899ad046f8|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e899ad046f8b60f3fc9c3cc523a51335f1961ce5] Thank you Raji 
I will try to validate this fix in current runs and update the results here","['AWS_Sanity', 'Auton', 'Hulk', 'Integration', 'sanity']",Raji Mukkamala,Resolved,Ashwini R Jadhav
SEEN-2056,https://miggbo.atlassian.net/browse/SEEN-2056,Auton: [Sanity] Test_TC202_site_tags_negative_operations/ test10_verify_removing_of_site_building_floor where there is a need of API and payload upliftment for changing APs location,"*Description:* The test [Test_TC202_site_tags_negative_operations|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=28343407&size=2430813&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug02_12:00:55.437774.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] which has sub-tc  [test10_verify_removing_of_site_building_floor|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=30275127&size=9480&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug02_12:00:55.437774.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] that is trying to delete the site/floor from DNAC which is not associated with any device. The deletion of this site/floor has been failing. 

The issue is with the api that is being used to change the AP location which needs upliftment on the current scripts. The API and payload for changing APs location has been totally changed for both Ghost and Hulk.

*Error Snippet:* 

*Task Completion: NCGR10013: Group contains one or more members. Hence can't be deleted.* {{Failed to delete a site!!}}

*Branch:* private/Ghost-ms/sanity_api_auto

*Script file:* solution_test_sanityecamb_lan.py 

*input file:* solution_test_input.json

*Failed log:*  [Test_TC202_site_tags_negative_operations|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=28343407&size=2430813&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug02_12:00:55.437774.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] / [test10_verify_removing_of_site_building_floor|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=30275127&size=9480&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug02_12:00:55.437774.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed*: Sanity Testbed TB3",2023-08-15T06:50:29.877+0000,"Moe can you prioritize it just an api upllift. PR: 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6628/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6628/overview]


Logs: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/moesaeed-sjc/pyats2/users/moesaeed/archive/23-08/sanity_TB17.2023Aug15_17:04:19.088967.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/moesaeed-sjc/pyats2/users/moesaeed/archive/23-08/sanity_TB17.2023Aug15_17:04:19.088967.zip&atstype=ATS] Thanks [~accountid:63f50bfce8216251ae4d59d5] 
","['Auton', 'Ghost', 'Hulk', 'Integration', 'Uplift', 'sanity']",Moe Saeed,Resolved,Ashwini R Jadhav
SEEN-2080,https://miggbo.atlassian.net/browse/SEEN-2080,[Halleck][AUTON]-Test_TC205_check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE,"Description

DNAC Release_Version Tested:Hulk ber ISO - 2.1.710.70497, Non-FIPS

Device Image Used: 17.12.1

Testbed: AWS-Multisite

Branch Used: private/Hulk-ms/api-auto

Script Name: solution_test_3sites_sjc_nyc_sf.py

Solution Input File : dnac-auto/configs/config_48hr_test/solution_test_input.json

Testcases Impacted:   Test_TC205_check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE",2023-08-16T08:46:25.469+0000,"Failed Log:[https://ngdevx.cisco.com/services/taas/results/323dd88d-a03f-48d5-b175-a1c1990596a4/run-results|https://ngdevx.cisco.com/services/taas/results/323dd88d-a03f-48d5-b175-a1c1990596a4/run-results]

13391: 2023-08-15T23:40:52:      raise e
13392: 2023-08-15T23:40:52:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/AWS-Multisite-Common-Job/services/dnaserv/client_manager.py"", line 326, in call_api
13393: 2023-08-15T23:40:52:      response.raise_for_status()
13394: 2023-08-15T23:40:52:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status
13395: 2023-08-15T23:40:52:      raise HTTPError(http_error_msg, response=self)
13396: [st-ds-2.cisco.com|http://st-ds-2.cisco.com]: 2023-08-15T23:40:52: %API-GROUP-ORCHESTRATION_ENGINE-3-ERROR: %[pid=3593707][pname=Task-1]: Error in deleting config preview activity id: 1d1ead08-1307-431e-b32b-e3cc6c13c92f
13397: 2023-08-15T23:40:52:  Library group ""orchestration_engine"" method ""delete_config_preview_actionable_items"" returned in 0:00:16.378443 Hi Balaji. I can not hit the error like you reported. can you please use the lastest code to trigger the testcase? since no log from reporter, i will close this auton. If anyone see error again, please re-open the auton @Quang  i executed the script in the Hulk P1RC2 2.3.7.3-70302 Testcases is failed [Test_TC205_configure_device_into_specific_NDG|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2022999&size=137844&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct30_00:48:34.845925.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  Tag of device and ise is not match error getting hi [~accountid:641058d57222b08f3e7064d0]. which branch did you using? since it’s not related to previous failed, so i closed this auton. please raised other auton if you needed","['Auton', 'Hulk']",QuangVinh Nguyen,Closed,Balaji Raju
SEEN-2081,https://miggbo.atlassian.net/browse/SEEN-2081,[Auton]:Ghost/Hulk[Optimization]: Test_TC2_DNAC_TSIM_static_onboarding_verifications /test7_prepare_aps_dot1x,"Hi  Team
We have observed that TCs are failing on optimized code  (Hulk/Ghost)due to the script being used wrong ID/Password. After some time, the AP goes into lock mode (inaccessible via telnet). However, when we run the same TC from the Main script, the test case is able to log in to the AP and execute the ""capwap ap erase all"" command.

*  Need  to add  validation ,If the ISE version is less than 3.1, the test case should be skipped.

+*Main script  Pass Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=552374&size=11439&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug10_02:26:23.005465.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=552374&size=11439&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug10_02:26:23.005465.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  

+*Optimation code Failed  Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding&begin=3741447&size=118048&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug14_23:37:19.337735.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding&begin=3741447&size=118048&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug14_23:37:19.337735.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Script Name:* Optimized code 
\testcases\sanityusecases\

{{TESTSCRIPT_FILE : testcases/forty_eight_hour/solution_test_sanityecamb_lan.py}}
{{SECURE_FABRIC_FILE : solution_test_input.json}}
[solution_test_input.json|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/config_48hr_test/solution_test_input.json?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto]
*Branch Name:*  private/Hulk-ms/sanity_api_auto 
private/Ghost-ms/sanity_api_auto

*Script file/Usecase:* solution_test_sanityecamb_lan.py

*Source Team:  Sanity*",2023-08-16T09:02:44.795+0000,"Ghost: [27fa8c40692|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/27fa8c40692bee823e6fd39a636b1710a84a43d0]

Hulk: [1662497b2a4|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/1662497b2a421bf4c2f11e25dc9a6eaa55f7c04d] Hi  [~accountid:63f50bf5e8216251ae4d59cf]  ,


TCs are failing on optimized code (Hulk/Ghost) due to the script using the wrong ID/Password, according to the log. After some time, the AP goes into lock mode (inaccessible via telnet). However, when we run the same TC from the Main script, the test case is able to log in to the AP and execute the ""capwap ap erase all"" command.
Could  please  fix this issue   in optimized  code 


+*Optimized  snip*+ 

!image-20230913-140542.png|width=968,height=848!

+*Main script  snip :*+  



!image-20230913-140650.png|width=1041,height=821!


 [~accountid:620b8357878c2f00729881c8] {{Connected to 10.30.0.71. Escape character is '^]'. % Authentication failed No valid user found, please configure a valid user from ControllerTrying 10.30.0.71..}} Looks like the controller does not have ap credentials configured. Next time test fails can you check on the controller if credentials are configured. test6_configure_ap_username_password configures ap credentials and the test passed.  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding&begin=3709466&size=31981&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug14_23:37:19.337735.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-125-SDAwiredHostOnboarding&begin=3709466&size=31981&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug14_23:37:19.337735.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  Hi [~accountid:63f50bf5e8216251ae4d59cf]  ,
Main script  =>
Test_TC42_ipphone_onboarding_verifications/test4_configure_ap_username_password

In the main script, sub TC is included in TC42, but in the optimized script, all AP TCs are running together in the same UCG. This might be causing issues.

  it's running the main script  Test_TC45_DNAC_TSIM_static_onboarding_verifications

+*Main script*+:
[solution_test_sanityecamb_lan.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py?at=refs%2Fheads%2Fprivate%2FHulkPatch-ms%2Fsanity_api_auto]

+*Optimized :*+
[SDAwiredHostOnboarding|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/sanityusecases/SDAwiredHostOnboarding?at=refs%2Fheads%2Fprivate%2FHulkPatch-ms%2Fsanity_api_auto]","['Auton', 'Ghost', 'Hulk', 'Optimized', 'Sanity']",Raji Mukkamala,Reopened,Omkar Sharad Wagh
SEEN-2082,https://miggbo.atlassian.net/browse/SEEN-2082,Auton:Guardian:Test_TC171_syslog_server_event_notification/test7_flap_interface_verify_client_dot1x,"*Reporter Analysis:* 

We observed that syslog sub testcases even if it is not flapping interface testcase is looping infinitely 
It was stuck in 17:18:48 , once i restarted clients only testcase failed.
Can we please add some check and not to loop like infinetly instead it should fail within some time limit.



*Description*:  

{noformat}575134: 
 #!!!Client TB6-wired-client3: Failed to flap interface LAB-NET-WiredClient!!!#{noformat}

{noformat}575135: 
 #############################################{noformat}

{noformat}575137: 
 HTTPConnectionPool(host='10.195.227.96', port=4000): Max retries exceeded with url: /jsonrpc (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f0ff5cd7280>: Failed to establish a new connection: [Errno 111] Connection refused')){noformat}

{noformat}575138: 
 #############################################{noformat}

{noformat}575139: 
 #!!!Client TB6-wired-client2: Failed to flap interface LAB-NET-WiredClient!!!#{noformat}

{noformat}575140: 
 #############################################{noformat}

{noformat}575141: 
 #######################################{noformat}

{noformat}575142: 
 Failed to flap interface for some clients. Failed list: ['TB6-wired-client1', 'TB6-wired-client3', 'TB6-wired-client2']{noformat}

{noformat}575143: 
 #######################################{noformat}

{noformat}575144: 
 Failed to flap wired client interfaces{noformat}

*Branch Name:* Ghost-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

*Failed Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=113924284&size=157019&archive=env_auto_job.2023Aug14_07:40:55.928552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=113924284&size=157019&archive=env_auto_job.2023Aug14_07:40:55.928552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=61233801&size=1739724&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May31_20:37:18.432295.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=61233801&size=1739724&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-05%2Fenv_auto_job.2023May31_20:37:18.432295.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-16T12:48:05.152+0000,"with reporter’s confirmation, i closed it because issue haven’t seen now, can not be re-produced. The reporter will reopen it if hitting again","['Auton', 'Ghost', 'Guardian', 'Hulk', 'Sanity']",QuangVinh Nguyen,Closed,Anusha John
SEEN-2083,https://miggbo.atlassian.net/browse/SEEN-2083,Auton:Guardian:Test_TC43_DNAC_EXT_NODE_interface_config_verifications/test10_AEN_onboarding_version_Validation,"*Reporter Analysis:* 

Once AEN node checks the ISE version and figures out it is not supported i.e, less than ISE 3.1 Version ,
It should skip remaining testcases related to AEN , observed it is continuing runs. I was using ISE2.7P10


Similar checks should be present for AEN node negative operations adn dot1x AP’s
[Test_TC157_DNAC_AEN_ext_nodes_assurance|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=109127021&size=1749075&archive=env_auto_job.2023Aug14_07:40:55.928552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[Test_TC45_DNAC_TSIM_static_onboarding_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=26985782&size=2453916&archive=env_auto_job.2023Aug14_07:40:55.928552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Description*:  

{noformat}149780: 
 Could not update the AEN on the segments as requested, reason: :Supplicant-Based Extended Node feature is only supported on Cisco ISE versions 3.1 and above.{noformat}

{noformat}149902: 
 Could not update the AEN on the segments as requested, reason: :Supplicant-Based Extended Node feature is only supported on Cisco ISE versions 3.1 and above.{noformat}

{noformat}149990: 
 Could not update the AEN on the segments as requested, reason: :Supplicant-Based Extended Node feature is only supported on Cisco ISE versions 3.1 and above.{noformat}

{noformat}149992: 
{noformat}

{noformat}149993: 
 ------------------------------------------------------------{noformat}

{noformat}149994: 
 Result: Failed to enable AEN on all segments POOLs on all sites on Virtual Network profile 'INFRA_VN'.'{noformat}

{noformat}149995: 
 ------------------------------------------------------------{noformat}

{noformat}150023: 
 None{noformat}

{noformat}150024: 
 Error: unable to upload CA PKI certificate to ISE, reason:[Errno Expecting value]   <!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.01//EN"" ""http://www.w3.org/TR/html4/strict.dtd"">{noformat}

{noformat}150025: 
 <!--**************************************************-->{noformat}

*Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=20750111&size=5883302&archive=env_auto_job.2023Aug14_07:40:55.928552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=20750111&size=5883302&archive=env_auto_job.2023Aug14_07:40:55.928552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*False Pass log for Dot1x AP’s:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=26985782&size=2453916&archive=env_auto_job.2023Aug14_07:40:55.928552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=26985782&size=2453916&archive=env_auto_job.2023Aug14_07:40:55.928552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*AEN Negative operation also should skip after the check:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=109127021&size=1749075&archive=env_auto_job.2023Aug14_07:40:55.928552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=109127021&size=1749075&archive=env_auto_job.2023Aug14_07:40:55.928552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-16T12:59:02.216+0000,"[~accountid:61efa8c457b25b006877eda3] , what script file is this? It should already skip the rest of the use cases.  [~accountid:63f50bfce8216251ae4d59d5] 


I am using 
Script: solution_test_sanityecamb_lan.py

Branch :  private/Guardian-ms/sanity_api_auto PR for Guardian: 

sync the changes to other script files 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6650/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6650/overview] Thanks [~accountid:63f50bfce8216251ae4d59d5]  for the fixes

Can you please add similar check for below testcases also:

Similar checks should be present for AEN node negative operations adn dot1x AP’s
[Test_TC157_DNAC_AEN_ext_nodes_assurance|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=109127021&size=1749075&archive=env_auto_job.2023Aug14_07:40:55.928552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[Test_TC45_DNAC_TSIM_static_onboarding_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=26985782&size=2453916&archive=env_auto_job.2023Aug14_07:40:55.928552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [dot1x ap]/[test5_onboard_ap_dot1x|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=29199650&size=3459&archive=env_auto_job.2023Aug14_07:40:55.928552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]  [~accountid:61efa8c457b25b006877eda3] ,

No need to check for T[+est_TC157_DNAC_AEN_ext_nodes_assurance+|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=109127021&size=1749075&archive=env_auto_job.2023Aug14_07:40:55.928552.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS], since it actually it would work either AEN or PEN, so it should be fine. It just test unreachability scenarios.  Please confirm so I can close this Ticket  [~accountid:63f50bfce8216251ae4d59d5]  AEN node negative operation is fine but can we add same check of ISE and block the testcases for AP-Dot1x Needs a confirmation if either needed for dot1x or not!?  Closing this issue since it has already been addressed the initial request. ","['Auton', 'Guardian', 'Sanity']",Moe Saeed,Closed,Anusha John
SEEN-2102,https://miggbo.atlassian.net/browse/SEEN-2102,Test_TC11_DNAC_Verify_adding_range_discovery_ssh_global_credentials,"Test_TC11_DNAC_Verify_adding_range_discovery_ssh_global_credentials

Sub test Effected : test1_verify_adding_range_discovery_ssh_global_credentials



Testcase is having LLDP discovery where it is doing the discovery for each and every device and it’s neighbors as well so it is taking around 6hrs of time to complete one single testcase.

Attached the Log captured for Hulk in a file above.

where in same testcase in Ghost doesn’t have the LLDP discovery included in it.

Ghost Log for TC11.1:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4296473&size=361099&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-07%2Fsanitycombine.2023Jul31_10:28:15.101087.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4296473&size=361099&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-07%2Fsanitycombine.2023Jul31_10:28:15.101087.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]

Branch used : bgl/Hulk-ms/api-auto - Hulk runs

Branch used : bgl/HulkPatch-ms/api-auto - Hulk Patch runs",2023-08-17T10:45:18.706+0000,Duplicate [https://miggbo.atlassian.net/browse/SEEN-1934|https://miggbo.atlassian.net/browse/SEEN-1934|smart-link] ,"['Auton', 'Hulk']",Raji Mukkamala,Closed,SAINATH CHATHARASI
SEEN-2150,https://miggbo.atlassian.net/browse/SEEN-2150,[Auton] - Provision of AP with Custom RF profile gets errored out with list index out of range ,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/Hulk-ms/api-auto

*Uber ISO tested:* Ghost Patch2 RC2 - 2.1.614.70836

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

When integrating the feature corresponding to Custom RF Profile, we see sub TC corresponding to Provision of APs with custom RF profile gets errored out due to list index out of range for wireless client check despite having wireless client associated to AP.

Based on the execution log, we see the TC logic has been implemented below way for this subTC2:

1) Check for presence of AI_RF_PROFILE tag on Wireless controllers (Aireos WLC or ECA 9K or EWLC 9800). If present need to skip and check for next wireless controller.

2) If condition 1) above is satisfied, On that wireless controller, again check is added for presence of APs which are not associated with any wireless clients. Only APs which are not associated with any wireless clients are used for provisioning. Not sure why this check has been implemented.

Given these above conditions, as per our failed log, we have AI_RF_PROFILE tag already added part of SF site (EWLC 9800) and NY sites (9400 ECA and 9300 ECA) part of basic SDA bringup for Kairos verification feature. Only SJ site with two Aireos WLC devices does not have AI_RF_PROFILE Tag and hence 5520-2 device got picked for provisioning of AP. But AP joined to this WLC is associated with wireless client. Due to this the TC is getting errored out as there no APs which are not associated with wireless clients. 

This issue needs to be fixed and logic needs to be revisited.

*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/Custom+RF+Profile+-+HD|https://wiki.cisco.com/display/EDPEIXOT/Custom+RF+Profile+-+HD]

*Failed log* - [Test_TC252_Provision_single_AP_with_custom_rf_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4677196&size=113074&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb_multi_sites_mdnac.2023Aug17_11:34:04.712928.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-18T13:11:42.113+0000,"Addressed [3ea7f94022b|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3ea7f94022bd1c17ab9fa827006b97a01efaae04] and cherry picked to Hulk, Hulk Patch1 and 2 branches","['Auton', 'Ghost', 'Integration', 'MSTB1', 'MSTB2', 'Multisite']",Raji Mukkamala,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2179,https://miggbo.atlassian.net/browse/SEEN-2179,[Auton][MSTB2] : Test_TC193_Exclude_interfaces_port_from_NBAR_usecase,"Hi [~accountid:63f50bcafb3ac4003fa2c6dd],

I’ve started integrating “Test_TC193_Exclude_interfaces_port_from_NBAR_usecase” testacse on MSTB2 but script fails with below error

Error:

=====

21349:  Resource path full url: [https://10.195.243.37/api/v1/ncp-node/store/autocomplete|https://10.195.243.37/api/v1/ncp-node/store/autocomplete]
21350:  Traceback (most recent call last):
21351:    File ""/auto/dna-sol/ws/divayada/Hulk-Main/dnac-auto/services/commonlibs/test_wrapper.py"", line 301, in wrapper
21352:      result = testfunc(func_self, **kwargs)
21353:    File ""/auto/dna-sol/ws/divayada/Hulk-Main/dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py"", line 18745, in test2_exclude_interfaces_port_from_NBAR
21354:      if dnac_handle.update_excluded_interfaces_list(dev_name=self.dev_name, interfaces_list=self.interfaces_list):
21355:    File ""/auto/dna-sol/ws/divayada/Hulk-Main/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper
21356:      result = method(*args, **kwargs)
21357:    File ""/auto/dna-sol/ws/divayada/Hulk-Main/dnac-auto/services/dnaserv/lib/api_groups/cfs_intent_apprecognition/group.py"", line 151, in update_excluded_interfaces_list
21358:      if dev_name not in response_full_name['response'][0]:
21359:  IndexError: list index out of range
21360:  Test returned in 0:00:00.142589

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3996458&size=503576&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb2_three_sites.2023Aug20_23:46:47.007699.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3996458&size=503576&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb2_three_sites.2023Aug20_23:46:47.007699.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Script Name : solution_test_3sites_sjc_nyc_sf

Branch : private/Hulk-ms/api-auto

Testacse Affected : 

[test2_exclude_interfaces_port_from_NBAR|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4009812&size=10272&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb2_three_sites.2023Aug20_23:46:47.007699.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

[test3_verify_there_is_no_config_ip_nbar_protocol_discovery|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4020084&size=236477&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb2_three_sites.2023Aug20_23:46:47.007699.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

[test4_enable_on_all_interfaces|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4256561&size=7288&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb2_three_sites.2023Aug20_23:46:47.007699.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-21T08:25:20.449+0000,"# PR Hulk-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6686/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6686/overview]
# Test Case:  {{TC_Exclude_interfaces_port_from_NBAR_usecase}}
# Testbed: INTG2
# Script files: testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py
# Trade log link Hulk-ms/api_auto:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-08/sanity-intg2.2023Aug21_02:37:44.825938.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-08/sanity-intg2.2023Aug21_02:37:44.825938.zip&atstype=ATS] [~accountid:63f50bcafb3ac4003fa2c6dd] , the complain is actually related to “ncp-node” API usage, which is limited on ESXi formfactor.

As a result, code should make use of different API to find the “full name” of the device.

Raised new PR for the mentioned complaint:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7443/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7443/overview] Thank [~accountid:62ab7a399cd13c0068b18fe0] for your comment and new PR. Required changes have been merged to private/HulkPatch-ms/api-auto and cherry-picked to private/Hulk-ms/api-auto and private/HulkPatch2-ms/api-auto.

Marking this Auton as “Closed”.","['Auton', 'Hulk', 'Integration', 'MSTB2']",NhanHuu Nguyen,Closed,Divakar Kumar Yadav
SEEN-2228,https://miggbo.atlassian.net/browse/SEEN-2228,[Auton]:Ghost_RC3 - TC17_DNAC_Site_Building_Floor_Addition / test1_verify_creating_of_site_building_floor,"Reporter Analysis:

Script is failing in Creating site/building/floor addition, please find the attached snaps for your detailed reference.

Branch : private/Ghost-ms/sanity_api_auto

DNAC Version - 2.3.5.4-70852

Cluster – 10.22.40.52 [admin/Maglev123]

Failed TC – TC17

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Script file /Use case -* *solution_test_sanityecamb_lan.py*

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal%2Fusers%2Fanusjohn%2Farchive%2F23-08%2Fenv_auto_job.2023Aug22_04:06:07.590054.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal%2Fusers%2Fanusjohn%2Farchive%2F23-08%2Fenv_auto_job.2023Aug22_04:06:07.590054.zip&atstype=ATS]",2023-08-23T08:29:39.246+0000,"Add a usecase to add Proxy in DNAC. There is a flag to be enabled in fabric input file to configure proxy.  Thank [~accountid:5f3c6ae932360700388f7b4b]  for the suggestion,

I've submitted PR - [Link|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6754/overview] to resolve this issue already. Reviewers, please review and approve it.

Thanks Needed one more update for optimized suite run.

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/51fe02868c89c1d19e0a6551f9cd0947c0ac7179|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/51fe02868c89c1d19e0a6551f9cd0947c0ac7179]","['BAPI', 'auton', 'express-sanity', 'ghost', 'optimized', 'sanity']",ThanhTan Nguyen,Resolved,KARTHIKEYAN KRISHNAMURTHY
SEEN-2229,https://miggbo.atlassian.net/browse/SEEN-2229,[Auton] [Sanity]:Hulk P1 Test_TC19_DNAC_Wireless_SSID_creation_open_enterprise/test1_configure_wireless_RF_profile which failed in creating custom rf profile,"*Description:* 
The test [test1_configure_wireless_RF_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=495928&size=981&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug23_04:13:36.151064.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] which failed while creating custom rf profile on Hulk P1.  The creation of custom rf profile payload is having parentProfileC to be TYPICAL where as it should be either CUSTOM or GLOBAL. 

This is a new validation that is been added as per the CDETS:  [*CSCwh00030*|https://cdetsng.cisco.com/summary/#/defect/CSCwh00030] : Custom 6GHz RF profile is not created when a system RF profile (LOW/TYPICAL/HIGH) is cloned
This CDET has been fixed on Hulk Build 70132 onwards

!custom_rf_profile_payload.png|width=542,height=332!


*Error Snip:* Message:{""response"":{""errorCode"":""NCND00049"",""message"":""NCND00049: Invalid request. Please check the request parameters and resubmit"",""detail"":""Permissible Parent Profile for 6Ghz is CUSTOM and GLOBAL for custom RF Profile testProfile ""} 

*LOG:* This has been observed on current Hulk P1 70139 runs 
 [Test_TC19_DNAC_Wireless_SSID_creation_open_enterprise|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1096340&size=446427&archive=%2Fusers%2Fajadhav2%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug22_08:01:31.566003.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=ajadhav2&from=trade&view=all&atstype=pyATS] / [test1_configure_wireless_RF_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1097088&size=171622&archive=%2Fusers%2Fajadhav2%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug22_08:01:31.566003.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=ajadhav2&from=trade&view=all&atstype=pyATS] 

we have a *pass log on the build Hulk P1 70125* : [https://ngdevx.cisco.com/services/taas/results/13bc766d-115b-4d81-8739-18f14349b89e/run-results|https://ngdevx.cisco.com/services/taas/results/13bc766d-115b-4d81-8739-18f14349b89e/run-results] 

Branch: *private/HulkPatch-ms/sanity_api_auto*

*Test script:* testcases/forty_eight_hour/solution_test_sanityecamb.py
*Usecase:* UC5.2 designWireless",2023-08-23T13:41:09.023+0000,"Hi Team ,
Build: Hulk Patch-1 2.1.713.70147
same issue is seen in the latest Hulk P1. A production test case has failed. Could you please check the priority

Failed  Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-base_wireless_ssid_profile_guest.py-53-designWireless&begin=4033&size=685234&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug28_08:17:39.169043.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-base_wireless_ssid_profile_guest.py-53-designWireless&begin=4033&size=685234&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug28_08:17:39.169043.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Required PR has been raised for *private/HulkPatch-ms/api-auto* branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6799/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6799/overview] PR got approved and merged to *private/HulkPatch-ms/api-auto* branch.

Same has been cherry-picked to private/HulkPatch2-ms/api-auto branch.

Marking this Auton as “Resolved”.","['Auton', 'Blocked', 'Ghost', 'Hulk', 'HulkPatch', 'Optimized', 'Uplift', 'exsivm', 'hulk-vm-sanity', 'sanity']",Amardeep Kumar,Closed,Ashwini R Jadhav
SEEN-2230,https://miggbo.atlassian.net/browse/SEEN-2230,[Auton]:  EXSI VM Upgrade: Test_TC7_DNAC_Device_Re_Provisioning_after_upgrade/test1_verify_provision_the_devices_fabric1,"*Reporter Analysis:* 

Observed in upgrade verify sanity script  *TC7 Device Re-provision* didn’t complete the execution even after more than 2 hours it’s keep on looping it with with below message:

_2023-08-24T01:07:04: %SERVICES-INFO: Index: 3 with empty Context_
_2023-08-24T01:07:04: %SERVICES-INFO: Sending newline to get proper prompt for next iteration_
_2023-08-24T01:07:06: %SERVICES-INFO: Encountered empty activity warning prompt with auto-auth_
_2023-08-24T01:07:06: %SERVICES-INFO: Index: 3 with empty Context_
_2023-08-24T01:07:06: %SERVICES-INFO: Sending newline to get proper prompt for next iteration_
_2023-08-24T01:07:07: %SERVICES-INFO: Encountered empty activity warning prompt with auto-auth_
_2023-08-24T01:07:07: %SERVICES-INFO: Index: 3 with empty Context_



*TC name:TC7_DNAC_Device_Re_Provisioning_after_upgrade/test1_verify_provision_the_devices_fabric1*  

*Build used:*  3.710.75530

*Branch Name:*  private/Ghost-ms/sanity_api_auto

*Script file/Usecase:* 
after_upgrade_verify.py

*Source Team:  EXSI Upgrade Sanity* 

*Failed  logs:* 

 Script execution aborted due to take more time. Even more 21 hours observed execution not completed

Attached Jenkins  execution logs

Please find jenkins log:

[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Upgrade-Verify-common-Multi-job/128/console|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Upgrade-Verify-common-Multi-job/128/console]



*Previous build passed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=539073&size=1731265&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul11_05:58:30.774068.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=539073&size=1731265&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_auto_job.2023Jul11_05:58:30.774068.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-24T10:02:36.293+0000,"*Build used:* Hulk RC4

*Branch Name:*  private/Hulk-ms/sanity_api_auto

*Script file/Usecase:* solution_test_sanityecamb_lan.py


*Source Team:  EXSI Upgrade Sanity*

Same issue is observed on sanity Hulk RC4  runs from UC 9-13 is stuck in looping:
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/379/console|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/379/console]
My job is looping more than 1 hour in :
2023-08-28T21:12:20: %SERVICES-INFO: Index: 3 with empty Context
2023-08-28T21:12:20: %SERVICES-INFO: Sending newline to get proper prompt for next iteration
2023-08-28T21:12:22: %SERVICES-INFO: Encountered empty activity warning prompt with auto-auth
2023-08-28T21:12:22: %SERVICES-INFO: Index: 3 with empty Context
2023-08-28T21:12:22: %SERVICES-INFO: Sending newline to get proper prompt for next iteration
2023-08-28T21:12:24: %SERVICES-INFO: Encountered empty activity warning prompt with auto-auth
2023-08-28T21:12:24: %SERVICES-INFO: Index: 3 with empty Context
2023-08-28T21:12:24: %SERVICES-INFO: Sending newline to get proper prompt for next iteration
Execution is still in progress we are unable to figure out TC name as it is optimised code


Aborted the job as it is looping and runs are blocked *Build used:* Ghost_RC3

*Branch Name:*  private/Hulk-ms/sanity_api_auto

*Script file/Usecase:* solution_test_sanityecamb_lan.py

*Source Team:   Sanity*

[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/Express-Sanity/job/Hulk/job/Hulk-BAPI-Optimized-Deployment_and_Express_Sanity/58/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/Express-Sanity/job/Hulk/job/Hulk-BAPI-Optimized-Deployment_and_Express_Sanity/58/]

The above job is looping more than 2 hours

h1. 
Progress:Console Output

Skipping 138,707 KB.. [Full Log|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/Express-Sanity/job/Hulk/job/Hulk-BAPI-Optimized-Deployment_and_Express_Sanity/58/consoleFull]

{noformat}ICES-INFO: Index: 3 with empty Context
2023-08-29T03:46:23: %SERVICES-INFO: Sending newline to get proper prompt for next iteration
2023-08-29T03:46:25: %SERVICES-INFO: Encountered empty activity warning prompt with auto-auth
2023-08-29T03:46:25: %SERVICES-INFO: Index: 3 with empty Context
2023-08-29T03:46:25: %SERVICES-INFO: Sending newline to get proper prompt for next iteration
2023-08-29T03:46:28: %SERVICES-INFO: Encountered empty activity warning prompt with auto-auth
2023-08-29T03:46:28: %SERVICES-INFO: Index: 3 with empty Context
2023-08-29T03:46:28: %SERVICES-INFO: Sending newline to get proper prompt for next iteration
2023-08-29T03:46:30: %SERVICES-INFO: Encountered empty activity warning prompt with auto-auth
2023-08-29T03:46:30: %SERVICES-INFO: Index: 3 with empty Context
2023-08-29T03:46:30: %SERVICES-INFO: Sending newline to get proper prompt for next iteration
2023-08-29T03:46:32: %SERVICES-INFO: Encountered empty activity warning prompt with auto-auth
2023-08-29T03:46:32: %SERVICES-INFO: Index: 3 with empty Context
2023-08-29T03:46:32: %SERVICES-INFO: Sending newline to get proper prompt for next iteration
2023-08-29T03:46:33: %SERVICES-INFO: Encountered empty activity warning prompt with auto-auth
2023-08-29T03:46:33: %SERVICES-INFO: Index: 3 with empty Context
2023-08-29T03:46:33: %SERVICES-INFO: Sending newline to get proper prompt for next iteration
2023-08-29T03:46:35: %SERVICES-INFO: Encountered empty activity warning prompt with auto-auth
2023-08-29T03:46:35: %SERVICES-INFO: Index: 3 with empty Context
2023-08-29T03:46:35: %SERVICES-INFO: Sending newline to get proper prompt for next iteration
2023-08-29T03:46:37: %SERVICES-INFO: Encountered empty activity warning prompt with auto-auth
2023-08-29T03:46:37: %SERVICES-INFO: Index: 3 with empty Context
2023-08-29T03:46:37: %SERVICES-INFO: Sending newline to get proper prompt for next iteration
2023-08-29T03:46:39: %SERVICES-INFO: Encountered empty activity warning prompt with auto-auth
2023-08-29T03:46:39: %SERVICES-INFO: Index: 3 with empty Context
{noformat} Hi [~accountid:63f50bcece6f37e5ed93c87e]  ,
Same issue observed in the Hulk P1 run, could  you please  check  on the priority 
*Build used:* Hulk P1- 2.3.7.3-70139
*Branch Name: private/Hulk-ms/sanity_api_auto*
*Source Team:  Sanity*

 *Jenkins jo : 1 )* [https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-sanity-common-Multi-job/801/console|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-sanity-common-Multi-job/801/console]

*2)* [https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/403/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/403/]

*Branch Name: private/HulkPatch-ms/sanity_api_auto*
*Optimized code*  
3) [https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/409/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/409/]

!image-20230831-115924.png|width=1031,height=599! [ENG-SDN / dnac-auto / 45bbcc4fd84 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/45bbcc4fd84553b7aae2253fa8a79507f1fe90fb] fixed here","['Auton', 'Blocked', 'Sanity', 'Upgrade', 'exsi', 'hulk-vm-sanity']",Andrew Chen,Resolved,Raghavendrachar Baraguru Mallesha Char
SEEN-2238,https://miggbo.atlassian.net/browse/SEEN-2238,[Auton][MSTB3] : Talos Features is failing not connected to AWS maglev cli,"DNAC Release_Version Tested:Hulk RC1 Uber ISO - 2.1.710.70479, Non-FIPS

Device Image Used: 17.12.1

Testbed: AWS-Multisite

Branch Used: private/Hulk-ms/api_auto

Script Name: solution_test_3sites_sjc_nyc_sf.py

Solution Input File : dnac-auto/configs/config_48hr_test/solution_test_input.json

Testcases Impacted: Test_TC166_cloud_Talos_anomaly_detection

Failed Trade Log: [https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/SR-CL-MS/job/AWS-Multisite-Common-Job/807/consoleFull|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/SR-CL-MS/job/AWS-Multisite-Common-Job/807/consoleFull]



Issue Analysis: Not able to connect aws maglev client establish connection.",2023-08-25T09:40:43.436+0000,"the aws login is already handled. just make sure the key is correctly added to the input file.

!image-20230829-003509.png|width=708,height=277! [~accountid:63f50bfce8216251ae4d59d5] Yes i checked the key details and executed its not login to the maglev cli.  Fail Log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fauto_MS_job.2023Sep26_03:58:09.143534.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fauto_MS_job.2023Sep26_03:58:09.143534.zip&atstype=ATS] As discussed with you i have modified the key  details ""key"": ""/auto/dna-sol/ws/pem_key_aws/key-n-california.pem"", and executed the script still facing the issue  

Failed log: [https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/SR-CL-MS/job/AWS-Multisite-Common-Job/853/console|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/SR-CL-MS/job/AWS-Multisite-Common-Job/853/console] [~accountid:63f50bfce8216251ae4d59d5]  I have executed in the latest Hulk P1 2.1.713.70263 still facing the same issue [Test_TC166_cloud_Talos_anomaly_detection|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1699330&size=309809&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct09_21:24:50.843008.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Could you please check and update on this. 2 defects found in the UI of cloud that can be filed as a defects. And other issue is change in region. 
- UI Breakage due to change in components
- Region changes that can be updated through input files
- defects can be found in the APIs too, and You can inspect the UI and see the errors.","['Auton', 'Guardian']",Moe Saeed,Cancelled,Balaji Raju
SEEN-2239,https://miggbo.atlassian.net/browse/SEEN-2239,[Auton]:Ghost_P2_RC3 -  Test_TC33_DNAC_configure_virtual_network_ip_pool_with_traffic_types  /   test1_device_onboarding_update_virtual_network_pool_and_authentication_fabric1 ,"Reporter Analysis:

Script is failing in configuring the VN {{“L2only VN}}"" in San_Jose site , throwing the error as {{vn-name is not present in input data}}

Action: Adding VN-Name: None on site: Global/USA/SAN_JOSE_US_SJ_Fabric1

Segment POOl: {'fabric-name': 'Global/USA/SAN_JOSE_US_SJ_Fabric1',
'segments': [{'isFloodAndLearn': 'true',
               'isSelectiveFloodingEnabled': 'true',
               'isWirelessPool': 'false',
               'trafficType': 'DATA',
               'vlanName': 'L2OnlyVlan_sub'}]}
VN Context ID reference retrieved: [{'siteNameHierarchy': 'Global/USA/SAN JOSE', 'virtualNetworkName': ''}]
{'vlanName': 'L2OnlyVlan_sub', 'trafficType': 'DATA', 'isWirelessPool': 'false', 'isFloodAndLearn': 'true', 'isSelectiveFloodingEnabled': 'true'}

Error:

{noformat}5002: 
 Traceback (most recent call last):{noformat}

{noformat}5003: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/Express-Sanity/Ghost/Ghost-Legacy-Deployment_and_Express_Sanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper{noformat}

{noformat}5004: 
     result = testfunc(func_self, **kwargs){noformat}

{noformat}5005: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/Express-Sanity/Ghost/Ghost-Legacy-Deployment_and_Express_Sanity/testcases/ext_api_test/solution_test_sanityext_api.py"", line 1272, in test1_device_onboarding_update_virtual_network_pool_and_authentication_fabric1{noformat}

{noformat}5006: 
     if (dnac_handle.ext_device_onboarding_virtual_segment_mapping()):{noformat}

{noformat}5007: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/Express-Sanity/Ghost/Ghost-Legacy-Deployment_and_Express_Sanity/services/dnaserv/lib/decorators.py"", line 32, in wrapper{noformat}

{noformat}5008: 
     result = method(*args, **kwargs){noformat}

{noformat}5009: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/Express-Sanity/Ghost/Ghost-Legacy-Deployment_and_Express_Sanity/services/dnaserv/lib/ext_api_groups/ext_onboarding/group.py"", line 76, in ext_device_onboarding_virtual_segment_mapping{noformat}

{noformat}5010: 
     pool_name = segment['IPPOOLNAME']{noformat}

{noformat}5011: 
 KeyError: 'IPPOOLNAME'{noformat}

{noformat}5013: 
 Errored reason: IPPOOLNAME{noformat}

Branch : private/Ghost-ms/sanity_api_auto

DNAC Version - 2.3.5.4-70852

Failed TC – TC33

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Script file /Use case -* *solution_test_sanityecamb_lan.py*

Log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug24_04:57:34.927891.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug24_04:57:34.927891.zip&atstype=ATS]",2023-08-25T13:12:29.679+0000,"Issue is fixed in PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6979/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6979/overview]

+ [~accountid:5f3c6ae932360700388f7b4b] please review it Update: PR merged and another issue is raised: [https://cdetsng.cisco.com/webui/#view=CSCwh54820|https://cdetsng.cisco.com/webui/#view=CSCwh54820]

+ [~accountid:620b8357878c2f00729881c8]  please check and merge to your sanity branch","['BAPI', 'auton', 'express-sanity', 'ghost', 'optimized', 'sanity']",ThanhTan Nguyen,Resolved,KARTHIKEYAN KRISHNAMURTHY
SEEN-2270,https://miggbo.atlassian.net/browse/SEEN-2270,Auton:Test_TC2_DNAC_configure_virtual_network_ip_pool_with_traffic_types/test5_remove_fabric_zones ,"*Reporter Analysis:* We have observed that testcase is passed but fabric zone is not deleted.



*Branch Name:* private/Hulk-ms/sanity_api_auto 

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:



*False Pass Log:* 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-virtual_networks_segments.py-72-SDAFabricCDVirtualNetworksAndSegmentOnboarding&begin=4645141&size=275719&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug28_05:25:10.586220.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-virtual_networks_segments.py-72-SDAFabricCDVirtualNetworksAndSegmentOnboarding&begin=4645141&size=275719&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug28_05:25:10.586220.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-29T05:23:46.770+0000,"!image-20230829-052541.png|width=1348,height=544! Observed same issue on EXSI VM Hulk-Patch1 *3.713.75085* build

*Branch Name:* private/HulkPatch-ms/api-auto

*Script* *file:* solution_test_sanityecamb_lan.py

Failed logs:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15184140&size=794831&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug27_22:02:55.149670.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15184140&size=794831&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug27_22:02:55.149670.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Observed same issue on Ghost_P2_RC3_BAPI - 2.3.5.4-70852

*Branch Name:* private/HulkPatch-ms/api-auto

*Script* *file:* solution_test_sanityecamb_lan.py

Failed logs:

[https://ngdevx.cisco.com/services/taas/results/d64e07c2-0849-40cf-85f8-2222f20abac0/run-results|https://ngdevx.cisco.com/services/taas/results/d64e07c2-0849-40cf-85f8-2222f20abac0/run-results]  I just run this use cases few times and all fz added and removed as expected. I checked the UI too, it is RC3. did you rerun it and you to tried to remove them? i do not see any issue in the script.
passed logs from Tb1:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/moesaeed-sjc/pyats2/users/moesaeed/archive/23-08/sanity_TB1.2023Aug29_10:00:32.209364.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/moesaeed-sjc/pyats2/users/moesaeed/archive/23-08/sanity_TB1.2023Aug29_10:00:32.209364.zip&atstype=ATS]

I will close this ticket for now, and if you still have the issue then you can open it back. 



thanks 

Moe","['Auton', 'Hulk', 'Sanity', 'exsivm', 'hulk-vm-sanity', 'optimized']",Moe Saeed,Closed,Anusha John
SEEN-2272,https://miggbo.atlassian.net/browse/SEEN-2272,[IBSTE-Optimized]  Config preview failures in optimized script ,"h2. Getting one or other config preview errs when we run optimized script in  IBSTE in recent times. 



*LOGS:*



[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-virtual_networks_segments.py-63-SDAFabricCDVirtualNetworksAndSegmentOnboarding&begin=7673&size=2564979&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Foptimized_ibste_job.2023Aug25_23:35:05.061643.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-virtual_networks_segments.py-63-SDAFabricCDVirtualNetworksAndSegmentOnboarding&begin=7673&size=2564979&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Foptimized_ibste_job.2023Aug25_23:35:05.061643.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] àconfig preview failed on [SDAFabricCDVirtualNetworksAndSegmentOnboarding|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-virtual_networks_segments.py-63-SDAFabricCDVirtualNetworksAndSegmentOnboarding&begin=0&size=-1&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Foptimized_ibste_job.2023Aug25_23:35:05.061643.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]  usecase .

 

*Branch:*  private/Hulk-ms/api-auto

*Script :*   [dnac-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse]/[job|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/job]/[sr_ibste|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/job/sr_ibste]/optimized_ibste_job.py

 

h2.  *Also, when we try to run the testcases sequentially from parallel set hitting variable reference err.*

 

Usecase list:

{'blocker_uc': ['SDAfusionPatching',

                'SDAExtnodeOnboarding',

                'FEWssidsegmentonboarding',

                'SDAwiredHostOnboarding'],

 'sequentialrun': [{'1': 'SDAfusionPatching'},

                   {'2': 'SDAExtnodeOnboarding'},

                   {'3': 'sensorOnboardingClaiming'},

                   {'4': 'FEWssidsegmentonboarding'},

                   {'5': 'SDAwiredHostOnboarding'}]}

Blocker Usecase list:

Following UCs will run in sequential order in parallel to parallel run set.

2023-08-27T18:29:54: %EASYPY-ERROR: Caught error in jobfile '/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/IBSTE/optimizedIbste_regression/job/sr_ibste/optimized_ibste_job.py':

2023-08-27T18:29:54: %EASYPY-ERROR:

2023-08-27T18:29:54: %EASYPY-ERROR: Traceback (most recent call last):

2023-08-27T18:29:54: %EASYPY-ERROR:   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/IBSTE/optimizedIbste_regression/job/sr_ibste/optimized_ibste_job.py"", line 91, in main

2023-08-27T18:29:54: %EASYPY-ERROR:     handle_type=""ibste"")

2023-08-27T18:29:54: %EASYPY-ERROR:   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/IBSTE/optimizedIbste_regression/job/jobutils.py"", line 283, in execute_parallel_usecases

2023-08-27T18:29:54: %EASYPY-ERROR:     seq_res = execute_parallel_usecases(test_handle, scriptlist[""sequentialrun""], testcase_run_data, scripts_map,

2023-08-27T18:29:54: %EASYPY-ERROR: UnboundLocalError: local variable 'testcase_run_data' referenced before assignment

  

*Usecase Definition :*

'9':

'sequentialrun':

'1': SDAfusionPatching

'2': SDAExtnodeOnboarding

'3': sensorOnboardingClaiming

'4': FEWssidsegmentonboarding

'5': SDAwiredHostOnboarding



[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/IBSTE/job/optimizedIbste_regression/529/console|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/IBSTE/job/optimizedIbste_regression/529/console]

[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/IBSTE/job/optimizedIbste_regression/532/console|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/job/IBSTE/job/optimizedIbste_regression/532/console]",2023-08-29T07:31:15.618+0000,"Config preview pending review  failures  seen in basic provisioning itself..

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Foptimized_ibste_job.2023Sep08_03:51:56.378214.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Foptimized_ibste_job.2023Sep08_03:51:56.378214.zip&atstype=ATS]

 Update Mapping file [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/47b8eb26f8f8fb7ac87ba47a4fd4d72554313532|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/47b8eb26f8f8fb7ac87ba47a4fd4d72554313532]","['Auton', 'IBSTE', 'Optimized', 'Yamlmapping', 'sanity']",Raji Mukkamala,Resolved,Karventhan Velusamy
SEEN-2273,https://miggbo.atlassian.net/browse/SEEN-2273,Auton:Hulk:Upgrade: Test_TC8_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision  /   Validate_nw_ping_tester_unitility,"*Reporter Analysis:* 

On upgrade Hulk execution we are observing after code chages on hulk branch TC8 was failed and same TC8 was passed on Ghost Branch 
 updated_dev_list= copy.deepcopy(device_list)

{noformat}19577: 
 NameError: name 'copy' is not defined{noformat}

*Description*:  
result = method(*args, **kwargs)

{noformat}19575: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/3N-Cluster-Hulk-Upgrade-Verify-common-Multi-job/services/dnaserv/lib/api_groups/network_ping/group.py"", line 33, in network_ping_test_result{noformat}

{noformat}19576: 
     updated_dev_list= copy.deepcopy(device_list){noformat}

{noformat}19577: 
 NameError: name 'copy' is not defined{noformat}

*Branch Name:* Hulk -ms/sanity_api_auto

**Script* *file:** {{testcases/upgrade/after_upgrade_verify.py}}

*Source Team:* Upgrade Sanity

Issue Seen first time or day0 issue:

Failed log from Hulk branch:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5244233&size=9385&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug29_21:25:45.333345.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5244233&size=9385&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug29_21:25:45.333345.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Pass log from Ghost Branch:
[Validate_nw_ping_tester_unitility|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2694431&size=61044&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug22_21:03:45.430475.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-30T07:54:45.870+0000,"add ‘import copy’

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/dbc504c6a7820dcb198d8536b0201b490a296977|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/dbc504c6a7820dcb198d8536b0201b490a296977] Hi Tran , 
We got a pass log from Hulkpatch-ms/sanity_api_auto branch , Thanks for fixing the issue 

Pass log:
[Validate_nw_ping_tester_unitility|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4235428&size=65357&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct20_02:37:15.775666.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Execution', 'Hulk', 'Sanity', 'Upgrade']",Tran Lam,Closed,Tulasi Reddy
SEEN-2274,https://miggbo.atlassian.net/browse/SEEN-2274,uplift library for Backup and Restore use-case for ESXi,Uplift library for Backup and Restore use-case for ESXi.,2023-08-30T20:12:44.409+0000,"Below is the summary of changes made during the uplift:

# generalize the methods to support On-Prem as well as ESXi APIs
# changes done to cover On-Prem Sanity related API changes
# added one more use-case to perform backup with non-assurance data on Remote backup server/Physical 4th Disk
# removed the use-case to get loopback ip here, as it's not relevant and required for B&R
# renamed {{wait_and_find_the_backup_after_addding_backup_server}} to {{wait_and_find_the_backup_after_adding_backup_server}}
# limited the deletion of backups only to the one that where backup name has strings from ""dailybackup"", ""solregrtestbackup"", ""weeklybackup""

Execution log with On-Prem Sanity TB7: [TC74, TC75|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fws%2Famardkum%2Fpyats%2Fusers%2Famardkum%2Farchive%2F23-11%2Fenv_auto_job.2023Nov02_00:54:43.362495.zip&atstype=ATS]

Execution log with ESXi Sanity TB4: [TC74, TC75|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov02_15:58:21.720367.zip&atstype=ATS]

Execution log with ESXi MS TB2: [TC74, TC75, TC76, TC78|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov01_21:51:49.116893.zip&atstype=ATS]

PR raised for *private/HulkPatch2-ms/api-auto*: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7701/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7701/overview] PR has been merged to *private/HulkPatch2-ms/api-auto* branch.

Marking this Uplift requirement as “Closed”.","['Auton', 'ESXi', 'Uplift']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-2275,https://miggbo.atlassian.net/browse/SEEN-2275,Ghost-TC74_backup_space_test_and_add_server_details,"TC Failed:TC74_backup_space_test_and_add_server_details

Subtest Failed:{{test1_set_temp_backup_server}}
Error Faced:{color:#bf2600} Error Code: 400 URL:{color}[{color:#bf2600}https://10.88.187.195/api/system/v1/maglev/backup/remote/settings{color}|https://10.88.187.195/api/system/v1/maglev/backup/remote/settings]{color:#bf2600} Data:{'timeout': 60} Headers:{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2NGVjNTFkODNmOGVlNzJjYzkxMzIyN2MiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjY0ZWM1MWQ3M2Y4ZWU3MmNjOTEzMjI3YiJdLCJ0ZW5hbnRJZCI6IjY0ZWM1MWQ2M2Y4ZWU3MmNjOTEzMjI3OSIsImV4cCI6MTY5MzI4OTQ4MCwiaWF0IjoxNjkzMjg1ODgwLCJqdGkiOiIyNzhiYjVmYy0zZjM3LTQyMDItOTJlMS0yOWVjNjE4NTY3NWIiLCJ1c2VybmFtZSI6ImFkbWluIn0.PybWHJ37qdbZ_R8DMEOhVNGmuNlawgZZz5om5Va93SbqjestTHWgqhXq_aRWQm0hlEKuvGk9q_nKv2DxBae2QuMhUTTf7sPGjr8R73GpYh_8cbMkA-rKrbRw5E-b2VnE0W5SZ6x87lfwL0C7g88JVG_WvIO92ELIjUgdkvN6LonBwqlWoBPlVh6dYEPw0vph9vZFUz9A9XTgV2ZsFbpA29z5RV_5AzVbCb6dq32c2btvJbQ-rb92KJlD9InYW90LyZ13wI5CgdwdL10JRm1xg_y3zMKauOs9rC_uVHJMcDY9T8oEWYFw1njNqfcmnUknkKTDyzKHbGv0Ub_q5kXD9w;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:{""response"": {""error"": ""Scheduled Backups found.  Can not reset remote server settings."", ""errorCode"": ""INVALID_REQUEST_ERROR""}, ""version"": ""1.5.1""}{color}
{color:#bf2600}9865: 2023-08-29T00:07:30:  Traceback (most recent call last):{color}
{color:#bf2600}9866: 2023-08-29T00:07:30:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod2/services/dnaserv/client_manager.py"", line 295, in call_api{color}
{color:#bf2600}9867: 2023-08-29T00:07:30:      response.raise_for_status(){color}
{color:#bf2600}9868: 2023-08-29T00:07:30:    File ""/home/wnbu/pythonenv/jenkins/sol_ats20/lib/python3.6/site-packages/requests/models.py"", line 953, in raise_for_status{color}
{color:#bf2600}9869: 2023-08-29T00:07:30:      raise HTTPError(http_error_msg, response=self){color}
{color:#bf2600}9870: 2023-08-29T00:07:30:  requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: {color}[{color:#bf2600}https://10.88.187.195/api/system/v1/maglev/backup/remote/settings{color}|https://10.88.187.195/api/system/v1/maglev/backup/remote/settings]
{color:#bf2600}9871: {color}[{color:#bf2600}wnbust-ats-26.cisco.com{color}|http://wnbust-ats-26.cisco.com]{color:#bf2600}: 2023-08-29T00:07:30: %API-GROUP-BACKUP_RESTORE-3-ERROR: %[part=9871.1/5][pid=27430][pname=Task-1]: Received bad response from server:{color}
{color:#bf2600}9872: {color}[{color:#bf2600}wnbust-ats-26.cisco.com{color}|http://wnbust-ats-26.cisco.com]{color:#bf2600}: 2023-08-29T00:07:30: %API-GROUP-BACKUP_RESTORE-3-ERROR: %[part=9871.2/5][pid=27430][pname=Task-1]: {'response': {'error': 'Scheduled Backups found.  Can not reset remote server '{color}
{color:#bf2600}9873: {color}[{color:#bf2600}wnbust-ats-26.cisco.com{color}|http://wnbust-ats-26.cisco.com]{color:#bf2600}: 2023-08-29T00:07:30: %API-GROUP-BACKUP_RESTORE-3-ERROR: %[part=9871.3/5][pid=27430][pname=Task-1]:                        'settings.',{color}
{color:#bf2600}9874: {color}[{color:#bf2600}wnbust-ats-26.cisco.com{color}|http://wnbust-ats-26.cisco.com]{color:#bf2600}: 2023-08-29T00:07:30: %API-GROUP-BACKUP_RESTORE-3-ERROR: %[part=9871.4/5][pid=27430][pname=Task-1]:               'errorCode': 'INVALID_REQUEST_ERROR'},{color}
{color:#bf2600}9875: {color}[{color:#bf2600}wnbust-ats-26.cisco.com{color}|http://wnbust-ats-26.cisco.com]{color:#bf2600}: 2023-08-29T00:07:30: %API-GROUP-BACKUP_RESTORE-3-ERROR: %[part=9871.5/5][pid=27430][pname=Task-1]:  'version': '1.5.1'}{color}
{color:#bf2600}9876: 2023-08-29T00:07:30:  Library group ""backup_restore"" method ""unconfigure_backup_server"" returned in 0:00:00.071769{color}
{color:#bf2600}9877: 2023-08-29T00:07:30:  Test returned in 0:00:00.184338{color}
{color:#bf2600}9878: 2023-08-29T00:07:30:  Errored reason: Failed to remove pre-existing backup server settings!{color}
{color:#bf2600}9879: 2023-08-29T00:07:30:  The result of section test1_set_temp_backup_server is => ERRORED{color}

Log: [https://ngdevx.cisco.com/services/taas/results/f9022896-649c-4977-8d14-456897835533/run-results|https://ngdevx.cisco.com/services/taas/results/f9022896-649c-4977-8d14-456897835533/run-results]

Branch Used: rcdn/Ghost-ms/api-auto

",2023-08-31T07:46:10.577+0000,"[~accountid:712020:64510349-9ea9-463d-8fb0-f4b0d76b65d5] This test needs a backup server [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/config_48hr_test/solution_test_input.json?at=refs%2Fheads%2Fprivate%2FGhost-ms%2Fapi-auto#10280|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/config_48hr_test/solution_test_input.json?at=refs%2Fheads%2Fprivate%2FGhost-ms%2Fapi-auto#10280]. Please use a  backup server for that is reachable to the testbed.   From the logs, test1_set_temp_backup_server PASSED

{noformat} Library group ""backup_restore"" method ""get_backup_server_settings"" returned in 0:00:00.239862{noformat}

{noformat}143557: 2023-08-28T11:27:56:  Resource path full url: https://10.88.187.195/api/system/v1/maglev/backup/remote/settings{noformat}

{noformat}143558: wnbust-ats-26.cisco.com: 2023-08-28T11:27:56: %API-GROUP-BACKUP_RESTORE-6-INFO: %[pid=18471][pname=Task-1]: Successfully unconfigured backup server{noformat}

{noformat}143559: 2023-08-28T11:27:56:  Library group ""backup_restore"" method ""unconfigure_backup_server"" returned in 0:00:00.349122{noformat}

{noformat}143560: 2023-08-28T11:27:56:  Resource path full url: https://10.88.187.195/api/system/v1/maglev/backup/remote/settings{noformat}

{noformat}143561: wnbust-ats-26.cisco.com: 2023-08-28T11:28:02: %API-GROUP-BACKUP_RESTORE-6-INFO: %[pid=18471][pname=Task-1]: {'response': {'status': 'ok', 'message': 'Remote server settings updated successfully.'}, 'version': '1.5.1'}{noformat}

{noformat}143562: wnbust-ats-26.cisco.com: 2023-08-28T11:28:02: %API-GROUP-BACKUP_RESTORE-6-INFO: %[pid=18471][pname=Task-1]: Backup server configured successfuly{noformat}

{noformat}143563: 2023-08-28T11:28:02:  Library group ""backup_restore"" method ""create_backup_server"" returned in 0:00:05.475137{noformat}

{noformat}143564: 2023-08-28T11:28:02:  Test returned in 0:00:06.066936{noformat}

{noformat}143565: 2023-08-28T11:28:02:  Passed reason: Configured backup server successfully{noformat}

{noformat}143566: 2023-08-28T11:28:02:  The result of section test1_set_temp_backup_server is => PASSED{noformat}



For test2_attempt_backup, looks like backup has failed. Please check on DNAC UI for backup failure.



{noformat}143587: 2023-08-28T11:30:02:  Traceback (most recent call last):{noformat}

{noformat}143588: 2023-08-28T11:30:02:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod2/services/commonlibs/test_wrapper.py"", line 301, in wrapper{noformat}

{noformat}143589: 2023-08-28T11:30:02:      result = testfunc(func_self, **kwargs){noformat}

{noformat}143590: 2023-08-28T11:30:02:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod2/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 5824, in test2_attempt_backup{noformat}

{noformat}143591: 2023-08-28T11:30:02:      assert result is False{noformat}

{noformat}143592: 2023-08-28T11:30:02:  AssertionError{noformat}

{noformat}143593: 2023-08-28T11:30:02:  Test returned in 0:02:00.615473{noformat}

{noformat}143594: 2023-08-28T11:30:02:  Failed reason: {noformat}

{noformat}143595: 2023-08-28T11:30:02:  {noformat}

{noformat}143596: 2023-08-28T11:30:02:  Exception:{noformat}

{noformat}143597: 2023-08-28T11:30:02:  Traceback (most recent call last):{noformat}

{noformat}143598: 2023-08-28T11:30:02:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod2/services/commonlibs/test_wrapper.py"", line 301, in wrapper{noformat}

{noformat}143599: 2023-08-28T11:30:02:      result = testfunc(func_self, **kwargs){noformat}

{noformat}143600: 2023-08-28T11:30:02:    File ""/home/wnbu/pythonenv/jenkins/workspace/DNAC_Jobs/DNAC_Solution_jobs/DNAC_solution_sanity_pod2/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 5824, in test2_attempt_backup{noformat}

{noformat}143601: 2023-08-28T11:30:02:      assert result is False{noformat}

{noformat}143602: 2023-08-28T11:30:02:  AssertionError{noformat} Fix: [410b2510817|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/410b2510817c599d9532f565bc5e5aa83f061f54]","['Auton', 'ghost']",Raji Mukkamala,Resolved,SONA MOGAL
SEEN-2277,https://miggbo.atlassian.net/browse/SEEN-2277,Auton:Hulk:Upgrade:Test_TC20_DNAC_provision_all_aps  /   test1_provision_all_aps,"*Reporter Analysis:* 
On Verify upgrade Hulk execution we are observing TC20 was failed due to  No AI RF Profile or Not Associated With Site!!!!. Falling Back To Basic Profile for Provisioning!! but on Dnac Ui provision of ap got success

{{ }}

{noformat}2474: 
 Failed reason: Result: AP Provision failed{noformat}

*Description*:  
1517: 
 No AI RF Profile or Not Associated With Site!!!!. Falling Back To Basic Profile for Provisioning!!

{{ }}

{noformat}2474: 
 Failed reason: Result: AP Provision failed{noformat}

*Branch Name:* HulkPatch-ms/sanity_api_auto

**Script* *file:** {{testcases/upgrade/after_upgrade_verify.py}}

*Source Team:* Upgrade Sanity

Issue Seen first time or day0 issue:

Failed log from Hulk branch:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=272148&size=351072&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug30_22:11:57.559749.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=272148&size=351072&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug30_22:11:57.559749.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

",2023-08-31T10:53:38.152+0000,"Trivial Failure, fixed here: [ENG-SDN / dnac-auto / 2cbe7828667 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/2cbe782866746118a8aeb305965756485f1c48ff]","['Auton', 'Execution', 'Hulk', 'Upgrade']",Andrew Chen,Closed,Tulasi Reddy
SEEN-2278,https://miggbo.atlassian.net/browse/SEEN-2278,Auton:Hulk:Upgrade: Test_TC35_DNAC_edit_border_attributes_SDA_transit  /   test1_verify_assign_remove_l2border_sdda_transit_ip_internet_options_from_border,"*Reporter Analysis:* 
On verify upgrade Hulk execution we are observing TC35 was failed duo to Fabric Provisioning of Devices got failed  with below error
”Device configuration failed while provisioning the highlighted capability(ies)”
*Description*:  

Config preview flow was failed for description: Scheduling task for Edit border properties for fabric ['Global/USA/New_York_US_SJ_Fabric1'] at time 1693459244.213865

{noformat}4909: 
 Result: Editing border's TB5-DM-NF-Switch L2HANDOFF, SDATRANSIT and Internet options failed{noformat}

{noformat}4911: 
 Failed reason: Result: Editing border's L2HANDOFF, SDATRANSIT and Internet options failed{noformat}

*Branch Name:* HulkPatch-ms/sanity_api_auto

**Script* *file:** {{testcases/upgrade/after_upgrade_verify.py}}

*Source Team:* Upgrade Sanity

Issue Seen first time or day0 issue:

Failed log from Hulk branch:
[Test_TC35_DNAC_edit_border_attributes_SDA_transit|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=326154&size=2138233&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug30_22:16:59.621322.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
",2023-08-31T11:04:15.730+0000,"{{no prefix-list SITE_LOCAL_EIDS_V6}} ← this is CLI which is rejected by NF switch based on device response: Cannot delete a prefix list used by import publication.



On eca it says internal error: Cannot delete a prefix list used by import publication. Was able to reproduce the issue manually. Have requested [~accountid:63f50bf84c355259db9ccc59]  to raise a defect. (Please link it here once it is raised too for tracking) Hi [~accountid:63f50bcece6f37e5ed93c87e] ,
We raised a bug 
[https://cdetsng.cisco.com/webui/#view=CSCwh45753|https://cdetsng.cisco.com/webui/#view=CSCwh45753]
","['Auton', 'Execution', 'Hulk', 'Upgrade']",Andrew Chen,Closed,Tulasi Reddy
SEEN-2279,https://miggbo.atlassian.net/browse/SEEN-2279,[Auton]  Task-border_fusion_patching_and_dhcp_server.py-121-SDAfusionPatching  /   Test_TC1_DNAC_Configuring_bgp_on_border_fusion_router  /   test1_configure_loopback_bgp_on_border,"We've noticed that the DHCP server configuration test case is failing during the Hulk Run due to issues with configuring BGP. As a result, the extended nodes and PEN nodes are not onboarding in the PnP page.

After pulling the latest code, we encountered this issue. last week, we ran the same build on the testbed, and that particular test case passed successfully.



*Branch: private/HulkPatch-ms/sanity_api_auto*

Failed reason:

{noformat}990: 
 Failed to create DHCP Server Configs{noformat}

{noformat}991: 
 Library group ""fusion_dhcp_server_config"" method ""genrate_dhcpserver_subpools"" returned in 0:00:00.029194{noformat}

{noformat}992: 
 Test returned in 0:00:11.127474{noformat}

{noformat}993: 
 Failed reason: Result: DHCP server config for POOL generation failed{noformat}



*Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-border_fusion_patching_and_dhcp_server.py-121-SDAfusionPatching&begin=5207&size=429866&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug30_07:37:04.275505.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-border_fusion_patching_and_dhcp_server.py-121-SDAfusionPatching&begin=5207&size=429866&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug30_07:37:04.275505.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


*Previous Hulk Passlog:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-border_fusion_patching_and_dhcp_server.py-121-SDAfusionPatching&begin=5207&size=1273317&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug23_06:44:14.600245.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-border_fusion_patching_and_dhcp_server.py-121-SDAfusionPatching&begin=5207&size=1273317&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug23_06:44:14.600245.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-08-31T11:56:40.755+0000,"One of the new pool with lower case ‘v6’ created the failure.
Added fix to handle either lower case or upper case.

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ee21455deffe629a3cbc074e264b866442aa1c46|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ee21455deffe629a3cbc074e264b866442aa1c46]
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/eabb21fddfdf7d3df2e6694905f1757267657459|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/eabb21fddfdf7d3df2e6694905f1757267657459]","['Auton', 'Blocked', 'HulkPatch', 'Optimized', 'hulk']",Tran Lam,Resolved,Elton GoldChristopher
SEEN-2281,https://miggbo.atlassian.net/browse/SEEN-2281,[Auton] - Script failures in Fabric Issue Consolidation feature related execution,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/HulkPatch-ms/api-auto

*Uber ISO tested:* Hulk Patch1 - 2.1.713.70147

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

When integrating the Hulk feature corresponding Fabric Issue Consolidation feature we see all the sub TCs getting errored out due to list index out of range and parameter errors.



*Feature wiki:* [https://wiki.cisco.com/pages/viewpage.action?pageId=1647748641|https://wiki.cisco.com/pages/viewpage.action?pageId=1647748641]

*Failed log* - [Test_TC279_DNAC_assurance_lisp_issue_check|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3102004&size=65379&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb_multi_sites_mdnac.2023Aug30_06:10:23.626932.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-01T13:54:09.117+0000,"Hi [~accountid:62a9d1d0192edb006f9f5332] ,
During solution sanity hulk testing ([Test_TC229_DNAC_assurance_lisp_issue_check|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2236142&size=80775&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep04_05:55:53.251796.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]) failed with Below Error, Could you please check :

{noformat}11207:  Traceback (most recent call last):
11208:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 11621, in checking_lisp_issue_generation
11209:      dev_result, out = self.add_or_remove_lisp_issue(device, cmd= ""ADDACL"")
11210:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
11211:      result = method(*args, **kwargs)
11212:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 11539, in add_or_remove_lisp_issue
11213:      ""ip access-list extended BLOCK-LISP-SESSION\n""
11214:  IndexError: list index out of range{noformat}

{{TESTSCRIPT_FILE : testcases/forty_eight_hour/solution_test_sanityecamb_lan.py}}
{{SECURE_FABRIC_FILE : solution_test_input.json}}
[solution_test_input.json|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/config_48hr_test/solution_test_input.json?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto]
*Branch Name:*  private/Hulk-ms/sanity_api_auto

*Script file/Usecase:* solution_test_sanityecamb_lan.py

*Source Team:  Sanity* PR link for the Fabric Lisp Issue: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6982/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6982/overview]
Fabric file input needed before starting the script, Please refer wiki for more details

wiki: [https://wiki.cisco.com/pages/viewpage.action?pageId=1647748641|https://wiki.cisco.com/pages/viewpage.action?pageId=1647748641]

Logs: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Froot%2Frel_automation%2Fdnac-auto%2Fpyats_rel%2Fusers%2Farchana%2Farchive%2F23-09%2Fthree_sites.2023Sep12_08:58:46.255418.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Froot%2Frel_automation%2Fdnac-auto%2Fpyats_rel%2Fusers%2Farchana%2Farchive%2F23-09%2Fthree_sites.2023Sep12_08:58:46.255418.zip&atstype=ATS] Hi [~accountid:62a9d1d0192edb006f9f5332]  ,
After pulling the latest code, we are getting the below error. Could you please check?

error  snip 

{noformat}21136:  Traceback (most recent call last):
21137:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper
21138:      result = testfunc(func_self, **kwargs)
21139:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 17175, in test1_Verify_lisp_issue_generation
21140:      self.result1, self.device, self.edge= dnac_handle.checking_lisp_issue_generation()
21141:  TypeError: cannot unpack non-iterable bool object
{noformat}

Failed  Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4990896&size=25382&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep24_09:19:13.871599.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4990896&size=25382&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep24_09:19:13.871599.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

PR merged Branch:
{{private/HulkPatch-ms/sanity_api_auto}}
PR:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/967e1d7e260929b21de0c53b95e310db2d8e8bb8#testcases/sanityusecases/assuranceClientHighestTxDrops/assurance_client_highest_tx_drops.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/967e1d7e260929b21de0c53b95e310db2d8e8bb8#testcases/sanityusecases/assuranceClientHighestTxDrops/assurance_client_highest_tx_drops.py] Issue is no more observed. Pass log on Hulk Patch1 - [Test_TC229_DNAC_assurance_lisp_issue_check|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1477542&size=168659&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct13_06:34:25.127914.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Hulk', 'HulkPatch1', 'Integration', 'MSTB1', 'Multisite', 'Optimized', 'Sanity']",Archana KM,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2282,https://miggbo.atlassian.net/browse/SEEN-2282,[Auton] - Verification of kairos events analytics TC errored out due to key errors,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/HulkPatch-ms/api-auto

*Uber ISO tested:* Hulk Patch1 - 2.1.713.70147

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

When integrating the Hulk feature- Verification of kairos events analytics, the corresponding subTC is getting errored out key errors. Looks like the corresponding payload block commit is missing on Hulk Patch1 branch.

*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/AI+Network+Analytics+Events|https://wiki.cisco.com/display/EDPEIXOT/AI+Network+Analytics+Events]

*Failed log* - [Test_TC215_assurance_health_validation|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=15536175&size=163173&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb_multi_sites_mdnac.2023Aug27_08:51:08.215027.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-01T14:07:12.710+0000,PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0a5be28493ed2aa752382a7a4ac1bff91b62ffd0|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0a5be28493ed2aa752382a7a4ac1bff91b62ffd0] [~accountid:63f50bfce8216251ae4d59d5] : Can you please commit the fix on Hulk Patch (Hulk Patch1 and Hulk Patch2) branches as well  ?,"['Auton', 'Hulk', 'HulkPatch', 'Integration', 'MSTB1', 'Multisite']",Moe Saeed,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2283,https://miggbo.atlassian.net/browse/SEEN-2283,[Auton] - AP config workflow fails with Key errors despite having APs available on inventory,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/HulkPatch-ms/api-auto

*Uber ISO tested:* Hulk Patch1 - 2.1.713.70147

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

When integrating the Groot+Hulk feature - Edit Site name, the AP config workflow subTC is getting errored out with key errors despite we have APs available and onboarded on DNAC.



*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/Sites+Name+Modification|https://wiki.cisco.com/display/EDPEIXOT/Sites+Name+Modification]

*Failed log* - [Test_TC236_edit_site_name|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2222242&size=5236947&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb_multi_sites_mdnac.2023Aug30_11:57:55.549610.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-01T14:56:42.547+0000,"this ticket for key error has been addressed. But, can you share recent logs on this? [~accountid:63f50bfce8216251ae4d59d5] : Thanks for the update! 
Will be using the latest code for execution in Hulk Patch1 and update the observations. [~accountid:63f50bfce8216251ae4d59d5] : As discussed, I tried executing the TC with Hulk Patch code base - private/HulkPatch-ms/api-auto (pulled on September 15th). Now we are not observing issue related to AP config workflow, but there are other sub TCs failing. 

*Uber ISO used* - 2.1.713.70207_Hulk_P1

*Failed log:* [Test_TC236_edit_site_name|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2236346&size=4851645&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac.2023Sep20_12:28:27.051611.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] Let’s close this since it is not seen. [~accountid:63f50bfce8216251ae4d59d5] : Have not got chance to retest this TC again due to back to back Regression testing and Customer build testings. will test and update by 10th July 2023. Issue is no more observed during *Hulk Patch1 RC1 - 2.1.713.70292 -*

Pass log:  [Test_TC236_edit_site_name|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2222221&size=4324238&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct10_22:01:07.334184.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Groot', 'Hulk', 'HulkPatch1', 'Integration', 'MSTB1', 'Multisite']",Moe Saeed,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2286,https://miggbo.atlassian.net/browse/SEEN-2286,[Auton]:Test_TC10_DNAC_Verify_adding_range_discovery_ssh_global_credentials/test4_verify_fqdn_devices,"In Recent Hulk P1 Execution while we are trying to integrate Feature [https://miggbo.atlassian.net/browse/SEEN-1996|https://miggbo.atlassian.net/browse/SEEN-1996|smart-link], we are facing an issue with the Verification of FQDN Devices

*Reporter analysis:*

Here are the files that are used
Script used: CERT_Lan Script --> solution_test_sanityecamb_cert.py
Branch used: private/Hulk-ms/sanity_delay_testing
FQDN changes made for ECA Device: ""device_fqdn"": ""[TB2-DM-eCA-BORDER.tb2sanity.com|http://TB2-DM-eCA-BORDER.tb2sanity.com]""

I have a pass log for the below list of test cases
test1_verify_adding_range_discovery_ssh_global_credentials, test2_add_device_inventory_fqdn, test3_verifications_all_devices_in_managed_state
However, verification of fqdn devices is failing as it is unable to find the loopback ip for the device
In TC2 underlay config generation we do not set the loopback IP for the devices
test1_assign_lp_ip2devices,
I did compare your pass log that is been attached in the wiki and I could see the script is run using regular script(solution_test_sanityecamb.py) in which the loopback ip is been configured in the underlay config generation

*Uber ISO Version tested:* Hulk P1 #2.3.7.3-70139

*Script Name:* Cert Lan Script

*Branch used:* private/Hulk-ms/sanity_delay_testing

*Script used:* solution_test_sanityecamb_cert_lan.py

*Snip from the log:* 

{noformat}3130: 
 Device list for FQDN devices ['TB2-DM-eCA-BORDER']{noformat}

{noformat}3131: 
 FQDN device list['TB2-DM-eCA-BORDER']{noformat}

{noformat}3132: 
 Device TB2-DM-eCA-BORDER{noformat}

{noformat}3133: 
{noformat}

{noformat}3134: 
{noformat}

{noformat}3135: 
  api_switch_call called:{noformat}

{noformat}3136: 
 {'params': {'offset': 1, 'limit': 100}}{noformat}

{noformat}3137: 
 Resource path full url: https://10.195.227.31/api/v1/network-device{noformat}



*Log*:[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=550662&size=7047&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep05_01:14:02.650267.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=550662&size=7047&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep05_01:14:02.650267.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-05T09:26:39.749+0000,"This is due to show running-config  interface Loopback0 | include address

show running-config  interface Loopback0 | include address

                                       ^

% Invalid input detected at '^' marker.



Please try to run the CLi manually, it’s not related to Device FQDN feature After Discussing with Raji on Space 
webexteams://im?space=7081a860-3779-11ee-9ec0-2fc836dd8d78 
It was concluded that, we cannot run this feature using Lan Script as the underlay config does not push loopback IP address to the device.
And for the Device FQDN Feature we are to configure the loopback ip of the device in the local DNS Servers.
We are attempted this feature using Regular script and observed issues where the Device with FQDN name were failing for Snmp and Netconf validation.
Raji has added Fix onto it need to run the feature and update. [~accountid:63f50be24e86f362d39acde8] , do we have any update on the test after last fix provided by Raji? This issue has been addressed. As the feature needs the loopback Ip of the device which in Lan script gets assigned only during device provisioning

We will reattempt this feature using the regular script.","['Auton', 'Feature', 'Ghost', 'Hulk', 'HulkPatch', 'Multisite', 'Optimized', 'Sanity', 'Uplift']",DeepakPratap Shinde,Closed,DeepakPratap Shinde
SEEN-2287,https://miggbo.atlassian.net/browse/SEEN-2287,[Legacy_Optimized_Code_mapping] Testcase and Usecase mapping are missed for few tests in HULK /Ghost ,"The intention of this ticket is to track the testcases that are not part of any usecase and are part of the usecases but are missed from mapping yaml files such as:

*usecasemaps/lansanity/lansanity_usecases_maps.yaml*
*usecasemaps/nonlansanitysuite/sanity_usecases_maps.yaml*

Branch referred here is: *private/Hulk-ms/sanity_api_auto*
 *private/Ghost-ms/sanity_api_auto*

legacy script referred to: *testcases/forty_eight_hour/solution_test_sanityecamb.py*
legacy script referred to: *testcases/forty_eight_hour/solution_test_sanityecamb_lan.py*



*Below are the tests that are NOT part of any usecase :*
Test_TC67_DNAC_CriticalVLAN_onboarding_ixia_scale
Test_TC68_configure_policy
Test_TC73_DNAC_verify_aaa_lisp_radius_configuration_on_devices_through_scheduled_job
{{Test_TC217_SDA_Wired_Host_Onboarding_Uplink_Interfaces}}",2023-09-05T13:56:08.182+0000,"[~accountid:620b8357878c2f00729881c8] Below usecases are already present in optimized folder and mapping in private/Hulk-ms/api-auto. If anything missing in Sanity branches , please sync with the main branch 

Test_TC67_DNAC_CriticalVLAN_onboarding_ixia_scale- [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/sanityusecases/trafficdot1x/traffic_unicst_multicast_dot1x_cp.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#1123|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/sanityusecases/trafficdot1x/traffic_unicst_multicast_dot1x_cp.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#1123]

Mapping(*usecasemaps/lansanity/lansanity_usecases_maps.yaml*) [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/lansanity/lansanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#143|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/lansanity/lansanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#143]

*usecasemaps/nonlansanitysuite/sanity_usecases_maps.yaml* [*https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/nonlansanitysuite/sanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#142*|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/nonlansanitysuite/sanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#142]

Test_TC68_configure_policy - [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/sanityusecases/configurePolicy/configure_policy.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/sanityusecases/configurePolicy/configure_policy.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto]

{{Test_TC217_SDA_Wired_Host_Onboarding_Uplink_Interfaces}} [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/sanityusecases/SDAWiredHostOnboardingUplinkInterfaces/SDA_wired_host_onboarding_uplink_interfaces.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/sanityusecases/SDAWiredHostOnboardingUplinkInterfaces/SDA_wired_host_onboarding_uplink_interfaces.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto]

Mapping *usecasemaps/nonlansanitysuite/sanity_usecases_maps.yaml* [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/nonlansanitysuite/sanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#115|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/nonlansanitysuite/sanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#115]

*usecasemaps/lansanity/lansanity_usecases_maps.yaml* [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/lansanity/lansanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#116|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/lansanity/lansanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#116]

{{Test_TC73_DNAC_verify_aaa_lisp_radius_configuration_on_devices_through_scheduled_job https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/sanityusecases/MSDnacVerifyLispAfterReload/verifylispafterreload.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto}}  Added usecase mapping 

Hulk: [0f4840b9788|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0f4840b978824e22289fe24298b833058549b228]

Ghost: [eeca2d223bb|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/eeca2d223bb79f6cb54e796c8bacd61b8f9f96fd]","['Auton', 'Blocked', 'Ghost', 'Hulk', 'Optimized', 'Sanity']",Raji Mukkamala,Resolved,Omkar Sharad Wagh
SEEN-2288,https://miggbo.atlassian.net/browse/SEEN-2288,[Hulk] [Ghost] Optimized: Task-templatehub.py-281-toolsTemplateHub,"Hi Team
We have observed that test cases are failing on optimized code (Hulk/Ghost) in  day 1. The 'toolsTemplateHub' use casebut the same test case is passing in the main script.  
           |


+*Failed Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-templatehub.py-281-toolsTemplateHub&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug09_05:19:59.904827.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-templatehub.py-281-toolsTemplateHub&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug09_05:19:59.904827.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

  +*Pass Log from Main LAN  script:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=33915405&size=1535118&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug09_10:26:43.039712.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=33915405&size=1535118&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug09_10:26:43.039712.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


*Script Name:* Optimized code 
\testcases\sanityusecases\

{{TESTSCRIPT_FILE : testcases/forty_eight_hour/solution_test_sanityecamb_lan.py}}
{{SECURE_FABRIC_FILE : solution_test_input.json}}
[solution_test_input.json|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/config_48hr_test/solution_test_input.json?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fsanity_api_auto]
*Branch Name:*  private/Hulk-ms/sanity_api_auto 
private/Ghost-ms/sanity_api_auto

*Script file/Usecase:* solution_test_sanityecamb_lan.py

*Source Team:  Sanity*",2023-09-05T15:05:10.538+0000,"Raised PR : [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6901/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6901/overview]

Issue : With optimized code UC27 is not running
Issue from Day1
Fix: Removed setup_parameters function from commonsetup which is not require for this usecase

Fail log : [https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-AUTO/job/vkuttykr/job/optimizedsanity/6/console|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-AUTO/job/vkuttykr/job/optimizedsanity/6/console]

After fix : [https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-AUTO/job/vkuttykr/job/optimizedsanity/8/console|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-AUTO/job/vkuttykr/job/optimizedsanity/8/console]","['Auton', 'Feature', 'Ghost', 'Hulk', 'Optimized', 'Sanity']",Vinoth Kumar Kutty Krishnamoorthy,Resolved,Omkar Sharad Wagh
SEEN-2295,https://miggbo.atlassian.net/browse/SEEN-2295,[Auton] - Need for addition of PDMT-4 specific subTCs to existing Prime feature related TC,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/HulkPatch-ms/api-auto

*Uber ISO tested:* Hulk Patch1 - 2.1.713.70147

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

When integrating the Ghost feature - PDMT-4 Features related to Prime Infra migration, we see related subTCs are not included as part of Multisite scripts. 

Below sub TCs needs to be included on Multisite scripts:

test9_remove_geo_cordinates
test12_dynamic_sync_issues
test11_verfify_audit_descr
test13_job_history_scan

Scripts to be used for inclusion of these TCs:

testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py
testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_dr.py
testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac.py
testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

Branches on which this changes are to be added:
private/Ghost-ms/api-auto
private/Hulk-ms/api-auto
private/HulkPatch-ms/api-auto



For more details please refer the below *Reference Feature Automation JIRAs:* 
[https://miggbo.atlassian.net/browse/SEEN-922|https://miggbo.atlassian.net/browse/SEEN-922|smart-link] 
[https://miggbo.atlassian.net/browse/SEEN-883|https://miggbo.atlassian.net/browse/SEEN-883|smart-link] 



*Reference Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/PDMT-4+Features|https://wiki.cisco.com/display/EDPEIXOT/PDMT-4+Features]",2023-09-06T02:40:23.748+0000,"Covering in sanity is good. Not needed in Multisite. [~accountid:63f50bf5e8216251ae4d59cf] : We have been asked to integrate Prime migration feature on all Multisite TBs - MSTB1 (On-Prem), MSTB2 (ESXI), MSTB3 (AWS). Hence needs to be included. *Commit for Hulk branch -* [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3eb135d3302392bc3ff5d251d0064e318577743c|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3eb135d3302392bc3ff5d251d0064e318577743c]

*Commit for Hulk Patch1 branch -* [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3eb135d3302392bc3ff5d251d0064e318577743c|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3eb135d3302392bc3ff5d251d0064e318577743c]","['AWS_MSTB', 'Auton', 'Ghost', 'Hulk', 'HulkPatch1', 'Integration', 'MSTB1', 'MSTB2', 'Multisite']",Vinay Raj V ,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2296,https://miggbo.atlassian.net/browse/SEEN-2296,[Auton] - AP zone feature specific TC fails during Aireos device provisioning post AP zone removal,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/Ghost-ms/api-auto

*Uber ISO tested:* Ghost Patch2 RC2

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

When integrating the Ghost feature - AP Zone, We have observed Aireos WLC provisioning failure after removal of AP zones.

*Reported defect to track this issue :*
[CSCwh30413|https://cdetsng.cisco.com/webui/#view=CSCwh30413] - [SR]Functional - Aireos WLC provision fails with NCSP11145 error related to Duplicate namespaces

Post Analysis from DE team, they have suggested to make automation side changes to address this issue.

Reference team space for more details - [https://eurl.io/#egjUJuH35|https://eurl.io/#egjUJuH35]

*Failed log :* [Test_TC251_AP_Zone|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2259442&size=2417754&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-08%2Fsr_mb_multi_sites_mdnac.2023Aug17_11:34:04.712928.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Reference Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/AP+Zones|https://wiki.cisco.com/display/EDPEIXOT/AP+Zones]

*Scripts to be used for addressing the issue:*

testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py
testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_dr.py
testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac.py
testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Branches on which this changes are to be added:*
private/Ghost-ms/api-auto",2023-09-06T03:08:23.794+0000,"[~accountid:63f50bf5e8216251ae4d59cf] : Thanks for looking into the issue. We Relavent fix has been been committed to Ghost branch - private/Ghost-ms/api-auto

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/603531fab5ed19897a656a9f4ec3760b53542c83|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/603531fab5ed19897a656a9f4ec3760b53542c83]

Could you please confirm if this fix applicable to Hulk and Hulk Patch1?  [~accountid:63f50bf5e8216251ae4d59cf] : As confirmed by you on relevant team space - [https://eurl.io/#egjUJuH35|https://eurl.io/#egjUJuH35], This fix is currently applicable for only Ghost branch. 

We will verify the fix once we consume next Ghost Patch build for the testing.","['Auton', 'Ghost', 'Hulk', 'HulkPatch1', 'Integration', 'MSTB1', 'Multisite', 'Sanity']",Raji Mukkamala,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2297,https://miggbo.atlassian.net/browse/SEEN-2297,[Auton]:Hulk: Task-virtual_networks_segments.py-72-SDAFabricCDVirtualNetworksAndSegmentOnboarding  /   Test_TC2_DNAC_configure_virtual_network_ip_pool_with_traffic_types  /   test4_enable_fabric_zones,"*Reporter Analysis:* We have observed that the test case for enabling fabric zones has failed, and it's impacting the run. Due to this issue, the run is blocking in the optimized code

*Branch Name:* private/HulkPatch-ms/sanity_api_auto 

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:

*Failed Log:*
[earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-virtual_networks_segments.py-72-SDAFabricCDVirtualNetworksAndSegmentOnboarding&begin=3612382&size=1097215&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep06_06:16:38.648034.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|http://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-virtual_networks_segments.py-72-SDAFabricCDVirtualNetworksAndSegmentOnboarding&begin=3612382&size=1097215&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep06_06:16:38.648034.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]




!image-20230906-162329.png|width=1918,height=746!",2023-09-06T16:15:27.580+0000,"this ticket is a duplicate. issue has already fixed

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/23506858b654dc66466d330ef7aba46d4952354f|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/23506858b654dc66466d330ef7aba46d4952354f]","['Auton', 'Hulk', 'HulkPatch1', 'Optimized', 'Sanity']",Moe Saeed,Closed,Omkar Sharad Wagh
SEEN-2298,https://miggbo.atlassian.net/browse/SEEN-2298,[Auton]:Hulk: Test_TC189_edit_site_name/ test15_provision_ap_with_rf_profile for Zero wait DFS feature-The AP provisioning is failing due to AI_RF_Profile is enabled,"*Reporter Analysis:*

For Zero wait DFS feature,  the custom rf profile has been created on DNAC. But while provisioning of the APs is not performed due to AI_RF_Profile is enabled. Due to this the test [test15_provision_ap_with_rf_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3489162&size=64169&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_07:45:28.958440.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] has failed to pick any AP and failed with reason as *“ No wireless controller device items found.“*

EXPECTATION here is: Before performing AP provisioning with rf profile, the AI_RF_Profile needs to be disabled on the sites and then proceed with AP provisioning.

LOG:  [Test_TC189_edit_site_name|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1482210&size=2737068&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_07:45:28.958440.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 
Branch: private/Hulk-ms/sanity_api_auto,  private/HulkPatch-ms/sanity_api_auto
TB3: LOG [Test_TC189_edit_site_name|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1141832&size=4413952&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_05:16:19.450144.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-06T17:42:56.767+0000,"the use case should already remove the AI profile before proceeding with this use case. 

!image-20230906-180111.png|width=742,height=627!

[~accountid:63f50bf5e8216251ae4d59cf] can you check why the following lib is not finding any wlc? this might be before you add your changes for unassign AI profile to the lib that removes AI profile> 

{noformat}def get_rf_profile_from_wireless_device(self):{noformat} Added new lib to uassign AI rf profile from sites [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/dcedb0c96e31ca5f3253155ed846c2b6d62e0197|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/dcedb0c96e31ca5f3253155ed846c2b6d62e0197] [~accountid:63f50bfce8216251ae4d59d5] Issue is with unassign AI RF profile library, return false pass/doesn’t unassign sites from the AI profile. {{def get_rf_profile_from_wireless_device}} picks wireless devices which doesn’t have AI Rf profile configured only. In this case , it did not find any wlc as AI RF profile was not removed.

  Closing the Jira as on aws sanity i got pass log for the subtc for which auton was raised:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1702930&size=8004213&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct10_00:15:37.986061.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1702930&size=8004213&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct10_00:15:37.986061.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Hulk', 'Integration', 'Sanity']",Raji Mukkamala,Closed,Ashwini R Jadhav
SEEN-2299,https://miggbo.atlassian.net/browse/SEEN-2299,[Auton]: [Sanity] Test_TC123_aaa_per_ssid /test3_onboard_wireless_segment_for_new_ssid-  where it failed to provision APs when AI PROFILE is enabled,"*Description:*  Test failed [Test_TC123_aaa_per_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=815837&size=3550794&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep05_12:41:35.708494.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] / [test3_onboard_wireless_segment_for_new_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3013504&size=1351257&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep05_12:41:35.708494.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] where AP provisioning failed due to 'failureReason': 'AP cannot be provisioned to Basic RF profile when AI is enabled.'
This is because the site where the APs and controller as assigned is still enabled with AI_RF_Profile. 


when checked on the DNAC where the AI_RF_Profile was still having as site/building mapped to it,

The test [test_clear_old_AI_RF_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=816401&size=15588&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep05_12:41:35.708494.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] doesn't clear the ai_rf_profile mapping from buildings/sites

*Error Snippet:* 

{noformat}Config Preview Activity failed with reason: AP cannot be provisioned to Basic RF profile when AI is enabled.

 Activity: 018a66ea-e3e9-77af-8305-4014c1bdc2c9 Trigger job: {'id': '20836e63-e8b4-4e6f-92c6-cf94fd20ec64', 'triggeredJobTaskId': '018a66ea-e43b-7e61-a309-62e236c167eb', 'triggeredTime': 1693943784511, 'status': 'FAILED', 'failureReason': 'AP cannot be provisioned to Basic RF profile when AI is enabled.', 'triggeredJobId': '20836e63-e8b4-4e6f-92c6-cf94fd20ec64'}
14471:  activity_id is False. Config preview task failed for description Provisioning Unified APs at time 1693943781.2151413 - Configuration Preview{noformat}

{{P}}

*Branch: private/Hulk-ms/sanity_api_auto, private/HulkPatch-ms/sanity_api_auto,*

*Script file:* solution_test_sanityecamb_lan.py /*Test_TC123_aaa_per_ssid / test3_onboard_wireless_segment_for_new_ssid*

*input file:* solution_test_input.json

*Failed log:* [Test_TC123_aaa_per_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=815837&size=3550794&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep05_12:41:35.708494.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed*: Sanity Testbed TB3",2023-09-06T18:18:53.415+0000,"Added new library to clear AI rf profile from sites [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/dcedb0c96e31ca5f3253155ed846c2b6d62e0197|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/dcedb0c96e31ca5f3253155ed846c2b6d62e0197] This fix worked fine. 
Log: [test_clear_old_AI_RF_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=809628&size=34571&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_23:06:02.962127.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

Thanks Raji","['AWS_Sanity', 'Auton', 'Hulk', 'Integration', 'sanity']",Raji Mukkamala,Resolved,Ashwini R Jadhav
SEEN-2300,https://miggbo.atlassian.net/browse/SEEN-2300,TC31_SWIM_UPGRADE_ECA_DEVICE-test12_verify_upgrading_os_image,"*Uber ISO Version tested :* 3.713.75106 - Hulk ESXI

*EXSI OVA build:* +assembly_release_dnac_hulk_converged_07-3.713.75106.ova+
*Branch*: private/HulkPatch-ms/api-auto
*Script Name :* solution_test_sanity_ecamb.py
*Testbed :* TB4
*Testcases Impacted :* test12_verify_upgrading_os_image

During Hulk-ESXI sanity while testing TC31_SWIM_UPGRADE_ECA_DEVICE, 

Please change the name of the testcase say “TC31_SWIM_UPGRADE” as this is not specific to eca device

*test12_verify_upgrading_os_image* fails after upgrading wlc and try to login into wlc without clearing the console.

due to this failure the next steps of devices image upgrade fail

*Fail Log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6750323&size=188584&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_07:44:44.502637.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6750323&size=188584&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_07:44:44.502637.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-06T20:22:34.597+0000,"[~accountid:63f50bf9e8216251ae4d59d4] Please uplift minor enhancements  [~accountid:63f50bf5e8216251ae4d59cf] the script is not clearing the console and script failed, please find the pass logs Addressed [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d45aca60d6dcbd1ad81bd448b2328abb2657a920|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d45aca60d6dcbd1ad81bd448b2328abb2657a920]","['Auton', 'ESXi', 'HulkPatch1']",Raji Mukkamala,Resolved,KRISHNA MUKKU
SEEN-2301,https://miggbo.atlassian.net/browse/SEEN-2301,Auton:Hulk:Test_TC20_DNAC_provision_all_aps  /   test1_provision_all_aps ,"*Reporter Analysis:* 
After the fix:

[+ENG-SDN / dnac-auto / 2cbe7828667 - Bitbucket Engineering - SJC1 (cisco.com)+|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/2cbe782866746118a8aeb305965756485f1c48ff]
 we are seeing error with respect to VCR changes.

After this failure WLC Client sumary mismatch came in picture which caused TC 22 failure:





*Description*:  

{noformat}3800: 
     raise HTTPError(http_error_msg, response=self){noformat}

{noformat}73801: 
 Encountered unhandled HTTPError in group ""vcr"" method ""get_config_preview_generated_for_dev""!{noformat}

{noformat}73802: 
 Flagging result as FAIL!{noformat}

{noformat}73803: 
 	Reason: 404 Client Error:  for url: https://10.195.227.92/apic-em-network-programmer-service/api/device-provisioning/v3/config-preview/activities/018a6e2e-f712-78e5-8aba-bf34aa03dfed/devices/a81326b4-17ee-4c75-acc7-16a36d1f8019?flatten=True&groupBy=CONFIGURATOR{noformat}

{noformat}73804: 
 	Args:   (<services.dnaserv.lib.api_groups.vcr.group.Group object at 0x7f41f8f14d30>, '018a6e2e-f712-78e5-8aba-bf34aa03dfed', ['TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC', 'TB6-DM-WLC']){noformat}

*Branch Name:* HulkPatch-ms/sanity_api_auto

**Script* *file:** {{testcases/upgrade/after_upgrade_verify.py}}

*Source Team:* Upgrade Sanity

Issue Seen first time or day0 issue:

*Failed log from Hulk branch:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17996738&size=1247120&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_21:28:12.726774.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17996738&size=1247120&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_21:28:12.726774.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*TC 22 failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19291701&size=153358&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_21:28:12.726774.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19291701&size=153358&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_21:28:12.726774.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-07T07:15:29.814+0000,"Fixed in local, waiting for tb availability to test. Note: its unrelated to the commit mentioned in the description.  [ENG-SDN / dnac-auto / 9a795885671 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9a7958856718747f84b27f0ad426e1d8a79a135a] ← fixed here, but not verified due to not being able to recreate the scenario. When I do ap re-provisioning, it shows a device list with vewlc, but it says no config generated. [~accountid:63f50bcece6f37e5ed93c87e] 
we have a cluster with Hulk P1 , it is not an upgrade cluster but we will try upgrade script specifically 



I cherry picked your commit to hulk-ms/sanity_api_auto branch and tried TC 20 from Upgrade script but now it failed with below error:

{noformat} No AI RF Profile or Not Associated With Site!!!!. Falling Back To Basic Profile for Provisioning!!{noformat}

Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=272850&size=413676&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep14_19:11:13.644101.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=272850&size=413676&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep14_19:11:13.644101.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



You can access TB5 cluster to validate your fixes:

10.22.40.63
admin/Maglev123





Thanks,
Anusha John [~accountid:63f50bcece6f37e5ed93c87e] 
Same issue seen on EXSI Hulk intra upgrade on *HULK Patch-1 build: 3.713.75131* [2.3.7.3] 

Here is the Failed log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=595669&size=1174708&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_04:41:57.197962.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=595669&size=1174708&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_04:41:57.197962.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [~accountid:63f50bd68ab3d6a635ecc29b] issue you are seeing is a different issue. Have provided fix for that one here: [ENG-SDN / dnac-auto / cc5825b29f6 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/cc5825b29f61089f50f28ba6590f19fec2a57a7b] [ENG-SDN / dnac-auto / c48b6227399 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c48b6227399a49b13f4cd1a2f2064cbfe63972ed] ← [~accountid:61efa8c457b25b006877eda3] added to ignore the ai error since ai rf profile is already deleted at the time of the upgrade script Hi [~accountid:63f50bcece6f37e5ed93c87e] 
I am seeing similar issue on AWS Ghost upgrade also 
Can you double commit fix on Ghost too.

TC 20 Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17388618&size=206634&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov14_07:25:58.324595.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17388618&size=206634&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov14_07:25:58.324595.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

{noformat}59378: 
 No AI RF Profile or Not Associated With Site!!!!. Falling Back To Basic Profile for Provisioning!!{noformat}

{noformat}59608: 
 Failed reason: Result: AP Provision failed{noformat}



TC 45:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6238875&size=174303&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov14_21:49:14.552073.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6238875&size=174303&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov14_21:49:14.552073.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



{noformat}11811: 
 Provisioning AP of device failed for reason:NCWL10307: APWirelessConfiguration FLOOR1_LEVEL1_TYPICAL_42015 with Id 4709c0d9-536f-441a-9571-b4b0a3c09cd0 SiteId d8e4bd27-3bee-424a-9924-9d7e96e22daa and RfProfile TYPICAL is already present in the database{noformat}

{noformat}11814: 
 Failed reason: Result: AP Provision failed{noformat}","['Auton', 'Hulk', 'Upgrade']",Andrew Chen,Reopened,Anusha John
SEEN-2302,https://miggbo.atlassian.net/browse/SEEN-2302,"[Auton]: [Sanity] Test_TC123_aaa_per_ssid /test3_onboard_wireless_segment_for_new_ssid provisioning has been done for only SAN jose and NEW york site, But the test is looking for site SAN FRANCISCO as well","*Description:*  The test [Test_TC123_aaa_per_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=809067&size=3289184&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_23:06:02.962127.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] / [test3_onboard_wireless_segment_for_new_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3466808&size=629584&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_23:06:02.962127.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] has {{Failed to onboard wireless segment at dev TB3-DM-WLC}}.  During provisioning of devices the test is looking for WLC /ECA sites and then performing segment onboarding. In this process the test is looking for site {{Global/USA/SAN-FRANCISCO}} where we don’t have any devices or APs assigned in Sanity profiles.

*CODE SNIPPET:* 

{noformat}for dev in self.services.dnaconfig.devlist:
    # Only onboard for WLCs    if self.services.dnaconfig.testbed.devices[dev].role.find(""WLC"") != -1 or ""ECA"" in \
                            self.services.dnaconfig.testbed.devices[dev].role:
        wlc_name = self.services.dnaconfig.testbed.devices[dev].name
        if ""BLD23"" in self.services.dnaconfig.testbed.devices[dev].site:
            segment1 = ""sub""            site = 'BLD23'        elif ""BLD_SF"" in self.services.dnaconfig.testbed.devices[dev].site:
            segment1 = ""sf""            site = 'BLD_SF'        else:
            segment1 = ""nyc""            site = 'BLD_SF'        segment = segment0 + segment1 + segment2
        self.log.info(f""Using segment pool {segment} for site {self.services.dnaconfig.testbed.devices[dev].site}"")
        self.log.info(f""Onboarding wireless segment for test_AAA on device: {wlc_name}"")
        if self.services.onboard_wireless_segment_for_ssid(segment=segment, ssid=ssid, device=wlc_name, site=site):
            self.log.info(f""Result: Device {wlc_name} - Device_onboarding adding wireless segment {segment} to SSID {ssid}.""){noformat}

 
*ERROR SNIPPET:*

{noformat}Segment WSClients_nyc-WirelessVNFB was not found in Global/USA/SAN-FRANCISCO.
 Existing l2segments in Global/USA/SAN-FRANCISCO:

Failed to onboard wireless segment at dev TB3-DM-WLC
16265:  Failed reason: Failed to onboard wireless segment for ssid test_AAA{noformat}

*DNAC Fabric Site SAN FRANCISCO with no devices :* 


!image-20230907-075501.png|width=1503,height=388!

*Branch:* private/Hulk-ms/sanity_api_auto, private/HulkPatch-ms/sanity_api_auto,

*Script file:* solution_test_sanityecamb_lan.py /*Test_TC123_aaa_per_ssid / test3_onboard_wireless_segment_for_new_ssid*

*input file:* solution_test_input.json

*Failed log on -Prem :*[*Test_TC123_aaa_per_ssid*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=809067&size=3289184&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_23:06:02.962127.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Failed log AWS TB11*: [Test_TC123_aaa_per_ssid|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1157836&size=2996706&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep07_00:24:17.599174.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed*: Sanity Testbed TB3",2023-09-07T07:48:02.456+0000,Fix [434cca55716|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/434cca5571640de8b971d585544c9152b0337aac],"['AWS_Sanity', 'Auton', 'Hulk', 'Integration', 'sanity']",Raji Mukkamala,Resolved,Ashwini R Jadhav
SEEN-2303,https://miggbo.atlassian.net/browse/SEEN-2303,Test_TC60_Verify_DHCP_server_change_on_segments/test3_verify_change_sharedsecret,"*Uber ISO Version tested :* 3.713.75106 - Hulk ESXI

*EXSI OVA build:* +assembly_release_dnac_hulk_converged_07-3.713.75106.ova+
*Branch*: private/HulkPatch-ms/api-auto
*Script Name :* solution_test_sanity_ecamb.py
*Testbed :* TB4

*Testcases Impacted :*

*Test_TC60_Verify_DHCP_server_change_on_segments/test3_verify_change_sharedsecret*

TC 60 is configuring shared secret and saving in Global settings , after saving it will be pushing to devices and ISE side.Enhance the script as after shared secret key is not applied on WLC public pushed  we want shared key to be displayed in logs and  validate and match every device key  with *""newcisco""*

*Description:*   

Pass Log :


[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=28407686&size=56367&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_auto_job.2023Mar01_05:25:46.847642.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=28407686&size=56367&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-03%2Fenv_auto_job.2023Mar01_05:25:46.847642.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Fail Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=34453112&size=110509&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_12:44:15.125712.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=34453112&size=110509&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_12:44:15.125712.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-08T02:52:16.612+0000,"{{Please rasise defect for this as the old shared secret is displayed on WLC TB4-DM-WLC# }} ({{client 10.195.227.68 server-key cisco}}) ← Should be newcisco here

{noformat}127361: 
 aaa server radius dynamic-author{noformat}

{noformat}127362: 
  client 10.195.227.68 server-key cisco{noformat}

{noformat}127363: 
  client 85.1.1.3 server-key newcisco{noformat} [~accountid:63f50bf5e8216251ae4d59cf] 
in HULK VA Patch2 build #3.714.75232  observed that {{shared secret}} key showing“{{newcisco}}“ but still got failed
log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-dhcp_server_sahred_secrect_change.py-132-designNWSettingsEditsDhcp&begin=2747445&size=46841&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov21_02:21:35.215260.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-dhcp_server_sahred_secrect_change.py-132-designNWSettingsEditsDhcp&begin=2747445&size=46841&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov21_02:21:35.215260.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] test3 got passed in  [test3_verify_change_sharedsecret] on latest Hulk p2 build

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-dhcp_server_sahred_secrect_change.py-132-designNWSettingsEditsDhcp&begin=476014&size=65020&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov28_15:35:28.078439.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-dhcp_server_sahred_secrect_change.py-132-designNWSettingsEditsDhcp&begin=476014&size=65020&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov28_15:35:28.078439.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Failed test5 due to [test5_verify_devices_status_after_change_shared_secret ]

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-dhcp_server_sahred_secrect_change.py-132-designNWSettingsEditsDhcp&begin=2381614&size=46837&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov28_15:35:28.078439.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-dhcp_server_sahred_secrect_change.py-132-designNWSettingsEditsDhcp&begin=2381614&size=46837&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov28_15:35:28.078439.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] On latest Hulk p2 build test5 [[test5_verify_devices_status_after_change_shared_secret|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-dhcp_server_sahred_secrect_change.py-131-designNWSettingsEditsDhcp&begin=10728&size=93258&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec05_03:31:01.767884.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]] failed due to same issue for EWLC 9800 controller others devices are updated shared secret key as expected 

{{Shared secret key 'cisco' configured on dev not as expected: newcisco}}

log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec05_03:31:01.767884.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec05_03:31:01.767884.zip&atstype=ATS]

*Failed to verify shard secret key when “sh run | s aaa server radius“ output printed as below :-* 

h5.  _TB4-DM-WLC#sh run | s aaa server radius_
_aaa server radius dynamic-author_
 *_client 10.195.227.68 server-key cisco_*
 _client 85.1.1.3 server-key newcisco_

When “client 85.1.1.3 server-key newcisco“ printed on the first line TC getting passed.

Below is th expected output when “_sh run | s aaa server radius_“ cli executed 

h5.  _TB4-DM-WLC#sh run | s aaa server radius_
_aaa server radius dynamic-author_
 *_client 85.1.1.3 server-key newcisco_*
 _client 10.195.227.68 server-key cisco_



I have modified _aaa server radius_ config manually on device & executed tc it’s passed. script needs to enhancement 

passed log:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec05_04:13:57.874295.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec05_04:13:57.874295.zip&atstype=ATS]","['Auton', 'ESXi', 'Hulk', 'HulkPatch1']",Raji Mukkamala,In Progress,KRISHNA MUKKU
SEEN-2304,https://miggbo.atlassian.net/browse/SEEN-2304,Auton:Hulk: Test_TC228_DNAC_assurance_pubsub_issue_check / Test_TC229_DNAC_assurance_lisp_issue_check,"*Reporter Analysis:*

Testcase is not working as expected ECA device[BORDER] config is not pushed and showing some ker error



*Description:*

{{TB2-DMZ-SJ-FIAB-ECA# }}

{noformat}16055: 
 fabric_pubsub_session_global_trigger issue generation failed adding back the removed CLI to Device TB2-DMZ-SJ-FIAB-ECA (alias=sj-fiab), type IOS-XE{noformat}

{noformat}16236: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Hulk/DMZ-Hulk-sanity-common-Multi-job/services/dnaserv/lib/api_groups/assurance/group.py"", line 11539, in add_or_remove_lisp_issue{noformat}

{noformat}16237: 
     ""ip access-list extended BLOCK-LISP-SESSION\n""{noformat}

{noformat}16238: 
 IndexError: list index out of range{noformat}

*Branch Name:* Hulk-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:

*Failed Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4107276&size=14307&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_00:25:29.677920.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4107276&size=14307&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_00:25:29.677920.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4121583&size=58669&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_00:25:29.677920.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4121583&size=58669&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_00:25:29.677920.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-08T03:35:41.974+0000,"PR link for the Fabric Lisp Issue: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6982/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6982/overview]
Fabric file input needed before starting the script, Please refer wiki for more details

wiki: [https://wiki.cisco.com/pages/viewpage.action?pageId=1647748641|https://wiki.cisco.com/pages/viewpage.action?pageId=1647748641]

Logs: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Froot%2Frel_automation%2Fdnac-auto%2Fpyats_rel%2Fusers%2Farchana%2Farchive%2F23-09%2Fthree_sites.2023Sep12_08:58:46.255418.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Froot%2Frel_automation%2Fdnac-auto%2Fpyats_rel%2Fusers%2Farchana%2Farchive%2F23-09%2Fthree_sites.2023Sep12_08:58:46.255418.zip&atstype=ATS] Thank you Archana for fixing the issue but seeing different issue in next testcase (Test_TC4_DNAC_assurance_lisp_issue_check )

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep29_08:32:45.282597.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep29_08:32:45.282597.zip&atstype=ATS]

Snip:
2607: Traceback (most recent call last):
2608: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/OptimizedSanity-for-ESXi/services/commonlibs/test_wrapper.py"", line 301, in wrapper
2609: result = testfunc(func_self, **kwargs)
2610: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/OptimizedSanity-for-ESXi/testcases/sanityusecases/assuranceMetricFabric/verify_assurance_metrics.py"", line 238, in test4_remove_lisp_issue_on_device
2611: if self.result1 and self.result2 and self.result3:
2612: AttributeError: 'Test_TC4_DNAC_assurance_lisp_issue_check' object has no attribute 'result3'
2613: Test returned in 0:00:00.001767
2614: Errored reason: 'Test_TC4_DNAC_assurance_lisp_issue_check' object has no attribute 'result3'
2615: [~accountid:61efa8c457b25b006877eda3]  / [~accountid:712020:a269cb40-4972-4cdd-b4be-1a9a0bc46160] / [~accountid:62a9d1d0192edb006f9f5332], I have raised another PR to avoid “AttributeError”: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7282/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7282/overview]

Please check your executions and confirm if this Auton can be concluded as “Closed”. Hi [~accountid:62a9d1d0192edb006f9f5332]  / [~accountid:62ab7a399cd13c0068b18fe0] 

I got a pass log for Feature on TB where we have no edge devices as we have an bug for feature with edge device:

BUG ID:
[https://cdetsng.cisco.com/webui/#view=CSCwh15522|https://cdetsng.cisco.com/webui/#view=CSCwh15522]

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1477542&size=168659&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct13_06:34:25.127914.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1477542&size=168659&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct13_06:34:25.127914.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

But I am seeing two subtc is skipped:
[Test_TC229_DNAC_assurance_lisp_issue_check|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1477542&size=168659&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct13_06:34:25.127914.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]  /   test3_verify_device_health_score
  [Test_TC229_DNAC_assurance_lisp_issue_check|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1477542&size=168659&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct13_06:34:25.127914.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]  /   test5_verify_device_health_score

Is this expected on TB with no edge device?

I see same TC is false passing on optimised code instead of skipping:
[Test_TC4_DNAC_assurance_lisp_issue_check|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_assurance_metrics.py-131-assuranceMetricFabric&begin=235696&size=484345&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep29_08:32:45.282597.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test3_verify_device_health_score
[Test_TC4_DNAC_assurance_lisp_issue_check|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_assurance_metrics.py-131-assuranceMetricFabric&begin=235696&size=484345&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep29_08:32:45.282597.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test5_verify_device_health_score



Thanks,
Anusha John
 Hi [~accountid:61efa8c457b25b006877eda3] 

If we are generating the issue on FIAB device, we cant apply ACL to stop the traffic between device and local cp because FIAB device acts as both here. So health score won’t go down. 

Thats the reason we are skipping the health score check.

I will update the same on optimisation as well

Thanks,

Archana I am marking Jira as  resolved per expectation  tc is passing as expected

Thanks [~accountid:62a9d1d0192edb006f9f5332]  Passing as expected as per below comments. 

Pass log on Hulk Patch1 - [Test_TC229_DNAC_assurance_lisp_issue_check|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1477542&size=168659&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct13_06:34:25.127914.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]","['AWS_Sanity', 'Auton', 'FEATURE', 'Hulk', 'Sanity']",Archana KM,Resolved,Anusha John
SEEN-2305,https://miggbo.atlassian.net/browse/SEEN-2305,Auton:Hulk:Test_TC214_template_conflicts_in_template_hub/test11_verify_provision_templates_inventory_provision_VCR/test16_validate_configuration_template_in_config_preview,"*Reporter Analysis:*

Initially testcase failed and 4-5 subtc failed :

We tried adding roles :
commit id:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ee4838617d5f4271164903771d662b77e193b048|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ee4838617d5f4271164903771d662b77e193b048]
Rerun log only two subtc failed:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1638800&size=3445670&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_01:32:41.800613.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1638800&size=3445670&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_01:32:41.800613.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*First Time Failed Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2708885&size=589281&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_01:32:41.800613.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2708885&size=589281&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_01:32:41.800613.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Branch Name:* Hulk-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:",2023-09-08T03:40:21.387+0000,"Hi [~accountid:61efa8c457b25b006877eda3], Which testbed did you use to run this script?

Can I use it to debug? I ran this on the INTG2 testbed and it worked fine. Hi [~accountid:63f50bcafb3ac4003fa2c6dd] 
I tried in dmz testbed 

You need to follow below wiki for accesing cluster:
[https://wiki.cisco.com/display/EDPEIXOT/Guidelines+-+DMZ+WoW|https://wiki.cisco.com/display/EDPEIXOT/Guidelines+-+DMZ+WoW] Hi [~accountid:61efa8c457b25b006877eda3], The test11_verify_provision_templates_inventory_provision_VCR belongs to Vinoth. So I think let him fix the test11, it will be easier and faster. And, I’m going to fix test16 with debug on my own testbed. I just added a few subtests into that usecase.

Vinoth PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6387/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6387/overview] sure [~accountid:63f50bcafb3ac4003fa2c6dd]  Andrew has already fixed this issue Yeah I believe this should be fixed in most recent for subtest 11 # PR Hulk-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7059/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7059/overview]
# Test Case:  Test_TC214_template_conflicts_in_template_hub/{{test16_validate_unexpected_configuration_not_found_in_config_preview}}
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link Hulk-ms/api_auto:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-09/sanity_TB1.2023Sep18_01:21:32.077644.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-09/sanity_TB1.2023Sep18_01:21:32.077644.zip&atstype=ATS] [~accountid:61efa8c457b25b006877eda3]: Could you cherry-pick the PR and check again if the TC run passes or fails? Hi [~accountid:63f50bcafb3ac4003fa2c6dd] 
In recent logs above mentioned One TC passed and another TC failed is due to the bug:

[https://cdetsng.cisco.com/webui/#view=CSCwh78443|https://cdetsng.cisco.com/webui/#view=CSCwh78443]


Log of template:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-templatehub.py-271-toolsTemplateHub&begin=3435&size=2681173&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct06_01:46:12.299148.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-templatehub.py-271-toolsTemplateHub&begin=3435&size=2681173&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct06_01:46:12.299148.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



Thanks,
Anusha John
 Hi [~accountid:61efa8c457b25b006877eda3], I saw the bug was ‘Resolved’ and waiting to ‘Verified’. Once that bug is verified, could you check and rerun the usecase? Thanks","['AWS_Sanity', 'Auton', 'Feature', 'Hulk', 'Sanity']",NhanHuu Nguyen,Resolved,Anusha John
SEEN-2306,https://miggbo.atlassian.net/browse/SEEN-2306,[Auton]: Hulk_P1_BAPI - Test_TC2_DNAC_configure_virtual_network_ip_pool_with_traffic_types/test1_device_onboarding_update_virtual_network_pool_and_authentication_fabric1,"*Reporter Analysis:*

Build : Hulk-P1_BAPI : 2.3.7.3-70190

In this UC7 [test1_device_onboarding_update_virtual_network_pool_and_authentication_fabric1|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-virtual_networks_segments.py-73-SDAFabricCDVirtualNetworksAndSegmentOnboarding&begin=1325344&size=1588625&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep07_01:19:58.156906.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] script is trying to add the Virtual Network to the sites below,

{{Action: Adding VN-Name: SGT_Port_test on site: Global/USA/SAN_JOSE}}

Segment POOl: {'fabric-name': 'Global/USA/SAN_JOSE',
'segments': [{'IPPOOLNAME': 'SGT_Port_test_sub',
                 'isApProvisioning': 'false',
                 'isCommonPool': 'false',
                 'isCriticalPool': 'false',
                 'isDefaultEnterprise': 'false',
                'isDefaultGuest': 'false',
                 'isExtendedProvisioning': 'false',
                 'isFloodAndLearn': 'true',
                'isWirelessPool': 'true',
                 'trafficType': 'DATA',
                 'wlan': []}],
   'vn-name': 'SGT_Port_test',
   'wirelessclientsegment': [{'IPPOOLNAME': 'WClients_sub'},
                                           {'IPPOOLNAME': 'WSClients_sub'}]}	

While adding the Virtual network getting the below error attached,

+*Error:*+

{noformat}Error Code: 400 for{noformat}

{noformat}3309: 
 URL:https://10.22.40.52/dna/intent/api/v1/business/sda/virtualnetwork/ippool Data:{'timeout': 100, 
 'data': '{""siteNameHierarchy"": ""Global/USA/SAN JOSE"", ""virtualNetworkName"": ""SGT_Port_test"", ""isLayer2Only"": ""false"", ""ipPoolName"": ""SGT_Port_test_sub"", ""vlanName"": ""SGT_Port_test_sub-SGT_Port_test"",
 ""autoGenerateVlanName"": ""false"", ""trafficType"": ""DATA"", ""isL2FloodingEnabled"": ""true"", ""isIpDirectedBroadcast"": ""false"", ""isThisCriticalPool"": ""false"", ""poolType"": ""Extended"", ""scalableGroupName"": """", 
 ""isWirelessPool"": ""true"", ""isFloodAndLearn"": true, ""isCommonPool"": ""false""}'
 
 description"" : ""NCSO10001: CFS validation failed due to invalid parameters. OperationalInfo in expanded collection inside domain Global/USA/SAN_JOSE is not valid. OI for domain CfsOperationalInfo[UPDATE,[fabric_segment_operation]]{noformat}

			

Log:[test1_device_onboarding_update_virtual_network_pool_and_authentication_fabric1|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-virtual_networks_segments.py-73-SDAFabricCDVirtualNetworksAndSegmentOnboarding&begin=1325344&size=1588625&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep07_01:19:58.156906.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Script file /Use case -* *solution_test_sanityecamb_lan.py*

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Branch* : private/Hulk-ms/sanity_api_auto",2023-09-08T05:00:14.447+0000,,"['BAPI', 'auton', 'express-sanity', 'hulk']",KARTHIKEYAN KRISHNAMURTHY,Cancelled,KARTHIKEYAN KRISHNAMURTHY
SEEN-2307,https://miggbo.atlassian.net/browse/SEEN-2307,Hulk - Failed in retry exceeded in getting CPU/Memory from NDP API,"*Regression:* SDA Solution Sanity
*DNAC Release_Version Used:* Hulk 2.1.710.70488
*Branch Used:* rcdn/Hulk-ms/api-auto
*Image Used on Polaris Devices:* V173_THROTTLE_LATEST_20230824_120026_V17_3_7_2
*Issue Faced:* Failed in retry exceeded in getting CPU/Memory from NDP API
*Fix Required Testcase:* TC36, TC43 

*Taas_Log:* [https://ngdevx.cisco.com/services/taas/results/cf6413bf-39d8-4984-9a85-5bfc8d250aff/run-results|https://ngdevx.cisco.com/services/taas/results/cf6413bf-39d8-4984-9a85-5bfc8d250aff/run-results]
*RCA:* /ws/smogal-bgl/rca/maglev-50.42.42.5-rca-2023-09-08_15-33-33_UTC.tar.gz   
*Note:* All testcases getting blocked after TC43",2023-09-11T03:02:31.032+0000,[ENG-SDN / dnac-auto / 2bc6d7c2b90 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/2bc6d7c2b908f90c00ed851e2f2ed0b12d702ffd] - fix here,"['Auton', 'hulk']",Andrew Chen,Resolved,SONA MOGAL
SEEN-2308,https://miggbo.atlassian.net/browse/SEEN-2308,Auton:Hulk:Test_TC11_DNAC_Verify_adding_range_discovery_ssh_global_credentials/test3_verify_discovery_netconf_status,"h2. Description:


Recently we observed during Hulk execution , In non optimized code where  [test3_verify_discovery_netconf_status|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=920031&size=6356&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep10_12:43:25.871638.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] tc was failing 

*Reporter analysis:*

1.At the first time NF switch discover was failed and same nf-switch discovery was passed in LLDP discovery, retried the nf switch discovery it was success

2. We are observing while LLDP discovery  ext node was trying to discover , I have attached snip 



*Uber ISO Version tested:* Hulk P1 #2.3.7.3-70203



*Script Name:* {{solution_test_sanityecamb.py}}

*Branch used:* HulkPatch-ms/sanity_api_auto



*Source Team:*  Sanity

Issue Seen first time or day0 issue:

**Fail Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=523416&size=448961&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep10_12:43:25.871638.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=523416&size=448961&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep10_12:43:25.871638.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

",2023-09-11T06:13:11.648+0000,"duplicate [https://miggbo.atlassian.net/browse/SEEN-1704|https://miggbo.atlassian.net/browse/SEEN-1704|smart-link]  Hi [~accountid:63f50bf5e8216251ae4d59cf] ,

Its a different issue because [https://miggbo.atlassian.net/browse/SEEN-1704|https://miggbo.atlassian.net/browse/SEEN-1704|smart-link]  in this Jira its taking 40 min to complete the discovery the task and its not taking ext node ip for LLDP discovering

But in this Jira our discovery task is completed with in 9 min and here its taking EXT node ip address while LLDP discovery was going 

can you please check","['Auton', 'Execution', 'Hulk', 'Sanity']",Raji Mukkamala,Closed,Tulasi Reddy
SEEN-2309,https://miggbo.atlassian.net/browse/SEEN-2309,"[Auton]Hulk P1: The tests are getting in loop during connecting to DNAC in this proc ""reconnect_clients"" and returning after 30mins , Due to this the tests executions are taking longer time than the usual","*Reporter Analysis:*  The execution time is taking more than the usual due to recent changes in the Hulk Patch code. The tests are getting in loop during connecting to DNAC in this proc ""reconnect_clients"" and returning after 30mins , Due to this the tests executions are taking longer time than the usual after recent code pull from main branch.

I am testing Hulk P1 70203 on TB8, Using REG script with optimized code


*Description:  The error from log or more info* 

{noformat}2023-09-11T04:15:19: %SERVICES-WARNING: Traceback (most recent call last):
2023-09-11T04:15:19: %SERVICES-WARNING:   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/dnaservices.py"", line 100, in __init__
2023-09-11T04:15:19: %SERVICES-WARNING:     super().__init__(
2023-09-11T04:15:19: %SERVICES-WARNING:   File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/dnacentersdk/api/__init__.py"", line 691, in __init__
2023-09-11T04:15:19: %SERVICES-WARNING:     raise VersionError(
2023-09-11T04:15:19: %SERVICES-WARNING: dnacentersdk.exceptions.VersionError: Unknown API version, known versions are 1.2.10, 1.3.0, 1.3.1, 1.3.3, 2.1.1, 2.1.2, 2.2.1, 2.2.2.3, 2.2.3.3 and 2.3.3.0.
2023-09-11T04:15:19: %SERVICES-WARNING: 
2023-09-11T04:15:19: %SERVICES-WARNING: During handling of the above exception, another exception occurred:
2023-09-11T04:15:19: %SERVICES-WARNING: 
2023-09-11T04:15:19: %SERVICES-WARNING: Traceback (most recent call last):
2023-09-11T04:15:19: %SERVICES-WARNING:   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/dnaservices.py"", line 889, in reconnect_clients
2023-09-11T04:15:19: %SERVICES-WARNING:     self._dnacSdk = DNACenterSDKManager(
2023-09-11T04:15:19: %SERVICES-WARNING:   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/dnaservices.py"", line 106, in __init__
2023-09-11T04:15:19: %SERVICES-WARNING:     print.error(traceback.format_exc())
2023-09-11T04:15:19: %SERVICES-WARNING: AttributeError: 'builtin_function_or_method' object has no attribute 'error'{noformat}

*Branch Name:  private/HulkPatch-ms/sanity-api-auto*
*Script file/Usecase:* UC0 

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: NA*

*Failed Log:*  [https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-sanity-common-Multi-job/910/consoleFull|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-sanity-common-Multi-job/910/consoleFull] 

*Testbed details:*  [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8]",2023-09-11T11:33:50.621+0000,[~accountid:5e1415780242870e996f0b2f] To Upgrade DNAcenter SDK pip install the dnacenter SDK to latest version pip install dnacentersdk --upgrade Resolved this issue after installing/upgrading our pyats env dnacentersdk to latest version  “pip install dnacentersdk --upgrade” ,"['Auton', 'Blocked', 'HulkPatch', 'optimized', 'sanity']",Ashwini R Jadhav,Resolved,Ashwini R Jadhav
SEEN-2310,https://miggbo.atlassian.net/browse/SEEN-2310,[Auton]:Hulk-P1_BAPI:Test_TC4_EXT_Add_Border_With_L2_L3_Handoff/test1_add_border_device_l2_l3_handoff_in_SDA_Fabric,"1)In UC10:[TC4_EXT_Add_Border_With_L2_L3_Handoff|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-sda_border_devices.py-101-bapi_SDABorderDevice&begin=708076&size=308125&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep11_02:49:50.361013.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]/[test1_add_border_device_l2_l3_handoff_in_SDA_Fabric|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-sda_border_devices.py-101-bapi_SDABorderDevice&begin=708775&size=281759&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep11_02:49:50.361013.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Script is trying to sync the Border device {{{'progress': 'Syncing devices [204.1.2.2]',}}}] 

{{progress': 'Synced devices: [204.1.2.2]',}} Device is re-synced successfully

2)Querying the loopback ip address in {{TB13-SJ-BORDER-ECA}}, {{TB13-NY-FIAB,TB13-Transit}}device
show running-config  interface Loopback0 | include address

ip address 204.1.2.2 255.255.255.255

show running-config  interface Loopback0 | include address
ip address 204.1.2.3 255.255.255.255

show running-config  interface Loopback0 | include address
ip address 204.1.2.4 255.255.255.255

3)Getting error in payload and Virtual network VN7 is not there in cluster.

{noformat} Modify payload for testing l2-handoff changes in 'nodeType'...
 Remove 'Edge_Node''role in payload...{noformat}

{noformat}l2Handoff': [{'virtualNetworkName': 'VN7',
'vlanName': 'VN7-POOL1_sub-VN7'}],{noformat}

Error:

{noformat}{'errorCode': 400,'reason': 'Bad Request',
'response': {'description': 'Could not find segment for given vlanName = ''VN7-POOL1_sub-VN7',
'executionId': '400c9c18-677d-42db-93d7-87c1a95d546f',
'executionStatusUrl': '/dna/intent/api/v1/dnacaap/management/execution-status/400c9c18-677d-42db-93d7-87c1a95d546f',
'status': 'failed',
'taskId': None
'taskStatusUrl': '/dna/intent/api/v1/task/null'},{noformat}

[{
""borderSessionType"": ""ANYWHERE"",
""borderWithExternalConnectivity"": true,
""connectedToInternet "": true,
""deviceManagementIpAddress"": ""[+204.1.2.2+|https://204.1.2.2]"",
""deviceRole"": [""Border_Node"", ""Control_Plane_Node""],
""externalConnectivityIpPoolName"": ""BorderHandOff_sub"",
""externalConnectivitySettings"": [{
""externalAutonomouSystemNumber"": ""200"",
""interfaceDescription"": ""GigabitEthernet1/0/48"",
""interfaceName"": ""GigabitEthernet1/0/48"",
""l2Handoff"": [{
""virtualNetworkName"": ""VN7"",
""vlanName"": ""VN7-POOL1_sub-VN7""
}],
""l3Handoff"": [{
""virtualNetwork"": {
""layer3Instance"": 4098,
""virtualNetworkContextId"": ""e08f6b9c-e411-44cb-9c71-ce04a451b4be"",
""virtualNetworkId"": ""6110d622-d304-4ad3-8ae2-61d206cd0ad5"",
""virtualNetworkName"": ""DEFAULT_VN"",
""virtualNetworkStatus"": ""ACTIVE"",
""vlanId"": ""3000""
}


""externalDomainRoutingProtocolName"": ""BGP"",
""internalAutonomouSystemNumber"": ""6100"",
""routeDistributionProtocol"": ""LISP_PUB_SUB"",
""siteNameHierarchy"": ""Global/USA/SAN JOSE/BLD23""



Log:[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep11_02:49:50.361013.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep11_02:49:50.361013.zip&atstype=ATS]

Branch : private/Hulk-ms/sanity_api_auto

DNAC Version - 2.3.7.3-70190

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:* 

*Script file /Use case -* *solution_test_sanityecamb_lan.py*",2023-09-11T13:00:53.481+0000,"Hi  [~accountid:712020:0b71b93e-0055-4f5e-aacf-044c85d28efd] 

RCA: 

IP Pools are not being mapped in Fabric Site. 

!image-20230912-030605.png|width=1759,height=1391!

Please ensure that UC 7 - {{SDAFabricCDVirtualNetworksAndSegmentOnboarding}}/ {{Test_TC2_DNAC_configure_virtual_network_ip_pool_with_traffic_types}} are PASSED as preconditions for this UC 10: {{TC4_EXT_Add_Border_With_L2_L3_Handoff/test1_add_border_device_l2_l3_handoff_in_SDA_Fabric}}

Hi [~accountid:5f3c6ae932360700388f7b4b] , this is not Auton, so I will mark this as Cancelled Hi [~accountid:63f50bd640328c12e4ec5b00] ,

In this UC10 
test1_add_border_device_l2_l3_handoff_in_SDA_Fabric 
Could not find segment for given vlanName = ''VN7-POOL1_sub-VN7', 

Checked in cluster Virtual network VN7 is not yet created



{noformat}deviceManagementIpAddress': '204.1.2.2',
deviceRole': ['Border_Node', 'Control_Plane_Node'],
externalConnectivityIpPoolName': 'BorderHandOff_sub',
[{'externalAutonomouSystemNumber': '200',
interfaceDescription': 'GigabitEthernet1/0/48',
interfaceName': 'GigabitEthernet1/0/48', 
'l2Handoff': [{'virtualNetworkName': 'VN7',vlanName': 'VN7-POOL1_sub-VN7'}]

Checked in this device 204.1.2.2 [TB13-SJ-BORDER-ECA-telnet 10.22.45.9 2012]
The interface GigabitEthernet1/0/48 is not there
sh run interface  GigabitEthernet1/0/48
                                      ^
% Invalid input detected at '^' marker.
Upto only Gig1/0/24 is there.


{noformat} Hi [~accountid:712020:0b71b93e-0055-4f5e-aacf-044c85d28efd] 
VN7 cannot be created because the previous UC7 failed by defect [https://cdetsng.cisco.com/webui/#view=CSCwh51995|https://cdetsng.cisco.com/webui/#view=CSCwh51995]. The image shows that your cluster has no enough pool that need be created successfully. Hi [~accountid:63f50bd640328c12e4ec5b00] ,

In this UC-[73-SDAFabricCDVirtualNetworksAndSegmentOnboarding|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-virtual_networks_segments.py-73-SDAFabricCDVirtualNetworksAndSegmentOnboarding&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep07_01:19:58.156906.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   [Test_TC1_DNAC_Connectivity_domain_creation_in_dnac|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-virtual_networks_segments.py-73-SDAFabricCDVirtualNetworksAndSegmentOnboarding&begin=4508&size=1319776&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep07_01:19:58.156906.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test4_adding_virtual_network_to_fabric 

{{Action: adding virtual network VN7 to site Global/USA/SAN JOSE!}}

{{'description': 'Virtual Network added successfully in SDA Fabric.',}}

{'params': {'virtualNetworkName': 'VN7', 'siteNameHierarchy': 'Global/USA/SAN JOSE'}, 'timeout': 50}

{'description': 'Virtual Network info retrieved successfully from SDA Fabric.',
fabricName': 'Default_Fabric_Lan',
'siteNameHierarchy': 'Global/USA/SAN JOSE',
'status': 'success',
'virtualNetworkName': 'VN7'}
Result: Get VN: VN7 from Fabric Site: Global/USA/SAN JOSE successfully
Result: Add virtual network VN7 to site: Global/USA/SAN JOSE successfully.

Log:[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep07_01:19:58.156906.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep07_01:19:58.156906.zip&atstype=ATS]

Subpool also configured please check below,

{'data': {'name': 'VN7-POOL1_sub', 'type': 'generic', 'ipv4GlobalPool': '40.40.52.0/24', 'ipv4Prefix': True, 'ipv6AddressSpace': True, 'ipv6Prefix': True, 'ipv4PrefixLength': '24', 'ipv4Subnet': '40.40.52.0', 'ipv4GateWay': '40.40.52.1', 'ipv4DhcpServers': ['204.192.3.40'], 'ipv4DnsServers': ['204.192.3.40'], 'ipv6GlobalPool': '2040:40:52::/64', 'ipv6PrefixLength': '96', 'ipv6Subnet': '2040:40:52::1:0:0', 'ipv6GateWay': '2040:40:52::1:0:1', 'ipv6DhcpServers': ['2004:192:3::40'], 'ipv6DnsServers': ['2006:1:1::1']}}

{""name"":""VN7-POOL1_sub"",""type"":""generic"",""result"":""NCIP10245: A group named VN7-POOL1_sub already exists."",""status"":""True""}'

subPool configured successfully!



Log:[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep08_09:17:26.939606.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep08_09:17:26.939606.zip&atstype=ATS] Hi [~accountid:712020:0b71b93e-0055-4f5e-aacf-044c85d28efd] 

# I’ve mentioned *UC7/TC2/test1* not UC7/TC1/test4 
# This Jira ticket mentions Optimized BAPI Use cases based on mapfile. Why did you provide a log of {{solution_test_sanityext_api.py}} file?
# Your cluster is current has no {{VN7-POOL1_sub-VN7}} . That why we cannot run UC10.
!image-20230912-150019.png|width=1885,height=558! Hi [~accountid:63f50bd640328c12e4ec5b00] ,

Could you please check my attachment , shows task id is null, i'm using the POST Method, could you check why taskid is showing null. Hi [~accountid:712020:0b71b93e-0055-4f5e-aacf-044c85d28efd] 
Your attachment shows status: failed, and schema states that {{For failed status, taskId may not be available}} so, I think this is fine.

We need confirmation from [~accountid:5f3c6ae932360700388f7b4b] for the case

!image-20230912-155810.png|width=1439,height=728! Task id none is ok for this when the reponse is an error. task id is generated only when response code is 200 or 202.  for all other it will be None and that is expected.","['BAPI', 'auton', 'express-sanity', 'hulk', 'junk', 'optimized', 'sanity']",ThanhTan Nguyen,Cancelled,KARTHIKEYAN KRISHNAMURTHY
SEEN-2311,https://miggbo.atlassian.net/browse/SEEN-2311,Auton:Hulk:Test_TC76_DNAC_verify_configuring_anchored_vn  /   test1_verify_add_segment_to_remote_anchored_vn,"h2. Description

*Reporter Analysis:*
We are observing on recent hulk execution [Test_TC76_DNAC_verify_configuring_anchored_vn|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=28385739&size=3196292&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep11_01:35:49.942750.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test1_verify_add_segment_to_remote_anchored_vn was failed, script is checking {{SAN-FRANCISCO }}site to add the l3 segment .
*Description*:  
130108: 
 WiredVNStatic1-Global/USA/SAN-FRANCISCO was not found

{noformat}130109: 
 WiredVNStatic2-Global/USA/SAN-FRANCISCO was not found{noformat}

{noformat}131463: 
{noformat}

{noformat}131464: 
 ############################################################{noformat}

{noformat}131465: 
 Adding L3 Segments failed.{noformat}

{noformat}131466: 
 ############################################################{noformat}

{noformat}131469: 
 Failed reason: Result: adding segment to remote anchored_vn failed{noformat}

*Branch Name:* HulkPatch-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:

**Fail Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=30686592&size=423399&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep11_01:35:49.942750.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=30686592&size=423399&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep11_01:35:49.942750.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-12T05:20:54.813+0000,"Hulk P1  RC3 
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-anchor_vn_validations.py-163-SDAFabricAnchorvn&begin=2579871&size=482748&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_07:09:33.075475.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-anchor_vn_validations.py-163-SDAFabricAnchorvn&begin=2579871&size=482748&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_07:09:33.075475.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]  
 [~accountid:63f50bf84c355259db9ccc59] , [~accountid:620b8357878c2f00729881c8] , can you follow this template when raising autons?

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] From the log: '{{9937: Pubsub is True in Anchored site Global/USA/New_York while False in Ultilized site Global/USA/SAN-FRANCISCO. Hence skip add anchored vn WiredVNStatic2 in uiltilized site Global/USA/SAN-FRANCISCO}}'

Why SF is not Pubsub {{'isPubSub': False}}? It should be some bug or some api changes.  Please compare with old passed log and debug why SF is not Pubsub while SJ and NYC are Pubsub and raise defect.

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-anchor_vn_validations.py-163-SDAFabricAnchorvn&begin=2113850&size=466021&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_07:09:33.075475.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-anchor_vn_validations.py-163-SDAFabricAnchorvn&begin=2113850&size=466021&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_07:09:33.075475.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] Can you share your last passed log and compare the pubsub info in SF? Root cause: This testbed has no device in SF so the pubsub is false in SF.

Fix: Add handle to skip add segment for remote anchorvn in SF in case pubsub state is different between anchor site and remote site.

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6a71aca270ff245c981ba0241a8755aa659a9e2d|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6a71aca270ff245c981ba0241a8755aa659a9e2d]","['Auton', 'Execution', 'Hulk', 'HulkPatch1', 'Optimized', 'Sanity']",Tran Lam,Resolved,Tulasi Reddy
SEEN-2312,https://miggbo.atlassian.net/browse/SEEN-2312,Auton-Hulk:Test_TC189_edit_site_name/ test16_configure_aps_with_work_flow,"*Reporter Analysis:*

Seeing key error on param:
{{params = {""key"": template['key']} on the path:}}
{{services/dnaserv/lib/api_groups/ap_profile/group.py}}



*Description*:  

{noformat}19362: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Hulk/DMZ-Hulk-sanity-common-Multi-job/services/dnaserv/lib/api_groups/ap_profile/group.py"", line 447, in save_ap_template{noformat}

{noformat}19363: 
     params = {""key"": template['key']}{noformat}

{noformat}19364: 
 KeyError: 'key'{noformat}

{noformat}19366: 
 Errored reason: key{noformat}

*Branch Name:* Hulk-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  AWS Sanity



*Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4392898&size=39960&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep11_07:19:42.300709.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4392898&size=39960&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep11_07:19:42.300709.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-12T06:36:39.048+0000,issue has been addressed: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3cac6cf4f21b50b5399f8fbf22d95ac4e2c1e037|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3cac6cf4f21b50b5399f8fbf22d95ac4e2c1e037],"['AWS_Sanity', 'Auton', 'Feature', 'Hulk']",Moe Saeed,Resolved,Anusha John
SEEN-2313,https://miggbo.atlassian.net/browse/SEEN-2313,Auton-Hulk:Test_TC219_APs_negative_operations/test12_reprovisioning_APs_after_adding_ap_zones /test14_reprovision_wlc_after_adding_ap_tags/test15_reprovisioning_APs_after_adding_ap_tags/test27_reprovision_aps_after_device_tags_removal/,"*Reporter Analysis:* 

During recent feature integration run we observed that AP_negative operation where AP Provision related TC is failing as AI_RF Profile is not  present on the site and AP_Zone related sub tc is”{{qualifier 'null' conflicts with existing user intent}}"".



*Description*:  

{noformat} No AI RF Profile or Not Associated With Site!!!!. Falling Back To Basic Profile for Provisioning!!{noformat}

{noformat}22861: 
 AttributeError: 'Test_TC219_APs_negative_operations' object has no attribute 'log'{noformat}

{noformat}22863: 
 Errored reason: 'Test_TC219_APs_negative_operations' object has no attribute 'log'{noformat}

{noformat} Config Preview Activity failed with reason: NCSP11030: Error occurred while processing the 'provision' request. Additional info for support: taskId: '3cef86db-630c-4e77-b3a5-4ad48f257f2a'. Incoming user intent with identifier '87776b57-7f27-4e0b-9822-236a3f1ba241' for type 'ApWirelessConfiguration' and qualifier 'null' conflicts with existing user intent.{noformat}




",2023-09-12T09:24:53.093+0000,"From the below error 22859:    File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Hulk/DMZ-Hulk-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 15850, in test15_reprovisioning_APs_after_adding_ap_tags

22860:      self.log.error(""Result: AP Provision failed "")

22861:  AttributeError: 'Test_TC219_APs_negative_operations' object has no attribute 'log'

22862:  Test returned in 0:00:50.303324

22863:  Errored reason: 'Test_TC219_APs_negative_operations' object has no attribute 'log'

There is no log  statement "" self.log.error(""Result: AP Provision failed "")"" in solution_test_sanityecamb_lan.py"" at line number 15850 [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#15850|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#15850]



Looks like it’s not run with latest code","['AWS_Sanity', 'Auton', 'Feature', 'Hulk']",Raji Mukkamala,Closed,Anusha John
SEEN-2314,https://miggbo.atlassian.net/browse/SEEN-2314,[Auton]:Test_TC122_verify_cmx_configs_push/test1_integrate_CMX_server,"In recent Hulk P1 Executions, We are Observing the CMX Server integration to be failing

*Reporter analysis:*

We are observing in recent Hulk P1 Executions we are observing the CMX Server Integration to be failing, On checking further we see that there has been a change in CMX Server Integration, where there are only 3 fields added, which include the server IP, username, and password.

We see 2 more fields are missing, i.e, SSH username and SSH password.  (Please refer Snippet for reference)
Initially, we thought it could be a Defect and Raised a Defect for this issue [*CSCwh30204*|https://cdetsng.cisco.com/webui/#view=CSCwh30204]
However, after discussing the DE [danijoh2@cisco.com|mailto:danijoh2@cisco.com]. We understand that there is a change in CMX integration where they do not connect to CMX server over SSH and the expectation is to provide the credentials only of the CMX Rest API account for querying data from the CMX, and not the CMX admin credentials

We see this change to have taken place only in Hulk Patch 1 and above

We have pass Log from Hulk RC Image and Ghost as well

[https://cdetsng.cisco.com/webui/#view=CSCwh55014|https://cdetsng.cisco.com/webui/#view=CSCwh55014]


*Design doc :* [https://mydna.cisco.com/documents/view-design-document?featurename=F116559|https://mydna.cisco.com/documents/view-design-document?featurename=F116559]

+*Pass Log:*+
Hulk RC4: [Test_TC122_verify_cmx_configs_push|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=68394213&size=1683124&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug07_04:32:03.564760.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Uber ISO Version tested:* Hulk Patch 1 {{2.3.7.3-70125}}

*Script Name:* Lan Script, Regular Script

*Branch used:* private/Hulk-ms/sanity_api_auto, private/Hulk-ms/sanity_delay_testing

*Script used:* solution_test_sanityecamb_cert_lan.py, {{solution_test_sanityeca}}.py, {{solution_test_sanityecamb_cert_lan}}.py

*Snip from the log:* 

{noformat}4982: st-ds-2.cisco.com: 2023-08-16T10:32:26: %API-GROUP-CMX-3-ERROR: %[pid=3887690][pname=Task-1]: CMX Integration Failed {'endTime': 1692207144919, 'progress': 'Add CMX Task initiated', 'data': 'Please verify the remote servers certificate alternative names are configured correctly.', 'version': 1692207144919, 'startTime': 1692207144589, 'errorCode': 'NCMP00042', 'serviceType': 'NCMP', 'failureReason': ""NCMP00042: The server https://85.1.1.15/api/config/v1/version/image authenticity could not be verified: SSLPeerUnverifiedException: Certificate for <85.1.1.15> doesn't match any of the subject alternative names: [TB8-CMX-Server]"", 'isError': True, 'instanceTenantId': '64d9e879e5163a5dac7c91b6', 'id': '0189ff67-de8d-7a9e-b3b7-7d27b1728bdf'}!!!{noformat}

{noformat}4983: 2023-08-16T10:32:26:  Test returned in 0:00:02.198514{noformat}

{noformat}4984: 2023-08-16T10:32:26:  Failed reason: Integrating CMX server failed!!{noformat}


+*Failed Log:*+

* [https://ngdevx.cisco.com/services/taas/results/093f21ca-61b5-4d2a-a96e-81d5c8ec72da/run-results/section/578/Task-1|https://ngdevx.cisco.com/services/taas/results/093f21ca-61b5-4d2a-a96e-81d5c8ec72da/run-results/section/578/Task-1]",2023-09-12T15:48:23.821+0000,"Hi [~accountid:63f50bf5e8216251ae4d59cf]  ,

Could  you  please  check  on priority ,

  HUlk P1  RC3  
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-cmx_config_push_and_device_validations.py-158-cmxConfigsAndValidations&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_04:11:44.631453.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-cmx_config_push_and_device_validations.py-158-cmxConfigsAndValidations&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_04:11:44.631453.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]","['Auton', 'Blocked', 'HulkPatch', 'Optimized', 'Sanity', 'Uplift']",Raji Mukkamala,In Progress,DeepakPratap Shinde
SEEN-2315,https://miggbo.atlassian.net/browse/SEEN-2315,Auton: Hulk: Test_TC71_DNAC_extended_node_link_failover_test  /   test70_reassign_ext_node_ip,"h2. Description

*Reporter Analysis*:
During Hulk and Ghost testing we are observing “test70_reassign_ext_node_ip” was failing duo to Extended node({{SN-FOC2311T18E}}) IP address was not changing after 3 attempts.

*Description*:  
Device Connectivity issue, not recoverable through resyn, check why device in this state.

{noformat}98645: 
 ERROR: Device SN-FOC2311T18E.cisco.com is not in managed state, current state:Partial Collection Failure, errorDescription:NCIM12013: SNMP timeouts are occurring with this device. Either the SNMP credentials are not correctly provided to Cisco DNA Center or the device is responding slow and SNMP timeout is low. If it’s a timeout issue, Cisco DNA Center will attempt to progressively adjust the timeout in subsequent collection cycles to get device to managed state. User can also run discovery again only for this device using the discovery feature after adjusting the timeout and SNMP credentials as required. Or user can update the timeout and SNMP credentials as required using update credentials., retrying{noformat}

{noformat}98718: 
 Failed reason: Extended node ""SN-FOC2311T18E"" IP did not change!{noformat}

*Branch Name:* HulkPatch-ms/sanity_api_auto and Ghost -ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:

**Fail Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=23498764&size=54427&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep11_01:35:49.942750.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=23498764&size=54427&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep11_01:35:49.942750.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

",2023-09-13T05:40:30.020+0000,"[~accountid:63f50bf84c355259db9ccc59] 

What is the issue? Did you debug it and what is your findings?  Needs more details and more debugging results, and what is the script issue here?  [~accountid:63f50bfce8216251ae4d59d5] 
Even though  Testcase is failed as IP is not changed on dnac ip is changing and testcase is not displaying that before displaying tc is failing with ip didn’t changed message:

I have raised an bug for testacase:
[https://cdetsng.cisco.com/webui/#view=CSCwh00379|https://cdetsng.cisco.com/webui/#view=CSCwh00379]


*shashank analysis:*


{adf:display=block}
{""type"":""table"",""attrs"":{""isNumberColumnEnabled"":false,""layout"":""default"",""localId"":""5d026bcc-c7d3-4448-85a0-de8a86ae9168""},""content"":[{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{""colspan"":2},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""From Shashank - \""Basically for this one, script is calculating the time to recover from an IP address change event (induced in the script by reloading the extended node and changing DHCP server configuration so that it gets a different IP address). Ideally the script should wait for up to 21 minutes after inventory has declared the device as unreachable since IP address recovery job runs every 20 mins and how soon the recovery happens depends on when in the 20 min cycle this event happened. In this case the IP address change happened correctly but script is reporting it took longer. But unable to see from the logs whether it really took that long. Thats where script team should confirm the analysis above and see if there is any discrepancy either in analysis or script. Since this is a timing issue the results will vary from execution to execution.\""""}]}]}]}]}
{adf}

Thanks,
Anusha John","['Auton', 'Execution', 'Ghost', 'Hulk', 'Sanity']",Moe Saeed,In Progress,Tulasi Reddy
SEEN-2316,https://miggbo.atlassian.net/browse/SEEN-2316,[Auton]:TC227_View_Issue_Consolidation_Of_Remote_Internet_Sessions_Between_Control_plane_And_Border_At_VN_level/test3_check_device_shown_unreachable ,"In a recent Hulk P1 Execution, We Tried integrating Feature [https://miggbo.atlassian.net/browse/SEEN-2223|https://miggbo.atlassian.net/browse/SEEN-2223|smart-link]+,+ But the Feature has been failing for the below Sub Test Cases

*Build:* Hulk Patch 1

*Uber ISO Version tested:* 2.3.7.3-70139

*Script Name:* Lan Script 

*Branch used:* private/Hulk-ms/sanity_api_auto

*Script used:* {{solution_test_sanityecamb_lan}}.py

*Snip from the log:* 

*test3_check_device_shown_unreachable:*

{noformat}92625: 
 AttributeError: 'DnaServices' object has no attribute 'check_reachability_device_in_inventory'{noformat}

{noformat}92627: 
 Errored reason: 'DnaServices' object has no attribute 'check_reachability_device_in_inventory'{noformat}

*test4_verify_remote_internet_session_from_cp_to_bn_after_bring_down_one_border*

{noformat}93124: 
 There is/are different value(s): [{'goodCount': '0', 'noHealthCount': '2'}]{noformat}

{noformat}93126: 
 Fail to verify Remote Internet Session from Control Plane to Border after change config.{noformat}

{noformat}93127: 
 Current Data: {'goodCount': '1', 'noHealthCount': '1'}. Expected Data: {'goodCount': '0', 'noHealthCount': '2'}.{noformat}

{noformat}93140: 
 Failed reason: Error: Data of Internet Session from Control Plane to Border at VN level are incorrect.{noformat}

*test6_check_device_shown_reachable*

{noformat}93645: 
 AttributeError: 'DnaServices' object has no attribute 'check_reachability_device_in_inventory'{noformat}

{noformat}93647: 
 Errored reason: 'DnaServices' object has no attribute 'check_reachability_device_in_inventory'{noformat}



+*Failed Log:*+

* [Test_TC227_View_Issue_Consolidation_Of_Remote_Internet_Sessions_Between_Control_plane_And_Border_At_VN_level|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=22045405&size=346803&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep07_19:43:54.737185.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-14T07:28:15.754+0000,"# PR Hulk-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7002/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7002/overview]
# Test Case:  Test_TC224_View_Issue_Consolidation_Of_Remote_Internet_Sessions_Between_Control_plane_And_Border_At_VN_level
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link Hulk-ms/api_auto:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-09/sanity_TB2.2023Sep12_21:01:53.489395.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-09/sanity_TB2.2023Sep12_21:01:53.489395.zip&atstype=ATS] The root cause:

Test 3 and test 6 got Errored because there is missing a library.

Test 4 failed because the assurance page took time to update the new data. [~accountid:63f50be24e86f362d39acde8]: Could you cherry-pick this PR and check again if the TC run passes or fails? Hello Nhan,

Will attempt this and update if this fix works, post we have the PR approved Hi [~accountid:63f50be24e86f362d39acde8], The PR was merged. Could you check? Hello [~accountid:63f50bcafb3ac4003fa2c6dd] 
We reattempted this feature with the latest code pull on Hulk P1 RC4 
Version: 2.3.7.3-70327

Branch: private/Hulk-ms/sanity_api_auto
Failed Log: [TC227_View_Issue_Consolidation_Of_Remote_Internet_Sessions_Between_Control_plane_And_Border_At_VN_level|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1439794&size=536222&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov09_00:35:33.698111.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Failed Sub Test Case: [test4_verify_remote_internet_session_from_cp_to_bn_after_bring_down_one_border|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1594188&size=106676&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov09_00:35:33.698111.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

{noformat}8351: 
 There is/are different value(s): [{'goodCount': '0', 'noHealthCount': '2'}]{noformat}

{noformat}8353: 
 Fail to verify Remote Internet Session from Control Plane to Border after change config.{noformat}

{noformat}8354: 
 Current Data: {'goodCount': '1', 'noHealthCount': '1'}. Expected Data: {'goodCount': '0', 'noHealthCount': '2'}.{noformat}

{noformat}8406: 
 There is/are different value(s): [{'goodCount': '0', 'noHealthCount': '2'}]{noformat}

We are re-opening this issue, could you please check Hi [~accountid:63f50be24e86f362d39acde8] ,After checking on TB2, I saw assurance page on your cluster takes a longer time to change the score than my Testbed. So I will increase the retry number for it. PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7854/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7854/overview]

Trade log link HulkPatch-ms/api-auto: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-11/sanity_TB2.2023Nov10_01:43:09.496880.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-11/sanity_TB2.2023Nov10_01:43:09.496880.zip&atstype=ATS] Hi [~accountid:63f50bcafb3ac4003fa2c6dd] 
We reattempted this feature with the latest changes as per the PR in our local Branch
And we now see the Test Case 7 to be failing for the same reason
When checked with you as mentioned it needs more wait time for the data to reflect

Failed Log: [Test_TC227_View_Issue_Consolidation_Of_Remote_Internet_Sessions_Between_Control_plane_And_Border_At_VN_level|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1451396&size=484027&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov14_23:44:25.654339.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

[test7_monitor_remote_internet_session_from_cp_to_bn_after_recover_device|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1778376&size=156809&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov14_23:44:25.654339.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

{noformat}0027: 
 Fail to verify Remote Internet Session from Control Plane to Border after change config.{noformat}

{noformat}10028: 
 Current Data: {'goodCount': '1', 'noHealthCount': '1'}. Expected Data: {'goodCount': '2', 'noHealthCount': '0'}.{noformat}

{noformat}10051: 
 Failed reason: Error: Data of Internet Session from Control Plane to Border at VN level are incorrect.{noformat} Hi [~accountid:63f50be24e86f362d39acde8], I have added one more way to get data from the assurance page. Could you check again on your testbed?","['Auton', 'HulkPatch', 'Optimized', 'Sanity', 'Uplift', 'hulk']",NhanHuu Nguyen,Pending Code Review,DeepakPratap Shinde
SEEN-2337,https://miggbo.atlassian.net/browse/SEEN-2337,Auton:Hulk: Cleanup script was not removing the User roles & Model configs ,"h2. Description

*Reporter Analysis:* 

On recent hulk recent execution we are observing the after full cleanup script was completed , Some of the configs was preset on the Cluster 

Areas we observed cleanup was not working : 

# System->User Roles--> Roles was not deleting 
# Design-->Network Profiles-->Profiles was not deleting
# Design-->Network Settings-->Wireless-> Basic RF Profiles was not deleting 

*Description*:  

*Branch Name:* HulkPatch-ms/sanity_api_auto

**Script* *file:** {{dnac_cleanup_script.py}}

*Source Team:*  Sanity",2023-09-15T12:53:20.023+0000,"[~accountid:63f50bf84c355259db9ccc59] Share logs for the above [~accountid:63f50bf84c355259db9ccc59] please share the logs to fix the auton Hi [~accountid:63f50bf5e8216251ae4d59cf] ,
We dont have a failure  logs, but after cleanup script we are observing above mentioned areas was not cleaned properly

Example Network profile cleanup was passed but when we are seeing on dnac we are able to see the network profiles 
Pass log:
[Test_TC26_delete_network_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1323409&size=70100&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep15_01:52:28.548951.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] [~accountid:63f50bf84c355259db9ccc59] Can you share in detail what components/profile names/roles were not cleaned up from DNAC , this would be helpful to fix the auton. please share UI screenshots Sure  [~accountid:63f50bf5e8216251ae4d59cf] ,  will try in next testing and share the details.
 Hi [~accountid:63f50bf5e8216251ae4d59cf] , 
I attached Vedio and snips which areas it was not cleaning , can you please check

!cleanup1.png|width=960,height=539!

!cleanup2 (cf77df4b-c68b-493d-adf4-345f2a45286e).png|width=960,height=540!

!cleanup.png|width=960,height=539!

  Hi [~accountid:63f50bf5e8216251ae4d59cf] 

Thanks, For adding additional testcases:

Today we triggered those new TC but some failed and was not able to delete properly 

1.user and roles were not delteed

2.RF Profiles from network settings

*Failed log and failed subtc:*
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct12_05:33:31.729449.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct12_05:33:31.729449.zip&atstype=ATS]

 

 [Test_TC27_delete_users|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2468471&size=23035&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct12_05:33:31.729449.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]  /   test1_delete_users 

 

[Test_TC34_cleanup_wireless_rf_profiles|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2834944&size=25353&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct12_05:33:31.729449.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]  /   test1_cleanup_rf_profiles

!user roles1.png|width=1920,height=1080! Addressed : [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c5c2c4a2a9ff315833826c1c665356f0c5e55512|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c5c2c4a2a9ff315833826c1c665356f0c5e55512]

Cherry-picked to other branches","['Auton', 'Cleanup', 'Execution', 'Ghost', 'Guardian', 'Hulk']",Raji Mukkamala,Resolved,Tulasi Reddy
SEEN-2338,https://miggbo.atlassian.net/browse/SEEN-2338,Auton:Hulk:Cleanup: Test_TC15_clear_device_onboarding_virtual_segment_mapping,"h2. Description:

*Reporter Analysis:* 
On Recent hulk execution we are observing TC15 was failed  duo to clearing the segments from vn on  SAN-FRANCISCO site on sanity site we are not using SAN-FRANCISCO site 

*Description*:  

{noformat}###########################################################{noformat}

{noformat}28399: 
 Config preview was failed{noformat}

{noformat}28400: 
 ############################################################{noformat}

{noformat}28402: 
 Config preview flow was failed for description: Scheduling task for Removing segments from VN{noformat}

{noformat}28407: 
 Failed reason: Result: Failed in clear_device_onboarding_virtual_segment_mapping{noformat}

*Branch Name:* HulkPatch -ms/sanity_api_auto

**Script* *file:** {{dnac_cleanup_script.py}}

*Source Team:*  Sanity

Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7392597&size=487960&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep14_20:34:43.502695.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7392597&size=487960&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep14_20:34:43.502695.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-15T13:05:24.437+0000,"[ENG-SDN / dnac-auto / 60a9ddd647a - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/60a9ddd647a0b76f543333d8d827fa75b393c620] ← try this commit and let me know. The problem was that bangalore had no VN, but it was still updating operational info. Corrected the check for empty VN Hi [~accountid:63f50bcece6f37e5ed93c87e] , 
unable to cherry pick to “Hulkpatch-ms/sanity_api_auto” branch , can you please double commit to “Hulkpatch-ms/sanity_api_auto” .","['Auton', 'Cleanup', 'Execution', 'Hulk']",Andrew Chen,Resolved,Tulasi Reddy
SEEN-2339,https://miggbo.atlassian.net/browse/SEEN-2339,[Auton]-Hulk VA-CSCwh58778 Test_TC29_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision/test3_cleanup_wireless_ssid failed due to  KeyError: value,"*In Hulk FCS OVA Build #3.710.75522 - 2.3.7.0*  Observed the TC29_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision/test3_cleanup_wireless_ssid failed due to {{KeyError: 'value'}}

{{NCSP11001: User intent validation failed while processing the 'modify' request. Additional info for support: taskId: '3ca22e61-02bf-4ead-ad55-ffbcb8199c02'.}}

*Branch:* private/Hulk-ms/api-auto

*Script file:* solution_test_sanityecamb.py

Failed log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=18253519&size=345683&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep14_03:01:56.554162.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=18253519&size=345683&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep14_03:01:56.554162.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Due to this issue, *TC 32* *TC32_DNAC_configure_virtual_network_ip_pool_with_traffic_types/test2_verify_devices_status_after_ip_pool_segments_added_to_vn*  also failed while config preview with below error message
""NCSP11001: User intent validation failed while processing the 'modify' request. Additional info for support: taskId: '3ca22e61-02bf-4ead-ad55-ffbcb8199c02'""

Other TC’s 36,39 & multiple TCs are failed this is blocking our execution.

*Passed log:*

Previous working build:
*Hulk patch1 #3.713.75113*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17506571&size=101857&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug09_08:20:58.660918.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17506571&size=101857&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug09_08:20:58.660918.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]ue



Webex team space link with DE discussions : - webexteams://im?space=c89b8990-5311-11ee-b5d0-f31264cffd1b",2023-09-15T16:28:53.311+0000,"PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7023/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7023/overview] Same issue is also observed during Solution testing on Multisite (DR) profile. 

Promoted Hulk Patch1 Uber ISO tested - 2.1.713.70207

*Failed log:* [Test_TC52_DNAC_all_aps_verification_in_wlc|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=93291627&size=5407208&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac.2023Sep18_13:31:26.691076.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] → Refer TC52.3 Fix Commits on private/HulkPatch-ms/api-auto branch - [3eb020dafdd|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3eb020dafdd2453bf8d8fc28e406eecee00ec1a2] , [8ebcdb1b93f|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/8ebcdb1b93f865b4bdfb474c97ddf4bd958fff19], [553f8ddcd84|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/553f8ddcd84a54d857b6bfb0329ceec20e4bd0bf], [c545eff1d9e|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c545eff1d9e7e58c0e21b24b62649042ee5b51f3]
","['Auton', 'Blocked', 'Execution', 'Hulk', 'HulkPatch', 'Integration', 'MSTB1', 'Multisite', 'exsivm', 'sanity']",Moe Saeed,Resolved,Raghavendrachar Baraguru Mallesha Char
SEEN-2340,https://miggbo.atlassian.net/browse/SEEN-2340,[Auton]Need to update alt ip address in TB4 yaml,"*Uber ISO Version tested :* 3.713.75122 - Hulk Patch ESXI

*Branch*: private/HulkPatch-ms/api-auto

*Script Name :* [SanityTB4.yaml|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb4/SanityTB4.yaml?at=refs%2Fheads%2Fprivate%2FHulkPatch-ms%2Fapi-auto]

*Testbed :* TB4

*Problem:*

Discovery failed due to wrong ip address updated in tb4 yaml file and need to update the correct ip address.







",2023-09-18T18:35:49.388+0000,"Not an Auton, Regression/Sanity Testbed files should be maintained/updated by regression team. ","['Auton', 'ESxi', 'Hulk']",Manoj Menakuri,Resolved,Manoj Menakuri
SEEN-2341,https://miggbo.atlassian.net/browse/SEEN-2341,private/Hulk-ms/api-auto-nfw branch needs update for getting DNAC Version,"*private/Hulk-ms/api-auto-nfw* branch needs update for getting DNAC Version.

Currently it is using “/v1/maglev/nodes/config” API which is not getting the intended info.

Instead, “/v1/maglev/release/current” should be in use.",2023-09-19T23:16:37.913+0000,"Commit for require change has been merged to *private/Hulk-ms/api-auto-nfw* branch:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ac8ef9a293283079fe34d43b656b6f36d0641663|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ac8ef9a293283079fe34d43b656b6f36d0641663]",['Auton'],Amardeep Kumar,Closed,Amardeep Kumar
SEEN-2342,https://miggbo.atlassian.net/browse/SEEN-2342,Test_TC142_Add_interace_description.,"*TC Failed:* Test_TC142_Add_interace_description.
*Subtest Failed:* test1_add_interface_description.
*Error:* 32011: wnbust-ats-26.cisco.com: 2023-09-14T07:28:24: %API-GROUP-ASSURANCE-3-ERROR: %[pid=373][pname=Task-1]: The expected Description to AP3800 and interface TwoGigabitEthernet1/0/6 NOT found in response [{'id': '40d8dee5-cd07-4e41-a60c-73f95c8333e8', 'name': 'global_ap_disconnect_trigger', 'flattened': True, 'enabled': True, 'entityType': 'Network Device', 'entityName': 'Unknown', 'entity': '7645933d-2893-4d7b-b905-c1db12202751', 'groupBy': 'Unknown', 'severity': 'HIGH', 'category': 'Availability', 'summary': 'AP(s) disconnected from WLC on Switch ""TB4-DM1-9KB1"".', 'rootCause': 'Unknown', 'timestamp': 1694691865233, 'startTime': 1694691865233, 'occurrences': 1, 'instances': 1, 'scope': 'GLOBAL', 'priority': 'P2', 'status': {'status': 'ACTIVE', 'updatedBy': 'Unknown', 'notes': 'Unknown', 'source': 'Unknown', 'updatedAt': None, 'ignoreSource': 'Unknown', 'ignoreValue': 'Unknown', 'ignoreStartTime': None, 'ignoreEndTime': None}, 'totalOccurrences': {'count': 1, 'startTime': 1694691865233, 'endTime': 1694691865233}}, {'id': '9bfb1f87-9bab-4c58-b0c8-b64dab72c745', 'name': 'dhcp_reachability_issue', 'flattened': True, 'enabled': True, 'entityType': 'Client', 'entityName': 'Unknown', 'entity': '88:43:E1:62:9C:06', 'groupBy': '7645933d-2893-4d7b-b905-c1db12202751', 'severity': 'HIGH', 'category': 'Onboarding', 'summary': 'This client 88:43:E1:62:9C:06 has failed to obtain an IP address from DHCP server.', 'rootCause': 'Unknown', 'timestamp': 1694691897404, 'startTime': 1694550755333, 'occurrences': 40, 'instances': 1, 'scope': 'LOCALIZED', 'priority': 'P3', 'status': {'status': 'ACTIVE', 'updatedBy': 'Unknown', 'notes': 'Unknown', 'source': 'Unknown', 'updatedAt': None, 'ignoreSource': 'Unknown', 'ignoreValue': 'Unknown', 'ignoreStartTime': None, 'ignoreEndTime': None}, 'totalOccurrences': {'count': 40, 'startTime': 1694550755333, 'endTime': 1694691897404}}, {'id': '5462b94f-7f96-436b-aa02-0db4840e9eac', 'name': 'dhcp_reachability_issue', 'flattened': True, 'enabled': True, 'entityType': 'Client', 'entityName': 'Unknown', 'entity': '88:43:E1:62:9C:04', 'groupBy': '7645933d-2893-4d7b-b905-c1db12202751', 'severity': 'HIGH', 'category': 'Onboarding', 'summary': 'This client 88:43:E1:62:9C:04 has failed to obtain an IP address from DHCP server.', 'rootCause': 'Unknown', 'timestamp': 1694691897403, 'startTime': 1694547798894, 'occurrences': 41, 'instances': 1, 'scope': 'LOCALIZED', 'priority': 'P3', 'status': {'status': 'ACTIVE', 'updatedBy': 'Unknown', 'notes': 'Unknown', 'source': 'Unknown', 'updatedAt': None, 'ignoreSource': 'Unknown', 'ignoreValue': 'Unknown', 'ignoreStartTime': None, 'ignoreEndTime': None}, 'totalOccurrences': {'count': 41, 'startTime': 1694547798894, 'endTime': 1694691897403}}] within time 35.666666666666664 in minutes

*Taas_Log:* [https://ngdevx.cisco.com/services/taas/results/294e8565-3833-458a-9f5b-7d575edc9c5f/run-results|https://ngdevx.cisco.com/services/taas/results/294e8565-3833-458a-9f5b-7d575edc9c5f/run-results]

*Branch Used:* rcdn/HulkPatch-ms/api-auto",2023-09-20T05:19:54.341+0000,"Hi [~accountid:712020:64510349-9ea9-463d-8fb0-f4b0d76b65d5], There was an Auton with the same TC just resolved and merged code today. I already tried Cherry-Pick that PR to run. Then, I didn’t get the error like you on the branch private/HulkPatch-ms/api-auto with Testbed TB4.

Please refer Auton/PR below:

[https://miggbo.atlassian.net/browse/SEEN-1992|https://miggbo.atlassian.net/browse/SEEN-1992|smart-link] 

PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6851/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/6851/overview] Hi [~accountid:712020:64510349-9ea9-463d-8fb0-f4b0d76b65d5], The branch HulkPatch just synced from branch Hulk. So you can sync between branch private/ and rcdn/. Then, recheck the TC with the latest code. Hi [~accountid:63f50bcafb3ac4003fa2c6dd] TC142 got failed in latest run after sync also.
*DNAC Release Version Used:* Hulk P1_2.1.713.70256
*Branch Used:* rcdn/HulkPatch-ms/api-auto
*Taas Log:* [https://ngdevx.cisco.com/services/taas/results/e7f2f86b-df25-400e-9e15-4883797e49c2/run-results|https://ngdevx.cisco.com/services/taas/results/e7f2f86b-df25-400e-9e15-4883797e49c2/run-results] Hi [~accountid:712020:64510349-9ea9-463d-8fb0-f4b0d76b65d5], There was an Auton: [SEEN-1162|https://miggbo.atlassian.net/browse/SEEN-1162] including Test_TC142_Add_interace_description. It also mentions behavior change and a bug for this TC. To avoid duplication code, I think we should let Anton: [SEEN-1162|https://miggbo.atlassian.net/browse/SEEN-1162] handle TC142_Add_interace_description. We can cancel this ticket.","['Auton', 'hulk']",NhanHuu Nguyen,Cancelled,SONA MOGAL
SEEN-2343,https://miggbo.atlassian.net/browse/SEEN-2343,[Auton] - SubTCs execution is not picking properly when TC number is specified with<TC_number>.<number>0 in the TC list,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/HulkPatch-ms/api-auto

*Uber ISO tested:* Hulk Patch1 - 2.1.713.70190

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Issues Faced:*

When executing script with subTCs numbering mentioned in TC Execution list, the sub TC corresponding to <TC_number>.<number>0 is not picked as it is, instead getting executed as <TC_number>.<number>0

Below refer below Example: 

*Reference log:* [Test_TC6_DNAC_Prime_Migration_DNAC_Server_Settings_NTP_DHCP_AAA_DNS_settings|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=893804&size=415934&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac.2023Sep13_07:42:32.920817.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] 

Here → ""skip_tc_list"": [1,30]  & ""exec_tc_list"": [2,6.3,6.4,6.5,6.6,6.7,6.8,6.9,6.10] in the job file.

With above TC run list when the script was executed, TC6.10 was picked as TC6.1 and hence TC6.1 got executed and TC6.10 was not executed. But we intended that TC6.1 should not be executed and TC6.10 had to be executed. This issue needs to be fixed. ",2023-09-20T13:23:23.316+0000,"By explicitly specifying the values as strings, they will be preserved in the output without any modification {{[2, '6.3', '6.4', '6.5', '6.6', '6.7', '6.8', '6.9', '6.10']}} testcase_run_data in job file is dictionary By explicitly specifying {{6.10}} as a string “6.10”, it will be preserved in the output without any modification. [~accountid:63f50bf5e8216251ae4d59cf] : Thanks for adding the fix. But we wanted another fix also where 

[2,”6.1-6.10”] should also work. Currently we are having issue with this kind of input also. This input runs TC6.1 only. 

Can you please add a fix for this as well? Try with all the TC number in string format 

for ex: {{""exec_tc_list"": [""2"",""230.1-230.10""]}}

This worked on my testbed  TCs marked for execution:

2023-09-27T16:36:59: %SERVICES-INFO: ['2', '230.1', '230.2', '230.3', '230.4', '230.5', '230.6', '230.7', '230.8', '230.9', '230.10’]  ","['Auton', 'Execution', 'Ghost', 'Hulk', 'HulkPatch', 'HulkPatch2', 'MSTB1', 'MSTB2', 'Multisite', 'Sanity']",Raji Mukkamala,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2344,https://miggbo.atlassian.net/browse/SEEN-2344,[Auton] - Script failures during Prime migration feature integration,"*Regression Profile:* Solution Regression Multisite (DR)

*Branch used:* private/HulkPatch-ms/api-auto

*Uber ISO tested:* Hulk Patch1 - 2.1.713.70190/2.1.713.70207

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Issues Faced:*

When integrating the TC related Prime Infra migration feature, we are observing some subTCs failures due to issues from script side. Below are the details:



a) [test3_add_ise_server|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=888527&size=9626&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac.2023Sep13_09:59:20.429801.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ If addition of ISE server on Prime Infra is failing, the dependent subTCs starting from   [test10_migrate_device_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=923969&size=12538&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac.2023Sep13_09:59:20.429801.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] and later should not be executed further and instead should be marked as blocked.

b) [test10_migrate_device_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=923969&size=12538&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac.2023Sep13_09:59:20.429801.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ For the Migration workflow, wrong DNAC cluster IP is being picked. Since this is DR Multisite testbed, the cluster IP has to be chosen based on which one is Active DR cluster node at that point of time. It should be 10.195.243.109 or 10.195.243.123 IPs depending upon which IP is Active DR cluster at that point of execution of the TC. Here instead IP - 10.195.243.110 is being picked which is incorrect.


Below is error snip from Prime Infra UI which shows script tried to input incorrect cluster IP:

!image-20230920-134111.png|width=1122,height=569!



→  Also the script is not complaining about GUI error corresponding usage of DNAC IP if incorrect IP used. It should print “Cisco DNAC Centre not reachable” message with failure as “Incorrect DNAC IP might have been used. Please check the DNAC reachability to Prime Infra and add the correct DNAC IP.”

c) [test11_verfify_audit_descr|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=936507&size=4709&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac.2023Sep13_09:59:20.429801.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]  & [test13_job_history_scan|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=944226&size=6061&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac.2023Sep13_09:59:20.429801.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ These subTCs are failing with variable assignment errors.

d) [test20_checking_assurance_data|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1180145&size=127851&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac.2023Sep15_00:22:19.870008.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ Observing 500 server error for neighbor topology validation check for vEWLC wireless controller device.


*Reference Feature wikis:* 

[https://wiki.cisco.com/display/EDPEIXOT/Prime+Infra+-+DNAC+Migration|https://wiki.cisco.com/display/EDPEIXOT/Prime+Infra+-+DNAC+Migration]
[https://wiki.cisco.com/display/EDPEIXOT/PDMT-4+Features|https://wiki.cisco.com/display/EDPEIXOT/PDMT-4+Features]",2023-09-20T13:58:33.286+0000,"*Update from Vinay over mailer:*


Enhancement Request for DR cluster has been added and  along with request to disable ISE check from fabric file has been addressed, please use *ise_check* as  false to disable to the ISE check. Please refer to the below PR.

 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7225/diff#configs/sanity_tb1/solution_sanityeca_SanityTB1.json|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7225/diff#configs/sanity_tb1/solution_sanityeca_SanityTB1.json]



!image-20231115-041059.png|width=383,height=124! Below commit has been added to *private/Hulk-ms/api-auto*, *private/HulkPatch-ms/api-auto* & *private/HulkPatch2-ms/api-auto*  branches  

Hi Vinay,

 

Thanks for the syncup over the Webex!

 

# [test3_add_ise_server|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=890320&size=9486&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov09_03:01:14.765596.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

* For this TC, its working as expected. As confirmed by you, *“ise_check”: false* flag only skips the ISE inclusion during migration. It does not skip ISE integration. In this case the failure is expected as we have known defect for ISE intehration issue with respect to ISE 3.3 P1.

 

# [test10_migrate_device_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=945953&size=12646&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov09_03:01:14.765596.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

* The changes committed for picking the cluster IP is not working as expected. So as agreed upon it needs changes. Please let us know once you have the changes added.

 

# [test11_verfify_audit_descr|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=958599&size=11004&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov09_03:01:14.765596.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] & [test13_job_history_scan|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=975596&size=10406&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov09_03:01:14.765596.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

* As we debugged on this issue, the failure is due to timing issue where sometimes migration page on Prime infra takes more time to load and due to this the job history click operation occurs even before the page load and fails. We have verified the same manually as well. As agreed upon please enhancement of the validation to be done by adding more sleep time to 2 minutes and iteration check for every 15 seconds so that we will avoid this kind of issues. We will report a different Automation JIRA for this issue to track this enhancement.

 

While adding the changes to code, please commit the changes to Ghost as well as Hulk production branches.

 

  Regards
Sandeep S

 

*From:* SANDEEP SHIVARAMAREDDY -X (sandshiv - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) 
*Sent:* Thursday, November 9, 2023 9:35 PM
*To:* Vinay Raj V (vinavasu) <[vinavasu@cisco.com|mailto:vinavasu@cisco.com]>
*Cc:* Karthika MM -X (muthukar - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[muthukar@cisco.com|mailto:muthukar@cisco.com]>; Madhusudhan Raghuveer (mraghuve) <[mraghuve@cisco.com|mailto:mraghuve@cisco.com]>; Shahin Laskar (shlaskar) <[shlaskar@cisco.com|mailto:shlaskar@cisco.com]>; Amardeep Kumar (amardkum) <[amardkum@cisco.com|mailto:amardkum@cisco.com]>; Santhosh Mounaswamy -X (smounasw - TERRALOGIC SOLUTIONS INC at Cisco) <[smounasw@cisco.com|mailto:smounasw@cisco.com]>; Solomon Raja Padavettan John Ganesan (sojohnga) <[sojohnga@cisco.com|mailto:sojohnga@cisco.com]>; Divakar Kumar Yadav -X (divayada - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[divayada@cisco.com|mailto:divayada@cisco.com]>; Balaji Raju -X (braju2 - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[braju2@cisco.com|mailto:braju2@cisco.com]>; Neelima Doddipalli -X (nedoddip - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[nedoddip@cisco.com|mailto:nedoddip@cisco.com]>; Jagadesh Kumar Enapanuri -X (jaenapan - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[jaenapan@cisco.com|mailto:jaenapan@cisco.com]>; Manjushree Saligrama -X (masaligr - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[masaligr@cisco.com|mailto:masaligr@cisco.com]>
*Subject:* RE: PRIME-Discussion Summary

 

Hi Vinay,

 

Sure. Will be sharing the details of the testbed over Webex with you now. The testbed will be moved out to Ghost Patch3 by tomorrow 11 AM.

 

  Regards
Sandeep S

 

*From:* Vinay Raj V (vinavasu) <[vinavasu@cisco.com|mailto:vinavasu@cisco.com]> 
*Sent:* Thursday, November 9, 2023 8:34 PM
*To:* SANDEEP SHIVARAMAREDDY -X (sandshiv - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[sandshiv@cisco.com|mailto:sandshiv@cisco.com]>
*Cc:* Karthika MM -X (muthukar - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[muthukar@cisco.com|mailto:muthukar@cisco.com]>; Madhusudhan Raghuveer (mraghuve) <[mraghuve@cisco.com|mailto:mraghuve@cisco.com]>; Shahin Laskar (shlaskar) <[shlaskar@cisco.com|mailto:shlaskar@cisco.com]>; Amardeep Kumar (amardkum) <[amardkum@cisco.com|mailto:amardkum@cisco.com]>; Santhosh Mounaswamy -X (smounasw - TERRALOGIC SOLUTIONS INC at Cisco) <[smounasw@cisco.com|mailto:smounasw@cisco.com]>; Solomon Raja Padavettan John Ganesan (sojohnga) <[sojohnga@cisco.com|mailto:sojohnga@cisco.com]>; Divakar Kumar Yadav -X (divayada - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[divayada@cisco.com|mailto:divayada@cisco.com]>; Balaji Raju -X (braju2 - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[braju2@cisco.com|mailto:braju2@cisco.com]>; Neelima Doddipalli -X (nedoddip - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[nedoddip@cisco.com|mailto:nedoddip@cisco.com]>; Jagadesh Kumar Enapanuri -X (jaenapan - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[jaenapan@cisco.com|mailto:jaenapan@cisco.com]>; Manjushree Saligrama -X (masaligr - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[masaligr@cisco.com|mailto:masaligr@cisco.com]>
*Subject:* Re: PRIME-Discussion Summary

 

Hello Sandeep,

 

Please share the TB for more triage.

 

Regards,

Vinay

 

*From:* SANDEEP SHIVARAMAREDDY -X (sandshiv - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[sandshiv@cisco.com|mailto:sandshiv@cisco.com]>
*Date:* Thursday, 9 November 2023 at 5:25 PM
*To:* Vinay Raj V (vinavasu) <[vinavasu@cisco.com|mailto:vinavasu@cisco.com]>
*Cc:* Karthika MM -X (muthukar - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[muthukar@cisco.com|mailto:muthukar@cisco.com]>, Madhusudhan Raghuveer (mraghuve) <[mraghuve@cisco.com|mailto:mraghuve@cisco.com]>, Shahin Laskar (shlaskar) <[shlaskar@cisco.com|mailto:shlaskar@cisco.com]>, Amardeep Kumar (amardkum) <[amardkum@cisco.com|mailto:amardkum@cisco.com]>, Santhosh Mounaswamy -X (smounasw - TERRALOGIC SOLUTIONS INC at Cisco) <[smounasw@cisco.com|mailto:smounasw@cisco.com]>, Solomon Raja Padavettan John Ganesan (sojohnga) <[sojohnga@cisco.com|mailto:sojohnga@cisco.com]>, Divakar Kumar Yadav -X (divayada - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[divayada@cisco.com|mailto:divayada@cisco.com]>, Balaji Raju -X (braju2 - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[braju2@cisco.com|mailto:braju2@cisco.com]>, Neelima Doddipalli -X (nedoddip - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[nedoddip@cisco.com|mailto:nedoddip@cisco.com]>, Jagadesh Kumar Enapanuri -X (jaenapan - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[jaenapan@cisco.com|mailto:jaenapan@cisco.com]>, Manjushree Saligrama -X (masaligr - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[masaligr@cisco.com|mailto:masaligr@cisco.com]>
*Subject:* RE: PRIME-Discussion Summary

Hi Vinay,

 

As our Multisite testbed (DR+MDNAC )was occupied with back to back Customer Hotfix, SMU & .bin Polaris image testing, we had not got a chance to test with Automation code changes that you had added so far.

 

I have currently tested with the below changes using the Hulk Patch2 code base which has all the changes implemented by you.

 

Code base used - *private/HulkPatch2-sand114/api-auto* (pulled from *private/HulkPatch2/api-auto* production branch and added some testbed specific changes)

 

*Reference log -* [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov09_03:01:14.765596.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov09_03:01:14.765596.zip&atstype=ATS]

 

Below are the list of issues observed around the area for which code changes were implemented:

 

# [test3_add_ise_server|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=890320&size=9486&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov09_03:01:14.765596.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

* Failed during addition of ISE server. We have included *“ise_check”: false* flag under testbed fabric json file. Still the ISE addition is attempted. Ideally it should have not been picked.

 

# [test10_migrate_device_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=945953&size=12646&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov09_03:01:14.765596.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

* Again the cluster IP is not getting picked properly for the migration workflow. 10.195.243.123 is the current Active cluster IP. But still 10.195.243.110 is getting picked for migration.

 

# [test11_verfify_audit_descr|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=958599&size=11004&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov09_03:01:14.765596.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] & [test13_job_history_scan|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=975596&size=10406&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov09_03:01:14.765596.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

* Xpath issues are observed for these TCs. Changes has been Merged to Hulk P1 and P2.. Please verify ","['Auton', 'Frey', 'Ghost', 'Hulk', 'HulkPatch', 'Integration', 'MSTB1', 'Multisite']",Vinay Raj V ,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2347,https://miggbo.atlassian.net/browse/SEEN-2347,[GHOST] N_plus_one_wlc cases is failed due to exsiting ssid are in dnac,"DNAC Release_Version Tested: Hulk RC3 Uber ISO - 2.1.710.70479, Non-FIPS

Device Image Used: 17.12.1

Testbed: AWS-Multisite

Branch Used: private/Hulk-ms/api_auto

Script Name: solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

Solution Input File : dnac-auto/configs/config_48hr_test/solution_test_input.json

Testcases Impacted:  Test_TC243_N_plus_one_wlc

Failed Trade Log:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_cl_ms_mdnac_mstb3.2023Sep21_02:55:54.062488.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_cl_ms_mdnac_mstb3.2023Sep21_02:55:54.062488.zip&atstype=ATS]

Failure Analysis:
Already custom_rf_ssid_profile is there, script is checked its already there so  its got errored other testcases are blocked.",2023-09-21T11:41:35.443+0000,"Addressed [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a0489efb3f50432a702440da7610403a70ab094a|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/a0489efb3f50432a702440da7610403a70ab094a] This issue was not observed, hitting another issue i have raised another Jira SEEN-2370","['AWS_MSTB', 'Auton', 'Ghost']",Raji Mukkamala,Resolved,Balaji Raju
SEEN-2348,https://miggbo.atlassian.net/browse/SEEN-2348,[Auton]Hulk: Test_TC1_DNAC_Wireless_SSID_creation_open_enterprise  /   test1_configure_wireless_RF_profile  ,"*Reporter Analysis:* 

 there are new api validations for user defined rf profiles, but those same validations dont seem to apply to the system defined rf profiles, so thru API we can modify the system ones in ways that the user defined ones cant be modified. And considering system defined ones probably shouldnt be able to be modified at all (you cannot modify them thru UI), being able to modify them and bypass validations seems like a defect.



*Description*: 

{noformat}585: 
 Error Code: 400 for{noformat}

{noformat}586: 
 URL:https://10.195.227.92/api/v1/commonsetting/wlan/-1 Data:{'timeout': 60, 'data': b'[{""instanceType"": ""rfprofile"", ""instanceUuid"": ""b4e2ebbc-3e39-4aee-adae-bb5517767950"", ""namespace"": ""wlan"", ""type"": ""rfprofile.setting"", ""key"": ""rfprofile.info"", ""version"": 1, ""value"": [{""rfProfileName"": ""LOW"", ""parentProfileA"": ""LOW"", ""parentProfileB"": ""LOW"", ""parentProfileC"": ""LOW"", ""isARadioType"": true, ""isBRadioType"": true, ""isCRadioType"": true, ""channelWidth"": ""best"", ""aRadioChannels"": ""36,40,44,48,52,56,60,64,149,153,157,161"", ""bRadioChannels"": ""1,6,11"", ""cRadioChannels"": ""5,21,37,53,69,85,101,117,133,149,165,181,197,213,229"", ""dataRatesA"": ""6,9,12,18,24,36,48,54"", ""dataRatesB"": ""1,2,5.5,6,9,11,12,18,24,36,48,54"", ""dataRatesC"": ""6,9,12,18,24,36,48,54"", ""mandatoryDataRatesA"": ""6,12,24"", ""mandatoryDataRatesB"": ""1,2,5.5,11""{noformat}

*Branch Name:* private/HulkPatch2-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:

**Fail Log:**

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-base_wireless_ssid_profile_guest.py-53-designWireless&begin=4741&size=238021&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep21_09:24:22.271427.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-base_wireless_ssid_profile_guest.py-53-designWireless&begin=4741&size=238021&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep21_09:24:22.271427.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-22T03:06:23.606+0000,"[ENG-SDN / dnac-auto / e314285fd36 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e314285fd36a50904416a43cc9ff80b015ecd32c] DE has said this is not an issue for system defined rf profiles, on script side have fixed for our custom rf profiles","['Auton', 'Hulk', 'Sanity']",Andrew Chen,Resolved,Anusha John
SEEN-2349,https://miggbo.atlassian.net/browse/SEEN-2349,address redundancy of Test-cases in Optimized Sanity executions,"# Fix non/lansanity_usecases_maps.yaml to avoid redundancy of Test-case execution.
## remove “{{siteLevelNWSettings}}"" from Group# 6 as the test-case mentioned inside it, is already covered in Group# 4 - {{“designSiteHeirarchySitesBuildingsAndFloors}}"" = 45 seconds
## remove “{{databaseMapping}}"" from Group# 26 as it is already included in Group# 12. = 1 minute 17 seconds
## replace {{“SDAfusionPatching""}} with {{“MSDnacConfigureBGPBorder""}} in Group# 12 as it has more use-cases and removed {{“MSDnacConfigureBGPBorder""}} from Group# 13. = 2 minutes 13 seconds

  2.  Under {{sensorOnboardingClaiming/sensor_pnp_claim_and_provisioning.py}}, rename “{{Test_TC1_DNAC_EXT_NODE_interface_config_verifications}}"" as
       ""{{Test_TC1_DNAC_SENSOR_onboarding_verifications}}"" to match the correct purpose.
       Also, correction on the use-case numbering is required.

  3. Add {{Test_TC3_DNAC_select_credentials_at_site_level}} from {{siteLevelNWSettings}} to {{designSiteHeirarchySitesBuildingsAndFloors/site_heirarchy_design.py}} as 
      {{siteLevelNWSettings}} is no more in use.

  4. Rename {{Test_TC1_DNAC_Device_Provisioning}} under {{provisionReprovisionConfigValidation/reprovisioning_provision_conifg_validation.py}} as {{Test_TC1_DNAC_Device_ReProvisioning}}
      to avoid duplicate name and have better explanation.

  5. Remove {{Test_TC3_aaa_per_ssid}} from sanityusecases/cmxConfigsAndValidations/cmx_config_push_and_device_validations.py as same is getting covered under
      sanityusecases/aaaPerSSID/aaa_per_ssid.py. = 35 seconds",2023-09-22T05:20:16.211+0000,"PR for the required changes have been raised for *private/HulkPatch-ms/api-auto* branch:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7120/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7120/overview] Raised another PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7174/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7174/overview] Raised PRs have been approved and merged to *private/HulkPatch-ms/api-auto* and cherry-picked to *private/Hulk-ms/api-auto* branch.
Marking this Auton as “Done”.",['Auton'],Amardeep Kumar,Closed,Amardeep Kumar
SEEN-2350,https://miggbo.atlassian.net/browse/SEEN-2350,[Auton]:Hulk:Test_TC226_import_static_route_to_database_on_border/test3_configure_route_and_instance_on_border,"*Reporter Analysis:*

we observed in TB3, the vlan identifier print in next line, due to that regex fails to match and fails to retrieve the identifer value ,
it works well in other onprem and AWS testbed where issue seen only this setup.

*Description*:

{noformat}57671: 
 Response received against GET Operation: [{'deviceUuid': '59e4c1fa-a044-49c6-b185-854d50bf5288', 'commandResponses': {'SUCCESS': {'show ip vrf WiredVNFBLayer2': 'show ip vrf WiredVNFBLayer2\n  Name                             Default RD            Interfaces\n  WiredVNFBLayer2                  1:4105                LI0.4105\n                                                         Vl1021\n                                                         Vl1022\n                                                         Vl1023\n                                                         Vl2046\n                                                         Vl3304\n                                                         Tu3\n                                                         Lo4105\nTB3-DM-eCA-BORDER#'}, 'FAILURE': {}, 'BLACKLISTED': {}}}]{noformat}

{noformat}157672: 
 Output of 'show ip vrf WiredVNFBLayer2' executed on TB3-DM-eCA-BORDER:{noformat}

{noformat}157673: 
 show ip vrf WiredVNFBLayer2{noformat}

{noformat}157674: 
   Name                             Default RD            Interfaces{noformat}

{noformat}157675: 
   WiredVNFBLayer2                  1:4105                LI0.4105{noformat}

{noformat}157676: 
                                                          Vl1021{noformat}

{noformat}157677: 
                                                          Vl1022{noformat}

{noformat}157678: 
                                                          Vl1023{noformat}

{noformat}157679: 
                                                          Vl2046{noformat}

{noformat}157680: 
                                                          Vl3304{noformat}

{noformat}157681: 
                                                          Tu3{noformat}

{noformat}157682: 
                                                          Lo4105{noformat}

{noformat}157683: 
 TB3-DM-eCA-BORDER#{noformat}

{noformat}157684: 
 Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:02.213721{noformat}

{noformat}157685: 
 find out the instance id , vlan on the WiredVNFBLayer2{noformat}

{noformat}157686: 
 Result: failed to fetch the instance_id & vlan for the vrf WiredVNFBLayer2{noformat}

{noformat}157687: 
 Library group ""device_config_validation"" method ""verify_vrf_instance_id_config_on_border"" returned in 0:00:06.481469{noformat}

{noformat}157688: 
 Test returned in 0:00:06.482781{noformat}

{noformat}157689: 
 Failed reason: Result: Failed to retrieve the instance id for the associated vrf{noformat}

{noformat}157690: 
 The result of section test3_configure_route_and_instance_on_border is => FAILED{noformat}

*Branch Name:* private/HulkPatch-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:

**Fail Log:**

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=41890568&size=44177&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_12:22:16.322366.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=41890568&size=44177&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep06_12:22:16.322366.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*Pass log from TB11:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3536414&size=994402&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep05_12:27:46.792675.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3536414&size=994402&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep05_12:27:46.792675.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-22T17:31:33.782+0000,"Identified the issue as show ip vrf <vrf> output print order for vlan identifier changed. 
We can retrieve the vlan identifer from the show run | section address-family ipvr vrf <vrf> cli and check the update-source vlan number . 

I will modify and update the auton  optimized code log  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-09/optimized_job.2023Sep22_13:33:46.027026.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-09/optimized_job.2023Sep22_13:33:46.027026.zip&atstype=ATS]
Regular code log  [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fvkuttykr-sjc%2Fpyats_dnac_kk%2Fusers%2Fvkuttykr%2Farchive%2F23-09%2Fsanity_TB23_standalone.2023Sep22_11:38:52.571625.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fvkuttykr-sjc%2Fpyats_dnac_kk%2Fusers%2Fvkuttykr%2Farchive%2F23-09%2Fsanity_TB23_standalone.2023Sep22_11:38:52.571625.zip&atstype=ATS] PR : [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7130/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7130/overview]","['Auton', 'Hulk', 'Sanity']",Vinoth Kumar Kutty Krishnamoorthy,Resolved,Anusha John
SEEN-2351,https://miggbo.atlassian.net/browse/SEEN-2351,Errored out while running test3_import_Ekahau_file,"*Uber ISO Version tested :* 3.713.75122- Hulk Patch ESXI

*Branch*: private/HulkPatch-ms/api-auto

*Script Name :* map.py

*Testbed :* TB4

*Testcases Impacted :*  [Test_TC2_DNAC_maps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-map.py-176-map&begin=4036&size=23014&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep20_08:15:48.447966.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

During Hulk Patch ESXI Optimized testing : test3_import_Ekahau_file is errored out with below issue



[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep20_08:15:48.447966.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep20_08:15:48.447966.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]



Snip:

{noformat}118:  Executing testcase Test_TC2_DNAC_maps test 2.3 ""test3_import_Ekahau_file"".{noformat}

119:  Import Ekahau file
120:  Traceback (most recent call last):
121:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/OptimizedSanity-for-ESXi/services/commonlibs/test_wrapper.py"", line 301, in wrapper
122:      result = testfunc(func_self, **kwargs)
123:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/OptimizedSanity-for-ESXi/testcases/sanityusecases/map/map.py"", line 134, in test3_import_Ekahau_file
124:      context_id = dnac_handle.ekahau_archives_import_async(file_path=file_path, site=site)
125:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/OptimizedSanity-for-ESXi/services/dnaserv/lib/api_groups/nw_heirarchy_sites/group.py"", line 607, in ekahau_archives_import_async
126:      lat_long_msg = error_messages[0]
127:  IndexError: string index out of range
128:  Test returned in 0:00:00.003445
129:  Errored reason: string index out of range
130:
131:  Exception:
132:  Traceback (most recent call last):
133:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/OptimizedSanity-for-ESXi/services/commonlibs/test_wrapper.py"", line 301, in wrapper
134:      result = testfunc(func_self, **kwargs)
135:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/OptimizedSanity-for-ESXi/testcases/sanityusecases/map/map.py"", line 134, in test3_import_Ekahau_file
136:      context_id = dnac_handle.ekahau_archives_import_async(file_path=file_path, site=site)
137:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/OptimizedSanity-for-ESXi/services/dnaserv/lib/api_groups/nw_heirarchy_sites/group.py"", line 607, in ekahau_archives_import_async
138:      lat_long_msg = error_messages[0]
139:  IndexError: string index out of range
140:  The result of section test3_import_Ekahau_file is => ERRORED",2023-09-22T20:34:49.615+0000,[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f29a8a83da5efd121fb29fc81ad4df070290e54c|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f29a8a83da5efd121fb29fc81ad4df070290e54c],"['Auton', 'ESxi', 'HulkPatch']",Raji Mukkamala,Resolved,Manoj Menakuri
SEEN-2352,https://miggbo.atlassian.net/browse/SEEN-2352,"[Auton] Test_TC1_wired_client_connect_behind_ip_phone_assurance_data
","*Uber ISO Version tested :* 3.713.75122 - Hulk Patch ESXI

*Branch*: private/HulkPatch-ms/api-auto

*Script Name :* WiredClientBehindIpphone.py

*Testbed :* TB4

*Testcases Impacted :*  

|[test4_scan_all_ipphone_in_testbed|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_wired_client_behind_ip_phone_assurance.py-291-WiredClientBehindIpphone&begin=2286807&size=7671&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep20_08:15:48.447966.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|

Unable to fetch IP-Phone information even after declared node role as EDGE.

Failed log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_wired_client_behind_ip_phone_assurance.py-291-WiredClientBehindIpphone&begin=2286807&size=7671&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep20_08:15:48.447966.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_wired_client_behind_ip_phone_assurance.py-291-WiredClientBehindIpphone&begin=2286807&size=7671&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep20_08:15:48.447966.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-22T22:29:48.063+0000,"Raised PR : [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7317/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7317/overview]

Requested [~accountid:712020:a269cb40-4972-4cdd-b4be-1a9a0bc46160]  to test and share the result as this issue specific to their setup.

Issue : in esxi vm testbed there is no separate edge device as they have a FIAB where in script checking the data for the ip phone from the self.edges category, added condition with self.borders to fetch the ip phone details from the FIAB. Testcase passed with local Branch & please merge the code to private/HulkPatch-ms/api-auto

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct12_14:28:34.648597.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct12_14:28:34.648597.zip&atstype=ATS]","['Auton', 'ESxi', 'Hulk']",Vinoth Kumar Kutty Krishnamoorthy,Resolved,Manoj Menakuri
SEEN-2353,https://miggbo.atlassian.net/browse/SEEN-2353,use common testcase name for all TCs that are calling assign_lb_ip_from_dnac() method,"Use common testcase name for all TCs that are calling assign_lb_ip_from_dnac() method.

# Test_TC1_loopback_interfaces_from_dnac
# Test_TC1_DNAC_assign_lb_ip_from_dnac
# Test_TC1_read_loopback_interfaces_from_dnac
# Test_TC0_loopback_interfaces_from_dnac

Same test-case with different name should be fixed.",2023-09-22T22:55:10.968+0000,"PR raised for required change: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7175/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7175/overview] PR approved and merged to private/HulkPatch-ms/api-auto branch and cherry-picked to private/Hulk-ms/api-auto branch.

Marking this Auton as “Done”.",['Auton'],Amardeep Kumar,Closed,Amardeep Kumar
SEEN-2354,https://miggbo.atlassian.net/browse/SEEN-2354,[Auton]Need to increase sleep time while verifying ip route for vrf VN1 & VN2,"*Uber ISO Version tested :* 3.713.75122 - Hulk Patch ESXI

*Branch*: private/HulkPatch-ms/api-auto

*Script Name :* defaultRouteVerification.py

*Testbed :* TB4

*Testcases Impacted :*  [test2_verify_default_route_issue_on_device|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-default_route_verification.py-341-defaultRouteVerification&begin=132985&size=197573&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep20_08:15:48.447966.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS], [test3_validate_default_route_issue_on_assurance|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-default_route_verification.py-341-defaultRouteVerification&begin=330558&size=368025&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep20_08:15:48.447966.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

We are able see route info in route summary through manually but it is failing through script. Need to increase sleep time



Log:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep20_08:15:48.447966.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep20_08:15:48.447966.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]

+*Manual output:*+
TB4-DM-eCA-BORDER#show ip route vrf VN1

Routing Table: VN1
Codes: L - local, C - connected, S - static, R - RIP, M - mobile, B - BGP
       D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area
       N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2
       E1 - OSPF external type 1, E2 - OSPF external type 2, m - OMP
       n - NAT, Ni - NAT inside, No - NAT outside, Nd - NAT DIA
       i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2
       ia - IS-IS inter area, * - candidate default, U - per-user static route
       H - NHRP, G - NHRP registered, g - NHRP registration summary
       o - ODR, P - periodic downloaded static route, l - LISP
       a - application route
       + - replicated route, % - next hop override, p - overrides from PfR
       & - replicated local route overrides by connected

Gateway of last resort is 204.1.16.42 to network 0.0.0.0

*B*    0.0.0.0/0 [20/0] via 204.1.16.42, 00:00:00*
      40.0.0.0/8 is variably subnetted, 4 subnets, 2 masks
C        40.40.40.0/24 is directly connected, Vlan1042
L        40.40.40.1/32 is directly connected, Vlan1042
C        40.40.41.0/24 is directly connected, Vlan1043
L        40.40.41.1/32 is directly connected, Vlan1043
      85.0.0.0/16 is subnetted, 1 subnets
B        85.1.0.0 [20/0] via 204.1.16.42, 1d01h
      204.1.16.0/24 is variably subnetted, 2 subnets, 2 masks
C        204.1.16.40/30 is directly connected, Vlan3310
L        204.1.16.41/32 is directly connected, Vlan3310
C     204.1.176.0/22 is directly connected, Vlan1087
      204.1.176.0/32 is subnetted, 2 subnets
L        204.1.176.1 is directly connected, Vlan1087
l        204.1.176.151 [10/1] via 204.1.176.151, 19:21:28, Vlan1087
      204.192.3.0/32 is subnetted, 1 subnets
B        204.192.3.40 [20/0] via 204.1.16.42, 1d01h
B     204.192.4.0/24 [20/0] via 204.1.16.42, 1d01h
TB4-DM-eCA-BORDER#show ip route vrf VN2

Routing Table: VN2
Codes: L - local, C - connected, S - static, R - RIP, M - mobile, B - BGP
       D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area
       N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2
       E1 - OSPF external type 1, E2 - OSPF external type 2, m - OMP
       n - NAT, Ni - NAT inside, No - NAT outside, Nd - NAT DIA
       i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2
       ia - IS-IS inter area, * - candidate default, U - per-user static route
       H - NHRP, G - NHRP registered, g - NHRP registration summary
       o - ODR, P - periodic downloaded static route, l - LISP
       a - application route
       + - replicated route, % - next hop override, p - overrides from PfR
       & - replicated local route overrides by connected

Gateway of last resort is 204.1.16.6 to network 0.0.0.0

*B*    0.0.0.0/0 [20/0] via 204.1.16.6, 00:00:45*
      40.0.0.0/8 is variably subnetted, 4 subnets, 2 masks
C        40.40.42.0/24 is directly connected, Vlan1021
L        40.40.42.1/32 is directly connected, Vlan1021
C        40.40.43.0/24 is directly connected, Vlan1022
L        40.40.43.1/32 is directly connected, Vlan1022
      85.0.0.0/16 is subnetted, 1 subnets
B        85.1.0.0 [20/0] via 204.1.16.6, 1d01h
      204.1.16.0/24 is variably subnetted, 2 subnets, 2 masks
C        204.1.16.4/30 is directly connected, Vlan3301
L        204.1.16.5/32 is directly connected, Vlan3301
C     204.1.176.0/22 is directly connected, Vlan1088
      204.1.176.0/32 is subnetted, 2 subnets
L        204.1.176.1 is directly connected, Vlan1088
l        204.1.176.212 [10/1] via 204.1.176.212, 09:07:27, Vlan1088
      204.192.3.0/32 is subnetted, 1 subnets
B        204.192.3.40 [20/0] via 204.1.16.6, 1d01h
B     204.192.4.0/24 [20/0] via 204.1.16.6, 1d01h
TB4-DM-eCA-BORDER#show ipv6 route vrf VN2
IPv6 Routing Table - VN2 - 7 entries
Codes: C - Connected, L - Local, S - Static, U - Per-user Static route
       B - BGP, R - RIP, H - NHRP, HG - NHRP registered
       Hg - NHRP registration summary, HE - NHRP External, I1 - ISIS L1
       I2 - ISIS L2, IA - ISIS interarea, IS - ISIS summary, D - EIGRP
       EX - EIGRP external, ND - ND Default, NDp - ND Prefix, DCE - Destination
       NDr - Redirect, RL - RPL, O - OSPF Intra, OI - OSPF Inter
       OE1 - OSPF ext 1, OE2 - OSPF ext 2, ON1 - OSPF NSSA ext 1
       ON2 - OSPF NSSA ext 2, la - LISP alt, lr - LISP site-registrations
       ld - LISP dyn-eid, lA - LISP away, le - LISP extranet-policy
       lp - LISP publications, ls - LISP destinations-summary
*B   ::/0 [20/0], tag 200*
     via FE80::4EBC:48FF:FEE7:3975, Vlan3301
C   2004:1:16::1:0:4/126 [0/0]
     via Vlan3301, directly connected
L   2004:1:16::1:0:5/128 [0/0]
     via Vlan3301, receive
C   2004:1:176::1:0/112 [0/0]
     via Vlan1088, directly connected
L   2004:1:176::1:1/128 [0/0]
     via Vlan1088, receive
B   2004:192:3::40/128 [20/0], tag 200
     via FE80::4EBC:48FF:FEE7:3975, Vlan3301
L   FF00::/8 [0/0]
     via Null0, receive
TB4-DM-eCA-BORDER#show ipv6 route vrf VN1
IPv6 Routing Table - VN1 - 9 entries
Codes: C - Connected, L - Local, S - Static, U - Per-user Static route
       B - BGP, R - RIP, H - NHRP, HG - NHRP registered
       Hg - NHRP registration summary, HE - NHRP External, I1 - ISIS L1
       I2 - ISIS L2, IA - ISIS interarea, IS - ISIS summary, D - EIGRP
       EX - EIGRP external, ND - ND Default, NDp - ND Prefix, DCE - Destination
       NDr - Redirect, RL - RPL, O - OSPF Intra, OI - OSPF Inter
       OE1 - OSPF ext 1, OE2 - OSPF ext 2, ON1 - OSPF NSSA ext 1
       ON2 - OSPF NSSA ext 2, la - LISP alt, lr - LISP site-registrations
       ld - LISP dyn-eid, lA - LISP away, le - LISP extranet-policy
       lp - LISP publications, ls - LISP destinations-summary
*B   ::/0 [20/0], tag 200*
     via FE80::4EBC:48FF:FEE7:3959, Vlan3310
C   2004:1:16::1:0:28/126 [0/0]
     via Vlan3310, directly connected
L   2004:1:16::1:0:29/128 [0/0]
     via Vlan3310, receive
C   2004:1:176::1:0/112 [0/0]
     via Vlan1087, directly connected
L   2004:1:176::1:1/128 [0/0]
     via Vlan1087, receive
B   2004:192:3::40/128 [20/0], tag 200
     via FE80::4EBC:48FF:FEE7:3959, Vlan3310
C   2040:40:40::1:0:0/96 [0/0]
     via Vlan1042, directly connected
L   2040:40:40::1:0:1/128 [0/0]
     via Vlan1042, receive
L   FF00::/8 [0/0]
     via Null0, receive
TB4-DM-eCA-BORDER#",2023-09-22T23:16:18.429+0000,"PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7562/diff#services/dnaserv/lib/api_groups/assurance/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7562/diff#services/dnaserv/lib/api_groups/assurance/group.py] Latest build Hulk patch1 RC2 #3.713.75176 build got passed:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-default_route_verification.py-341-defaultRouteVerification&begin=430323&size=31659&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct23_09:31:12.123183.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-default_route_verification.py-341-defaultRouteVerification&begin=430323&size=31659&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct23_09:31:12.123183.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'ESxi', 'Hulk']",Moe Saeed,Closed,Manoj Menakuri
SEEN-2355,https://miggbo.atlassian.net/browse/SEEN-2355,[Auton]Script is checking AP which is not available in yaml,"*Uber ISO Version tested :* 3.713.75122 - Hulk Patch ESXI

*Branch*: private/HulkPatch-ms/api-auto

*Script Name :* FEWapReachabilityIssue.py

*Testbed :* TB4

*Problem:*

I haven’t add below AP in yaml file but script is looking for that AP, not getting expected information and script is failing for the same.



{{AP3C41.0EFE.219C}}



203:  FAILED: Blank value in RxUtil:-- for device AP3C41.0EFE.219C
204:  RxUtil:--
205:  FAILED: Blank value in TxUtil:-- for device AP3C41.0EFE.219C
206:  TxUtil:--
207:  Radio client count:--
208:  Radio Util:--
209:  Radio Interference:--
210:  Radio Noise:--
211:  Radio Air Quality:--
212:  Total Mgmt Frames:--
213:  Total Dtal Frames:--
214:  Tx Error Frames:--
215:  Rx Error Frames:--
216:  Tx power:--
217:  Channels:--
218:  FAILED: Blank value in RxUtil:-- for device AP3C41.0EFE.219C
219:  RxUtil:--
220:  FAILED: Blank value in TxUtil:-- for device AP3C41.0EFE.219C
221:  TxUtil:--
222:  Radio client count:--
223:  Radio Util:--
224:  Radio Interference:--
225:  Radio Noise:--
226:  Radio Air Quality:--
227:  Total Mgmt Frames:--
228:  Total Dtal Frames:--
229:  Tx Error Frames:--
230:  Rx Error Frames:--
231:  Tx power:--
232:  Channels:--
233:  Device completed for:AP3C41.0EFE.219C
234:  Following device will be verified:AP502f.a857.c9d4
235:  Using the URL /assurance/v1/time



Log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_reachability_issue.py-161-FEWapReachabilityIssue&begin=134222&size=106185&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep20_08:15:48.447966.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_reachability_issue.py-161-FEWapReachabilityIssue&begin=134222&size=106185&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep20_08:15:48.447966.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-23T04:07:34.059+0000,"[~accountid:712020:a269cb40-4972-4cdd-b4be-1a9a0bc46160] The below information are required when you file an autons. Please provide.

* Your debug/Analysis for the issue.
* Was the testcase passed before in this testbed? When? Please provide Passed log.
* Was the testcase passed in different testbeds? Raised PRs:

* HulkPatch1: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7309/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7309/overview]
* HulkPatch2: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7310/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7310/overview] Latest build Hulk patch1 RC2 #3.713.75176 build got passed:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_reachability_issue.py-161-FEWapReachabilityIssue&begin=562767&size=80214&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct23_00:17:06.669159.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_reachability_issue.py-161-FEWapReachabilityIssue&begin=562767&size=80214&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct23_00:17:06.669159.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'ESxi', 'Hulk']",ThangQuoc Tran,Closed,Manoj Menakuri
SEEN-2358,https://miggbo.atlassian.net/browse/SEEN-2358,[Auton]:HulkP1: Task-ssid_edits_special_char.py-1713-ssidEditSpecialChar  /   Test_TC5_Verify_special_char_SSID  /   test3_reassign_site_nw_profile ,"{quote}*Reporter Analysis:*

Test3_reassign_site_nw_profile, tc failed  with the below error :
{{/wlan-cfg-data/wlan-cfg-entries/wlan-cfg-entry[profile-name='CiscoSensorProvisioning']/wlan-id is not configured"", 'triggeredJobId': 'fbfe84a9-e086-4a06-87b1-c7faf139c100'}}} 
Due to this issue, eWLC provisioning has failed, and the sanity execution is blocked. Could you please check the priority?

*Hulk P1 -2.1.713.70255*
*Failed Log:*
[test3_reassign_site_nw_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_edits_special_char.py-1713-ssidEditSpecialChar&begin=1768722&size=705226&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep23_06:04:20.686044.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Hulk P1 -2.1.713.70190*
*Pass Log  :*  

[test3_reassign_site_nw_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_edits_special_char.py-1713-ssidEditSpecialChar&begin=1795205&size=399281&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep07_08:30:17.626506.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]


*Found on:*
Uber ISO : Hulk P1 -2.1.713.70255
Polaris version: 17.12.1 (cco)

*Script file/Usecase :* lansanity_usecases_maps.yaml

*Source Team:  Sanity*

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] 
{quote}

!image-20230923-171630.png|width=1887,height=364!

!image-20230923-172015.png|width=1887,height=714!",2023-09-23T17:15:27.347+0000,"*Hulk P1 -2.1.713.70263*

+*optimized  code*+ 
Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-09/env_optimized_auto_job.2023Sep27_19:24:38.875555.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-09/env_optimized_auto_job.2023Sep27_19:24:38.875555.zip&atstype=ATS]

rerun Pass log from main script  : 
[test3_reassign_site_nw_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=874190&size=372708&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep27_23:31:36.981017.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

!image-20230928-071538.png|width=1395,height=554! 
Same issue  observed  reg , script  


*Script file/Usecase :* usecasemaps\nonlansanitysuite

Faile  log:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep29_23:07:29.907461.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep29_23:07:29.907461.zip&atstype=ATS] Hi [~accountid:63f50bcece6f37e5ed93c87e]  ,
 as  per  Naveen Engg .note, 
Payload submitted to SPF provisioning API  by automation script has missing owning entity ID.

Please see attachments 
1.ProvisionPayloadFromGUI.rtf
         Payload submitted by GUI has owning entity id populated.

# ProvisionPayloadByAutomationScript

Payload submitted by automation script has owning entity id missing

*Defect:* [https://cdetsng.cisco.com/webui/#view=CSCwh76870|https://cdetsng.cisco.com/webui/#view=CSCwh76870]
 *Webex Space :*
webexteams://im?space=f711bc20-6367-11ee-9637-45fc7156c0f1

Could  you please  check  Same issue observed on EXSI VM Sanity as well using optimized script.

*Used Optimized Script.*

*OVA:*   3.713.75159

*Branch:  private/HulkPatch-ms/api-auto*

*Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_edits_special_char.py-171-ssidEditSpecialChar&begin=402622&size=794781&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct13_00:13:24.945407.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_edits_special_char.py-171-ssidEditSpecialChar&begin=402622&size=794781&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct13_00:13:24.945407.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Same issue is observed on ESXI-VM MSTB2 regression testbed.

Script : solution_test_3sites_sjc_nyc_sf.py

Branch : private/HulkPatch-ms/api-auto
Ova#3.713.75159 (FIPS_Enabled)
Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=83372911&size=68840&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_three_sites_FIPS.2023Oct18_08:25:39.041752.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=83372911&size=68840&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_three_sites_FIPS.2023Oct18_08:25:39.041752.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] Hi [~accountid:63f50bcece6f37e5ed93c87e] ,
still we are still seeing issues on Hulk p2 AWS Sanity runs:
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_edits_special_char.py-1714-ssidEditSpecialChar&begin=1456372&size=833191&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec06_20:10:20.181595.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_edits_special_char.py-1714-ssidEditSpecialChar&begin=1456372&size=833191&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec06_20:10:20.181595.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



Due to this so may testcases are getting affected including feature testcases , please look into the issue with high priority","['AWS_Sanity', 'Auton', 'Blocked', 'ESXi', 'Execution', 'HulkPatch1', 'HulkPatch2', 'MSTB2', 'Optimized', 'Sanity']",Andrew Chen,Backlog,Omkar Sharad Wagh
SEEN-2360,https://miggbo.atlassian.net/browse/SEEN-2360,[Auton]:Hulk P1: Test_TC103_DNAC_External_Authentication / test2_enable_tacacs_attributes,"*Reporter Analysis:*

In the UI , tacacs is active but log we are getting error

*Description*:  

{noformat}273229: 
 Waiting for AAA settings to get updated.....{noformat}

{noformat}273230: 
 Library group ""aaa"" method ""enable_TACACS_ise"" returned in 0:02:40.522209{noformat}

{noformat}273231: 
 Test returned in 0:02:41.248350{noformat}

{noformat}273232: 
 Failed reason: Error enabling TACACS protocol on AAA server{noformat}

{noformat}273233: 
 The result of section test2_enable_tacacs_attributes is => FAILED{noformat}



*Branch Name:*private/HulkPatch-ms/api-auto

**Script* [*file:*|file:*][*|file:*]solution_test_sanityecamb.py

*Source Team:*  SDA Solution Sanity

*Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=64131535&size=15341&archive=%2Fhome%2Fsda-pyats%2Fusers%2Fapic%2Farchive%2F23-09%2FSanity_TB4.2023Sep22_07:55:17.217943.zip&ats=%2Fhome%2Fsda-pyats&submitter=raksrao&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=64131535&size=15341&archive=%2Fhome%2Fsda-pyats%2Fusers%2Fapic%2Farchive%2F23-09%2FSanity_TB4.2023Sep22_07:55:17.217943.zip&ats=%2Fhome%2Fsda-pyats&submitter=raksrao&from=trade&view=all&atstype=pyATS]",2023-09-25T07:11:50.224+0000,Addressed [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6ce18650037c65d11c6b8b662a8069c820e082b7|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/6ce18650037c65d11c6b8b662a8069c820e082b7],"['Auton', 'Hulk']",Raji Mukkamala,Resolved,Anusha John
SEEN-2361,https://miggbo.atlassian.net/browse/SEEN-2361,skip execution of Test_TC1_DNAC_assign_lb_ip_from_dnac if the purpose is already served,"With Optimized Sanity executions, it has been observed that Test_TC1_DNAC_assign_lb_ip_from_dnac is executed multiple times if whole suite is triggered at once. This is unnecessary killing ~30 minutes of overall execution time that can be saved.

This Auton is to request for an enhancement in the Framework to skip execution of Test_TC1_DNAC_assign_lb_ip_from_dnac if the purpose is already served.



Please refer sheet - *3.713.75122Full*: [https://cisco-my.sharepoint.com/:x:/r/personal/amardkum_cisco_com/_layouts/15/Doc.aspx?sourcedoc=%7B2AF4EE4D-F21D-4922-A0E7-FBE29AC409A5%7D&file=ESXiLANSanityTB4.xlsx&action=default&mobileredirect=true|https://cisco-my.sharepoint.com/:x:/r/personal/amardkum_cisco_com/_layouts/15/Doc.aspx?sourcedoc=%7B2AF4EE4D-F21D-4922-A0E7-FBE29AC409A5%7D&file=ESXiLANSanityTB4.xlsx&action=default&mobileredirect=true|smart-link]  for trade logs.",2023-09-25T22:11:12.328+0000,,"['Auton', 'Enhancements']",Raji Mukkamala,Backlog,Amardeep Kumar
SEEN-2363,https://miggbo.atlassian.net/browse/SEEN-2363,Auton- [CSCwh65150] VM Hulk Upgrade: TC35_DNAC_edit_border_attributes_SDA_transit/test1_verify_assign_remove_l2border_sdda_transit_ip_internet_options_from_border,"*Branch*: private/Hulk-ms/sanity_api_auto

*Script file*: after_upgrade_verify.py 

*OVA build*: Hulk Patch1 2.3.7.3 #3.713.75131


*VM Hulk Upgrade:* *TC35*_DNAC_edit_border_attributes_SDA_transit/test1_verify_assign_remove_l2border_sdda_transit_ip_internet_options_from_border



*DE analysis - please refer Eng- note on defect CSCwh65150*


*In TC35*, when border edit is done, the wlans within the DeviceInfo are being passed as idRefs which is causing the wlans to go down. UI sends the Wlans in minified format and same should be incorporated in the script too. 

API payload : 

""type"": ""DeviceInfo"",
""wlan"": [
        {
          ""idRef"": ""b18e146e-8e51-4add-bdb3-8d6901f0b840""
        },
        {
          ""idRef"": ""61ca4c1a-17bf-49c7-a174-71b2a622ed18""
        },
        {
          ""idRef"": ""ffe46e3d-966b-4852-9671-2f1abb24d383""
        },
        {
          ""idRef"": ""503444cc-1d44-4dc3-8785-f7071c3ef1d0""
        },
        {
          ""idRef"": ""e2a092a7-e969-4f30-bc50-f3029563129f""
        },
        {
          ""idRef"": ""d7fc2ff1-77b3-4cbb-91d6-adef677344fe""
        },
        {
          ""idRef"": ""be75455b-f0b5-42eb-8e27-c82730d91a3b""
        },
        {
          ""idRef"": ""dbee94b0-1ead-4d0b-9715-f2a581b82866""
        }
      ],

UI Payload (sample) : 

""type"": ""DeviceInfo"",
                ""wlan"": {
                  ""__isArray"": true,
                  ""__link"": ""/api/v2/data/lazy-load/com.cisco.dnac.model.DeviceInfo/97d59dc9-a5c6-4443-8674-0da46e65114f/wlan""
                }

Script needs to be enhanced to send the wlan in minified format. Moving to autons.

Logs:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=32238693&size=1096279&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep22_03:55:58.218397.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=32238693&size=1096279&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep22_03:55:58.218397.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-26T07:18:27.896+0000,"[ENG-SDN / dnac-auto / 229cac2b72e - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/229cac2b72ea8c71a72cf3ba1352be6a75d2b386] ← added fix to hulk main, please give a try","['Auton', 'Upgrade', 'exsivm', 'hulk-vm-sanity']",Andrew Chen,Resolved,Raghavendrachar Baraguru Mallesha Char
SEEN-2368,https://miggbo.atlassian.net/browse/SEEN-2368,"[IBSTE-Optimized] -inventoryAndRoles & Provisioning,FEWssidsegmentonboarding  Usecase  script failures","h3. +1)setting device roles  testcase is getting failed  but in dnac  could see devices roles are set.+

h4.          *Usecase:*  [*inventoryAndRoles*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-inventory.py-61-inventoryAndRoles&begin=0&size=-1&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Foptimized_ibste_job.2023Sep21_20:43:01.489978.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]

                 Test_TC3_DNAC_Device_Inventory_verifications_configure_roles_on_devices



Error snip:



2169:  configure_device_role result is failed for site:SAN JOSE
2170:  configure_device_role result is failed for site:New York
2311:  configure_device_role result is failed for site:SAN-FRANCISCO
2312:  configure_device_role result is failed for site:Australia
2315:  configure_device_role result is failed for site:Cuba
2319:  Failed reason: Result: Some or all devices could not be set roles


LOG:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-inventory.py-61-inventoryAndRoles&begin=302644&size=157456&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Foptimized_ibste_job.2023Sep21_20:43:01.489978.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-inventory.py-61-inventoryAndRoles&begin=302644&size=157456&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Foptimized_ibste_job.2023Sep21_20:43:01.489978.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]


*2)*     +*Adding  devices to fabric site is throwing 404 err*+.

       *USECASE* :Provisioning    Test_TC1_DNAC_adding_fabric_devices_to_site



503:  Error Code: 404 for
2504:  URL:[https://10.22.40.43/api/v1/task/018abafe-6459-7ec2-968d-29e302640a22|https://10.22.40.43/api/v1/task/018abafe-6459-7ec2-968d-29e302640a22] Data:{'timeout': 60} Headers:{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2NTBjMjUzNDE4YTk5YjBjODg4Y2MxZWEiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjY1MGMyNTMzMThhOTliMGM4ODhjYzFlOSJdLCJ0ZW5hbnRJZCI6IjY1MGMyNTMyMThhOTliMGM4ODhjYzFlNyIsImV4cCI6MTY5NTM1Nzg4NCwiaWF0IjoxNjk1MzU0Mjg0LCJqdGkiOiI4NTdiYmM3My0wMzFhLTQzMTEtODA1OS1lNmE5ODk0ZGY1YzAiLCJ1c2VybmFtZSI6ImFkbWluIn0.bcjxo4K3IMxg9gpvCB_KnH_Ry-XenXpPeOrZrvLItZXoFRuXW_Nj-Rpa0tff0ZkTehVqoQz0InbglwIKIFs4qt3F5WgpItgKx5hlUScqtyPYSsTbjv-6BC55KkBHMc23IVo9mbTDIGDhvFVFW9n3gd3z1Mqb5OSvtnHino9O7mUCt_bx6_whGCo7zdz921BrcEiGXoPiZetMfM_iZ9cMcdltTw0omrmboKOYAfWdVTivHF_-Mvkq9WEn3MDJVf5wyBO0NKx8JWqWZmd946O2t6q4n_vtGozslnDnkx9n1pWjwygKIFfvvYWTgUU0vzk-oQ0I02sjRkTZxm6fSDiKRw;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:{""response"":{""errorCode"":""INVALID_TASK"",""message"":"""",""detail"":""The specified task is either invalid, or information about the task is not available yet, please try again later"",""href"":""""},""version"":""1.0""}
2505:  Traceback (most recent call last):
2506:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/IBSTE/optimizedIbste_regression/services/dnaserv/client_manager.py"", line 326, in call_api
2507:      response.raise_for_status()
2508:    File ""/ws/divayada-sjc/Solution_pyatsenv/lib/python3.6/site-packages/requests/models.py"", line 960, in raise_for_status
2509:      raise HTTPError(http_error_msg, response=self)
2510:  requests.exceptions.HTTPError: 404 Client Error: Not Found for url: [https://10.22.40.43/api/v1/task/018abafe-6459-7ec2-968d-29e302640a22|https://10.22.40.43/api/v1/task/018abafe-6459-7ec2-968d-29e302640a22]
2511:  Encountered unhandled HTTPError in Internal API Call
2512:  Flagging result as FAIL!
2513:  	Reason: 404 Client Error: Not Found for url: [https://10.22.40.43/api/v1/task/018abafe-6459-7ec2-968d-29e302640a22|https://10.22.40.43/api/v1/task/018abafe-6459-7ec2-968d-29e302640a22]
2514:  Kwargs:
2515:  {}
2517:  Adding 0b0be0ea-1440-45f5-87f8-8c1321579e8f device to site failed
2592:  add_devices_to_site result is failed for site:Cuba
3013:  Failed reason: Result: Device addition to the site failed


*LOG:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assign_devices_to_site_and_provision.py-62-provisioning&begin=8935&size=640756&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Foptimized_ibste_job.2023Sep21_20:43:01.489978.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assign_devices_to_site_and_provision.py-62-provisioning&begin=8935&size=640756&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Foptimized_ibste_job.2023Sep21_20:43:01.489978.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



+*3)  SID_IP_Pool_Mapping_ failures*+ 
  *USECASE* : [FEWssidsegmentonboarding|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_segment_onboarding_wireless.py-93-FEWssidsegmentonboarding&begin=0&size=-1&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Foptimized_ibste_job.2023Sep24_01:00:57.602096.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]       Test_TC1_DNAC_configure_SSID_IP_Pool_Mapping_for_Wireless



ERROR SNIP:

Action: Perform device onboarding, add traffic type, ip pool authentication type on fabric1
47:  Initializing function group ""wireless""
48:  Group ""wireless"" initialized successfully
49:  Initializing function group ""wlan_segment_onboarding""
50:  Group ""wlan_segment_onboarding"" initialized successfully
51: {color:#ff991f} Failed to get cdname for SAN JOSE. Please check SITE_CD_MAPPING again!!{color}
52:  Library group ""wlan_segment_onboarding"" method ""onboard_all_wireless_segment"" returned in 0:00:00.000446
53:  Library group ""wlan_segment_onboarding"" method ""onboard_wireless_segment_for_ssid"" returned in 0:00:00.000736
54:  {color:#ff991f}Failed to get cdname for SAN JOSE. Please check SITE_CD_MAPPING again!!{color}
55:  Library group ""wlan_segment_onboarding"" method ""onboard_all_wireless_segment"" returned in 0:00:00.000300
56:  Library group ""wlan_segment_onboarding"" method ""onboard_wireless_segment_for_ssid"" returned in 0:00:00.000469
57: {color:#ff991f} Failed to get cdname for SAN JOSE. Please check SITE_CD_MAPPING again!!{color}
58:  Library group ""wlan_segment_onboarding"" method ""onboard_all_wireless_segment"" returned in 0:00:00.000288
59:  Library group ""wlan_segment_onboarding"" method ""onboard_wireless_segment_for_ssid"" returned in 0:00:00.000470
60:  {color:#ff991f}Failed to get cdname for SAN JOSE. Please check SITE_CD_MAPPING again!!{color}
61:  Library group ""wlan_segment_onboarding"" method ""onboard_all_wireless_segment"" returned in 0:00:00.000242
62:  Library group ""wlan_segment_onboarding"" method ""onboard_wireless_segment_for_ssid"" returned in 0:00:00.000419
63:  {color:#ff991f}Failed to get cdname for SAN JOSE. Please check SITE_CD_MAPPING again!!{color}
64:  Library group ""wlan_segment_onboarding"" method ""onboard_all_wireless_segment"" returned in 0:00:00.000244
65:  Library group ""wlan_segment_onboarding"" method ""onboard_wireless_segment_for_ssid"" returned in 0:00:00.000409
66: {color:#ff991f} Failed to get cdname for SAN JOSE. Please check SITE_CD_MAPPING again!!{color}
67:  Library group ""wlan_segment_onboarding"" method ""onboard_all_wireless_segment"" returned in 0:00:00.000274
68:  Library group ""wlan_segment_onboarding"" method ""onboard_wireless_segment_for_ssid"" returned in 0:00:00.000441
69:  {color:#ff991f}Failed to get cdname for SAN JOSE. Please check SITE_CD_MAPPING again!!{color}
70:  Library group ""wlan_segment_onboarding"" method ""


*LOG:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_segment_onboarding_wireless.py-93-FEWssidsegmentonboarding&begin=7166&size=70149&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Foptimized_ibste_job.2023Sep24_01:00:57.602096.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_segment_onboarding_wireless.py-93-FEWssidsegmentonboarding&begin=7166&size=70149&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Foptimized_ibste_job.2023Sep24_01:00:57.602096.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-26T10:34:59.516+0000,"[~accountid:712020:96c61f39-1766-4002-9406-bfc1c806f040] 

1.Are the same issues observed with regular IBSTE?

2.  Did you try these manually : adding device roles for devices in San Jose, NY, SF and other sites.

3.  Please share your debug/Analysis for the issue. 

4. Was the test case passed before in this testbed? When? Please provide Passed log. Hi [~accountid:63f50bf5e8216251ae4d59cf] ,
For issue 1:Adding device roles for devices in San Jose, NY, SF and other sites.

Even though that use case getting failed, Manually we observed that roles configured for all the devices ,but not sure why its showing failed via script. Only sometimes we are hitting this issue and sometimes we are not hitting it.

I am not sure whether it has worked or not in regular IBSTE.

Please find the latest pass log on Hulk P2:[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-inventory.py-61-inventoryAndRoles&begin=344806&size=229879&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Foptimized_ibste_job.2023Nov13_07:54:07.767370.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-inventory.py-61-inventoryAndRoles&begin=344806&size=229879&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Foptimized_ibste_job.2023Nov13_07:54:07.767370.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Issue 2:+*Adding devices to fabric site is throwing 404 err*+.
This issue also we are hitting only few times.

Please find the latest pass log:[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assign_devices_to_site_and_provision.py-71-provisioning&begin=9666&size=540014&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Foptimized_ibste_job.2023Nov13_20:19:39.422718.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assign_devices_to_site_and_provision.py-71-provisioning&begin=9666&size=540014&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Foptimized_ibste_job.2023Nov13_20:19:39.422718.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Issue 3:+*SID_IP_Pool_Mapping_ failures*+

Every time we are hitting this issue “Failed to get cdname for SAN JOSE. Please check SITE_CD_MAPPING again!!” for al most all the sites.

Please find the latest failed log as well:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_segment_onboarding_wireless.py-104-FEWssidsegmentonboarding&begin=8189&size=78179&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Foptimized_ibste_job.2023Nov14_02:07:02.328018.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_segment_onboarding_wireless.py-104-FEWssidsegmentonboarding&begin=8189&size=78179&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Foptimized_ibste_job.2023Nov14_02:07:02.328018.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

","['Auton', 'IBSTE', 'Optimized', 'Usecase', 'script']",Raji Mukkamala,Open,Karventhan Velusamy
SEEN-2370,https://miggbo.atlassian.net/browse/SEEN-2370,[GHOST] N_plus_one_wlc cases is failed due to  CUSTOM_RF_SSID errored,"DNAC Release_Version Tested: Ghost P1 RC6 Uber ISO - 2.1.613.70194, Non-FIPS

Device Image Used: 17.11.1

Testbed: AWS-Multisite

Branch Used: private/Ghost-ms/api_auto

Script Name: solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

Solution Input File : dnac-auto/configs/config_48hr_test/solution_test_input.json

Testcases Impacted:  Test_TC184_N_plus_one_wlc  

Failed Trade Log: [Test_TC184_N_plus_one_wlc|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1753900&size=210827&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fauto_MS_job.2023Sep26_05:14:51.588122.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  

Failure Analysis:
Getting  Errored reason: CUSTOM_RF_SSID i have checked the pre-requisites are added in solution and fabric json file.",2023-09-26T15:20:53.012+0000,"[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=0&size=592305&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fauto_MS_job.2023Sep26_05:14:51.588122.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=0&size=592305&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fauto_MS_job.2023Sep26_05:14:51.588122.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

From the logs, {{dnac_input': './configs/sr_cl_ms/solution_test_input-DMZ_MS.json}} is used. And this file is not updated [~accountid:63f50bf5e8216251ae4d59cf] I have executed the script with this input file testcases is failed. As discussed with you After your modifications still testcase is failed please find the failed log:  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1795197&size=160593&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fauto_MS_job.2023Sep27_19:50:42.104216.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1795197&size=160593&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fauto_MS_job.2023Sep27_19:50:42.104216.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] topology link is incorrect/ not same for the AP and  {{NYC-FE-9300}}

{noformat}AP488b.0af0.caec:
  interfaces:
      GigabitEthernet0:
          link: eth-NYC-FE-9300-2-NYC-AP488b.0af0.caec-0
          type: ethernet{noformat}

{noformat}NYC-FE-9300:
  interfaces:
      GigabitEthernet1/0/12:
          link: eth-NYC-FE-9300-2-AP488b.0af0.caec-0
          type: ethernet{noformat}

Please have the same link name for both the devices","['Auton', 'Ghost']",Raji Mukkamala,Resolved,Balaji Raju
SEEN-2372,https://miggbo.atlassian.net/browse/SEEN-2372,[Auton] Hulk P1 VA - Wireless Solution Sanity - Script Fix Required for Report Generation TCs,"*Regression:* SDA Solution Sanity (SSR)

*DNAC Type:* DNAC-VM

*DNAC Release_Version Used:* assembly_release_dnac_hulk_converged_07-3.713.75131.ova

*Cluster:* 10.88.187.190 – admin/Maglev123 for UI and maglev/Maglev123 for SSH

* For device access, SSH to device ip from DNAC CLI via login
** cisco/Cisco#123/Cisco#123 for 204.1.2.1/204.1.2.3
** wlcaccess/Lablab#123/Cisco#123 for 204.1.2.2/204.192.2.1

*Branch Used:* rcdn/HulkPatch-ms/api-auto  (local branch that will be synced to main branch Private/HulkPatch-ms/api-auto before every reg run start)

*Script Used:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

*Taas Log:* [https://ngdevx.cisco.com/services/taas/results/f99452d9-25a1-43f6-a199-d6caae79eece|https://ngdevx.cisco.com/services/taas/results/f99452d9-25a1-43f6-a199-d6caae79eece]

*Impacted (Reports Generation Related) TCs: 4*

* *Errored TCs:* TC148, TC153, TC221
* *Failed TCs:* TC190

Sent error snapshots from log, on mail with sub 'Hulk P1 VA SSR - Need Help on Report Generation / Assurance failed TCs' to solution-automation-team(mailer list) [solution-automation-team@cisco.com|mailto:solution-automation-team@cisco.com]",2023-09-27T15:11:09.521+0000,"As per Armadeep reply to the email:

# TC148 - generate_ap_report. Solutions ESXi Sanity has CSCwh64142.
# TC153 - test4_generate_network_devices_CSV_report. Solutions ESXi Sanity has CSCwh64142.
# TC153 - test5_generate_network_devices_JSON_report. Solutions ESXi Sanity has CSCwh64142.
# TC190 - test1_generate_Port_Reclaim_CSV_report and test2_generate_Port_Reclaim_JSON_report. Solutions ESXi Sanity has CSCwh64142.
# TC221 - generate_network_device_poe_report. Solutions ESXi Sanity has CSCwh64142.","['Auton', 'Hulk-P1-VA']",Unassigned,Cancelled,Yuvarani Iyamperumal
SEEN-2373,https://miggbo.atlassian.net/browse/SEEN-2373,[Auton] Hulk P1 VA - Wireless Solution Sanity - Script Fix Required for Assurance TCs,"*Regression:* SDA Solution Sanity (SSR)

*DNAC Type:* DNAC-VM

*DNAC Release_Version Used:* assembly_release_dnac_hulk_converged_07-3.713.75131.ova

*Cluster:* 10.88.187.190 – admin/Maglev123 for UI and maglev/Maglev123 for SSH

* For device access, SSH to device ip from DNAC CLI via login
** cisco/Cisco#123/Cisco#123 for 204.1.2.1/204.1.2.3
** wlcaccess/Lablab#123/Cisco#123 for 204.1.2.2/204.192.2.1

*Branch Used:* rcdn/HulkPatch-ms/api-auto  (local branch that will be synced to main branch Private/HulkPatch-ms/api-auto before every reg run start)

*Script Used:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

*Taas Log:* [https://ngdevx.cisco.com/services/taas/results/f99452d9-25a1-43f6-a199-d6caae79eece|https://ngdevx.cisco.com/services/taas/results/f99452d9-25a1-43f6-a199-d6caae79eece]

*Impacted (Assurance Related) TCs: 2*

* *Errored TCs:* TC218
* *Failed TCs:* TC171

Sent error snapshots from log, on mail with sub 'Hulk P1 VA SSR - Need Help on Report Generation / Assurance failed TCs' to solution-automation-team(mailer list) [solution-automation-team@cisco.com|mailto:solution-automation-team@cisco.com]",2023-09-27T15:14:23.017+0000,"As per Armadeep reply to the email:

TC218 - test1_get_node_and_check_name_before_change. Solutions ESXi Sanity has [CSCwh32084 - API issue with Neighbor-topology|https://cdetsng.cisco.com/webui/#view=CSCwh32084]

TC171 - test1_assurance_health_validation. Disabled in Halleck; CSCwe13443","['Auton', 'Hulk-P1-VA']",Unassigned,Cancelled,Yuvarani Iyamperumal
SEEN-2374,https://miggbo.atlassian.net/browse/SEEN-2374,[Auton] Hulk P1 VA - Wireless Solution Sanity - Script Fix Required TCs,"*Regression:* SDA Solution Sanity (SSR)

*DNAC Type:* DNAC-VM

*DNAC Release_Version Used:* assembly_release_dnac_hulk_converged_07-3.713.75131.ova

*Cluster:* 10.88.187.190 – admin/Maglev123 for UI and maglev/Maglev123 for SSH

* For device access, SSH to device ip from DNAC CLI via login
** cisco/Cisco#123/Cisco#123 for 204.1.2.1/204.1.2.3
** wlcaccess/Lablab#123/Cisco#123 for 204.1.2.2/204.192.2.1

*Branch Used:* rcdn/HulkPatch-ms/api-auto  (local branch that will be synced to main branch Private/HulkPatch-ms/api-auto before every reg run start)

*Script Used:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

*Taas Log:* [https://ngdevx.cisco.com/services/taas/results/f99452d9-25a1-43f6-a199-d6caae79eece|https://ngdevx.cisco.com/services/taas/results/f99452d9-25a1-43f6-a199-d6caae79eece]

*Impacted TCs: 5*

* *Errored TCs:* *TC7, TC8, TC43, TC107*
* *Failed TCs: TC28*

Sent error snapshots from log, on mail with sub 'Hulk P1 VA SSR - Need Help on Report Generation / Assurance failed TCs' to solution-automation-team(mailer list) [solution-automation-team@cisco.com|mailto:solution-automation-team@cisco.com]",2023-09-27T15:17:52.302+0000,"Armadeep reply to the email:

# TC7 - test33_set_proxy - require update for this TC with ESXi specific APIs. Raised [SEEN-2377|https://miggbo.atlassian.net/browse/SEEN-2377] to get it addressed.
# TC8 - test2_verify_remote_sftp_settings: the used API is no more in use in ESXi. Need to find the UI options first.
# TC28 - test1_create_model_config - Solutions ESXi Sanity has pass log.

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assign_devices_to_site_and_provision.py-82-provisioning&begin=265874&size=132103&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep25_09:46:18.293895.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assign_devices_to_site_and_provision.py-82-provisioning&begin=265874&size=132103&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep25_09:46:18.293895.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

After reviewing the execution log shared by you, Andrew will be adding more retries to check the status of the “Model Config” before marking it as “Failed”. Raised [SEEN-2378|https://miggbo.atlassian.net/browse/SEEN-2378]. Andrew will provide a fix for it.

# TC43 - test9_configure_wireless_sensor_ssh_credentials - Sensors are not fully supported on ESXi at the moment, so we are ignoring the failure.
# TC107 - test8_verify_Networkprofile_compliance: Exception Handling needs to be done for better logging about issue. As per Amardeep info below, close this ticket as it will be followed by separated ticket individually.
[~accountid:636ce33a6bbefce0aca3df70] please open separated ticket for each issue if needed. Please follow our template check list when you report issues/autons [https://wiki.cisco.com/display/EDPEIXOT/Solution+Engineering+Team|https://wiki.cisco.com/display/EDPEIXOT/Solution+Engineering+Team]","['Auton', 'Hulk-P1-VA']",Unassigned,Cancelled,Yuvarani Iyamperumal
SEEN-2375,https://miggbo.atlassian.net/browse/SEEN-2375,[Auton]:Test_TC1_generate_link_flap_issues,"*Reporter Analysis:* 

For any device role link flap issue should be generated

*Description:*  
Message:{""message"":""Role does not have valid permissions to access the API""}

*Branch Name:  private/HulkPatch-ms/api-auto*

*Script file/Usecase:* 

generateLinkFlapIssues.py

*Source Team:  Sanity*



*Fail Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-generate_link_flap_issues.py-184-generateLinkFlapIssues&begin=4785&size=63560&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep25_21:04:32.896955.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-generate_link_flap_issues.py-184-generateLinkFlapIssues&begin=4785&size=63560&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep25_21:04:32.896955.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:* NA",2023-09-27T16:41:28.882+0000,fix [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f8608e6df41fe056a657664018de786d2044257b|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f8608e6df41fe056a657664018de786d2044257b],"['Auton', 'Ghost', 'Issue', 'Sanity']",Raji Mukkamala,Closed,Manoj Menakuri
SEEN-2376,https://miggbo.atlassian.net/browse/SEEN-2376,[Auton]Test_TC4_DNAC_verify_anchorvn_withIXIA_clients_and_traffic_noauth/verify_anchorvn_withIXIA," 

Reporter Analysis:  Ports are down at the time of traffic start. script is not enabling auto negotiation on ixia ports. 

Description: 
40104: 
 Send ARP for all interfaces

{noformat}40105: 
 Send traffic interfaces{noformat}

{noformat}40106: 
 Exception in starting traffic{noformat}

{noformat}40107: 
 None{noformat}

{noformat}40108: 
 Test returned in 1:48:52.458604{noformat}

{noformat}40109: 
 Failed reason: Result:  Failed to start traffic{noformat}



 
*Branch Name:* private/HulkPatch-ms/api-auto

*Script file/usecase:*  SDAFabricAnchorvn.py

*Source Team:*  Sanity



Fail Log: 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-anchor_vn_validations.py-163-SDAFabricAnchorvn&begin=6942453&size=3293413&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep25_21:04:32.896955.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-anchor_vn_validations.py-163-SDAFabricAnchorvn&begin=6942453&size=3293413&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep25_21:04:32.896955.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Testbed details: NA",2023-09-27T17:35:11.913+0000,"Was any Ixia test passed before?
Are your Ixia ports fiber or copper, 1G or 10G? Hulk P1  RC3   
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-anchor_vn_validations.py-163-SDAFabricAnchorvn&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_07:09:33.075475.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-anchor_vn_validations.py-163-SDAFabricAnchorvn&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_07:09:33.075475.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS] Hulk  P1  RC3  
Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-anchor_vn_validations.py-163-SDAFabricAnchorvn&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_07:09:33.075475.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-anchor_vn_validations.py-163-SDAFabricAnchorvn&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_07:09:33.075475.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS] 

Media =Fiber  
TB 7 Wiki : 
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] [~accountid:712020:a269cb40-4972-4cdd-b4be-1a9a0bc46160] , [~accountid:620b8357878c2f00729881c8] , can you follow this template when raising autons?

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist]","['Auton', 'Ghost', 'Groot', 'Issue', 'Optimized', 'Sanity']",Tran Lam,Backlog,Manoj Menakuri
SEEN-2377,https://miggbo.atlassian.net/browse/SEEN-2377,test33_set_proxy related lib uplift is required for ESXi,"test33_set_proxy related lib uplift is required for ESXi.

Working APIs for ESXi:

Request Method: GET
Request URL: [https://10.22.45.61/api/v1/system-orchestrator/cluster/outgoing-proxy|https://10.22.45.61/api/v1/system-orchestrator/cluster/outgoing-proxy]


Request Method: PUT
Request URL: [https://10.22.45.61/api/v1/system-orchestrator/cluster/outgoing-proxy|https://10.22.45.61/api/v1/system-orchestrator/cluster/outgoing-proxy]
payload: {""proxy"":""[http://proxy-wsa.esl.cisco.com:80|http://proxy-wsa.esl.cisco.com:80]"",""password"":"""",""username"":""""}",2023-09-27T23:42:12.776+0000,"Raised PR for the required change: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7234/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7234/overview] against *private/HulkPatch-ms/api-auto* branch. Mentioned PR has been approved and merged to “private/HulkPatch-ms/api-auto” branch and cherry-picked to “private/Hulk-ms/api-auto” and “private/HulkPatch2-ms/api-auto” branches.

Marking this Auton as “Closed”.","['Auton', 'ESXi', 'Uplift']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-2378,https://miggbo.atlassian.net/browse/SEEN-2378,"test1_create_model_config requires more retries before concluding it as ""FAILED""","test1_create_model_config requires more retries before concluding it as ""FAILED""",2023-09-27T23:44:11.653+0000,"[ENG-SDN / dnac-auto / 7e4d1f3b312 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/7e4d1f3b3121eda21639057c2273e6b43a512c54]
Added more retries ← hulk main line, cherry picked to p1 and p2 main branches","['Auton', 'ESXi']",Andrew Chen,Resolved,Amardeep Kumar
SEEN-2379,https://miggbo.atlassian.net/browse/SEEN-2379,[Auton][MSTB2] : Test_TC204_APs_negative_operations / test1_setup_test_cases,"Hi 

On latest hulk-Patch1 ESXI runs we are seeing below error

219473:  ERROR changing the site in the input file!!, please check if the input file has changed!!
219474:  Reason key error: dictionary keys changed during iteration
219475:  None
219476:  Library group ""inventory"" method ""set_site_with_max_aps"" returned in 0:00:00.549974
219477:  Test returned in 0:00:03.402855
219478:  Failed reason: Failed setting up use case to choose a site that has APs!
219479:  The result of section test1_setup_test_cases is => FAILED

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=53299241&size=79089&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fauto_MS_job.2023Sep26_09:34:59.552694.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=53299241&size=79089&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fauto_MS_job.2023Sep26_09:34:59.552694.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

The error message is not clear, it says script is trying to modify Input file.

Branch : private/HulkPatch-ms/api-auto

Script : solution_test_3sites_sjc_nyc_sf.py

OVA#3.713.75131

IOS#17.12.2 PRD2",2023-09-28T09:10:44.278+0000,"Hi [~accountid:62d2fed26eba7198372366ca]  ,

In the sanity execution, we are encountering the same issue Hi [~accountid:63f50bf0e8216251ae4d59ca]  and [~accountid:620b8357878c2f00729881c8] 

Please try from the Branch

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse?at=refs%2Fheads%2Fprivate%2FHulk-CFI-ind|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse?at=refs%2Fheads%2Fprivate%2FHulk-CFI-ind] Hi [~accountid:62d2fed26eba7198372366ca],

I’ve executed script on ESXI with private/Hulk-CFI-ind branch, and script is failing is failing with same error

{noformat}11134: 
 ERROR changing the site in the input file!!, please check if the input file has changed!!{noformat}

{noformat}11135: 
 Reason key error: dictionary keys changed during iteration{noformat}



Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1839925&size=100624&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct13_06:16:17.178601.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1839925&size=100624&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct13_06:16:17.178601.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Hi [~accountid:63f50bf0e8216251ae4d59ca] ,

Ashwini And I had a discussion and it could be an issue with Jenkins as the Code is not being pulled properly and Jenkins is trying to cherry-pick a commit instead pull complete branch.



{noformat} > git config core.sparsecheckout # timeout=10
 > git checkout -f 4d886f3c9ed109867d11b1c1937c15f551fe9905 # timeout=10
Commit message: ""updated missing ap""
 > git rev-list --no-walk 11a84e40f4b75543e7c83a9fa7e61038d757fc17 # timeout=10
New run name is '#108  Script:  LAN TB11 1.1 TC : 2,219'
[DMZ-Hulk-sanity-common-Multi-job] $ /bin/sh -xe /tmp/jenkins8075454574082183400.sh
+ echo /ws/owagh-sjc/Solution_pyatsenv
/ws/owagh-sjc/Solution_pyatsenv{noformat} Raised PR 



[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7092/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7092/overview] Patch 1



[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7451/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7451/overview]","['Auton', 'ESXi', 'Execution', 'Hulk-Patch1', 'MSTB2', 'Optimized', 'Sanity']",Vinay Raj V ,Backlog,Divakar Kumar Yadav
SEEN-2380,https://miggbo.atlassian.net/browse/SEEN-2380,Auton-Hulk EXSI VM Upgrade:- TC31_onboard_device_clear_all_interfaces_all_edges/test1_onboard_device_clear_all_interfaces_all_edges,"During EXSI upgrade verify script execution observed,*TC31_onboard_device_clear_all_interfaces_all_edges/test1_onboard_device_clear_all_interfaces_all_edges:- It’s* Clearing all port configuration on the FIAB device on Newyork Site on port assignment page it’s causing to TSIM AP’s going down & unreachable on EWLC controller. It’s should be clear only Extended node connected ports not TSIM connected port on the FIAB device. TC need to enhance it.

*Please find below snippet:* 

_Scheduling task for Clearing all interface Onboarding for device_ [_TB18-NY-FIAB.cisco.com_|http://TB18-NY-FIAB.cisco.com] _for fabric ['Global/USA/New_York'] at time 1695380394.2342381 - Configuration Preview,_

'Global/USA/New_York'] at time 1695380394.2342381 - Configuration Preview

4383: Task Found :{'activityId': '018abc8b-d133-7a37-8bb8-fcfc29948e4a',

_interface GigabitEthernet1/0/24\n no access-session inherit disable autoconf\n no access-session inherit disable interface-template-sticky\n macro auto processing\ninterface GigabitEthernet1/0/39\n no access-session inherit disable autoconf\n no access-session inherit disable interface-template-sticky\n macro auto processing\ninterface range Gigab1/0/39,Gigab1/0/9,Gigab1/0/24,Gigab1/0/3-4\n no load-interval\n no spanning-tree portfast\n no spanning-tree bpduguard enable\n no switchport voice vlan\n no switchport access vlan\n no description\n switchport mode dynamic auto\n exit',_

*VM Build used:  #*3.710.75131 build

*Branch Name:*  private/Hulk-ms/sanity_api_auto

*Script file/Usecase:* 
after_upgrade_verify.py

*Source Team:  EXSI Upgrade Sanity*

Log:-  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=344402&size=1101024&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep22_03:55:58.218397.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=344402&size=1101024&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep22_03:55:58.218397.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Regards,

Raghavendra B. M",2023-09-28T11:26:25.353+0000,,"['Auton', 'Upgrade', 'exsivm', 'hulk-vm-sanity']",Moe Saeed,Backlog,Raghavendrachar Baraguru Mallesha Char
SEEN-2381,https://miggbo.atlassian.net/browse/SEEN-2381,Auton-Hulk EXSI VM Upgrade:- Test_TC24_verify_assurance_health_nw_health_border_edge_wlc_ext_node/test7_verify_all_edge_devices_poe_details,"[~accountid:63f50bfce8216251ae4d59d5] 


During EXSI upgrade verify script execution observed *TC24_verify_assurance_health_nw_health_border_edge_wlc_ext_node/test7_verify_all_edge_devices_poe_detail*s failed due to {{HTTPError: 400 Client Error:}}

Affected below both TCs:

*Test_TC24_verify_assurance_health_nw_health_border_edge_wlc_ext_node/test7_verify_all_edge_devices_poe_details*

*Test_TC50_verify_assurance_health_nw_health_border_edge_wlc_ext_node/test7_verify_all_edge_devices_poe_details*



 _6062:  Resource path full url:_ [_https://10.22.45.217/api/v1/network-device/ip-address_|https://10.22.45.217/api/v1/network-device/ip-address]
_6063:  Error Code: 400 for_
_6064:  URL:_[_https://10.22.45.217/api/v1/network-device/ip-address_|https://10.22.45.217/api/v1/network-device/ip-address] _Data:{'timeout': 60} tgPbdmgwm_eV6owCkJbb5XGv9gHSy0lYYwpmU5jWXSa4PILSMYDX7kXvvtpCQgXqUFeu-OdHeg'} Message:{""response"":{""errorCode"":""Bad request"",""message"":""Invalid input request"",""detail"":""ip-address is not a valid UUID of device""},""version"":""1.0""}_
_6065:  Traceback (most recent call last):_




Checked Manually on the cluster POE details EDEG device it’s showing fine.

*Failed log:*  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3964724&size=15051&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep27_22:32:54.272967.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3964724&size=15051&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep27_22:32:54.272967.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*VM Build used: #*3.710.75131 build

*Branch Name:*  private/Hulk-ms/sanity_api_auto

*Script file/Usecase:* 
after_upgrade_verify.py

*Source Team:  EXSI Upgrade Sanity*



Regards,

Raghavendra B. M",2023-09-28T13:45:54.423+0000,"[~accountid:63f50bd68ab3d6a635ecc29b] The below information are required when you file an autons. Please provide.

* Your debug/Analysis for the issue.
* Was the testcase passed before in this testbed? When? Please provide Passed log.
* Was the testcase passed in different testbeds? Hi [~accountid:63f50bd68ab3d6a635ecc29b]  , Are this testcase passed earlier ? please share pass log if you have. are the pre-sanity case passed as these function call exist there as well.  [~accountid:5fe224a53b5e47013862f185] 


Pre-Sanity script TC got passed Vinod:

Here is the Pass log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1182671&size=180937&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep26_11:48:25.724554.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1182671&size=180937&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep26_11:48:25.724554.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Note: - In EXSI VM upgrade testbed we are testing first time upgrade scripts. Do you have the logs for TC5,6,7 in upgrade script ?  from the logs shows there is no device exist . need to check whether device is exist in inventory or not as there is no code changes went in this library. Here full trade log link:

TC 24.5,6 &7 are passed. 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep27_22:32:54.272967.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep27_22:32:54.272967.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS]



Here is Main TCs 5,6, & 7 passed trade log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=527002&size=18424&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep27_10:47:33.487042.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=527002&size=18424&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep27_10:47:33.487042.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Regards,

Raghavendra B. M Raghav, you mentioned the upgrade script running first time in esxi vm TB18, but the below logs executed on july 11th, Not clear . TC24 also failed with below error in which you shared the log 

{noformat}6127: 
     raise AttributeError(""'Device' object has no attribute '%s'""{noformat}

{noformat}16128: 
 AttributeError: 'Device' object has no attribute 'lb_ip'{noformat}

{noformat}16129: 
 Test returned in 0:00:00.007886{noformat}

{noformat}16130: 
 Errored reason: 'Device' object has no attribute 'lb_ip'{noformat}

{noformat}16131: {noformat}



We can have call and discuss as this is something relate to the testcase execution order only. 

Let me know when you have the testbed ready with validating the upgrade script Typo error i have shared dry runs logs the upgrade was not done that we have executed.

Now i have updated recent executed trade logs after EXSI vm upgrade. with new logs, assigning it back to [~accountid:5fe224a53b5e47013862f185] . Testcase passed with latest runs and please find the below log for reference.



[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct24_09:35:51.652800.zip&reqseq=&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct24_09:35:51.652800.zip&reqseq=&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=PYATS] Thanks [~accountid:712020:a269cb40-4972-4cdd-b4be-1a9a0bc46160]  for the latest run , as the failed log error code is 400 which is bad request might be devices were down that time. ","['Auton', 'ESXi', 'Sanity', 'Upgrade']",Vinoth Kumar Kutty Krishnamoorthy,Closed,Raghavendrachar Baraguru Mallesha Char
SEEN-2382,https://miggbo.atlassian.net/browse/SEEN-2382,[Hulk] [P1] Optimized:  Task-verify_border_edge_kpi.py-172-verifyBorderEdgeKPI,"*Sub TC failed*:
test1_verify_border_edge_kpi

*Error snip shot* :
105: 
 Resource path full url: https://10.30.0.100/api/assurance/v1/network-device/assurance-metric

{noformat}105:  Resource path full url: https://10.30.0.100/api/assurance/v1/network-device/assurance-metric
106:  ************************************************************
107:   Response: {'isMetricApplicable': True, 'records': [{'modificationtime': '1695867300000', 'overallScore': '8', 'tcpConnScore': '-1', 'time': '2023-09-28T02:15:00.000+0000', 'enIseConnScore': '10'}, {'modificationtime': '1695867600000', 'overallScore': '8', 'tcpConnScore': '-1', 'time': '2023-09-28T02:20:00.000+0000', 'enIseConnScore': '10'}], 'request': {'entity': {'_id': None, 'managementIpAddr': '204.1.1.69', 'macAddress': None, 'uuid': None, 'uuidFieldName': 'uuid'}, 'name': 'allDeviceHealthScoreAgg', 'dimensions': None, 'window': '1 min', 'timeRange': {'start': '2023-09-28T02:12:24.731Z', 'end': '2023-09-28T02:27:24.767Z', 'current': None}, 'field': None, 'type': None, 'fields': ['dhcpConnScore', 'dnsConnScore', 'tcpConnScore', 'overallScore', 'enIseConnScore']}, 'pagination': {'page': 1, 'pageSize': 300, 'order': 'ASC', 'totalResults': 2}}
108:   ************************************************************
109:  Calculating If the required result found  in the Response!!
110:  Result Generated is ['1695867300000', '1695867600000']
111:  Fabric Control Plane Scores or ISE are not as Expected!! on TB7-SJ-EDGE {'modificationtime': '1695867600000', 'overallScore': '8', 'tcpConnScore': '-1', 'time': '2023-09-28T02:20:00.000+0000', 'enIseConnScore': '10'}{noformat}



*Analysis*:

The test case failed on the EDGE node with a 360-page health score of 8. Could you please check and let me know if there's a minimum score required? Additionally, could you please add log info indicating the 'expected score'? It will be useful for debugging.

*Trade logs*:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_border_edge_kpi.py-172-verifyBorderEdgeKPI&begin=4653&size=103152&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_19:24:38.875555.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_border_edge_kpi.py-172-verifyBorderEdgeKPI&begin=4653&size=103152&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_19:24:38.875555.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

 



Branch used:

private/HulkPatch-ms/sanity_api_auto

Pass Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_border_edge_kpi.py-172-verifyBorderEdgeKPI&begin=4653&size=27724&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep23_06:04:20.686044.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_border_edge_kpi.py-172-verifyBorderEdgeKPI&begin=4653&size=27724&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep23_06:04:20.686044.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

!edge_kpi.mp4|width=1920,height=890!",2023-09-28T14:51:51.498+0000,"The test case passed in the recent run. If the issue reoccurs, I will reopen the Jira ticket:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_border_edge_kpi.py-172-verifyBorderEdgeKPI&begin=4653&size=27325&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct04_09:16:04.537477.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_border_edge_kpi.py-172-verifyBorderEdgeKPI&begin=4653&size=27325&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct04_09:16:04.537477.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Hulk', 'HulkPatch1', 'Optimized', 'Sanity']",NhanHuu Nguyen,Closed,Omkar Sharad Wagh
SEEN-2383,https://miggbo.atlassian.net/browse/SEEN-2383,Auton-Hulk EXSI VM Upgrade:-Test_TC14_DNAC_configure_multicast_primary_border_as_rp/test1_configure_multicast_on_site_sjc,"[~accountid:63f50bcece6f37e5ed93c87e] 


During Upgrade Verify Script execution observed failed below TCs  TC14_DNAC_configure_multicast_primary_border_as_rp/test1_configure_multicast_on_site_sjc & 

Test_TC42_DNAC_configure_SSID_IP_Pool_Mapping_for_Wireless/test1_device_onboarding_update_virtual_network_pool_and_authentication_fabric1

Test_TC37_DNAC_TSIM_static_onboarding_verifications/test1_tsim_static_onboarding_verifications/test1_onboard_all_wired_clients

TCs failed due to script looking fabric site id “Global/USA/SAN_JOSE_US_SJ_Fabric1“ which is not found

Hence it’s failed both TCs.

_68414:  {'params': {'name': 'Global/USA/SAN_JOSE_US_SJ_Fabric1'}}_
_68415:  Resource path full url:_ [_https://10.22.45.217/api/v2/data/customer-facing-service/ConnectivityDomain_|https://10.22.45.217/api/v2/data/customer-facing-service/ConnectivityDomain]
_68416:  Fabric was not found with input fabric name Global/USA/SAN_JOSE_US_SJ_Fabric1 id None. Response: {'response': [], 'version': '1.0'}_
_68417:_
_68418:_
_68419:   api_switch_call called:_
_68420:  {'params': {'name': 'Global/USA/SAN_JOSE_US_SJ_Fabric1_US_SJ_Fabric1'}}_
_68421:  Resource path full url:_ [_https://10.22.45.217/api/v2/data/customer-facing-service/ConnectivityDomain_|https://10.22.45.217/api/v2/data/customer-facing-service/ConnectivityDomain]
_68422:  Fabric was not found with input fabric name Global/USA/SAN_JOSE_US_SJ_Fabric1_US_SJ_Fabric1 id None. Response: {'response': [], 'version': '1.0'}_
_68423:  Library group ""connectivity_domain"" method ""get_cd_info"" returned in 0:00:00.024920_

_68425:  The fabric Global/USA/SAN_JOSE_US_SJ_Fabric1 was not found._

 



Failed log  TC 14: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14470305&size=6029&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep27_10:47:33.487042.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14470305&size=6029&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep27_10:47:33.487042.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Failed log: TC 42: 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1304165&size=5825&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_04:02:30.818210.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1304165&size=5825&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_04:02:30.818210.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

TC 37 : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=613365&size=422795&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_01:19:37.791351.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=613365&size=422795&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_01:19:37.791351.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] ",2023-09-28T16:09:54.416+0000,"{{configs/upgrade/ghost_to_hulk_solution_test_input_upgrade.json}} This is the solution input file it says its using, are you upgrading from ghost? If not, this will fail. In ghost, those fabric names are present in the codebase, but in hulk they are not. So if you are upgrading from ghost, __US_SJ_Fabric1_ suffix should be present, but if you are upgrading from hulk (for example intra upgrade), the suffix will not be there, and you need to use the correct solution input json. We have done Hulk intra upgrade from from *Hulk FCS 2.3.7.0* build to *HULK Patch-1 build: 3.713.75131*

Using Upgrade regular json file {color:#ffc400}_“_{color}*_solution_test_input_upgrade.json“_* it got passed

Now Script picking fabric site id: Global/USA/SAN_JOSE

_""MULTICAST_IPPOOL_VN_MAP"":_
_[_
_{ ""vn-name"":""WiredVNFBLayer2"",_
_""fabric-name"":""Global/USA/SAN_JOSE""_,



pass log:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_22:11:51.789028.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_22:11:51.789028.zip&atstype=ATS]","['Auton', 'ESXi', 'Sanity', 'Upgrade']",Andrew Chen,Closed,Raghavendrachar Baraguru Mallesha Char
SEEN-2384,https://miggbo.atlassian.net/browse/SEEN-2384,[Auton]:HulkP1 [Optimization]:  Task-SDA_wired_host_onboarding_uplink_interfaces.py-127-SDAWiredHostOnboardingUplinkInterfaces,"*Reporter Analysis:*

We observed in TB7, during Hulk P1 optimization script execution,  test2_onboard_port_with_user_device  New feature  TC case  failed with below error  could please check 



{noformat}1724: 
1725:  TB7-SJ-EDGE -ERROR Following line ip flow monitor dnacmonitor input not present on device unexpectedly
1726: 
1727:  TB7-SJ-EDGE -ERROR Following line ip flow monitor dnacmonitor_dns input not present on device unexpectedly
1728: 
1729:  TB7-SJ-EDGE -ERROR Following line ip flow monitor dnacmonitor output not present on device unexpectedly
1730: 
1731:  TB7-SJ-EDGE -ERROR Following line ip flow monitor dnacmonitor_dns output not present on device unexpectedly
1732: 
1733:  TB7-SJ-EDGE -ERROR Following line ipv6 flow monitor dnacmonitor_v6 input not present on device unexpectedly
1734: 
1735:  TB7-SJ-EDGE -ERROR Following line ipv6 flow monitor dnacmonitor_dns_v6 input not present on device unexpectedly
1736: 
1737:  TB7-SJ-EDGE -ERROR Following line ipv6 flow monitor dnacmonitor_v6 output not present on device unexpectedly
1738: 
1739:  TB7-SJ-EDGE -ERROR Following line ipv6 flow monitor dnacmonitor_dns_v6 output not present on device unexpectedly{noformat}

*Hulk P1 -2.1.713.70263*
*Failed Log :*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-SDA_wired_host_onboarding_uplink_interfaces.py-127-SDAWiredHostOnboardingUplinkInterfaces&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep26_21:39:36.380307.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-SDA_wired_host_onboarding_uplink_interfaces.py-127-SDAWiredHostOnboardingUplinkInterfaces&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep26_21:39:36.380307.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]


*Script file/Usecase :* lansanity_usecases_maps.yaml

*Source Team:  Sanity*

*Branch  Deatils :* private/HulkPatch-ms/sanity_api_auto

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] 


+*Feature  Wiki :*+ [+*https://wiki.cisco.com/display/EDPEIXOT/Onboard+Uplink+and+Network+module+port+for+all+types+of+endpoints*+|https://wiki.cisco.com/display/EDPEIXOT/Onboard+Uplink+and+Network+module+port+for+all+types+of+endpoints]",2023-09-28T19:08:01.178+0000,"Hi [~accountid:63f50bcafb3ac4003fa2c6dd] ,

Seeing same error on AWS Sanity:
Failed Log from Hulk P1#:2.3.7.3-70263
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-10/env_optimized_auto_job.2023Oct04_20:45:20.038628.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-10/env_optimized_auto_job.2023Oct04_20:45:20.038628.zip&atstype=ATS]

*Script file/Usecase :* lansanity_usecases_maps.yaml

*Source Team:  Sanity*

*Branch Deatils :* private/HulkPatch-ms/sanity_api_auto



Pass log from ESXI VM Hulk p1 where they have no EDGE Device but BORDER is acting as edge:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-SDA_wired_host_onboarding_uplink_interfaces.py-128-SDAWiredHostOnboardingUplinkInterhttps://engci-private-sjc.cisco.com/jenkins/sol-eng/job/DMZ-SANITY/job/Hulk/job/Hulk_Optimized-Sanity/51/consolefaces&begin=4717&size=4570202&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct02_14:59:07.079619.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-SDA_wired_host_onboarding_uplink_interfaces.py-128-SDAWiredHostOnboardingUplinkInterhttps://engci-private-sjc.cisco.com/jenkins/sol-eng/job/DMZ-SANITY/job/Hulk/job/Hulk_Optimized-Sanity/51/consolefaces&begin=4717&size=4570202&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct02_14:59:07.079619.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

NOTE:
I tried to do manually as per the wiki:
[https://wiki.cisco.com/display/EDPEIXOT/Onboard+Uplink+and+Network+module+port+for+all+types+of+endpoints|https://wiki.cisco.com/display/EDPEIXOT/Onboard+Uplink+and+Network+module+port+for+all+types+of+endpoints]

 even after that EDGE Device was not able to getting these configs
Is it a bug ?

Thanks,
Anusha John Hi [~accountid:61efa8c457b25b006877eda3], As discussed before with Omkar via Webex, the root cause is:

The ‘config_to_check’ in the code library no longer matches with the output config in Hulk Patch1. Needs to change a bit in the code library. I will fix this one asap.

Thanks,

Nhan Nguyen # PR HulkPatch-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7324/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7324/overview]
# Test Case:  {{Test_TC215_SDA_Wired_Host_Onboarding_Uplink_Interfaces}}
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Testbed: TB1, INTG2
# Trade log link HulkPatch-ms/api_auto:
## TB1: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-nhanhngu%2Fusers%2Fnhanhngu%2Farchive%2F23-10%2Fsanity_TB1.2023Oct05_23:26:00.635993.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-nhanhngu%2Fusers%2Fnhanhngu%2Farchive%2F23-10%2Fsanity_TB1.2023Oct05_23:26:00.635993.zip&atstype=ATS]
## INTG2: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-10/sanity-intg2.2023Oct06_01:27:37.273519.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-10/sanity-intg2.2023Oct06_01:27:37.273519.zip&atstype=ATS] The Root Cause: The output config of a device that has both border and edge roles is different from a device that has an edge role only.
Solution: Add if caluse to check the device have border role or not. Hi [~accountid:620b8357878c2f00729881c8] [~accountid:61efa8c457b25b006877eda3], Could you cherry-pick this PR and check again? Sure [~accountid:712020:857b82fe-8242-464d-b7ca-d81c683a08eb]  Hi  [~accountid:63f50bcafb3ac4003fa2c6dd]  ,
We  got the Passlog in the sanity  Hulk P1 branch with on-prem cluster  & AWS cluster ,

Merged  PR: 
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/003b2373a0992b9d7a4129c587050f8d41cff105|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/003b2373a0992b9d7a4129c587050f8d41cff105]

On-prem Passlog:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-10/env_optimized_auto_job.2023Oct09_06:33:02.748849.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-10/env_optimized_auto_job.2023Oct09_06:33:02.748849.zip&atstype=ATS]

AWS Passlog:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1468346&size=449824&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct09_08:25:26.632649.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1468346&size=449824&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct09_08:25:26.632649.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Feature', 'HulkPatch1', 'Optimized', 'Sanity']",NhanHuu Nguyen,Resolved,Omkar Sharad Wagh
SEEN-2385,https://miggbo.atlassian.net/browse/SEEN-2385,[Auton]:HulkP1 [Optimization]:  Task-ssid_segment_onboarding_wireless.py-124-FEWssidsegmentonboarding  /   Test_TC4_DNAC_verify_SSID_lan_on_ECA_device  /   test3_cleanup_wireless_ssid,"*Reporter Analysis:*

We observed in TB7, during Hulk P1 optimization script execution, that the 'cleanup SSID' test case failed on ECA. As a result, ECA Provision  failed. Interestingly, the same sub-test case ran from the main script, and it passed. In the last few releases, we have consistently encountered this issue. Could you please check its priority?


*Hulk P1 -2.1.713.70263*
*Failed Log  :*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_segment_onboarding_wireless.py-124-FEWssidsegmentonboarding&begin=888490&size=1584977&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep26_21:39:36.380307.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ssid_segment_onboarding_wireless.py-124-FEWssidsegmentonboarding&begin=888490&size=1584977&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep26_21:39:36.380307.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]


+*Pass log  from Main script*+ 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=745018&size=835906&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep27_06:41:24.029172.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=745018&size=835906&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep27_06:41:24.029172.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]




*Found on:*
Uber ISO : Hulk P1 -2.1.713.70263
Polaris version: 17.12.1 (cco)

*Script file/Usecase :* lansanity_usecases_maps.yaml

*Source Team:  Sanity*

*Branch  Deatils :* private/HulkPatch-ms/sanity_api_auto

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] ",2023-09-28T19:11:46.488+0000,"This is a bug: similar to [https://cdetsng.cisco.com/webui/#view=CSCwh64084|https://cdetsng.cisco.com/webui/#view=CSCwh64084]

closing this auton ","['Auton', 'Blocked', 'HulkPatch1', 'Optimized', 'Sanity']",Moe Saeed,Cancelled,Omkar Sharad Wagh
SEEN-2386,https://miggbo.atlassian.net/browse/SEEN-2386,[Auton]:HulkP1 [Optimization]:Task-few_ap_clients_roaming_by_wpagent.py-146-FEWAPClientsWpagentTrafficRoaming,"*Reporter Analysis:*

We observed in TB7, during Hulk P1 optimization script execution, that the following TCs failed on *wpagent*.
Unfortunately, we don't have the full pass logs & wiki  &  feature  recording.
Could you please help us in fixing these issue:
[test1_start_wpagent_clients_and_validate_clients_joined|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-few_ap_clients_roaming_by_wpagent.py-146-FEWAPClientsWpagentTrafficRoaming&begin=978578&size=33179&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_11:08:09.452398.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
[test1_validate_client_roaming_stats|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-few_ap_clients_roaming_by_wpagent.py-146-FEWAPClientsWpagentTrafficRoaming&begin=1022225&size=9261&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_11:08:09.452398.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
[test1_start_wpagent_clients_and_validate_clients_joined_psk|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-few_ap_clients_roaming_by_wpagent.py-146-FEWAPClientsWpagentTrafficRoaming&begin=2002579&size=33413&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_11:08:09.452398.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
[test1_start_wpagent_clients_and_validate_clients_joined_dot1x|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-few_ap_clients_roaming_by_wpagent.py-146-FEWAPClientsWpagentTrafficRoaming&begin=2999753&size=33310&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_11:08:09.452398.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]




{noformat}
APA023.9F11.A6E8#
4246:  Not all clients found, expected: 0 , found:0 Failed to get DHCP success clients from output Print all MACs : ALL
show wpagent dp-stats all
Print all MACs : ALL


APA023.9F11.A6E8#
4252:  Not all clients are connected to the APs, expected 0 clients
4255:  Failed reason: Failed to start wpagent clients and validate clients joined{noformat}

*Hulk P1 -2.1.713.70263*
*Failed Log :*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-few_ap_clients_roaming_by_wpagent.py-146-FEWAPClientsWpagentTrafficRoaming&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_11:08:09.452398.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-few_ap_clients_roaming_by_wpagent.py-146-FEWAPClientsWpagentTrafficRoaming&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_11:08:09.452398.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]

*Script file/Usecase :* lansanity_usecases_maps.yaml

*Source Team:  Sanity*

*Branch  Deatils :* private/HulkPatch-ms/sanity_api_auto

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] ",2023-09-28T19:44:05.445+0000,,"['Auton', 'Feature', 'HulkPatch1', 'Optimized', 'Sanity']",Pawan Singh,Backlog,Omkar Sharad Wagh
SEEN-2387,https://miggbo.atlassian.net/browse/SEEN-2387,[Auton][Hulk]-[P1]Task-aca_security_groups_policy.py-155-policytAcaValidations  / Test_TC2_aca_test  /   test11_aca_create_policy,"*Reporter Analysis:*
We observed in TB7, during Hulk P1 optimization script execution with ISE 3.1 P6, ISE 3.3, and ISE 3.2, that 'test11_aca_create_policyr' TC failed with the following error

{noformat}2399: 
2400:  ############################################################
2401:  Some Policies don't exist in ISE or have different information [{'name': 'guest-guest', 'contract': 'contract_sol1'}, {'name': 'finance-finance', 'contract': 'contract_sol1'}, {'name': 'management-management', 'contract': 'contract_sol1'}, {'name': 'guest-finance', 'contract': 'contract_sol2'}, {'name': 'finance-guest', 'contract': 'contract_sol2'}, {'name': 'guest-management', 'contract': 'contract_sol1'}, {'name': 'management-guest', 'contract': 'contract_sol1'}, {'name': 'management-finance', 'contract': 'contract_sol1'}, {'name': 'finance-management', 'contract': 'contract_sol1'}]
2402:  ############################################################
2407: 
2408:  ############################################################
2409:  Some Policies don't exist in ISE or have different information [{'name': 'management-management', 'contract': 'contract_sol1'}, {'name': 'guest-finance', 'contract': 'contract_sol2'}, {'name': 'guest-management', 'contract': 'contract_sol1'}, {'name': 'management-finance', 'contract': 'contract_sol1'}]
2410:  ############################################################
2415: {noformat}

Could you  please  check  , 

+Pass LOG: ISE  2.1 P6+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-aca_security_groups_policy.py-152-policytAcaValidations&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul03_11:26:14.526196.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-aca_security_groups_policy.py-152-policytAcaValidations&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul03_11:26:14.526196.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] 

+*FAiled Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-aca_security_groups_policy.py-155-policytAcaValidations&begin=1139338&size=119138&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_11:08:09.452398.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-aca_security_groups_policy.py-155-policytAcaValidations&begin=1139338&size=119138&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_11:08:09.452398.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Found on:*
*Hulk P1 -2.1.713.70263*
Polaris version: 17.12
ISE :3.1 P6  (10.30.0.101= admin/Lablab123)
Cluster  details  :10.30.0.100(admin/Maglev123)

*Branch Name:*  private/HulkPatch-ms/sanity_api_auto

*Script file/Usecase:* optimization  Code  [-policytAcaValidations|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-aca_security_groups_policy.py-152-policytAcaValidations&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul03_11:26:14.526196.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*",2023-09-28T19:58:06.145+0000,"Hulk P1  RC3
 Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-aca_security_groups_policy.py-155-policytAcaValidations&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_04:11:44.631453.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-aca_security_groups_policy.py-155-policytAcaValidations&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_04:11:44.631453.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
 [~accountid:620b8357878c2f00729881c8]  can you follow this template when raising autons?
[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] Root Cause: Based on latest log from [~accountid:620b8357878c2f00729881c8]  for Hulk P1 RC3, the console was conflicted between {{policytAcaValidations}} and {{SDAFabricAssurance}}.

Solution: Moved {{policytAcaValidations}} to run separately since it needs console, VCR, IXIA.

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/741f33dd6b84dfcc26a0d94ce3d2faf594dd1fb1|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/741f33dd6b84dfcc26a0d94ce3d2faf594dd1fb1]","['Hulk', 'Optimized', 'Sanity', 'Uplift', 'auton']",Tran Lam,Resolved,Omkar Sharad Wagh
SEEN-2388,https://miggbo.atlassian.net/browse/SEEN-2388,[Auton]:[Optimized ]Hulk P1: Task-assurance_fabric_assurance.py-156-SDAFabricAssurance  /   Test_TC7_generate_ap_reachability_events  /   test1_generate_ap_reachability_event,"*Reporter Analysis:*
""During solution sanity in Hulk Optimization, we observed that '*test1_generate_ap_reachability_events* ' and '{{test1_generate_link_flap_issues}}' are failing in the optimized code. Those test cases are passing when rerun in legacy   script .  We need to fix the YAML mapping on the optimized code.""

*Branch Name:*  private/Hulk-ms/sanity_api_auto

*Script file/Usecase:* lansanity_usecases_maps.yaml

*UCG: 18=>“generateLinkFlapIssues” ,  15=>”SDAFabricAssurance*

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log: 1)*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-156-SDAFabricAssurance&begin=732155&size=243321&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_11:08:09.452398.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-156-SDAFabricAssurance&begin=732155&size=243321&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_11:08:09.452398.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*2)* {{Test_TC94_generate_link_flap_issues}}
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=730722&size=220853&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_10:10:01.749264.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=730722&size=220853&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_10:10:01.749264.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:*[*https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7*|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] 

{{TESTSCRIPT_FILE : testcases/forty_eight_hour/solution_test_sanityecamb_lan.py}}
*Passlog:*
[Test_TC94_generate_link_flap_issues|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=719378&size=80426&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug09_10:26:43.039712.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

2)[Test_TC93_generate_ap_reachability_events|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5891498&size=190381&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_05:04:47.281218.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-09-28T20:07:54.787+0000,,"['Auton', 'Hulk', 'Optimized', 'Sanity', 'Yamlmapping']",QuangVinh Nguyen,Backlog,Omkar Sharad Wagh
SEEN-2389,https://miggbo.atlassian.net/browse/SEEN-2389,[Auton]:Hulk P1: (9800) Radio Channel for 5GHZ (Maui AP) =Quad radio support on 9136 (slot-2 5GHz)  is coming in 17.13,"We observed in TB7, during Hulk P1/Hulk optimization script execution, that the following TCs failed on  *Maui AP C9136I-B -* AP687D.B45C.2054, 


|[test3_verify_AP_radio_utilization|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-171-assuranceHealthMetrics&begin=2449883&size=412343&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_19:24:38.875555.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]|
|[test4_verify_ap_radio_interference_scores|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-171-assuranceHealthMetrics&begin=2862226&size=411035&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_19:24:38.875555.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]|
|[test5_verify_AP_radio_noise|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-171-assuranceHealthMetrics&begin=3273261&size=404472&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_19:24:38.875555.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]|
|[test6_verify_ap_radio_air_quality|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-171-assuranceHealthMetrics&begin=3677733&size=408848&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_19:24:38.875555.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]|


As discussed with DE ([srevure.cisco.com|http://srevure.cisco.com]), Quad radio support on 9136 (slot-2 5GHz) is only coming in 17.13. Could you please add a condition for this AP? If the image is below 17.13, it should skip checking the TCs above  

*Hulk P1 -2.1.713.70263*


*Script file/Usecase :* lansanity_usecases_maps.yaml

*Source Team:  Sanity*

*Branch Deatils :* private/HulkPatch-ms/sanity_api_auto

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] 

!Manui_AP.mp4|width=1280,height=592!",2023-09-29T13:41:02.849+0000,"Hi [~accountid:620b8357878c2f00729881c8] 

As I observed on Sanity TB7, the {{C9130}} APs have triple radio option but it’s also no statistics of slot 2 (5 GHz) on AP360. Could you please check the image version we need to validate metric for those APs?

!image-20231006-070456.png|width=1869,height=757!

!image-20231006-070503.png|width=1832,height=871! Hi [~accountid:63f50bd34c355259db9ccc4d]  ,
As discussed on Webex, we are hitting an issue with the below AP. Please check the snippet below


!image-20231006-132423.png|width=1920,height=898!







!image-20231006-132546.png|width=1920,height=880! Raised PRs:

* HukP1: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7334/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7334/overview]
* HulkP2: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7336/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7336/overview]
* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7335/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7335/overview] Hi  [~accountid:63f50bd34c355259db9ccc4d]  ,

in recent  hulk  P1  RC3  tc failed  on  different AP  , could you  please  check 

Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-171-assuranceHealthMetrics&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_10:06:54.734792.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-171-assuranceHealthMetrics&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct21_10:06:54.734792.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS] Hi [~accountid:620b8357878c2f00729881c8] 

This is not an issue belonging to Radio Channel for 5GHZ 9136 AP series (Maui AP) as raised in this ticket. It failed due to unhandled scenarios after the nfw PR merged to the main line. Please create a new auton for a different issue.","['Auton', 'Hulk', 'HulkPatch1', 'Optimized', 'Sanity']",ThangQuoc Tran,Resolved,Omkar Sharad Wagh
SEEN-2390,https://miggbo.atlassian.net/browse/SEEN-2390,[Auton]:HulkTask-itsm_validations.py-231-itsmValid /test8_schedule_dev_deletionations/,"*Reporter Analysis:*
we  observed  in the latest  hulk  P 1  run  build  in    test8_schedule_dev_deletion failed with the below  error:
5917: 
Library group ""task"" method ""wait_for_task_complete"" returned in 0:00:00.029494

{code:python}2626:  Scheduling for """" failed for reason ""NCSS10065: Failed to validate schedule ITSM App ServiceNow is enabled, please use Schedule Later option instead of Now.""
2629:  Failed to schedule config preview deploy task!{code}

*Hulk  Patch ! Version* : 2.1.713.70263
*Script Name* :\solution_test_sanityecamb_lan.py
*script  file :* \services\dnaserv\lib\api_groups\itsm\group.py
+*Branch :*+ private/Hulk-ms/sanity_api_auto

*Testcases Impacted :* 
   [Test_TC2_ITSM_ticket_generation_test|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=23564&size=3692686&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep28_00:32:55.123854.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]  /  test8_schedule_dev_deletion

+*Fail Log :*+

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=370560&size=661586&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep28_00:32:55.123854.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=370560&size=661586&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep28_00:32:55.123854.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]


*Script file/Usecase :* lansanity_usecases_maps.yaml

*Source Team:  Sanity*

*Branch Deatils :* private/HulkPatch-ms/sanity_api_auto

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] 

*testbed is available today for debugging. (29th sep)*



!image-20230929-143805.png|width=1221,height=816!",2023-09-29T13:51:51.485+0000,"Hi [~accountid:63f50bcece6f37e5ed93c87e]  ,
Continuous TC failures in Hulk P1 , even when I tried the main branch

Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep28_00:32:55.123854.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep28_00:32:55.123854.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

##############################
[test8_schedule_dev_deletion|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=370560&size=661586&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep28_00:32:55.123854.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

2626: 
 Scheduling for """" failed for reason ""NCSS10065: Failed to validate schedule ITSM App ServiceNow is enabled, please use Schedule Later option instead of Now.""

{noformat}2629: 
 Failed to schedule config preview deploy task!{noformat}


Could you please prioritize ? You can use the cluster below  for  debugging
10.30.0.100(admin/Maglev123) Hi [~accountid:63f50bcece6f37e5ed93c87e]  could please check on priority , 
  *Hulk Patch 1 RC2 Version* : 2.1.713.70302
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=389161&size=654578&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct12_09:10:06.804017.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=389161&size=654578&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct12_09:10:06.804017.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] Hi [~accountid:620b8357878c2f00729881c8] , working on it. Have resolved the issue described in this auton but finding some other issues. Will post when done. Thanks [ENG-SDN / dnac-auto / e2b1c9bc95b - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e2b1c9bc95b16bb6845cfa2e8eead795afe22fb7] ← did you have this commit when you ran? Can you check? The above commit should fix tc8. On the testbed I was using, saw some other issues non-script related, so let me know if you run into issues in further testcases. Hi [~accountid:63f50bcece6f37e5ed93c87e] 
After  pulling the latest   code,  TC Failed  with the below  error,   
Please  use  TB 7  for  debugging 
*Cluster  details:  10.30.0.100(admin/Maglev123)*
 +*Optimized Failed log:*+
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-10/env_optimized_auto_job.2023Oct13_00:54:13.265275.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-10/env_optimized_auto_job.2023Oct13_00:54:13.265275.zip&atstype=ATS] 

+*Main script  failed Log:*+
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-10/env_auto_job.2023Oct13_01:32:31.576411.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-10/env_auto_job.2023Oct13_01:32:31.576411.zip&atstype=ATS]

*Merge code:*  [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7421/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7421/overview] 

[test9_reject_dev_deletion_request|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=664444&size=10771&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct13_00:54:13.265275.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] => Failed with  below  response  :

{code:python}1662:   External API handler called:
1663: 
1664:  Resource path: /v1/integration-event-details/
1665:  Method: GET

1666:  {'params': {'eventName': 'SDA Provision Creation Request', 'timeFilter': 1697183611759}}
1667:  Resource path full url: https://10.30.0.100/dna/intent/api/v1/integration-event-details
1668:  Library method ""ext_api_call_handle"" returned in 0:00:00.044219
1669:  {'eventName': 'SDA Provision Creation Request', 'headers': [{'title': 'Event Id', 'key': 'DNAEventID', 'link': 'viewHistory'}, {'title': 'Source', 'key': 'source'}, {'title': 'Destination', 'key': 'destination'}, {'title': 'ITSM Workflow', 'key': 'ITSMWorkflow'}, {'title': 'DNA Event Status', 'key': 'DNAEventStatus'}, {'title': 'ITSM Status', 'key': 'ITSMStatus'}, {'title': 'ITSM Id', 'key': 'ITSMEntityId'}, {'title': 'ITSM Link', 'key': 'ITSMLink', 'link': 'self'}, {'title': 'ITSM Ticket Assigned To', 'key': 'assignedTo'}, {'title': 'ITSM Last UpdatedTime', 'key': 'ITSMLastUpdatedTime', 'format': 'MMMM Do YYYY, h:mm:ss a'}, {'title': 'ITSM Entity Severity/Priority', 'key': 'ITSMEntitySeverity'}, {'title': 'DNA Event Severity', 'key': 'DNAEventPriority'}, {'title': 'ITSM Message', 'key': 'ITSMMessage'}], 'rows': [], 'groupedColumns': [{'title': 'Last In-Event Flow', 'keys': ['source', 'destination']}], 'sortBy': 'ITSMLastUpdatedTime', 'sortOrder': 'desc'}
1670:  Library group ""itsm"" method ""get_integration_events"" returned in 0:00:00.044749
1671:  No ITSM integration event for group based policy found within the timeframe
1672:  Library group ""itsm"" method ""get_latest_event"" returned in 0:00:00.045305
1673:  Latest event found with id: None
1674:  Latest event id is 'None'. Please check the status of MidServer, if it's UP.
1675:  Library group ""itsm"" method ""reject_latest_event"" returned in 0:00:00.045851
1676:  Test returned in 0:00:02.164240
1677:  Failed reason: SGT change request approved unsuccessfully
1678:  The result of section test9_reject_dev_deletion_request is => FAILED{code}


+*Mid Server  is  up:*+


!image-20231013-085341.png|width=1282,height=138!

  [ENG-SDN / dnac-auto / e7a9ac9b93c - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e7a9ac9b93c633d671e69defe2033be7ef10a3b6] ← The below was a different issue than the original auton, addressed here. It was an api upliftment, so i have updated the label for this auton [~accountid:63f50bcece6f37e5ed93c87e]  

Observed the same issue again today on EXSI VM *Hulk-Patch1 #3.713.75159* build 

_280:   External API handler called:_
_281:_
_282:  Resource path: /v1/integration-event-details/_
_283:  Method: GET_

_284:  {'params': {'eventName': 'Group Based Policy', 'timeFilter': 1697637949222}}_
_285:  Resource path full url:_ [_https://10.22.45.61/dna/intent/api/v1/integration-event-details_|https://10.22.45.61/dna/intent/api/v1/integration-event-details]
_286:  Library method ""ext_api_call_handle"" returned in 0:00:00.240389_
_287:  {'eventName': 'Group Based Policy', 'headers': [{'title': 'Event Id', 'key': 'DNAEventID', 'link': 'viewHistory'}, {'title': 'Source', 'key': 'source'}, {'title': 'Destination', 'key': 'destination'}, {'title': 'ITSM Workflow', 'key': 'ITSMWorkflow'}, {'title': 'DNA Event Status', 'key': 'DNAEventStatus'}, {'title': 'ITSM Status', 'key': 'ITSMStatus'}, {'title': 'ITSM Id', 'key': 'ITSMEntityId'}, {'title': 'ITSM Link', 'key': 'ITSMLink', 'link': 'self'}, {'title': 'ITSM Ticket Assigned To', 'key': 'assignedTo'}, {'title': 'ITSM Last UpdatedTime', 'key': 'ITSMLastUpdatedTime', 'format': 'MMMM Do YYYY, h:mm:ss a'}, {'title': 'ITSM Entity Severity/Priority', 'key': 'ITSMEntitySeverity'}, {'title': 'DNA Event Severity', 'key': 'DNAEventPriority'}, {'title': 'ITSM Message', 'key': 'ITSMMessage'}], 'rows': [], 'groupedColumns': [{'title': 'Last In-Event Flow', 'keys': ['source', 'destination']}], 'sortBy': 'ITSMLastUpdatedTime', 'sortOrder': 'desc'}_
_288:  Library group ""itsm"" method ""get_integration_events"" returned in 0:00:00.241072_
_289:  No ITSM integration event for group based policy found within the timeframe_
_290:  Library group ""itsm"" method ""get_latest_event"" returned in 0:00:00.242232_
_291:  Latest event found with id: None_
_292:  Latest event id is 'None'. Please check the status of MidServer, if it's UP._
_293:  Hulk api has different event_name, trying with latest description of task instead_
_294:  itsm latest description is set to: Adding sgt 21 to test ITSM 1697638506_8289258_



*Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct18_07:12:44.429231.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct18_07:12:44.429231.zip&atstype=ATS] [~accountid:63f50bd68ab3d6a635ecc29b] this looks like a different issue… did you test manually? And was there a pass log previously for this testbed? [~accountid:63f50bd68ab3d6a635ecc29b] looks like some issue with enabling itsm, which branch did you use? This seems like a defect on esxivm side. Basically, the api to configure the bundle only works when it has cookies, which should not be the expected behavior. 

[https://10.22.45.61/api/dnacaap/v1/dnacaap-app-services/bundle-settings/5b93-7b65-49d8-b1f0|https://10.22.45.61/api/dnacaap/v1/dnacaap-app-services/bundle-settings/5b93-7b65-49d8-b1f0]

Above is the api in question. It needs X-JWT-ACCESS-TOKEN to work for some reason… otherwise without the token, the response is still successful: {{""Configuration saved successfully""}} but when you go to Visibility and Control of Configurations, ITSM is not able to be configured.

I am reaching out to devs. Hi [~accountid:63f50bcece6f37e5ed93c87e] 

I have used branch is - *private/HulkPatch-ms/api-auto*

Don’t have pass logs for this TCS, integrating the TC in EXSI VM testbed.

Regards,

Raghavendra B. M  Hi [~accountid:63f50bcece6f37e5ed93c87e] 
Recent  Hulk   P1  RC 3  below tc skipped  , &  cleanup  tc is failed 
could please  check 


||[test14_request_provision_wlc|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=2181647&size=2031&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct22_07:33:29.228418.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]     => Skipped||


[test17_cleanup|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=3377993&size=1514938&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct22_07:33:29.228418.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]  =failed  

{noformat}11403:  Task is still running, wait for some time and retry.
11404:  Retrying

11475:  No AI RF Profile or Not Associated With Site!!!!. Falling Back To Basic Profile for Provisioning!!
11969:  No conflict found for ['TB7-eWLC.cisco.com'] and namespaces APG_e4764d97-e72f-4ce8-ab51-4279140adb23_TYPICAL
12773:  Task is still running, wait for some time and retry.
12774:  Retrying
12781:  Task is still running, wait for some time and retry.
12782:  Retrying

12840:  Error in provisioning unified aps
12841:  Wireless segments for ny dev onboarded and aps provisioned failure
12843:  Failed reason: Device not added back correctly{noformat} [~accountid:620b8357878c2f00729881c8] provision wlc is skipping because theres no aireos I guess. 

As for cleanup part: [ENG-SDN / dnac-auto / ea210a2494a - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/ea210a2494ad22b7374e777972f5dbf6524f6967] ← this should resolve it, its a trivial failure. Thanks [~accountid:63f50bcece6f37e5ed93c87e]  ,

We  got  full pass log Hulk P1  RC5, hence  moving  close state 
Passlog:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct29_03:55:46.515036.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct29_03:55:46.515036.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Blocked', 'Feature', 'Hulk', 'Hulk-Patch1', 'HulkPatch', 'Optimized', 'Sanity', 'Uplift']",Andrew Chen,Closed,Omkar Sharad Wagh
SEEN-2394,https://miggbo.atlassian.net/browse/SEEN-2394,[Auton]:Test_TC2_wired_app_policy ,"*Reporter Analysis:*
TB7, during Hulk P1/Hulk optimization script, 'Test_TC2_wired_app_policy' failed to change device roles
{{1435: Device role is as expected for dev :: TB7-SJ-eCA-BORDER-CP and role :: BORDER ROUTER}}

*NOTE ::* Devices roles should be *ACCESS or DISTRIBUTION* for the policy to be applied*.* 

+*wiki*+:[https://wiki.cisco.com/pages/viewpage.action?pageId=682726437|https://wiki.cisco.com/pages/viewpage.action?pageId=682726437]


+*Failed Log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_23:52:44.876851.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep27_23:52:44.876851.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]


+*After  changing  role Manually  verified :*+


*Hulk Patch 1  Version* : 2.1.713.70263

*script file :* \services\dnaserv\lib\api_groups\itsm\group.py
+*Branch:*+ private/Hulk-ms/sanity_api_auto

*Script file/Usecase:* lansanity_usecases_maps.yaml

*Source Team:  Sanity*

*Branch Details:* private/HulkPatch-ms/sanity_api_auto




!TC96_script  issue.png|width=1152,height=529!




+*After  changing roles Manually  verified :*+

!TC96_manullay _verfied .png|width=1152,height=532!",2023-09-29T15:45:31.989+0000,"[~accountid:620b8357878c2f00729881c8] The below information are required when you file an autons. Please provide:

* Your debug/Analysis for the issue.
* Was the testcase passed before in this testbed? When? Please provide Passed log.
* Was the testcase passed in different testbeds? Hi [~accountid:62d2fe9f8afb5805e5d5af49]  ,
{{1435: Device role is as expected for dev :: TB7-SJ-eCA-BORDER-CP and role :: BORDER ROUTER}}
After changing the role for ECA  &  Transit Border to Distribution, I was able to manually deploy the wired policy. We have attached a screenshot in the attachment. Please check

+*Passlog:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=407812&size=1058452&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep23_06:04:20.686044.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=407812&size=1058452&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep23_06:04:20.686044.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] Hi  [~accountid:63f50bd34c355259db9ccc4d]  ,

I haven't observed the issue during the latest Hulk P1 RC1 run. If the issue occurs next   hulk run   , I will report it in Jira

Pass log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=339791&size=1077185&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct04_23:58:41.744143.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-application_policy.py-183-policyApplicationPolicy&begin=339791&size=1077185&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct04_23:58:41.744143.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]","['Auton', 'Blocked', 'HulkPatch1', 'Optimized', 'Sanity']",ThangQuoc Tran,Closed,Omkar Sharad Wagh
SEEN-2395,https://miggbo.atlassian.net/browse/SEEN-2395,Auton-Hulk EXSI VM Upgrade: Test_TC14_remove_devices_from_fabric/test1_remove_devices_from_fabric,"During clean up script  execution observed the, Script failing to remove ECA BORDER device from fabric page.

Tried to manually able to delete the ECA BORDER device successfully. 

*Failed log:*  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1602504&size=802826&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep29_04:53:48.591515.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1602504&size=802826&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep29_04:53:48.591515.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*Branch Name:  private/Hulk-ms/api-auto*

*Script file/Usecase:*  dnac_cleanup_script.py

*Source Team:  Upgrade Sanity*



Regards,

Raghavendra B.M

  ",2023-09-29T17:15:16.983+0000,"why the error is complaining that device can not be added without control plane if we are doing removal: 


{noformat}""Device with a Border Node or Edge Node role can't be added without a Control Plane in the Fabric Site. Add a Control Plane to the Fabric Site before adding devices with a Border Node or Edge Node role to the Fabric Site."", 'triggeredJobId': '196cecec-4875-49fa-9350-c0e076d22029'}
6933:  activity_id is False. Config preview task failed for description Scheduling task for Removing devices from fabric for fabric ['Global/USA/New_York'] at time 1695989512.1515138 - Configuration Preview{noformat} [~accountid:63f50bd68ab3d6a635ecc29b] 
I need your help here, how did you remove the border, and why you got this error? 

Did you have to remove the edges before the control plane/FIAB? I see this provsiioning error: 

{{'status': 'FAILED', 'failureReason': ""NCSP11017: Operation failed on '1' devices.""}}

is this a defect? and what is the reason it failed.  Removed ECA BORDER device manually from inventory page, it’s deleted successfully.  

Manually didn’t remove any other  control plane/FIAB  device used script only. closing the ticket:

Please follow the template:

[Automation support Checklist - EDPEIXOT Solution Engineering Team Wiki - IT Wiki (cisco.com)|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist]","['Auton', 'Cleanup', 'ESXi', 'Sanity', 'Upgrade']",Moe Saeed,Cancelled,Raghavendrachar Baraguru Mallesha Char
SEEN-2396,https://miggbo.atlassian.net/browse/SEEN-2396,Auton-Hulk Upgrade: Test_TC5_remove_all_ext_node_from_fabric/test3_clear_subtended_devices_from_fabrics_and_inventory,"*Reporter Analysis:*

 During Clean up script execution observed, failed to clear all Extended node interface config from fabric post assignent.

Observed  below EXT node & interface clear port config not done

{{EXT node name : SN-FDO2417J8N2}}  - This is REP RING feature one of the EXT node

{{Interface:- TenGigabitEthernet1/26}}



_7319:  Resource path full url:_ [_https://10.22.45.217/api/v1/network-device/901/100_|https://10.22.45.217/api/v1/network-device/901/100]
_7320:  Library group ""extended_node"" method ""check_devices_sync_status"" returned in 0:00:00.323240_
_7321:  Clients:{'name': 'SN-FDO2433J87B', 'type': 'exnode', 'intf': [<Interface object 'TenGigabitEthernet1/26' on 'SN-FDO2433J87B' at 0x7f68ec0471f0>], 'remote_intf_list': [<Interface object 'TenGigabitEthernet1/26' on 'SN-FDO2417J8N2' at 0x7f68ec047310>]}_



TC:-  Test_TC5_remove_all_ext_node_from_fabric/test3_clear_subtended_devices_from_fabrics_and_inventory

Failed log: - [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=548686&size=1814623&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep29_02:08:49.525852.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=548686&size=1814623&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_auto_job.2023Sep29_02:08:49.525852.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*Branch Name:  private/Hulk-ms/api-auto*

*Script file/Usecase:*  dnac_cleanup_script.py

*Source Team:  Upgrade Sanity*

*Executing on first time clean up script on EXSI VM testbed*

*On prem testbed it’s got passed:*

Trade log:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_20:45:08.147352.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_20:45:08.147352.zip&atstype=ATS]

 

Regards,

Raghavendra B.M",2023-09-29T17:38:21.549+0000,"[~accountid:63f50bd68ab3d6a635ecc29b] 
The below information are required when you file an autons. Please provide.
- Your debug/Analysis for the issue.
- Was the testcase passed before in this testbed? When? Please provide Passed log.
- Was the testcase passed in different testbeds? Updated Required info as mentioned below. I do not see this {{SN-FDO2417J8N2}} in the input file? and Neither S_N-FDO2433J87B_ Need more info Cancel this Auton, since no more info received. 
Please update the ticket with more info.

Follow the template: [Automation support Checklist - EDPEIXOT Solution Engineering Team Wiki - IT Wiki (cisco.com)|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist]","['Auton', 'Cleanup', 'hulk-vm-sanity']",Moe Saeed,Cancelled,Raghavendrachar Baraguru Mallesha Char
SEEN-2397,https://miggbo.atlassian.net/browse/SEEN-2397,"[Auton]:[Optimized ]Hulk P1:Task-generate_link_flap_issues.py-184-generateLinkFlapIssues
","*Reporter Analysis:*
During solution sanity in Hulk Optimization & Main script, the flap issue event is not generating. We tried manually, but we're not receiving any API response. Could you please check and confirm whether it's a defect or a script issue?""'{{test1_generate_link_flap_issues}}' are failing in the optimized code. 

*Branch Name:*  private/Hulk-ms/sanity_api_auto

*Script file/Usecase:* lansanity_usecases_maps.yaml

*UCG: 18=>“generateLinkFlapIssues” ,*  

*Source Team:  Sanity*

*Issue Seen first time or day0 issue:*

*Fail Log:*

*2)* {{Test_TC94_generate_link_flap_issues}}
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=730722&size=220853&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_10:10:01.749264.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=730722&size=220853&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fenv_auto_job.2023Sep28_10:10:01.749264.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed details:*[*https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7*|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] 

{{TESTSCRIPT_FILE : testcases/forty_eight_hour/solution_test_sanityecamb_lan.py}}
*Passlog:*
[Test_TC94_generate_link_flap_issues|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=719378&size=80426&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug09_10:26:43.039712.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Hulk P1 -2.1.713.70263*
*Device image => 12.12.1  cco* 
*Ise = 3.1 P6* ",2023-09-29T17:38:59.191+0000,"UC18.4 [""generateLinkFlapEvent""]
Pass log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-generate_link_flap_event.py-184-generateLinkFlapEvent&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct12_01:59:06.035352.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-generate_link_flap_event.py-184-generateLinkFlapEvent&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct12_01:59:06.035352.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Hulk', 'Optimized', 'Sanity', 'Yamlmapping']",Amardeep Kumar,Closed,Omkar Sharad Wagh
SEEN-2402,https://miggbo.atlassian.net/browse/SEEN-2402,"[Auton] - Script failures under DR Cleanup, DR ping TCs which also results incorrect Active DR node IP selection during execution","*Regression & Sanity:* Solution Regression Multisite - DR+MDNAC

*Branch Used:* 

*private/HulkPatch-sand927/api-auto* which was pulled from *private/HulkPatch-ms/api-auto* branch, has all the correct data pulled as of 27th September. Since we had to make temporary testbed file related changes to skip PRIME related TCs, we created a temporary branch to execute. Please note that this branch has all the necessary latest data as per what is available on main code base branch at that point.

*Uber ISO tested:* Hulk Patch1 - 2.1.713.70263

*Scripts Used:*

testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

*Description :* 

1) During the regular script execution we see TCs related DR cleanup and DR ping, where “Checking reachability from fusion to main, recovery, witness” is being performed are getting errored out due to incorrect ping cli ping passed on the Fusion device. This always used to work fine, and somehow now it has got broken.

2) Due to this we see DR specific TCs are getting affected getting blocked. Further strangely the execution has continued and picked 10.195.243.123 as cluster IP instead of 10.195.243.109 ,to execute further which should not be done. As per script design and implementation, 10.195.243.109 IP has to be used as Active DR node cluster IP and 10.195.243.123 has to be used as Recovery DR node cluster IP. As a consequence of this our further executions got impacted.

3) Also upon any DR Basic TCs are getting failed, further TCs execution get blocked instead of propagating. This also needs to be fixed.



*Failed log 1:*  Initial execution - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac_dr.2023Sep28_10:24:51.720126.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac_dr.2023Sep28_10:24:51.720126.zip&atstype=ATS] → Refer TC4.1, TC5, TC6.24

TC6.28 → It has picked 10.195.243.123 as cluster IP to proceed with execution instead of 10.195.243.109. And all further TCs execution till TC35 has picked .123 IP address and used it for DNAC configuration.

*Failed log 2*: Re-execution to check if its reproducing - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac_dr.2023Sep28_21:19:46.660398.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac_dr.2023Sep28_21:19:46.660398.zip&atstype=ATS] → Refer TC4.1 

*Failed log 3:* DR specific TCs execution - [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac.2023Oct02_22:02:18.975974.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac.2023Oct02_22:02:18.975974.zip&atstype=ATS] → Refer TC200.1



*Previous Pass log on Hulk Patch1- 2.1.713.70207:* 

 [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac.2023Sep18_13:31:26.691076.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac.2023Sep18_13:31:26.691076.zip&atstype=ATS] → Refer TC4, TC5, TC6.28
",2023-10-03T12:23:19.089+0000,"The issue happens since there were changes in fusion lp_ip due to this commit [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9611df41cd815b361f5e6689ff710b65c999c1ac#services/commonlibs/sftopology.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/9611df41cd815b361f5e6689ff710b65c999c1ac#services/commonlibs/sftopology.py]

Added handle for it: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/90b73e52ba40962e4014e0e59f4359f48b41d048|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/90b73e52ba40962e4014e0e59f4359f48b41d048]","['Auton', 'Execution', 'HulkPatch', 'MSTB1', 'Multisite']",Tran Lam,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2409,https://miggbo.atlassian.net/browse/SEEN-2409,AWS Sanity :Hulk[Auton]:Test_TC1_enhanced_lan_automation  /   test2_lan_automation_level2  ,"*Reporter Analysis:*

Recently on AWS Sanity we observed that before edge device coming to inventory lan automation stopped abruptly.
All other devices onboarded via pnp successfully. Manually i tried it went fine



*Description*:  

{{Reload command is being issued on Active unit, this will reload the whole stack Proceed with reload? [confirm] }}

{noformat}3038: 
 Timeout occurred : Timeout value : 5{noformat}

{noformat}3039: 
  Target: TB2-DMZ-SJ-EDGE{noformat}

{noformat}3040: 
  Command sent: n{noformat}

{noformat}3041: 
  Pattern: 'Proceed with reload? [confirm]'{noformat}

{noformat}3042: 
  Got: 'wr erase\r\n************************************************************************************************************\r\nErasing Nvram will not clear license trust code.\r\n************************************************************************************************************\r\nErasing the nvram filesystem will remove all configuration files! Continue? [confirm]y[OK]\r\nErase of nvram: complete\r\nSwitch#\r\nSwitch#reload\r\n\r\nSystem configuration has been modified. Save? [yes/no]: n\r\nReload command is being issued on Active unit, this will reload the whole stack\r\nProceed with reload? [confirm]'{noformat}

{noformat}3085: 
 Device is not in expected state :Provisioned,current state:Unclaimed{noformat}

{noformat}3095: 
 Device is not in expected state :Provisioned,current state:Unclaimed{noformat}

{noformat}3097: 
 Device is not in expected state :Provisioned,current state:Unclaimed{noformat}

{noformat}3107: 
 Device is not in expected state :Provisioned,current state:Planned{noformat}

{noformat}3109: 
 Device is not in expected state :Provisioned,current state:Unclaimed{noformat}

{noformat}3119: 
 Device is not in expected state :Provisioned,current state:Planned{noformat}

{noformat}3131: 
 Device is not in expected state :Provisioned,current state:Onboarding{noformat}

{noformat}3165: 
 ERROR: Device host name is null{noformat}

{noformat}3196: 
 Devices in managed state: 3, expected in managed state 4{noformat}

{noformat}4005: 
 ERROR: Device host name is null{noformat}

{noformat}4027: 
 ERROR: Device host name is null{noformat}

{noformat}4049: 
 ERROR: Device host name is null{noformat}

{noformat}4071: 
 ERROR: Device host name is null{noformat}

{noformat}4201: 
 Device Connectivity issue, not recoverable through resyn, check why device in this state.{noformat}

{noformat}4202: 
 ERROR: Device TB2-DMZ-SJ-EDGE is not in managed state, current state:Partial Collection Failure, errorDescription:NCIM12013: SNMP timeouts are occurring with this device. Either the SNMP credentials are not correctly provided to Cisco DNA Center or the device is responding slow and SNMP timeout is low. If it’s a timeout issue, Cisco DNA Center will attempt to progressively adjust the timeout in subsequent collection cycles to get device to managed state. User can also run discovery again only for this device using the discovery feature after adjusting the timeout and SNMP credentials as required. Or user c{noformat}

*Branch Name:* private/HulkPatch-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  AWS Sanity

*Please find details in below webex space:*
webexteams://im?space=b5afae10-61ad-11ee-9f59-ddc0ce99b1d4

Issue Seen first time or day0 issue

**Fail Log:**Hulk P1 #2.3.7.3-70263
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-underlay_automation_and_validations.py-71-LANAutomation&begin=506200&size=693447&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct03_06:03:43.826982.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-underlay_automation_and_validations.py-71-LANAutomation&begin=506200&size=693447&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct03_06:03:43.826982.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Pass Log:*Hulk RC3
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-underlay_automation_and_validations.py-71-LANAutomation&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug04_05:17:16.833956.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-underlay_automation_and_validations.py-71-LANAutomation&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-08%2Fenv_optimized_auto_job.2023Aug04_05:17:16.833956.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-10-04T08:14:23.358+0000,"Reassigning to Moe as looks like its related to [Source of group.py - dnac-auto - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/diff/services/dnaserv/lib/api_groups/inventory/group.py?until=e0b33aa2a49fcc0835f6b363bdbd8a077453a47f&at=refs%2Fheads%2Fprivate%2FHulkPatch-ms%2Fapi-auto] Ill add an enhancement to support the changes. A small change added: 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/8e2f589f8f163d5097388dade86834f4cb98b996|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/8e2f589f8f163d5097388dade86834f4cb98b996] [~accountid:63f50bcece6f37e5ed93c87e] I have observed the same issue in the Hulk Patch 2 
Branch: *private/HulkPatch2-ms/sanity_api_auto*
*Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=374725&size=1262440&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct10_03:11:02.103814.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=374725&size=1262440&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct10_03:11:02.103814.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] Hi [~accountid:63f50bcece6f37e5ed93c87e]  / [~accountid:63f50bfce8216251ae4d59d5] 

AWS Sanity my device came to inventory and pnp or lan was susccess on UI this time  but my device went to unreachable state as provision failed:

*Failed Log from private/HulkPatch-ms/sanity_api_auto  Optimised suite:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-underlay_automation_and_validations.py-71-LANAutomation&begin=4071&size=1068213&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct12_02:47:19.634353.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-underlay_automation_and_validations.py-71-LANAutomation&begin=4071&size=1068213&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct12_02:47:19.634353.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



*Same TC Passed log from private/Hulk-ms/sanity_api_auto Non-Optimised code:*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct12_05:53:19.038653.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct12_05:53:19.038653.zip&atstype=ATS]



Please prioritize the auton [ENG-SDN / dnac-auto / d913bc51db3 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/d913bc51db35cd356f3d448ed581007593a76a04] ← Please try with this commit, which has been added to Hulk, p1 and p2 branches. I added 5 min of retry which should be enough, but if not let me know.","['AWS_Sanity', 'Auton', 'HulkPatch1']",Andrew Chen,Resolved,Anusha John
SEEN-2410,https://miggbo.atlassian.net/browse/SEEN-2410,[Auton]:Guardian: Test_TC45_DNAC_TSIM_static_onboarding_verifications/test5_onboard_ap_dot1x/test6_prepare_aps_dot1x/test7_pnp_onboard_dot1x_ap/test8_wait_ap_to_be_provisioned,"*Reporter Analysis:*

TC is not even fetched pnp AP from YAML but Testcase passed.

It should not false pass either skip or block the tc

*Description*:  

{noformat}223686: 
 Cannot track test: tracking auth info must be set in order to transfer test tracking data{noformat}

{noformat}223687: 
 Executing testcase Test_TC45_DNAC_TSIM_static_onboarding_verifications test 45.8 ""test8_wait_ap_to_be_provisioned"".{noformat}

{noformat}223688: 
 ISE version: 3.1.0.518 patch: 3{noformat}

{noformat}223689: 
 Library group ""ise_integration"" method ""is_ISE_meet_requirement"" returned in 0:00:00.000249{noformat}

{noformat}223690: 
 Waiting for the APs to Provisioned{noformat}

{noformat}223691: 
 No pnp APs are expected to be provisioned, please add some pnp APs to the testbed{noformat}

{noformat}223692: 
 Test returned in 0:00:00.001847{noformat}

{noformat}223693: 
 Passed reason: Provisioning AP is Success{noformat}

{noformat}223694: 
 The result of section test8_wait_ap_to_be_provisioned is => PASSED{noformat}



*Branch Name:* Guardain-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

*False Pass log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=37731986&size=2510905&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct03_03:30:08.924356.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=37731986&size=2510905&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct03_03:30:08.924356.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
",2023-10-04T12:32:03.733+0000,"As discussed, the issue with not picking the pnp AP type is due to wrong name added in link interface section in yaml. need to give both link name as same for both ends of link. 



Will check on the condition to be skip if none of the AP type claim PNP in yaml  This issue solve by adding below line of code in subtest onboard_ap_dot1x

if not dnac_handle.dnaconfig.pnp_ap_clients():

     self.skipped(“message”,goto=[“next_tc”]
 Thanks [~accountid:61efa8c457b25b006877eda3]  for quickly verify the local fix , i will open a PR 
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-10/env_auto_job.2023Oct10_20:46:48.428667.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-10/env_auto_job.2023Oct10_20:46:48.428667.zip&atstype=ATS] 2410 - added check to verify if pnp_ap_list is empty or not before proceed rest of the sub case , else go to next_tc [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7387/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7387/overview]","['AWS_Sanity', 'Auton', 'Ghost', 'Guardian', 'Hulk', 'HulkPatch1', 'Sanity']",Vinoth Kumar Kutty Krishnamoorthy,Resolved,Anusha John
SEEN-2411,https://miggbo.atlassian.net/browse/SEEN-2411,[Auton]:Guardian: DOT1X Testcase should be skipped and subtc should be blocked or skipped if ISE version didn't meet 3.1,"*Reporter Analysis:*

TC Should be skipped or blocked accordingly  if TB is with ISE<3.1 version  but observed TC is executing



*Branch Name:* Guardain-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

*TC log which should be modified:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=37731986&size=2510905&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct03_03:30:08.924356.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=37731986&size=2510905&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct03_03:30:08.924356.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-10-04T12:43:21.689+0000,"I see the supported code is already present to check the ISE version in that case, 
In this current log the pnp list is empty thats why the case has executed and pass, if you fix your yaml then the PnPlist will pick correctly  [~accountid:5fe224a53b5e47013862f185] please add it to Guardian Branches also as sanity we are testing with older releases also [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7388/diff#testcases/forty_eight_hour/solution_test_sanityecamb_cert_lan.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7388/diff#testcases/forty_eight_hour/solution_test_sanityecamb_cert_lan.py]","['AWS_Sanity', 'Auton', 'Ghost', 'Guardian', 'Hulk', 'HulkPatch1', 'Sanity']",Vinoth Kumar Kutty Krishnamoorthy,Resolved,Anusha John
SEEN-2412,https://miggbo.atlassian.net/browse/SEEN-2412,Auton-HulkP1: Test_TC28_DNAC_Device_Provisioning  /   test2_verify_provision_the_devices_fabric1 ,"*Reporter Analysis:*

Filed a bug:

[https://cdetsng.cisco.com/webui/#view=CSCwh36735|https://cdetsng.cisco.com/webui/#view=CSCwh36735]

which moved to Auton.

Advanced SSID Model Config cannot be provisioned against a plain switch. Auton script needs to be checked to correct the same.

the removed SSIDs from needs to be removed from the model configs



*Description:*

{noformat}21808: 
 Generating config previews in error for dev TB8-SJ-BORDER-CP-9400-SVL : {'version': 1692717893778, 'startTime': 1692717891776, 'data': 'workflow_id=0;cfs_id=0;rollback_status=not_supported;rollback_taskid=0;failure_task=NA;processcfs_complete=false', 'endTime': 1692717893778, 'progress': 'TASK_PREVIEW_GENERATION', 'errorCode': 'NCNP50000', 'serviceType': 'NCSP', 'failureReason': 'Unable to push configuration to device 204.1.2.2', 'isError': True, 'instanceTenantId': '64e3444cd615a745e3117801', 'id': '018a1dd9-40c0-7174-8d28-7032fdc7eb6a'}{noformat}

{noformat}21812: 
 Config preview generation failure{noformat}

*Branch Name:* Hulk-ms/sanity_api_auto[not sure about branch used]

**Script* *file:** solution_test_sanityecamb.py

*Source Team:*  Sanity

*NOTE:*

Issue Not seen now in Sanity



*Failed Log from Hulk #*{{2.3.7.3-70139}}

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5395401&size=1139260&archive=%2Fusers%2Fajadhav2%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug22_08:01:31.566003.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=ajadhav2&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5395401&size=1139260&archive=%2Fusers%2Fajadhav2%2F.pyats%2Farchive%2F23-08%2Fenv_auto_job.2023Aug22_08:01:31.566003.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=ajadhav2&from=trade&view=all&atstype=pyATS]",2023-10-04T13:03:14.619+0000,"[~accountid:63f50bcece6f37e5ed93c87e] : Could you please have a look into this on priority?



*Branch used -* private/HulkPatch-ms/api-auto

*Testbed affected -* Multisite DR testbed

*Description:*
During Solution Regression testing on *Hulk Patch1 Uber ISO - 2.1.713.70263*, we had observed Model config provision failure and it impacted multiple TCs during execution as well as feature integration. On confirmation with Pushpa from Wireless team it was confirmed that its issue related to - *CSCwh36735.* Reference Team space - webexteams://im?space=00bf3090-61ec-11ee-8bc4-49ac8587966e

RCA, relevant logs, Failed task details and Failed script logs details attached to Box link -
[https://cisco.box.com/s/nt0aysipvnxgmzdxf6k0okwx5pf3em45|https://cisco.box.com/s/nt0aysipvnxgmzdxf6k0okwx5pf3em45|smart-link] 

Below are set of logs in chronology leading to the issue:

1)Initial basic device provisioning - 

[Test_TC27_DNAC_Device_Provisioning|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5019208&size=1816097&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac_dr.2023Sep29_08:51:29.024392.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]->Refer TC27.1

2)Device Re-provisioning - 

[Test_TC28_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6835305&size=9837792&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac_dr.2023Sep29_08:51:29.024392.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]-> Refer TC28.1 & TC28.10

 3)Fabric provisioning of Devices - 

[Test_TC32_DNAC_assigne_dhcp_role_deploy_device_in_fabric|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6810859&size=3549537&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac_dr.2023Sep29_10:33:20.745594.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]-> Refer TC32.1 to TC32.4

 4)Enable fabric wireless on ECA devices - 

[Test_TC33_enable_fabric_wireless_eca|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10360396&size=2626915&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac_dr.2023Sep29_10:33:20.745594.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]-> Refer TC33.1

 5)Fabric re-provision after segment to SSID mapping - 

[Test_TC48_DNAC_Verify_provision_status_after_segments_added|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=38016437&size=15075233&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac_dr.2023Sep29_10:33:20.745594.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]-> Refer TC48.4

 6)Cleanup SSIDs scenario - 

[Test_TC52_DNAC_all_aps_verification_in_wlc|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=56128341&size=1631893&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac_dr.2023Sep29_10:33:20.745594.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]-> Refer TC52.3

 7) Provision devices after interface onboarding for wired client interfaces and ixia interfaces - 

[Test_TC81_dcs_magellan_onboarding_clients_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=21238668&size=15422739&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-09%2Fsr_mb_multi_sites_mdnac_dr.2023Sep30_08:51:09.461021.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]-> Refer 81.10



We are currently ongoing Solution Regression testing *Hulk Patch1 RC1 Uber ISO - 2.1.713.70292 on Multisite DR testbed* and its going to get impacted due to this. So request to help to prioritize on this. 



  Regards
Sandeep S","['Auton', 'Execution', 'HulkPatch1', 'MSTB1', 'Multisite', 'Sanity']",Andrew Chen,Backlog,Anusha John
SEEN-2413,https://miggbo.atlassian.net/browse/SEEN-2413,[Auton]TC3_TSIM_verify_tsim_client_start handle exception for prompt,"Test_TC3_TSIM_verify_tsim_client_start handle exception for prompt.

Branch : private/HulkPatch-ms/api-auto

Script name : FEWAccessPointAndCLients.py

Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=3311160&size=27781&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct02_17:22:08.452584.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=3311160&size=27781&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct02_17:22:08.452584.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Snippet from fail log:

{noformat}8666: 
 Starting TSIM Clients:sim client start 1 100 100 1{noformat}

{noformat}8667: 
 +++ TB4-DM-TSIM with via 'a': executing command 'sim client start 1 100 10 1' +++{noformat}

{{sim client start 1 100 10 1(Cisco Capwap Simulator) >Client 1 to 100 started}}

{noformat}8669: 
 Traceback (most recent call last):{noformat}

{noformat}8670: 
   File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 730, in call_service{noformat}

{noformat}8671: 
     dialog_match = dialog.process({noformat}

{noformat}8672: 
   File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialogs.py"", line 476, in process{noformat}

{noformat}8673: 
     return dp.process(){noformat}

{noformat}8674: 
   File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialog_processor.py"", line 326, in process{noformat}

{noformat}8675: 
     self.timeout_handler(){noformat}

{noformat}8676: 
   File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/eal/dialog_processor.py"", line 273, in timeout_handler{noformat}

{noformat}8677: 
     raise TimeoutError('Prompt timeout occured, please check the hostname\n \{noformat}

{noformat}8678: 
 unicon.core.errors.TimeoutError: Prompt timeout occured, please check the hostname{noformat}

{noformat}8679: 
                                  hostname: TB4-DM-TSIM{noformat}

{noformat}8680: 
                                  timeout value: 10{noformat}

{noformat}8681: 
                                  last_command: 'sim client start 1 100 10 1\r'{noformat}

{noformat}8682: 
                                  pattern: ['^.*\\[confirm\\]\\s*\\[y/n\\].*$', '^.*\\[confirm(\\(y/n\\))?\\].*$', '^.*\\[yes[/,][Nn][Oo]\\]\\s?:?\\s*$', '^.*?(%\\w+(-\\S+)?-\\d+-\\w+|Guestshell destroyed successfully|%Error opening tftp:\\/\\/255\\.255\\.255\\.255).*$', '(.*?)Press any key to continue', '(.*?)Are you sure.*\\([yY]/[nN]\\)\\s*$', '(.?)Press Enter to continue.*', '^.*--\\s?[Mm]ore\\s?--.*$', '^(.*?)\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\)\\s*>\\s*$', '^.*?User:\\s*$', '^(.*?)\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\)\\s*show>\\s*$', '^(.*?)\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\)\\s*config>\\s*$', '^(.*?)\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\)\\s*debug>\\s*$', '^(.*?)\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\)\\s*test>\\s*$', '^(.*?)\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\)\\s*transfer>\\s*$', '^(.*?)\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\)\\s*license>\\s*$', '^(.*?)\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\)\\s*reset>\\s*$', '^(.*?)\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\)\\s*save>\\s*$', 'bash.*#\\s*$']{noformat}

{noformat}8683: 
                                  buffer:'sim client start 1 100 10 1\r\r\n\r\r\n\r\r\n(Cisco Capwap Simulator) >Client 1 to 100 started\r\n'{noformat}

{noformat}8684: 
{noformat}

{noformat}8685: 
 The above exception was the direct cause of the following exception:{noformat}

{noformat}8686: 
{noformat}

{noformat}8687: 
 Traceback (most recent call last):{noformat}

{noformat}8688: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/OptimizedSanity-for-ESXi/services/commonlibs/test_wrapper.py"", line 301, in wrapper{noformat}

{noformat}8689: 
     result = testfunc(func_self, **kwargs){noformat}

{noformat}8690: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/OptimizedSanity-for-ESXi/testcases/sanityusecases/FEWAccessPointAndCLients/ap_sync_provisioning_clients_roaming.py"", line 251, in test1_verify_tsim_client_start{noformat}

{noformat}8691: 
     tsimdev.execute(""sim client start {} {} {} {}"".format({noformat}

{noformat}8692: 
   File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/bases/routers/services.py"", line 259, in __call__{noformat}

{noformat}8693: 
     self.call_service(*args, **kwargs){noformat}

{noformat}8694: 
   File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/unicon/plugins/generic/service_implementation.py"", line 743, in call_service{noformat}

{noformat}8695: 
     raise SubCommandFailure(""Command execution failed"", err) from err{noformat}

{noformat}8696: 
 unicon.core.errors.SubCommandFailure: ('Command execution failed', TimeoutError(""Prompt timeout occured, please check the hostname\n                                 hostname: TB4-DM-TSIM\n                                 timeout value: 10\n                                 last_command: 'sim client start 1 100 10 1\\r'\n                                 pattern: ['^.*\\\\[confirm\\\\]\\\\s*\\\\[y/n\\\\].*$', '^.*\\\\[confirm(\\\\(y/n\\\\))?\\\\].*$', '^.*\\\\[yes[/,][Nn][Oo]\\\\]\\\\s?:?\\\\s*$', '^.*?(%\\\\w+(-\\\\S+)?-\\\\d+-\\\\w+|Guestshell destroyed successfully|%Error opening tftp:\\\\/\\\\/255\\\\.255\\\\.255\\\\.255).*$', '(.*?)Press any key to continue', '(.*?)Are you sure.*\\\\([yY]/[nN]\\\\)\\\\s*$', '(.?)Press Enter to continue.*', '^.*--\\\\s?[Mm]ore\\\\s?--.*$', '^(.*?)\\\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\\\)\\\\s*>\\\\s*$', '^.*?User:\\\\s*$', '^(.*?)\\\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\\\)\\\\s*show>\\\\s*$', '^(.*?)\\\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\\\)\\\\s*config>\\\\s*$', '^(.*?)\\\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\\\)\\\\s*debug>\\\\s*$', '^(.*?)\\\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\\\)\\\\s*test>\\\\s*$', '^(.*?)\\\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\\\)\\\\s*transfer>\\\\s*$', '^(.*?)\\\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\\\)\\\\s*license>\\\\s*$', '^(.*?)\\\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\\\)\\\\s*reset>\\\\s*$', '^(.*?)\\\\((TB4-DM-TSIM|Cisco Capwap Simulator)\\\\)\\\\s*save>\\\\s*$', 'bash.*#\\\\s*$']\n                                 buffer:'sim client start 1 100 10 1\\r\\r\\n\\r\\r\\n\\r\\r\\n(Cisco Capwap Simulator) >Client 1 to 100 started\\r\\n'"")){noformat}

{noformat}8697: 
 Test returned in 0:01:41.526805{noformat}

{noformat}8698: 
 Errored reason: Command execution failed{noformat}",2023-10-04T15:28:17.384+0000,"Required code change has been added to “private/HulkPatch-ms/api-auto” and cherry-picked to “private/Hulk-ms/api-auto” branch.

[~accountid:712020:a269cb40-4972-4cdd-b4be-1a9a0bc46160] , pls. validate the change and confirm for the closure. Marking this Auton as “Closed” with successful execution log: [Test_TC4_TSIM_verify_tsim_client_start  /   test1_verify_tsim_client_start|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=3050784&size=5620&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct10_08:34:39.507175.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['AWS_Sanity', 'Auton', 'ESXi', 'Ghost', 'Groot', 'Issue', 'Optimized', 'Sanity']",Amardeep Kumar,Closed,Manoj Menakuri
SEEN-2415,https://miggbo.atlassian.net/browse/SEEN-2415, [Auton]:Hulk P1: Test_TC43_DNAC_EXT_NODE_interface_config_verifications/ test12_dnac_ext_node_onboarding_enabling_AEN During AEN onboarding Config Preview Activity failed,"*Reporter Analysis:* During AEN Onboarding, the test is enabling PNP device authorization in system setting and then Updating Authentication Template on Fabric sites to 'Closed Authentication' went fine. But for enabling AEN on segments POOLs for Virtual Network profiles has failed for config preview with error as: 

 *""NCSO10001: CFS validation failed due to invalid parameters. OperationalInfo in expanded collection inside domain Global/USA/SAN_JOSE is not valid. OI for domain CfsOperationalInfo[UPDATE,[fabric_segment_operation]]""*

 The error is because the operationalInfo is not correctly populated in VirtualNetwork of the CD. Please refer to UI or wiki below to make the correct change. 

[https://confluence-eng-sjc1.cisco.com/conf/display/APICEMUCI/OperationalInfo+property+for+SDA+2.0+UI+operations|https://confluence-eng-sjc1.cisco.com/conf/display/APICEMUCI/OperationalInfo+property+for+SDA+2.0+UI+operations]

The required changes are from the CDET [CSCwh42136|https://cdetsng.cisco.com/webui/#view=CSCwh42136]

*Error Snip:*
{{Config Preview Activity failed with reason: NCSO10001: CFS validation failed due to invalid parameters. OperationalInfo in expanded collection inside domain Global/USA/SAN_JOSE is not valid. OI for domain CfsOperationalInfo[UPDATE,[fabric_segment_operation]]}} 

*Branch Name:* private/HulkPatch-ms/sanity_api_auto 

*Script* *file:* solution_test_sanityecamb.py
Optimized usecase: [SDAExtnodeOnboarding|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-123-SDAExtnodeOnboarding&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct03_12:36:59.580366.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] / Test_TC1_DNAC_EXT_NODE_interface_config_verifications

*Source Team:*  Sanity

Issue Seen first time or day0 issue:

*Failed Log:*
[Test_TC43_DNAC_EXT_NODE_interface_config_verifications|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=582247&size=1305463&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct04_19:51:09.887018.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] / [test12_dnac_ext_node_onboarding_enabling_AEN|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=594276&size=1084659&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct04_19:51:09.887018.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-10-05T07:39:07.295+0000,PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7383/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7383/overview],"['Auton', 'Hulk', 'HulkPatch1', 'Optimized', 'Sanity']",Moe Saeed,Resolved,Ashwini R Jadhav
SEEN-2416,https://miggbo.atlassian.net/browse/SEEN-2416,[Auton]:Hulk P1: Test_TC7_DNAC_Prime_Migration_DNAC_Server_Settings_NTP_DHCP_AAA_DNS_settings/test10_migrate_device_configs,"*Reported Analysis:*
Testcase should be skipped if testbed is not with prime feature , I am observing in a non-prime TB TC is executing without any checks

*Description*: 

{noformat}1924: 
 +------------------------------------------------------------------------------+{noformat}

{noformat}1925: 
 Cannot track test: tracking auth info must be set in order to transfer test tracking data{noformat}

{noformat}1926: 
 Executing testcase Test_TC7_DNAC_Prime_Migration_DNAC_Server_Settings_NTP_DHCP_AAA_DNS_settings test 7.10 ""test10_migrate_device_configs"".{noformat}

{noformat}1927: 
 Traceback (most recent call last):{noformat}

{noformat}1928: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job@2/services/commonlibs/test_wrapper.py"", line 301, in wrapper{noformat}

{noformat}1929: 
     result = testfunc(func_self, **kwargs){noformat}

{noformat}1930: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job@2/testcases/forty_eight_hour/solution_test_sanityecamb.py"", line 640, in test10_migrate_device_configs{noformat}

{noformat}1931: 
     ise_check =  dnac_handle.dnaconfig.testbed.custom[dnac_handle.dnaconfig.sid]['ise_check']{noformat}

{noformat}1932: 
 KeyError: 'ise_check'{noformat}

*Branch Name:* private/HulkPatch-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:

**Fail Log:**

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=330514&size=3994&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct04_23:27:44.874935.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=330514&size=3994&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct04_23:27:44.874935.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-10-05T08:28:46.042+0000,"[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7237/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7237/overview] [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7452/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7452/overview] [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7237/diff#testcases/forty_eight_hour/solution_test_sanityecamb_cert_lan.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7237/diff#testcases/forty_eight_hour/solution_test_sanityecamb_cert_lan.py] [~accountid:61efa8c457b25b006877eda3] , If you are not any issues ,please resolve the Auton","['Auton', 'Hulk', 'Sanity']",Vinay Raj V ,Resolved,Anusha John
SEEN-2417,https://miggbo.atlassian.net/browse/SEEN-2417,[Auton]:Hulk:Test_TC1_advanced_wlan_configs/test3_verify_device_configs,"*Reporter Analysis:*

output from ECA device is not getting while giving command:
{{show wireless profile policy detailed Adv_wlan_configs_2dmz_tb2_profile}}

But i am seeing ssid is up on both network profile and fabric sites /  SSID,
{{6857: 18 Adv_wlan_configs_2_profile Adv_wlan_configs_2dmz_tb2 UP [WPA2][802.1x][AES][PMF 802.1X]}}



*Description*:  

{noformat}7111: 
 TB2-DMZ-SJ-FIAB-ECA#{noformat}

{noformat}7112: 
 Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:02.277551{noformat}

{noformat}7113: 
 Wireless policy profile not found for Adv_wlan_configs_2dmz_tb2_profile{noformat}

{noformat}7114: 
 Some configs not verified on device{noformat}

{noformat}7115: 
 Library group ""advanced_wlan_configs"" method ""verify_advanced_wlan_configs"" returned in 0:08:40.102823{noformat}

{noformat}7116: 
 Test returned in 0:08:40.103932{noformat}

{noformat}7117: 
 Failed reason: Configs not verified on wlc{noformat}

{noformat}7118: 
 The result of section test3_verify_device_configs is => FAILED{noformat}





*Failed Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=1694839&size=435640&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct04_23:23:53.583374.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=1694839&size=435640&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct04_23:23:53.583374.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Branch used*: private/HulkPatch-ms/sanity_api_auto

*Script used*:advanced_wlan_configs optimised code

*Testbed details:*
[https://wiki.cisco.com/display/EDPEIXOT/Guidelines+-+DMZ+WoW|https://wiki.cisco.com/display/EDPEIXOT/Guidelines+-+DMZ+WoW]
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-11|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-11?src=contextnavpagetreemode]

Testbed is avialble in read only access as execution is in progress
",2023-10-05T16:49:54.117+0000,"Hi  [~accountid:63f50bfce8216251ae4d59d5]  ,

In recent  *hulk   P1  RC2  2373-70302* run     test3_verify_device_configs failed   in snity  run ,
could you please  check :

*Failed:*
*show wireless profile policy detailed Adv_wlan_configs_2tb7_profile*
+*Failed  log*+ 
 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=1214385&size=166271&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct11_10:10:30.681960.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=1214385&size=166271&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct11_10:10:30.681960.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Hulk P1 Rc1 2373-70289*
 verification   cmd   ==>
*show wireless profile policy detailed Adv_wlan_configs_2_profile*
+*Pass log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=1271611&size=198080&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct03_08:42:59.924152.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=1271611&size=198080&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct03_08:42:59.924152.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

{noformat}TB7-SJ-eCA-BORDER-CP#show wireless profile policy detailed Adv_wlan_configs_2_profile

Policy Profile Name                 : Adv_wlan_configs_2_profile
Description                         : Adv_wlan_configs_2_profile
Status                              : ENABLED
VLAN                                : 1
Multicast VLAN                      : 0
OSEN client VLAN                    : 
Wireless management interface VLAN  : 0
Multicast Filter                    : DISABLED
QBSS Load                           : ENABLED
Passive Client                      : DISABLED
ET-Analytics                        : DISABLED
StaticIP Mobility                   : DISABLED
WLAN Switching Policy
  Flex Central Switching            : DISABLED
  Flex Central Authentication       : ENABLED
  Flex Central DHCP                 : DISABLED
  Flex NAT PAT                      : DISABLED
WLAN Flex Policy
  VLAN based Central Switching      : DISABLED
WLAN ACL
  IPv4 ACL                          : Not Configured
  IPv6 ACL                          : Not Configured
  Layer2 ACL                        : Not Configured
  Preauth urlfilter list            : Not Configured
  Postauth urlfilter list           : Not Configured
WLAN Timeout
  Session Timeout                   : 1800
  Idle Timeout                      : 300
  Idle Threshold                    : 0
  Guest LAN Session Timeout         : DISABLED
WLAN Local Profiling
  Subscriber Policy Name            : Not Configured
  RADIUS Profiling                  : DISABLED
  HTTP TLV caching                  : ENABLED
  DHCP TLV caching                  : ENABLED
  DOT11 TLV accounting              : DISABLED
CTS Policy
  Inline Tagging                    : DISABLED
  SGACL Enforcement                 : DISABLED
  Default SGT                       : 0
WLAN Mobility
  Anchor                            : DISABLED
AVC VISIBILITY                      : Disabled
IPv4 Flow Monitors
  Ingress
  Egress
IPv6 Flow Monitors
  Ingress
  Egress
NBAR Protocol Discovery             : Disabled
Reanchoring                         : Disabled
Classmap name for Reanchoring
  Reanchoring Classmap Name         : Not Configured
QOS per SSID
  Ingress Service Name              : platinum-up
  Egress Service Name               : platinum
QOS per Client
  Ingress Service Name              : Not Configured
  Egress Service Name               : Not Configured
Umbrella information
  Cisco Umbrella Parameter Map      : Not Configured
  DHCP DNS Option                   : ENABLED
  Mode                              : ignore
Autoqos Mode                        : None
Call Snooping                       : Disabled
Tunnel Profile
  Profile Name                      : Not Configured
Calendar Profile
Fabric Profile
  Profile Name                      : Adv_wlan_configs_2_profile
Accounting list
  Accounting List                   : dnac-acct-Adv_wlan_c-b5a5e01e
DHCP
  required                          : DISABLED
  server address                    : 0.0.0.0
 Opt82
  DhcpOpt82Enable                   : DISABLED
  DhcpOpt82Ascii                    : DISABLED
  DhcpOpt82Rid                      : DISABLED
  APMAC                             : DISABLED
  SSID                              : DISABLED
  AP_ETHMAC                         : DISABLED
  APNAME                            : DISABLED
  POLICY TAG                        : DISABLED
  AP_LOCATION                       : DISABLED
  VLAN_ID                           : DISABLED
  VRF_NAME                          : DISABLED
Exclusionlist Params
  Exclusionlist                     : ENABLED
  Exclusion Timeout                 : 180
AAA Policy Params
  AAA Override                      : ENABLED
  NAC                               : DISABLED
  AAA Policy name                   : default-aaa-policy
  Vlan Fallback                     : DISABLED
WGB Policy Params
  Broadcast Tagging                 : DISABLED
  Client VLAN                       : DISABLED
Interim Accounting Updates       : ENABLED
Hotspot 2.0 Server name             : Not Configured
Mobility Anchor List
  IP Address                                  Priority
  -------------------------------------------------------
mDNS Gateway
  mDNS Service Policy name          : default-mdns-service-policy
User Defined (Private) Network              : Disabled
User Defined (Private) Network Unicast Drop  : Disabled
Policy Proxy Settings
  ARP Proxy State                   : DISABLED
  IPv6 Proxy State                  : None
ARP Activity Limit
  Exclusion                          : ENABLED
  PPS                                : 100
  Burst Interval                     : 5
NDP Activity Limit
  Exclusion                          : ENABLED
  PPS                                : 100
  Burst Interval                     : 5
Airtime-fairness Profile
  2.4Ghz ATF Policy                 : default-atf-policy
  5Ghz ATF Policy                   : default-atf-policy
Link-local bridging                 : DISABLED
IP mac-binding                      : ENABLED{noformat} Same issue seen on EXSI VM Hulk P1 Build #3.713.75159

Failed log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=1656496&size=163366&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct10_08:34:39.507175.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=1656496&size=163366&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct10_08:34:39.507175.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Passed log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=1663919&size=210394&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep25_09:46:18.293895.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=1663919&size=210394&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fenv_optimized_auto_job.2023Sep25_09:46:18.293895.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] [~accountid:63f50bd68ab3d6a635ecc29b] [~accountid:620b8357878c2f00729881c8] 

why you opened this ticket? Is it a defect or what is the issue here? 

If it was working fine and the ssid is up, then this is a defect? [~accountid:63f50bfce8216251ae4d59d5] 
SSID is up and running but from the pass log it si taking different command to verify on eca device and failed log different command:
n recent *hulk P1 RC2 2373-70302* run test3_verify_device_configs failed in snity run ,
could you please check :

*Failed:*
*show wireless profile policy detailed Adv_wlan_configs_2tb7_profile*
+*Failed log*+ 
 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=1214385&size=166271&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct11_10:10:30.681960.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=1214385&size=166271&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct11_10:10:30.681960.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Hulk P1 Rc1 2373-70289*
 verification cmd ==>
*show wireless profile policy detailed Adv_wlan_configs_2_profile*
+*Pass log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=1271611&size=198080&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct03_08:42:59.924152.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=1271611&size=198080&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct03_08:42:59.924152.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Can you please confirm whether this is expected to take the other cli also

Thanks,
Anusha John PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/b5c1d23bb0a35e9d1bd23ae80e8d6cee774d1190|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/b5c1d23bb0a35e9d1bd23ae80e8d6cee774d1190]","['AWS_Sanity', 'Auton', 'ESXi', 'Hulk', 'HulkPatch1', 'MSTB2', 'Optimized', 'Sanity']",Moe Saeed,Resolved,Anusha John
SEEN-2418,https://miggbo.atlassian.net/browse/SEEN-2418,[AUTON][NFW]-EWLC devices getting NETCONF Access Denied though devices is netconf reachable from DNAC,"{{During Hulk P1 testing on NFW profile, we see that after discovery of EWLC device ,the device not coming to managed state with Device NFW-EWLC-1.cisco.cloud and NFW-EWLC-2.cisco.cloud is not in managed state, current state: ""Netconf Access Denied"", errorDescription: NCIM12028: Netconf connection could not be established for device. Please ensure that the user privilege level is set to 15 for netconf connection to be successful. You can ensure correct user with right privileges is available in global credentials or in discovery job and run discovery again. You can also update the credentials of the device using update credentials option. }}
{{Cluster details:10.4.23.12(admin/Maglev123) Device details: EWLC-1 Telnet 10.4.2.27 2029 and EWLC-2 Telnet 10.4.2.27 2032 (wlcaccess/Lablab#123/Cisco#123)}}



{{Failed Log : https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/divayada-sjc/Solution_pyatsenv/users/admin/archive/23-09/env_auto_job.2023Sep07_03:24:55.201997.zip&atstype=ATS}}",2023-10-06T05:31:22.051+0000,,"['Auton', 'NFW', 'ghost', 'hulk', 'regression']",DatChi Pham,Backlog,JagadeshKumar Enapanuri
SEEN-2419,https://miggbo.atlassian.net/browse/SEEN-2419, [Auton][MSTB2] - Test_TC206_check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE  /   test2_remove_an_edge_from_the_fabric,"Hi

With the recent execution we are seeing deletion of device from fabric in “[Test_TC206_check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10354275&size=2895180&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct05_07:40:11.070698.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test2_remove_an_edge_from_the_fabric” testcase is failing due to device is having port assignment. Generally before deleting the device from  fabric script should clear the port assignment and then proceed with Fabric device removing.

Error:

45690:  Config Preview Activity failed with reason: Cannot delete device [SJC-FE-9300-1.cisco.com|http://SJC-FE-9300-1.cisco.com] from the Fabric Site/Zone because it has port assignments. Remove the port assignments from the device before deleting it from the Fabric Site/Zone.
45691:
45692:  Activity: 018b00a5-0ba7-7740-ac78-79ccef3fdf79 Trigger job: {'id': 'bd7eef69-a72e-4ee9-8fac-96d5d2362360', 'triggeredJobTaskId': '018b00a5-0bfc-7794-b845-311528fbe020', 'triggeredTime': 1696522898434, 'status': 'FAILED', 'failureReason': 'Cannot delete device [SJC-FE-9300-1.cisco.com|http://SJC-FE-9300-1.cisco.com] from the Fabric Site/Zone because it has port assignments. Remove the port assignments from the device before deleting it from the Fabric Site/Zone.', 'triggeredJobId': 'bd7eef69-a72e-4ee9-8fac-96d5d2362360'}

Failed log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10602557&size=560688&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct05_07:40:11.070698.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10602557&size=560688&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct05_07:40:11.070698.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Script : solution_test_3sites_sjc_nyc_sf.py

Branch : private/HulkPatch-ms/api-auto

Ova#3.713.75142

Passlog : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=53597209&size=26880&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fauto_MS_job.2023Sep26_09:34:59.552694.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=53597209&size=26880&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fauto_MS_job.2023Sep26_09:34:59.552694.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-10-06T09:28:36.213+0000,"PR-Link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7574/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7574/overview] Hi [~accountid:63f50bcf4e86f362d39acde5],

Still we are seeing same issue on latest Hulk-Patch-2-ESXI#3.714.75232 build

156985:  Activity: 018bf67a-cfd4-7644-a110-7d696d367ecf Trigger job: {'id': 'effe74ac-462e-4d5c-b3c4-7d131a557d4e', 'triggeredJobTaskId': '018bf67a-d014-7fbd-86ce-cf9a23111318', 'triggeredTime': 1700647325723, 'status': 'FAILED', 'failureReason': 'Cannot delete device [SJC-FE-9300-1.cisco.com|http://SJC-FE-9300-1.cisco.com] from the Fabric Site/Zone because it has port assignments. Remove the port assignments from the device before deleting it from the Fabric Site/Zone.', 'triggeredJobId': 'effe74ac-462e-4d5c-b3c4-7d131a557d4e'}

Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=35075037&size=571063&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov21_21:16:54.552925.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=35075037&size=571063&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov21_21:16:54.552925.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

I’ve checked PR and same fix is already part of HulkPatch2 branch
Script File : solution_test_3sites_sjc_nyc_sf.py

Branch : private/HulkPatch2-ms/api-auto
Note : DNAC is a ESXI based DNAC. Hi Divakar. i can not catch the error you mentioned when i used my cluster. Can you please share your testbed?","['Auton', 'ESXi', 'Execution', 'Hulk-Patch1', 'MSTB2']",QuangVinh Nguyen,Reopened,Divakar Kumar Yadav
SEEN-2420,https://miggbo.atlassian.net/browse/SEEN-2420,[Auton] [Optimized] Issue In  YAML mapping,"* 
*In the optimized code, below UCs are skipping. We tried ran  with  a single UC, but it didn't work. Could you please check the priority?*

 *There's also an issue with UCs highlighted in blue,*

+*I've attached the log and Jenkins job URL.*+

##############################################################################
""17"": [     {         ""parallelrun"": [             ""FEWKairos"", ""assuranceHealthMetrics"", ""verifyBorderEdgeKPI"",             ""generateAPReport"", ""verifyAIAnaliticData"", ""map"", ""assuranceGlobalDeviceCount"",          {color:#ff991f} {color}{color:#4c9aff}  *""toolsNetworkReasoner/MREWirelessAPDataCollection"",             ""toolsNetworkReasoner/MREWirelessClientDataCollection"",*{color}             ""ISENDG/tagDeviceInISENDG""        ],         ""blocker_uc"": [ ]     },

  ""parallelrun"": [
              {color:#ff991f} {color}{color:#ff991f} {color}{color:#4c9aff}*""toolsNetworkReasoner/MREAssuranceTelemetryAnalysis""*{color}
            ],
            ""blocker_uc"": [ ]


 {
          # Remove/readd Edge: ISENDG/deletedDeviceInISENDG
            ""parallelrun"": [
             {color:#ff991f}   *""*{color}{color:#4c9aff}*ISENDG/deletedDeviceInISENDG""*{color}
            ],
            ""blocker_uc"": [ ""ISENDG/deletedDeviceInISENDG"" ]
        }
#########################################################################

*Hulk P1 -2.1.713.70263*



*Script file/Usecase :* lansanity_usecases_maps.yaml

*Source Team:  Sanity*

*Branch Deatils :* private/HulkPatch-ms/sanity_api_auto

*Jenkins Job :*   [engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/602/parameters/|http://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/602/parameters/]


*EXEC_UC_LIST* :

!image-20231006-145414.png|width=1498,height=156!

+*Log:*+
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-10/env_optimized_auto_job.2023Oct05_05:07:29.991264.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-10/env_optimized_auto_job.2023Oct05_05:07:29.991264.zip&atstype=ATS]



+*ISENDG:*+

!image-20231006-150720.png|width=512,height=164!



+*ToolsNetworkResaoner*+

!image-20231006-150846.png|width=461,height=186!",2023-10-06T14:45:13.257+0000,[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/4db69e7afb2e4008a4a81dc8b94ccba7fd43ab94|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/4db69e7afb2e4008a4a81dc8b94ccba7fd43ab94] Cherry picked the fix to all hulk branches. Please modify other job files based on the fix ,"['Auton', 'Blocked', 'Hulk-Patch1', 'Optimized', 'Sanity', 'Yamlmapping']",Raji Mukkamala,Resolved,Omkar Sharad Wagh
SEEN-2424,https://miggbo.atlassian.net/browse/SEEN-2424,CSCwh23676:AWS Sanity:Hulk P1:Test_TC12_verify_fabric_360/test1_verify_fabric_SD_access,"*Reporter Analysis*:



we are observing {{AAA Server Status and Extended Node Connectivity status on device 360 is accurate and properly displaying with score 10}} but still TC Failing with error as below:

Check with DE “kpsam” who was the engineer for bug:

[https://cdetsng.cisco.com/webui/#view=CSCwh23676|https://cdetsng.cisco.com/webui/#view=CSCwh23676]


As per Karthik:

TC related to extended node KPI is good.
The fix on script side is needed for AAA KPI for this device.

you should assert for valid AAA score on this device platform as this does not support TDL subscriptions so need to remove that assertion.



Bug space: and added [~accountid:62d2fe9f8afb5805e5d5af49]  to the space:
webexteams://im?space=67048720-3752-11ee-bb22-d917cacd7322



*Description*:  

{noformat}4496: 
 'SN-FDO2415JBFJ.cisco.com' device has valid fabric role 'EXTENDED-NODE' with InValidvalid scores:- AAA Server Status:-1.0, Extended Node Connectivity:10.0{noformat}

{noformat}4497: 
 'SN-FDO2416J86H.cisco.com' device has valid fabric role 'EXTENDED-NODE' with InValidvalid scores:- AAA Server Status:-1.0, Extended Node Connectivity:10.0{noformat}

{noformat}4499: 
 'SN-FDO2416J86F.cisco.com' device has valid fabric role 'EXTENDED-NODE' with InValidvalid scores:- AAA Server Status:-1.0, Extended Node Connectivity:10.0{noformat}

{noformat}4531: 
 noHealthDevices:0=23, monitoredHealthyDevices:6=6, latestHealthScore:100.0=100.0{noformat}

{noformat}4532: 
 Un structure data from '/assurance/v1/network-device'  API response{noformat}

{noformat}4534: 
 Failed reason: Unable to verify_fabric_SD_access parameters{noformat}

!06_35_06.jpg|width=1784,height=821!

!06_24_22.jpg|width=1605,height=595!

*Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-156-SDAFabricAssurance&begin=1160763&size=155080&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct05_02:56:36.017061.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-156-SDAFabricAssurance&begin=1160763&size=155080&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct05_02:56:36.017061.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-10-10T14:52:18.731+0000,"Raised PRs:

* Hulk Patch1: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7533/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7533/overview]
* Hulk Patch2: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7534/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7534/overview]
* Ghost: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7880/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7880/overview]","['AWS_Sanity', 'Auton', 'Ghost', 'Guardian', 'Hulk', 'HulkPatch1', 'Sanity']",ThangQuoc Tran,Resolved,Anusha John
SEEN-2439,https://miggbo.atlassian.net/browse/SEEN-2439,[Hulk Patch2][Implicit provisioning] Twj900558r CFI FEAT-6880 Implicit Provisioning : Validate DNAC will not implicit provision fabric devices of DHCP server get updated,"Twj900558r CFI FEAT-6880 Implicit Provisioning : Validate DNAC will not implicit provision fabric devices of DHCP server get updated.

[https://wiki.cisco.com/display/APICEMUCI/F155700++Stop+implicit+fabric+provisioning+on+Design+parameter+update|https://wiki.cisco.com/display/APICEMUCI/F155700++Stop+implicit+fabric+provisioning+on+Design+parameter+update]",2023-10-10T18:03:20.833+0000,"Testcase - [Hulk Patch2][Implicit provisioning] Twj900558r CFI [FEAT-6880|https://bitbucket-eng-sjc1.cisco.com/bitbucket/plugins/servlet/jira-integration/issues/FEAT-6880] Implicit Provisioning : Validate DNAC will not implicit provision fabric devices of DHCP server get updated

JIRA ID - [SEEN-2439|https://bitbucket-eng-sjc1.cisco.com/bitbucket/plugins/servlet/jira-integration/issues/SEEN-2439]

PR Link : 
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8127/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8127/overview]

Logs:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/root/new_pyats/users/root/archive/23-11/sanity_TB1.2023Nov29_12:15:45.444030.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/root/new_pyats/users/root/archive/23-11/sanity_TB1.2023Nov29_12:15:45.444030.zip&atstype=ATS]","['Auton', 'HulkPatch2', 'ReleaseUseCases', 'Uplift']",Shubham Varfa,Resolved,Tran Lam
SEEN-2473,https://miggbo.atlassian.net/browse/SEEN-2473,[Auton][MSTB2] - Test_TC207_enable_maintenance_mode_on_aps_and_validate_maintenance_mode/test18_verify_ssid_broadcast_when_ap_in_mm,"Hi [~accountid:63f50be71223974bc04b0534],

We have tested “[Test_TC207_enable_maintenance_mode_on_aps_and_validate_maintenance_mode|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=49937132&size=1801667&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct04_09:50:10.588385.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  /   test18_verify_ssid_broadcast_when_ap_in_mm” on Hulk-ESXI build#3.713.75142 build and seeing below issue

{noformat}223736: 
 +++ APA488.73CF.1A70 with via 'a': executing command 'debug capwap console cli' +++{noformat}

{{debug capwap console cli ^% Invalid input detected at '^' marker. APA488.73CF.1A70#}}

After checking with you there was library change for verification and actually script is looking for below CLI output

show capwap client detailrcb | include SSID

AP1416.9D2E.1EE0#show capwap client detailrcb | include SSID
   SSID                           : SSIDDot1XIndiaMS2
   Broadcast SSID                 : Enabled
   Profile Name                   : SSIDDot1XIndia_profile
   SSID                           : OPENMS2
   Broadcast SSID                 : Enabled
   SSID                           : Radius_ssidMS2
   Broadcast SSID                 : Enabled
   SSID                           : CiscoSensorProvisioning
   Broadcast SSID                 : Disabled
   SSID                           : Random_macMS2
   Broadcast SSID                 : Enabled
   SSID                           : postureMS2
   Broadcast SSID                 : Enabled
   SSID                           : GUESTMS2
   Broadcast SSID                 : Enabled
   SSID                           : SSIDDot1XIndiaMS2
   Broadcast SSID                 : Enabled
   Profile Name                   : SSIDDot1XIndia_profile
   SSID                           : OPENMS2
   Broadcast SSID                 : Enabled
   SSID                           : Radius_ssidMS2
   Broadcast SSID                 : Enabled
   SSID                           : CiscoSensorProvisioning
   Broadcast SSID                 : Disabled
   SSID                           : Random_macMS2
   Broadcast SSID                 : Enabled
   SSID                           : postureMS2
   Broadcast SSID                 : Enabled
   SSID                           : Single5KBandMS2
   Broadcast SSID                 : Enabled
AP1416.9D2E.1EE0#

Log : [Test_TC207_enable_maintenance_mode_on_aps_and_validate_maintenance_mode|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=49937132&size=1801667&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct04_09:50:10.588385.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]
Script : solution_test_3sites_sjc_nyc_sf.py
Branch : private/HulkPatch-ms/api-auto",2023-10-11T04:13:48.189+0000,"Issue is due to AP model - for that AP need to change model from ios to airos in the yaml file for the specific testbed that is seeing the issue.  Made necessary changes in yaml. And it worked fine

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/05d99983f3fc04f28a808a4e3b4793c87a1f5e87#configs/sr_mb2/sr_mb2_testbed.yaml|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/05d99983f3fc04f28a808a4e3b4793c87a1f5e87#configs/sr_mb2/sr_mb2_testbed.yaml] Issue is not seen after making yaml changes.

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1909080&size=1964937&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct15_23:34:34.442278.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1909080&size=1964937&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct15_23:34:34.442278.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'ESXi', 'Hulk-Patch1', 'Integration', 'MSTB2']",Majlona 'Luna' Aliaj,Closed,Divakar Kumar Yadav
SEEN-2488,https://miggbo.atlassian.net/browse/SEEN-2488,[Auton][MSTB2] -  Test_TC39_DNAC_Configuring_bgp_on_border_fusion_router  /   test1_configure_loopback_bgp_on_border_for_reader,"Hi,

We have executed “solution_test_3sites_sjc_nyc_sf_mdnac_dr.py” script on latest Hulk-P1--ESXI build#3.713.75159 and Polaris version#17.12.2 on Fiab device.

 

But during Configuring BGP on Reader Node-Fusion(MD-Fusion), we are seeing script is trying to create sub-interface and script is getting errored.

 

We are having Reader Node Fusion(MD-Fusion) as 3k box.

 

MD-Fusion(config-vrf-af)#

MD-Fusion(config-vrf-af)#default interface GigabitEthernet4/0/4

Interface GigabitEthernet4/0/4 set to default configuration

MD-Fusion(config)# interface GigabitEthernet4/0/4.3313

interface GigabitEthernet4/0/4.3313

^ % Invalid input detected at '^' marker.

MD-Fusion(config)#end

MD-Fusion#

 

Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13462585&size=680843&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_multi_sites_mdnac.2023Oct10_09:19:44.872417.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13462585&size=680843&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_multi_sites_mdnac.2023Oct10_09:19:44.872417.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

 

After debugging further on MSTB1 we can see same command works fine on Reader Node-Fusion(MD-FR-ASR) of MSTB1 as sub-interface is supported on ASR device and same CLI works as expected.

MD-FR-ASR(config-subif)# interface TenGigabitEthernet0/0/1.3313

MD-FR-ASR(config-subif)# encapsulation dot1Q 3313

MD-FR-ASR(config-subif)# vrf forwarding DEFAULT_VN

MD-FR-ASR(config-subif)# ip address 205.1.16.54 255.255.255.252

MD-FR-ASR(config-subif)# ip pim sparse-mode

 

From logs we can see that “interface GigabitEthernet4/0/4.3313” is unsupported on 3k switch. Please help us in fixing the issue.",2023-10-11T13:28:12.963+0000,You should use wrong type for your fusion in yaml. Please correct it.,"['Auton', 'ESXi', 'Hulk-Patch1', 'Integration', 'MSTB2']",Tran Lam,Cancelled,Divakar Kumar Yadav
SEEN-2489,https://miggbo.atlassian.net/browse/SEEN-2489,[Auton]:Guardian:Test_TC45_DNAC_TSIM_static_onboarding_verifications/test5_onboard_ap_dot1x/test6_prepare_aps_dot1x/test7_pnp_onboard_dot1x_ap/test8_wait_ap_to_be_provisioned,"*Reporter Analysis:*

Testcase failed from logs but testcase gave me again false pass log even when AP didn’t even attempted to onboard via pnp

I am using lan script and we are cleaning ECA device so tc should be able to resolve the below error:


*Description:*

226388: 
 Config Preview Activity failed with reason: Provisioning failed due to invalid request. Connected Device Type for an interface cannot be changed. To change the type, first clear the interface and then try again.

{noformat}226399: 
 Config preview task for provision failed for device:TB6-DM-eCA-BORDER{noformat}

{noformat}226400: 
 Not Valid Activity-ID{noformat}

{noformat}226402: 
 Failed to receive config preview{noformat}



*Branch Name:* Guardian -ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

*False Pass log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=37052128&size=3785600&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct09_10:20:44.424099.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=37052128&size=3785600&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct09_10:20:44.424099.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*Last True pass log from Guardian P4RC4:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=568428&size=1043545&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct03_21:58:33.516386.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=568428&size=1043545&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct03_21:58:33.516386.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]",2023-10-11T14:02:39.149+0000,,"['Auton', 'Guardian', 'Sanity']",Andrew Chen,Backlog,Anusha John
SEEN-2490,https://miggbo.atlassian.net/browse/SEEN-2490,[Auton] - Verification of sgt mapping fails due to variable assignment errors,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/HulkPatch-ms/api-auto

*Uber ISO tested:* Hulk Patch1 RC1 - 2.1.713.70292

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

When integrating the Hulk feature - Trustsec policy enforcement on Border, we see verification of SGTs are failing with variable assignment errors.

*Error snip:*

36193:  Traceback (most recent call last):
36194:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/services/commonlibs/test_wrapper.py"", line 301, in wrapper
36195:      result = testfunc(func_self, **kwargs)
36196:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py"", line 35228, in test3_verify_sgt_map_for_clients
{color:#bf2600}36197:      if not dnac_handle.dnaconfig.verify_sgtmap(oRtr, sg=sg_tag, vn=vn_name, count=1, retries=5):{color}
{color:#bf2600}36198:  UnboundLocalError: local variable 'vn_name' referenced before assignment{color}
36199:  Test returned in 0:00:01.493290
36200:  Errored reason: local variable 'vn_name' referenced before assignment



*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/Trustsec+policy+enforcement+on+Border|https://wiki.cisco.com/display/EDPEIXOT/Trustsec+policy+enforcement+on+Border]

*Failed log* - [Test_TC281_Trustsec_Policy_enforcement_on_Border|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7956801&size=4991109&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct10_22:01:07.334184.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-10-11T14:22:53.622+0000,"Hi [~accountid:62d2fec15d6f5fd2c3db8f9f] 

This testcase is blocked due to your testbed is missing update Solution test input changes, so it could not find SG and VN data. Could you please update the solution input changes?

{noformat}{
    ""sg_name"": ""sg_soltest"",
    ""sg_tag"": ""30005"",
    ""sg_condition"": [
        {
          ""description"": ""TC_Trustsec_Policy_enforcement"",
          ""name"": ""sg_soltest"",
          ""scalableGroupType"": ""USER_DEVICE"",
          ""securityGroupTag"": ""30005""
        }
    ],
      ""vn_name"": ""WiredVNFBLayer2""
  }{noformat} Also raised PRs to check that SG data have been updated in solution input changes:

* Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7457/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7457/overview]
* HulkPatch1: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7455/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7455/overview]
* HulkPatch2: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7481/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7481/overview] [~accountid:63f50bd34c355259db9ccc4d] : Thanks for looking into the issue further ! 

Solution input json file parameters are not testbed specific. It always generic and its being used by everyone across all profiles including Sanity and Regression. 

Since its generic, it needs to be included as part of *configs/config_48hr_test/solution_test_input.json*

Could you please add the corresponding necessary changes to this file as part of below raised PRs for both branches?



 Regards

Sandeep S Hi [~accountid:62d2fec15d6f5fd2c3db8f9f] 

My apologies for causing some confusion, I mean the testbed fabric input file same as one of Sanity TB1 *configs/sanity_tb1/solution_sanityeca_SanityTB1.json*

Also added the required fabric input changes in the wiki. Please update and re-run this testcase. [~accountid:63f50bd34c355259db9ccc4d] : Thanks for further confirmation! Sure we will be making the required changes and will be re-running the TC. Will update the observations once its completed.","['Auton', 'Hulk', 'HulkPatch', 'Integration', 'MSTB1', 'Multisite']",ThangQuoc Tran,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2491,https://miggbo.atlassian.net/browse/SEEN-2491,[Auton] - Device reachability check is failing due object attribute errors,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/HulkPatch-ms/api-auto

*Uber ISO tested:* Hulk Patch1 RC1 - 2.1.713.70292

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

When integrating the Hulk feature - View issue consolidation, we see device reachability check is failing due to object attribute instantiation. Looks like proc itself is missed to be included in library. 

*Error snip:*

32763:  Successfully wrote map
32764:  Traceback (most recent call last):
32765:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/services/commonlibs/test_wrapper.py"", line 301, in wrapper
32766:      result = testfunc(func_self, **kwargs)
32767:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py"", line 34552, in test3_check_device_shown_unreachable
32768:      if dnac_handle.check_reachability_device_in_inventory(device_name=self.device_name, reachability_is_up=False):
32769:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC {color:#bf2600}DR/services/dnaserv/dnaservices.py"", line 337, in {color}{color:#bf2600}*getattr*{color}
{color:#bf2600}32770:      raise AttributeError(err_msg){color}
{color:#bf2600}32771:  AttributeError: 'DnaServices' object has no attribute 'check_reachability_device_in_inventory'{color}
{color:#bf2600}32772:  Test returned in 0:00:00.474336{color}
{color:#bf2600}32773:  Errored reason: 'DnaServices' object has no attribute 'check_reachability_device_in_inventory'{color}



*Feature wiki:* [https://wiki.cisco.com/display/EDPEIXOT/View+issue+consolidation+of+Remote+Internet+sessions+from+Control+plane+to+Border+at+VN+level|https://wiki.cisco.com/display/EDPEIXOT/View+issue+consolidation+of+Remote+Internet+sessions+from+Control+plane+to+Border+at+VN+level]

*Failed log -* [Test_TC277_View_Issue_Consolidation_Of_Remote_Internet_Sessions_Between_Control_plane_And_Border_At_VN_level|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7449859&size=476733&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct10_22:01:07.334184.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-10-11T14:37:15.533+0000,"# PR HulkPatch-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7389/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7389/overview]
# Test Case:  Test_TC224_View_Issue_Consolidation_Of_Remote_Internet_Sessions_Between_Control_plane_And_Border_At_VN_level
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-09/sanity_TB2.2023Sep12_21:01:53.489395.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-09/sanity_TB2.2023Sep12_21:01:53.489395.zip&atstype=ATS] The root cause:
Test 3 and test 6 got Errored because there is missing a library.
Test 4 failed because the assurance page took time to update the new data. Change another way to get data on assurance page. [~accountid:62d2fec15d6f5fd2c3db8f9f]: Could you cherry-pick this PR and check again if the TC run passes or fails? This Auton was fixed in another PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7002/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7002/overview]","['Auton', 'Hulk', 'HulkPatch', 'Integration', 'MSTB1', 'Multisite']",NhanHuu Nguyen,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2492,https://miggbo.atlassian.net/browse/SEEN-2492,[Auton]:Hulk P1: Test_TC1_template_conflicts_in_template_hub  /   test18_re_provision_the_wireless_device_and_validate_configuration_template_not_be_pushed ,"*Reporter Analysis:*

TC was not able to fetch ewlc device name properly

*Description:*

{noformat}5530: 
     result = method(*args, **kwargs){noformat}

{noformat}5531: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Hulk/Hulk_Optimized-Sanity/services/dnaserv/lib/api_groups/provision/group.py"", line 323, in provision_devices{noformat}

{noformat}5532: 
     if self.services.dnaconfig.testbed.devices[dev].role.find(""WLC"") != -1 :{noformat}

{noformat}5533: 
   File ""/ws/owagh-sjc/Solution_pyatsenv/lib/python3.10/site-packages/pyats/topology/bases.py"", line 101, in __getitem__{noformat}

{noformat}5534: 
     return super().__getitem__(key){noformat}

{noformat}5535: 
 KeyError: 'TB2-DMZ-eWLC.cisco.com'{noformat}



*Failed Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-templatehub.py-271-toolsTemplateHub&begin=2132198&size=17838&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct06_01:46:12.299148.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-templatehub.py-271-toolsTemplateHub&begin=2132198&size=17838&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct06_01:46:12.299148.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Branch Used:*private/HulkPatch-ms/sanity_api_auto

*Script used*:Optimised solution_test_sanityecamb_lan.py",2023-10-11T15:02:32.585+0000,"issue is with the return value of assign_tag_to_site function call as it returns {{TB2-DMZ-eWLC.cisco.com}} PR : [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7387/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7387/overview]

2492 - in template hub testcase, self.wlc_device list has overwrite with the hostname append "".cisco.com"" where in next sub case it fails to match the name from the device list - Fix : rename the variable form the function call return

Fail log : [https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/vkuttykr-sjc/pyats_dnac_kk/users/vkuttykr/archive/23-10/sanity_TB23_standalone.2023Oct11_14:40:50.446477.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/vkuttykr-sjc/pyats_dnac_kk/users/vkuttykr/archive/23-10/sanity_TB23_standalone.2023Oct11_14:40:50.446477.zip&atstype=ATS]

Pass log : [https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/vkuttykr-sjc/pyats_dnac_kk/users/vkuttykr/archive/23-10/sanity_TB23_standalone.2023Oct11_15:07:38.735026.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/vkuttykr-sjc/pyats_dnac_kk/users/vkuttykr/archive/23-10/sanity_TB23_standalone.2023Oct11_15:07:38.735026.zip&atstype=ATS]

There is one case failed due to AAA config push due to the existing bug CSCwh75996 [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7387/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7387/overview]","['AWS_Sanity', 'Auton', 'HulkPatch1', 'Sanity']",Vinoth Kumar Kutty Krishnamoorthy,Resolved,Anusha John
SEEN-2495,https://miggbo.atlassian.net/browse/SEEN-2495,[Auton] Hulk P2 - Wireless Solution Sanity - Script Enhancement Required for Config Push Verifications at TC41,"*Regression:* SDA Solution Sanity (SSR)

*DNAC Type:* On-Prem

*DNAC Release_Version Used:* assembly_release_dnac_hulk-patch2_07-2.1.714.70203.iso

*Branch Used:* rcdn/HulkPatch2-ms/api-auto  (local branch that will be synced to main branch Private/HulkPatch2-ms/api-auto before every reg run start)

*Script Used:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

*Taas Log:* [https://ngdevx.cisco.com/services/taas/results/633577a5-bdd0-4edf-9dff-2fa434b0ae6f|https://ngdevx.cisco.com/services/taas/results/633577a5-bdd0-4edf-9dff-2fa434b0ae6f]

*Enhancement Required:* Script missed to capture config push verifications at TC41 and failed to capture DNAC bug CSCwh65210 and CSCwh74027 (that leaded to Aireos WLC AP join failure) in script execution logs. Missed clis should be verified by script and right failure reason/message should be thrown at TC41, to capture this product bug via automation. This helps to confirm product bug or script bug or testbed issue, saving time of Test-Team/DE-Team/Automation-Team spent on debugging such issues.

*Additional Request:* Please analyze such possible bugs at all applicable TCs and enhance script coverage to capture more product bugs via automated runs",2023-10-12T07:57:16.683+0000,,"['Auton', 'Hulk-P2']",Andrew Chen,Backlog,Yuvarani Iyamperumal
SEEN-2496,https://miggbo.atlassian.net/browse/SEEN-2496,Need to remove UC /testcases completely from Sanity which is dependent on additional packages or which is only on regression profile,"*Reporter Analysis*:

All the testcases which requires additional packages / which testcases moved to regression or only on regression needs to be completely removed from sanity.

*Description*:  

UC14-""SDASAPDelayUsecase""

UC16-""FEWAccessPointOeapMode""

UC17-""verifyAIAnaliticData""

UC19-”talosSecurity"", ""MSDnacVerifyLispAfterReload""

UC 28.2 ""wirelessMobility"" ,
UC 30.1 ""assuranceTrueTrace"",

UC 30.4 -[MSDnacVerifyLispAfterReload|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verifylispafterreload.py-304-MSDnacVerifyLispAfterReload&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct11_21:47:13.305019.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

UC 31.1 ""wiredWirelessPosture""
UC34.1 ""rlan""
UC 35 ""nplus1wlc""

Test_TC233_flexible_reports_check

TC53 [DNAC_provision_all_aps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1250658&size=1256226&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct11_12:45:11.242624.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]  /   test2_enable_kairos_cs_details 



*Branch:*private/HulkPatch-ms/sanity_api_auto 

*Script used:* solution_test_sanityecamb_lan.py and solution_test_sanityecamb.py",2023-10-12T13:15:24.551+0000,"I have removed KairOS and TALOS related use-cases from lansanity and nonlansanity maps as they involve cost.
Limiting the execution of the same with MultiSite Profiles only.

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/5f943939a5c8ce9fa40de00ba2cb229ddc20acf3|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/5f943939a5c8ce9fa40de00ba2cb229ddc20acf3]","['AWS_Sanity', 'Auton', 'Ghost', 'Hulk', 'HulkPatch1', 'Sanity']",Raji Mukkamala,Backlog,Anusha John
SEEN-2497,https://miggbo.atlassian.net/browse/SEEN-2497,Auton: Upgrade:Test_TC53_SWIM_UPGRADE_ECA_DEVICE,"*Reporter Analysis:* 
On Recent Hulk execution we are observing , [Test_TC53_SWIM_UPGRADE_ECA_DEVICE|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=372839&size=11839403&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct12_04:02:12.221534.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] was failing and it was not attempting fon ECA &NF switches it was attempting and upgrading only Transit , after Transit was upgraded it was stopping the execution and after that tc is failing , we tried manually to image upgrade on ECA & NF it got success.

In error code also we are seeing error like “   Image Upgrade not needed as device already has the same image, or device not supported on this image.“ but ECA & NF was not having the same image which we were planning  to upgrade .



Issue is seen only on Hulk Patch1 branch



*Description*:  

{noformat}11702: 
 Image Upgrade not needed as device already has the same image, or device not supported on this image.{noformat}

{{ }}

{noformat}12614: 
 Traceback (most recent call last):{noformat}

{noformat}12615: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/3N-Cluster-Hulk-Upgrade-Verify-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper{noformat}

*Branch Name:* HulkPatch -ms/sanity_api_auto

**Script* *file:** {{testcases/upgrade/after_upgrade_verify.py}}

*Source Team:* Upgrade Sanity

Issue Seen first time or day0 issue:

**Fail Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11999618&size=212460&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct12_04:02:12.221534.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=11999618&size=212460&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct12_04:02:12.221534.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Passlog from private/Hulk-ms/sanity_api_auto:

[Test_TC53_SWIM_UPGRADE_ECA_DEVICE|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=379099&size=8760270&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-06%2Fenv_auto_job.2023Jun28_21:03:19.434165.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-10-12T15:17:15.089+0000,"Addressed [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/79e56d8acfd3a9971f7bdb296d70885a30dd0cad|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/79e56d8acfd3a9971f7bdb296d70885a30dd0cad] Hi [~accountid:63f50bf5e8216251ae4d59cf] , 
We have tried to run the swim tc , TC was passed but

1. It was not attempting to the eca device 

2.After NF was upgraded , NF connected devices taking time to come reachable state

!1914_623_1.png|width=1436,height=467!



Pass log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=430337&size=12204844&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct23_04:56:59.194111.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=430337&size=12204844&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct23_04:56:59.194111.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

[^pen_afterswim.log]

RCA:
[http://172.21.236.183/sanity_rca/hulkp1rc3_swim/|http://172.21.236.183/sanity_rca/hulkp1rc3_swim/]","['Auton', 'Execution', 'Hulk', 'Sanity', 'Upgrade']",Raji Mukkamala,Reopened,Tulasi Reddy
SEEN-2498,https://miggbo.atlassian.net/browse/SEEN-2498,[Auton] Hulk P2 and Hulk P1 - Wireless Solution Sanity - Script Fix Required for Interface Onboarding TCs,"# *Regression Team Actions Checklist:*
Debugged/Analyzed the issue: Y
Compared with the last passed log: Y
Checked and compared the configuration: Y
Confirmed with DE on the root cause: Not required as Same ISO/Testbed passed on Oct11 triggered run. This confirms recent script breakage
# *Issue faced:* Message:{""response"":{""errorCode"":""NCSS10016"",""message"":""NCSS10016: Start time is in the past: October 13, 2023 04:35 AM PDT."",""detail"":""NCSS10016: Start time is in the past: October 13, 2023 04:35 AM PDT.""},""version"":""1.0""} - Leads to WorkItem in Pending Review and next set of TCs gets impacted due to pending operation blocking other TCs
# *Failure/Errors snip from log:* Message:{""response"":{""errorCode"":""NCSS10016"",""message"":""NCSS10016: Start time is in the past: October 13, 2023 04:35 AM PDT."",""detail"":""NCSS10016: Start time is in the past: October 13, 2023 04:35 AM PDT.""},""version"":""1.0""} - Leads to WorkItem in Pending Review and next set of TCs gets impacted due to pending operation blocking other TCs
# *Regression team debug analysis with details:* Same ISO/Testbed passed on Oct11 triggered run. This confirms recent script breakage
# *DE analysis/confirmation with details:* Not required as Same ISO/Testbed passed on Oct11 triggered run. This confirms recent script breakage
# *What do Automation team need to look into?* Fix script breakage
# *Failed log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/23-10/sanitycombine.2023Oct13_04:18:59.528473.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/23-10/sanitycombine.2023Oct13_04:18:59.528473.zip&atstype=ATS] with Hulk P2 and [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/23-10/sanitycombine.2023Oct12_13:19:22.969452.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/wnbu/pythonenv/jenkins/sol_ats20/users/wnbu/archive/23-10/sanitycombine.2023Oct12_13:19:22.969452.zip&atstype=ATS] with Hulk P1
# *Last passed log:* [https://ngdevx.cisco.com/services/taas/results/633577a5-bdd0-4edf-9dff-2fa434b0ae6f|https://ngdevx.cisco.com/services/taas/results/633577a5-bdd0-4edf-9dff-2fa434b0ae6f]
# *Failed DNAC version/ISO:* assembly_release_dnac_hulk-patch2_07-2.1.714.70203.iso
# *Last Passed DNAC version/ISO:* assembly_release_dnac_hulk-patch2_07-2.1.714.70203.iso
# *Script/Mapping file Used:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py
# *TCs Impacted:* test1_extnode_ap_static_onboarding_verifications on TC42 and TC44
# *Branch Used:* rcdn/HulkPatch2-ms/api-auto  (local branch that will be synced to main branch Private/HulkPatch2-ms/api-auto before every reg run start)
# *DNAC Type:* On-Prem
# *Testbed details:* NA
# *Team:* Working under client manager Loi Le and running SDA Solution Sanity Regression",2023-10-13T13:41:49.380+0000,[ENG-SDN / dnac-auto / 62d96573648 - Bitbucket Engineering - SJC1 (cisco.com)|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/62d965736487470796b5249b3bdbc6e47ed4f9a3] ← reverting changes that caused the issue. ,"['Auton', 'Hulk-P1', 'Hulk-P2']",Andrew Chen,Resolved,Yuvarani Iyamperumal
SEEN-2499,https://miggbo.atlassian.net/browse/SEEN-2499,sda_hitless_auth_swith_on_fabric_sites.py-361-sdaHitlessAuthSwitch/Test_TC1_hitless_authentication/test10_dot1xtoMABfallback,"*Reporter Analysis:* 

Observed the in *ESXI VM* *Hulk-Patch1 #3.713.75159* build sda_hitless_auth_swith_on_fabric_sites.py-361-sdaHitlessAuthSwitch/Test_TC1_hitless_authentication/test10_dot1xtoMABfallback failed due to unable fetch the network device from ISE via API & hitting below error message.

{{Error Code: 400 URL:https://10.195.227.68:9060/ers/config/networkdevice/91dc86f0-66fa-11ee-80f1-d2d68398ba1d Data:{'json': {'NetworkDevice': {'id': '91dc86f0-66fa-11ee-80f1-d2d68398ba1d', 'name': 'TB4-DM-eCA-BORDER.cisco.com',}}

_YWRtaW46TGFibGFiMTIz'} Message:{_
_25454:    ""ERSResponse"" : {_
_25455:      ""operation"" : ""PUT-update-networkdevice"",_
_25456:      ""messages"" : [ {_
_25457:        ""title"" : ""The specified NDG cannot be found in DB: Location#All Locations#SJ"",_
_25458:        ""type"" : ""ERROR"",_
_25459:        ""code"" : ""Application resource validation exception""_



*Used Optimized Script.*

*OVA:* 3.713.75159

*Branch: private/HulkPatch-ms/api-auto*

*Issue seen from day one in EXSI VM setup*

*Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-sda_hitless_auth_swith_on_fabric_sites.py-361-sdaHitlessAuthSwitch&begin=7120681&size=4323019&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct10_19:15:39.384070.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-sda_hitless_auth_swith_on_fabric_sites.py-361-sdaHitlessAuthSwitch&begin=7120681&size=4323019&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct10_19:15:39.384070.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-10-13T17:29:04.510+0000,,"['Auton', 'Sanity', 'exsivm', 'hulk-vm-sanity']",Raji Mukkamala,Backlog,Raghavendrachar Baraguru Mallesha Char
SEEN-2513,https://miggbo.atlassian.net/browse/SEEN-2513, Auton-Hulk EXSI VM - Task-trustsec_policy_enforcement_on_border.py-401-trustsecPolicyEnforcementOnBorder/Test_TC1_Trustsec_Policy_enforcement_on_Border,"

Observed the TC  execution stuck more than hours at the  flap interface  for the LAB-NET-WiredClient for TB4-wired-client1 on wired client during test2_connect_dot1x_wired_client subTC.



{noformat}TB4-DM-eCA-BORDER#
2023-10-15T23:49:26: %SCRIPT-INFO: Starting flap interface wired clients
2023-10-15T23:49:26: %SERVICES-INFO: Start to flap interface LAB-NET-WiredClient for TB4-wired-client1{noformat}



*Please find below jenkins job:*

[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/OptimizedSanity-for-ESXi/212/console|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/OptimizedSanity-for-ESXi/212/console]

*Used Optimized Script.*

*OVA:* 3.713.75159

*Branch: private/HulkPatch-ms/api-auto*



Regards,

Raghavendra B. M",2023-10-16T10:44:31.469+0000,"Hi [~accountid:63f50bd68ab3d6a635ecc29b] 

The below information are required when you file an autons. Please provide:

* Failed log.

* Your debug/Analysis for the issue.
* Was the testcase passed before in this testbed? When? Please provide Passed log.

Checked in Sanity TB1, this testcase is working fine, so that mark this ticket as Cancelled. Passed log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-thangqtr/users/thangqtr/archive/23-10/sanity_TB1.2023Oct17_00:39:09.750447.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-thangqtr/users/thangqtr/archive/23-10/sanity_TB1.2023Oct17_00:39:09.750447.zip&atstype=ATS]

Please check your connection between your execution server and the wired-client.

Thanks Hi [~accountid:63f50bd34c355259db9ccc4d] 

This TC running first time in EXSI Testbed, we don’t previous passed logs. Already shared jenkins logs Please take look failures  

Attached snippet of the reachability from execution server and the wired-client as discussed. Hi [~accountid:63f50bd34c355259db9ccc4d] 

We have reopened the Auton Please work on this priority on the issue.

Using Regular script also observed the same issue:

Please find the Jenkins job.

[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/VM-sanity-common-Multi-job/1153/console|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/VM-sanity-common-Multi-job/1153/console]



Regards,

Raghavendra B. M



   Hi [~accountid:63f50bd68ab3d6a635ecc29b] 

Could you please share your testbed details? Hi [~accountid:63f50bd34c355259db9ccc4d] 

Please find below Cluster details:

10.22.45.61 (admin/Maglev123)

*Note: Pleas use Read only mode  access & don’t modify fusion device configs*

Regards,

Raghavendra B. M [~accountid:63f50bd68ab3d6a635ecc29b] , please follow this template when raising Autons.

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] Hi [~accountid:63f50bd68ab3d6a635ecc29b] 

Checked that {{flap_interface}} for TB4-wired-client1 was successful.

!image-20231030-101217.png|width=777,height=145!

As we discussed in space, this is not a script issue, so mark this ticket as Cancelled. Please re-run this usecase on your setup.

If there is any issue, please give Failed log and Reopend this.

Thanks","['Auton', 'Sanity', 'exsivm', 'hulk-vm-sanity']",ThangQuoc Tran,Cancelled,Raghavendrachar Baraguru Mallesha Char
SEEN-2514,https://miggbo.atlassian.net/browse/SEEN-2514,"[Auton] [Optimized] We're facing an issue with optimized code in DNAC Version 2.3.5.3, where a ghost is still present even when running the hulk build","We're facing an issue with optimized code in DNAC Version 2.3.5.3, where a ghost is still present even when running the Hulk build

Branch: private/HulkPatch-ms/api-auto

*Log:* 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-configurebgpborder.py-122-MSDnacConfigureBGPBorder&begin=3563&size=1030461&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct02_14:59:07.079619.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-configurebgpborder.py-122-MSDnacConfigureBGPBorder&begin=3563&size=1030461&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct02_14:59:07.079619.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

{noformat}40:  External APIs are not enabled, to enable : set use_external_api=True in fabric input file.
41:  Resource path full url: https://10.22.45.61/api/v1/system-orchestrator/software-management/releases/installed
42:  DNAC Version: 2.3.5.3
43: 
44: 
45:  Connected to  DNAC
46: 
47:  Library method ""reconnect_clients"" returned in 0:00:00.125007
48:  Action: Configire bgp on border and fusion for configured vrfs border
49:  Initializing function group ""fabric_wired""
50:  Group ""fabric_wired"" initialized successfully
51:  Initializing function group ""virtual_network""
52:  Group ""virtual_network"" initialized successfully{noformat}",2023-10-16T11:59:08.914+0000,"+*PR links :*+  
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7461/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7461/overview]","['Auton', 'Sanity', 'exsi', 'junked']",Omkar Sharad Wagh,Cancelled,Omkar Sharad Wagh
SEEN-2530,https://miggbo.atlassian.net/browse/SEEN-2530,[Auton] [Hulk P1]Test_TC230_SDA_AP_mesh/ test7_configure_aps_with_work_flow,"!Mesh feature.png|width=2436,height=751!

*Reporter Analysis:*

Hi Shubham ,

In Hulk Patch1 RC2 testing, We have observed,

test7_configure_aps_with_work_flow tc failing below error :

The Schedduled Job failed: with reason {'id': '4d7ad838-4cca-4c7f-9162-dfeb9b9ead4f', 'triggeredJobTaskId': '018b3d75-8237-7638-a7ab-316db35acfa8', 'triggeredTime': 1697543193272, 'status': 'FAILED', 'failureReason': 'Unable to push to device 204.192.4.2 using protocol ssh2 the CLI do ap name APF01D.2D1C.1AA8 mode bridge', 'triggeredJobId': '4d7ad838-4cca-4c7f-9162-dfeb9b9ead4f'}

as per  space  discussion  APs rebooted an hour ago. Should the script wait for the APs to rejoin the controller,  

could you please check

webEX Space:[https://eurl.io/#vRvJ4eXf2|https://eurl.io/#vRvJ4eXf2]



RAP Role==>
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb7/SanityTB7.yaml?at=refs%2Fheads%2Fprivate%2FHulkPatch-ms%2Fsanity_api_auto#975-1000|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb7/SanityTB7.yaml?at=refs%2Fheads%2Fprivate%2FHulkPatch-ms%2Fsanity_api_auto#975-1000]

MAP Role=>:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb7/SanityTB7.yaml?at=refs%2Fheads%2Fprivate%2FHulkPatch-ms%2Fsanity_api_auto#725-750|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb7/SanityTB7.yaml?at=refs%2Fheads%2Fprivate%2FHulkPatch-ms%2Fsanity_api_auto#725-750]

Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1981819&size=503706&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct17_04:33:31.890115.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1981819&size=503706&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct17_04:33:31.890115.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



*Found on:*
Uber ISO : Hulk  P1 RC2 
Polaris version: 17.12

*Script file/Usecase :*  solution_test_sanityecamb_lan.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0 issue*

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] ",2023-10-18T07:30:37.935+0000,"sure [~accountid:620b8357878c2f00729881c8]  
will work on this issue. Hi [~accountid:712020:0c874fb3-6015-47af-93ac-7ea20beedaf7] 
Same issue seen on AWS Sanity also:
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13423570&size=1050698&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct23_02:19:30.444558.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13423570&size=1050698&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct23_02:19:30.444558.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
*Found on:*
Uber ISO : Hulk  P1 RC2  AMI VM
Polaris version: 17.12.2

*Script file/Usecase :*  solution_test_sanityecamb_lan.py

*Source Team:  Sanity* Hi [~accountid:620b8357878c2f00729881c8] 



Fix : SDA AP Mesh Auton [SEEN-2530|https://bitbucket-eng-sjc1.cisco.com/bitbucket/plugins/servlet/jira-integration/issues/SEEN-2530] (Auton)

PR Link : [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7638/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7638/overview]

Jira id - [SEEN-2530|https://bitbucket-eng-sjc1.cisco.com/bitbucket/plugins/servlet/jira-integration/issues/SEEN-2530]

Test Cases Summary - SDA wireless mesh :A network admin should be able to enable mesh AP
in fabric wireless. Wireless clients connected to Mesh Aps or Root AP should be
able to communicate to other wired & wireless clients part of same FE and different
FEs.

Trade log link - [https://earms-trade.cisco.com/tradeui/logs/details?archive=/root/new_pyats/users/root/archive/23-10/sanity_TB1.2023Oct27_12:05:16.452827.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/root/new_pyats/users/root/archive/23-10/sanity_TB1.2023Oct27_12:05:16.452827.zip&atstype=ATS]

Wiki link - [https://wiki.cisco.com/pages/viewpage.action?pageId=1644954407&src=contextnavpagetreemode|https://wiki.cisco.com/pages/viewpage.action?pageId=1644954407&src=contextnavpagetreemode] Hi [~accountid:712020:0c874fb3-6015-47af-93ac-7ea20beedaf7]  ,
 TCs are still failing after pulling   latest  code , could you please chec 
PR Merged in sanity_api_auto Branch:


|{{SEEN-2530_Mesh_AP_Auton}}|→|{{private/HulkPatch-ms/sanity...}}|

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7782/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7782/overview]
*(*[ *datetime error*|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e6367e5528bdefea39b2fb633ba2dc5e86d813ae]*)*
[e6367e5528b|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e6367e5528bdefea39b2fb633ba2dc5e86d813ae] ([ datetime error|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/e6367e5528bdefea39b2fb633ba2dc5e86d813ae])

+*Failed log:*+
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov08_03:59:29.536685.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov08_03:59:29.536685.zip&atstype=ATS]

Thanks,
Omkar  Hi [~accountid:620b8357878c2f00729881c8] 

Datetime error is fixed and PR is merged.
Also can you check why the code is not synced properly i see there is some code is missing in platform lib uni_connect method Hi  [~accountid:712020:0c874fb3-6015-47af-93ac-7ea20beedaf7] ,
I executed   main branch (*private/Hulk-ms/api-auto*) with  Hulk Patch-2 2.1.714.70284 build, TCs are still failing could please  check 

Failed log:
[https://ngdevx.cisco.com/services/taas/results/c3170302-6464-4872-92e3-67f0f55ecc2c|https://ngdevx.cisco.com/services/taas/results/c3170302-6464-4872-92e3-67f0f55ecc2c]

2023-11-17T03:35:26: %AETEST-ERROR: re.error: missing ), unterminated subpattern at position 15
2023-11-17T03:35:26: %AETEST-INFO: The result of section t*est2_check_wlan_status_on_wlc is* => ERRORED




{{C9130AXE-B , APF01D.2D1C.1AA8:==>RAP}}
APF01D.2D1C.1AA8#

2023-11-17 03:52:27,393: %UNICON-INFO: +++ initializing handle +++

2023-11-17 03:52:27,458: %UNICON-INFO: +++ APF01D.2D1C.1AA8 with via 'a': executing command 'debug capwap console cli' +++
debug capwap console cli
                               ^
% Invalid input detected at '^' marker.

{{nicon.core.errors.SubCommandFailure: ('sub_command failure, patterns matched in the output:', ['% Invalid input detected at'], 'service result', ""debug capwap console cli\r\n                               ^\r\n% Invalid input detected at '^' marker.\r\n\rAPF01D.2D1C.1AA8#"")}}

  Hi [~accountid:620b8357878c2f00729881c8] 

Can you give me the complete logs??
ngdevx log link is not working Hi [~accountid:620b8357878c2f00729881c8] 

As per discussed on webex can you share me the full logs??
also i never faced this issue can you recheck once ?

also i pushed latest changes for AP Mesh in PR for Auton SEEN-2671
please have changes and run and let me know the results  
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7932/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7932/overview]

As of now i’m closing this auton if you face again raise a auton with full logs

Thanks

Shubham Varfa","['Auton', 'Blocker', 'Feature', 'HulkPatch1', 'Optimized', 'Sanity']",Shubham Varfa,Resolved,Omkar Sharad Wagh
SEEN-2531,https://miggbo.atlassian.net/browse/SEEN-2531,[Auton]Hulk-P2:Test_TC156_Validate_sda_multicast_external_apis/test5_add_multicast_with_asm_external_rp_method_ipv4,"*Reporter Analysis:* 

The Script is using array instead of string “externalRpIpAddress”

*Description:* 
API is working as designed.

In payload, you are not using need following element correctly:

externalRpIpAddress ==> is string

And you are using it as array.

*Branch Name:  bgl/HulkPatch2-ms/api_auto*

*Script file/Usecase:* 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=refs%2Fheads%2Fbgl%2FHulkPatch2-ms%2Fapi-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=refs%2Fheads%2Fbgl%2FHulkPatch2-ms%2Fapi-auto]

*Fail Log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2310505&size=42505&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-10%2Fsanitycombine.2023Oct18_13:26:27.707843.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2310505&size=42505&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-10%2Fsanitycombine.2023Oct18_13:26:27.707843.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]",2023-10-18T10:15:50.845+0000,"Same solution with ticket [https://miggbo.atlassian.net/browse/SEEN-1508|https://miggbo.atlassian.net/browse/SEEN-1508|smart-link] 

Fixed in PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7510/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7510/overview]","['Auton', 'Hulk-P2', 'HulkPatch2', 'Sanity']",ThanhTan Nguyen,Resolved,SAINATH CHATHARASI
SEEN-2532,https://miggbo.atlassian.net/browse/SEEN-2532,[Auton]:hulk P1: Test_TC234_email_notifier_for_events / test3_enable_magctl_service_in_dnac,"*Reporter Analysis:*

Observing aws cluster is not able to login via ssh to cluster and try to execute magctl command
Manually command is taking

*Description*:  

{noformat}0229: 
 Using persistent connection.{noformat}

{noformat}10230: 
 Calling method `send_cmd`.{noformat}

{noformat}10231: 
 ('172.35.16.150', 'maglev', 'xxxxxxxx'){noformat}

{noformat}10232: 
 Encountered error on login. Check login details or try again.  Error details:{noformat}

{noformat}10233: 
 Could not establish connection to host{noformat}

{noformat}10234: 
 Encountered error during excecution of `send_cmd`{noformat}

{noformat}10235: 
 Could not establish connection to host{noformat}

{noformat}10236: 
 Traceback (most recent call last):{noformat}

{noformat}10237: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Hulk/DMZ-Hulk-sanity-common-Multi-job/services/maglev_cli/utils.py"", line 34, in wrapper{noformat}

{noformat}10238: 
     self.connect(){noformat}

{noformat}10239: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Hulk/DMZ-Hulk-sanity-common-Multi-job/services/maglev_cli/maglevclihandler.py"", line 141, in connect{noformat}

{noformat}10240: 
     if not self.ssh.login(*args, **kwargs):{noformat}

{noformat}10241: 
   File ""/ws/owagh-sjc/Solution_pyatsenv/lib/python3.10/site-packages/pexpect/pxssh.py"", line 424, in login{noformat}

{noformat}10242: 
     raise ExceptionPxssh('Could not establish connection to host'){noformat}

{noformat}10243: 
 pexpect.pxssh.ExceptionPxssh: Could not establish connection to host{noformat}

{noformat}10244: 
 Last ssh response:{noformat}

{noformat}10245: 
 Could not establish connection to host{noformat}

{noformat}10246: 
 Error in executing command on DNAC Maglev CLI.{noformat}

{noformat}10247: 
 Test returned in 0:00:00.274843{noformat}

{noformat}10248: 
 Failed reason: Failed to execute the magctl service log level set dna-event-runtime enabler{noformat}

{noformat}10249: 
 The result of section test3_enable_magctl_service_in_dnac is => FAILED{noformat}

*Branch Name:* private/HulkPatch-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity",2023-10-18T11:57:22.711+0000,"Debugged the issue and found it is local json file , please modify 



Changes in the ssh key
from : /home/admin/key-n-california.pem
to:
/auto/dna-sol/ws/pem_key_aws/key-n-california.pem [https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-10/auto_MS_job.2023Oct18_10:29:53.527462.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-10/auto_MS_job.2023Oct18_10:29:53.527462.zip&atstype=ATS] [~accountid:5fe224a53b5e47013862f185] 
Even after changing key i faced issue wrt testcase:
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14512995&size=23264&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct23_02:19:30.444558.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14512995&size=23264&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct23_02:19:30.444558.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



As per my analysis:
1._shell was enabled on cli cluster that is why subtc [test3_enable_magctl_service_in_dnac|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14512995&size=23264&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct23_02:19:30.444558.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] failed for me even after key was changed

2.When i disbaled shell and rerun sub testcase passed for me:
Passed log:

[test3_enable_magctl_service_in_dnac|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8568200&size=7858&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct23_04:00:58.515528.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

3.New two subtc failed:
[Test_TC234_email_notifier_for_events|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8551401&size=80031&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct23_04:00:58.515528.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]  /   test6_verify_the_events_counts_summary_after_flap

[Test_TC234_email_notifier_for_events|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8551401&size=80031&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct23_04:00:58.515528.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]  /   test7_verify_dnac_debug_email_instance



Thanks,
Anusha John waiting for cluster as this code works fine on other clusters,  once in problematic state share to to check the issue ","['AWS_Sanity', 'Auton', 'Feature', 'HulkPatch1']",Anusha John,Reopened,Anusha John
SEEN-2533,https://miggbo.atlassian.net/browse/SEEN-2533,Missing Testcases on YAML Mapping ,"*Reporter Analysis:*

Recently while executing observed some of the basic testcases is missing from usecase mappings


*Description:*

[Test_TC3_generate_dhcp_server_config_on_fusion|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=320939&size=4618&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct12_11:39:47.242488.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] / [test1_generate_dhcp_server_config_on_fusion|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=321503&size=994&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct12_11:39:47.242488.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

[Test_TC29_DNAC_verify_aaa_lisp_radius_configuration_on_devices_after_provision|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1212846&size=6037&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct12_11:39:47.242488.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]  / [test5_verify_single_connection_config_on_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1217638&size=1037&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct12_11:39:47.242488.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]  ------>Hulk Feature



*Branch Used:*
private/HulkPatch-ms/sanity_api_auto

private/Hulk-ms/sanity_api_auto

private/HulkPatch2-ms/sanity_api_auto

private/Ghost-ms/sanity_api_auto

**Script* [*file:*|file:*][*|file:*] solution_test_sanityecamb_lan.py and  *solution_test_sanityecamb.py*

*Source Team:*  Sanity

Issue Seen first time or day0 issue:",2023-10-18T12:36:37.903+0000,"TC3  generate_dhcp_server_config_on_fusion is already present in the usecase mappings

Non-LAN Sanity:

goldenConfigOverlayBrownfieldConfigCleanup. [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/nonlansanitysuite/sanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#11|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/nonlansanitysuite/sanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#11]

LAN Sanity:

goldenConfigoverlayConfigCleanup [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/lansanity/lansanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#11|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/lansanity/lansanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto#11]



Added vverify_single_connection_config_on_devices to sanity optimized folder [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/62f57582df1b40773ee0cc24c167ec00f9e6f93a|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/62f57582df1b40773ee0cc24c167ec00f9e6f93a]","['AWS_Sanity', 'Auton', 'Ghost', 'Hulk', 'HulkPatch1', 'Sanity']",Raji Mukkamala,Resolved,Anusha John
SEEN-2537,https://miggbo.atlassian.net/browse/SEEN-2537,[Auton]:hulk feature: ewlc PNP Onboarding Validation needs a check for device status,"*Reporter Analysis:*

Device pnp onboarding is successful and device is in inventory with managed state.
Once after the device is claimed during the PNP process - the script is looking for the device in managed state and not waiting for the device to be in provisioned state. 
There is a need of check for the device to be in provision state and then the script should look for device in managed state. 

*Code Snip:* 

!image-20231019-071851.png|width=1430,height=597!


Testcase: [Test_TC27_DNAC_adding_fabric_devices_to_site|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=304017&size=54605&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct18_05:26:52.604988.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] / [test7_claim_wlc_to_site_with_pnp_onboarding|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=331492&size=26957&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct18_05:26:52.604988.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity",2023-10-19T07:15:02.748+0000,"Tried adding a sleep for 3mins which didnt help - As device onboarding took around 15mins to onboard DNAC in provisioned state.  
[89e5b680e1e|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/89e5b680e1e8acca0a75f1677c9c4fe3d7189597]

!image-20231019-085714.png|width=1525,height=647!

!image-20231019-085653.png|width=1535,height=606!

[~accountid:62a9d1d0192edb006f9f5332]  Could you please check this  [~accountid:5e1415780242870e996f0b2f] , please follow this template when raising Autons.

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] Hi [~accountid:5e1415780242870e996f0b2f] / [~accountid:61efa8c457b25b006877eda3] / [~accountid:641058d57222b08f3e7064d0] / [~accountid:63f50bf0e8216251ae4d59ca] 
The code is merged now for the Auton, Please check it out 

PR:  [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7432/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7432/overview]

Thanks,

Archana Thank you [~accountid:62a9d1d0192edb006f9f5332]  
Currently running with Ghost P3, in upcoming hulk cadence will try with latest code!","['AWS_Sanity', 'Auton', 'Feature', 'Hulk', 'HulkPatch1', 'Sanity', 'Uplift']",Archana KM,Resolved,Ashwini R Jadhav
SEEN-2538,https://miggbo.atlassian.net/browse/SEEN-2538,[Auton] - BGP session status between Border and control plan fails due to presence of Border devices with Polaris version less than 17.10.1,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/HulkPatch-ms/api-auto

*Uber ISO tested:* Hulk Patch1 RC1 - 2.1.713.70292 (with PUBSUB enabled )& Hulk Patch2 RC1 - 2.1.713.70302 (with PUBSUB enabled)

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

When integrating the Ghost feature - Monitor and Debug BGP session states, we see that subTC corresponding to BGP session status between Border and control plan with PUBSUB enabled fails due to presence of Border devices with Polaris version lower than 17.10.1 version. 

1) The logic implemented for calculating the scores Fabric side has to be enhanced if Border devices are with Polaris version is lower than 17.10.1. This corresponds to 256.3 subTC. Accordingly to be updated to handle also in 256.7 subTC.

2) Also we need logging improvements in subTCs - 256.1, 256.2, 256.5, 256.6,

→ where the scores obtained per fabric site along with device details has to be printed.

→ Need to print that scores are from Assurance and from Fabric provision seperately so that user can have better view of results.

*Failed log -* [Test_TC256_Monitor_And_Debug_BGP_Session_Status_Issues_Between_Border_And_Control_Plane|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2391856&size=127061&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct17_07:01:40.877686.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



Testbed was shared in same state and also had a joint syncup session on this and agreed upon with above changes with [~accountid:63f50bcafb3ac4003fa2c6dd] ",2023-10-19T10:28:25.960+0000,"The root cause: The library ""compare_image_version"" was compared to the wrong type value. Need to compare the 'integer' not 'string'
Code changes: 

# Fix the library ""compare_image_version"" to work correctly.
# Improve the log message so the user can better view the results. # PR HulkPatch-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7575/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7575/overview]
# Test Case:  {{Test_TC256_Monitor_And_Debug_BGP_Session_Status_Issues_Between_Border_And_Control_Plane}}
# Script files: testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
# Trade log link HulkPatch-ms/api_auto: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-10/sanity_TB7.2023Oct23_02:30:03.045767.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-10/sanity_TB7.2023Oct23_02:30:03.045767.zip&atstype=ATS] [~accountid:63f50bcafb3ac4003fa2c6dd] : Tried executing from your Private branch code - private/HulkPatch-ms/SEEN-2538

Still observing same failure

*Failed log -* [Test_TC256_Monitor_And_Debug_BGP_Session_Status_Issues_Between_Border_And_Control_Plane|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2375122&size=114851&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct25_10:49:52.693722.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Jenkins link -* [https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/view/MSTB1/job/MSTB1%20MDNAC%20DR/98/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-REG/view/MSTB1/job/MSTB1%20MDNAC%20DR/98/] [~accountid:62d2fec15d6f5fd2c3db8f9f], Can I borrow that cluster to check? Oh, never mind. I miss 1 parameter in the 'if' statement. I already updated it in PR. Could you please take a look again? This is the pass log from the sr_mb1 testbed (same as your testbed below):

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-10/sr_mb_multi_sites_mdnac.2023Oct25_21:08:10.789127.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-10/sr_mb_multi_sites_mdnac.2023Oct25_21:08:10.789127.zip&atstype=ATS]

Sorry for my mistake. [~accountid:63f50bcafb3ac4003fa2c6dd] : Thanks for adding the changes!  Tried re-execution with the below updated changes. Its working fine now.

*Pass log -* [Test_TC256_Monitor_And_Debug_BGP_Session_Status_Issues_Between_Border_And_Control_Plane|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2385590&size=114859&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct25_22:10:18.630064.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Also can we add these changes on Ghost branch - *private/Ghost-ms/api-auto* as well ? We are observing same issue during Ghost testing as well. 

*Failed log on Ghost Patch2 HF2 - 2.1.614.70852-HF2 :* [Test_TC258_Monitor_And_Debug_BGP_Session_Status_Issues_Between_Border_And_Control_Plane|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9035323&size=114095&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct20_03:55:58.335972.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] PR for *private/Ghost-ms/api-auto*: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7621/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7621/overview]

Trade log link: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-10/sr_mb_multi_sites_mdnac.2023Oct26_00:30:06.385411.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-10/sr_mb_multi_sites_mdnac.2023Oct26_00:30:06.385411.zip&atstype=ATS]","['Auton', 'Ghost', 'Hulk', 'HulkPatch', 'Integration', 'MSTB1']",NhanHuu Nguyen,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2539,https://miggbo.atlassian.net/browse/SEEN-2539,[Auton]:[Ghost]: Task-1  /   Test_TC53_DNAC_provision_all_aps  /   test1_provision_all_aps,"Debugged/Analyzed the issue: Y
Compared with the last passed log: Y
Checked and compared the configuration: Y
Confirmed with DE on the root cause: N
Issue faced: we have observed that  APs assigning to site and AP provisioning  is Failing  due to which AP related testcases are failing
Failure/Errors snip from log: 

{noformat}5411: 
 Config Preview Activity failed with reason: NCWL10307: APWirelessConfiguration FLOOR1_LEVEL1_TYPICAL_235f0 with Id 852a98b0-b7d6-453b-b682-ab8b1a1d98bf SiteId 35bbfb6a-0fa4-4df3-bacc-82a361961af4 and RfProfile TYPICAL is already present in the database{noformat}

{noformat}5412: 
{noformat}

{noformat}5413: 
 Activity: 018bfac9-abba-7e98-81a1-208ecfd79859 Trigger job: {'id': '4b09c7af-b2f1-4019-8b59-9960525475c3', 'triggeredJobTaskId': '018bfac9-ac07-7b60-a0d0-a2d0bdc64fdd', 'triggeredTime': 1700719602701, 'status': 'FAILED', 'failureReason': 'NCWL10307: APWirelessConfiguration FLOOR1_LEVEL1_TYPICAL_235f0 with Id 852a98b0-b7d6-453b-b682-ab8b1a1d98bf SiteId 35bbfb6a-0fa4-4df3-bacc-82a361961af4 and RfProfile TYPICAL is already present in the database', 'triggeredJobId': '4b09c7af-b2f1-4019-8b59-9960525475c3'}{noformat}

{noformat}5498: 
 activity_id is False. Config preview task failed for description Provisioning Unified APs at time 1700719602.3597207 - Configuration Preview{noformat}


Regression team debug analysis with details: Manually Proviosioning aps passing when we try via script its failing .
DE analysis/confirmation with details: NA
What do Automation team need to look into: after TC52 Increase the timer 3 to 4 min , after Execution should start for TC53 .  if it is not passing check the script issue and fix .

Failed log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1523740&size=1214971&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_21:51:23.597471.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1523740&size=1214971&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_21:51:23.597471.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Last passed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=32411778&size=464855&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov21_08:46:27.332358.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=32411778&size=464855&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov21_08:46:27.332358.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Failed DNAC version/ISO: Hulk P1 RC7# 2.3.7.3-70332
Last Passed DNAC version/ISO: Ghost P2 RC3 # 2.3.5.4-70852-HF4
Script/Mapping file Used: dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py  (Optimized LAN Script)
TCs Impacted: 1
Branch Used: private/HulkPatch-ms/sanity_api_auto
Testbed details: NA
Team: DNAC Solution Sanity",2023-10-19T10:52:46.876+0000,"[~accountid:63a2a522082abdd71bb36e09] , please follow this template when raising Autons.

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] [~accountid:62d2fe9f8afb5805e5d5af49] I have updated the details as per checklist .","['Auton', 'Ghost', 'Hulk', 'HulkPatch', 'optimized', 'sanity']",Andrew Chen,Backlog,Elton GoldChristopher
SEEN-2540,https://miggbo.atlassian.net/browse/SEEN-2540,Revert SEEN-2514 and update `get_dnac_version`,"* Enhance: [https://miggbo.atlassian.net/browse/SEEN-2514|https://miggbo.atlassian.net/browse/SEEN-2514|smart-link]
** Issue is about log message → shouldn’t fix the logic → revert it
* Update lib {{get_dnac_version}}
** the actual version is a string with the format: “2.3.7.4.70203” → enhance script to get the best matching or get the latest supported version, current is 2.3.5.3",2023-10-19T11:16:43.541+0000,"* Update for Hulk: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7529/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7529/overview]
* Update for HulkPatch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7530/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7530/overview]
* Update for HulkPatch2: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7531/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7531/overview]","['Auton', 'Enhancement']",ThanhTan Nguyen,Resolved,ThanhTan Nguyen
SEEN-2541,https://miggbo.atlassian.net/browse/SEEN-2541,[EXSI VM Sanity Integration] Hulk patch1 - Wireless Controller Mobility group check,"Add first wlc to mobility group. Then add second wlc to second mobility group, and add the first one as a member. Ensure that the name of the mobility group of the first WLC remains the same and does not get changed when adding it to the second wlc's mobility group.

PR link:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7524/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7524/overview]

Wiki link:

[https://wiki.cisco.com/display/EDPEIXOT/Wireless+Controller+Mobility+group+check|https://wiki.cisco.com/display/EDPEIXOT/Wireless+Controller+Mobility+group+check]



Passed log:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct18_21:43:21.689384.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct18_21:43:21.689384.zip&atstype=ATS]",2023-10-19T12:00:02.074+0000,,"['Auton', 'Sanity', 'hulk', 'hulk-vm-sanity']",Raghavendrachar Baraguru Mallesha Char,Resolved,Raghavendrachar Baraguru Mallesha Char
SEEN-2542,https://miggbo.atlassian.net/browse/SEEN-2542,[EXSI VM Sanity Integration] Hulk patch1 -Test_TC1_edit_site_name ,"In a migrated cluster, after a network admin modified the fabric site name .

Fabric wireless should continue work for new onboarded AP and AP onboarded before migration

Wiki link:

[https://wiki.cisco.com/display/EDPEIXOT/Edit+sites+name|https://wiki.cisco.com/display/EDPEIXOT/Edit+sites+name]

pass log:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%  2Fenv_optimized_auto_job.2023Oct16_04:21:58.808640.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%25%20%202Fenv_optimized_auto_job.2023Oct16_04:21:58.808640.zip&atstype=ATS]



Reagrds,

Raghavendra B. M",2023-10-19T12:04:21.551+0000,,"['Auton', 'Sanity', 'hulk-vm-sanity']",Raghavendrachar Baraguru Mallesha Char,Resolved,Raghavendrachar Baraguru Mallesha Char
SEEN-2543,https://miggbo.atlassian.net/browse/SEEN-2543,[Auton]:[hulk]: Test_TC232_Propagate_SGT_on_SDA_ip_handoff /test1_enable_cts_sgt_monitor_on_border_fusion,"*Reporter Analysis:*

Hi *ThangQuoc Tran* ,

In Hulk Patch1 RC2 testing, We have observed,
[Test_TC232_Propagate_SGT_on_SDA_ip_handoff|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2125174&size=397848&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct16_22:58:33.870624.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] /test1_enable_cts_sgt_monitor_on_border_fusion
test case unable to  fusion the Fusion interface.

{{TB7-Fusion# }}

{noformat}Total cdp entries displayed : 5
TB7-Fusion#
11496:  Could not find fusion interfaces!!!
11497:  TB7-Fusion
11498:  Capability Codes: R - Router, T - Trans Bridge, B - Source Route Bridge
11499:                    S - Switch, H - Host, I - IGMP, r - Repeater, P - Phone,
11500:                    D - Remote, C - CVTA, M - Two-port Mac Relay
11501: 
11502:  Device ID        Local Intrfce     Holdtme    Capability  Platform  Port ID
11503:  TB7-eWLC.cisco.com
11504:                   Ten 1/1/4         171              R I   C9800-40- Ten 0/0/1
11505:  TB7-NY-FIAB.cisco.com
11506:                   Ten 1/1/2         161             R S I  C9404R    Ten 2/0/1
11507:  MGMT_SW3_10.30.0.64
11508:                   Gig 0/0           147              S I   WS-C2960X Gig 1/0/8
11509:  TB7-SJ-eCA-BORDER-CP.cisco.com
11510:                   Ten 1/0/48        132             R S I  C9500-48Y Twe 2/0/1
11511:  TB7-DM-L2-SWITCH.cisco.com
11512:                   Ten 1/1/1         124              S I   WS-C3850- Ten 1/1/1
11513: 
11514:  Total cdp entries displayed : 5
11515:  Library group ""cli_check"" method ""config_cts_sgt_monitor_on_dev"" returned in 0:00:18.912919
11516:  Test returned in 0:00:19.313501
11517:  Failed reason: Result: Failed to enable monitor capture on borders fusion{noformat}






could you please check


Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2125174&size=397848&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct16_22:58:33.870624.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2125174&size=397848&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct16_22:58:33.870624.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
*Found on:*
Uber ISO : Hulk P1 RC2 
Polaris version: 17.12

*Script file/Usecase :*  solution_test_sanityecamb_lan.py

*Source Team:  Sanity*

*Issue Seen first time or day0 issue: day0 issue*

*Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-7] 
",2023-10-19T12:51:33.981+0000,"[~accountid:620b8357878c2f00729881c8] , please follow this template when raising Autons.

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] Sanity TB7 is being used for Hulk RC 5 execution. Will start working on this ticket when the testbed is in an ideal state. Hi [~accountid:620b8357878c2f00729881c8], How about Sanity TB7 status? Can I use it for debugging this issue? Hi [~accountid:620b8357878c2f00729881c8] 

Could you please share the cluster which being observed for this issue? Hi [~accountid:63f50bd34c355259db9ccc4d] ,
 the testbed is been redeployed for the next Sanity request.(Hulk P2Rc2)

Currently, we do not have a testbed in the issue stat,

could you please try on your local testbed ","['Auton', 'Feature', 'HulkPatch1', 'Optimized', 'Sanity']",ThangQuoc Tran,Backlog,Omkar Sharad Wagh
SEEN-2544,https://miggbo.atlassian.net/browse/SEEN-2544,[Auton] - Provision AP with RF profile verification TC with respect to Zero Wait DFS feature not working as expected,,2023-10-19T12:58:47.391+0000,"[~accountid:62d2fec15d6f5fd2c3db8f9f] , please follow this template when raising Autons.

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] We have wlc role check before executing the cli, can you check if this commit was present wit last run. [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/services/dnaserv/lib/api_groups/provision_wireless/group.py?at=refs%2Fheads%2Fprivate%2FHulkPatch-ms%2Fapi-auto#1538|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/services/dnaserv/lib/api_groups/provision_wireless/group.py?at=refs%2Fheads%2Fprivate%2FHulkPatch-ms%2Fapi-auto#1538]","['Auton', 'Halleck', 'Hulk', 'HulkPatch', 'Integration', 'MSTB1']",Raji Mukkamala,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2546,https://miggbo.atlassian.net/browse/SEEN-2546,[Auton][MSTB2] -  Test_TC224_flexible_reports_check  /   test1_generate_and_verify_flexible_swim_report,"Hi,

We have started integrating “[Test_TC224_flexible_reports_check|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2452511&size=156987&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_three_sites_FIPS.2023Oct19_07:44:02.653415.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]” usecase on MSTB2 but it failing while querying for image upgrade, API is failing with below error
13696:  Resource path full url: [https://10.195.243.37/api/v1/ncp-node/graphql|https://10.195.243.37/api/v1/ncp-node/graphql]
13697:  response for the image upgrade api call:
13698:   {'response': []}
13699:  Traceback (most recent call last):
13700:    File ""/auto/dna-sol/ws/divayada/Hulk-patch1latest/dnac-auto/services/commonlibs/test_wrapper.py"", line 301, in wrapper
13701:      result = testfunc(func_self, **kwargs)
13702:    File ""/auto/dna-sol/ws/divayada/Hulk-patch1latest/dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py"", line 23378, in test1_generate_and_verify_flexible_swim_report
13703:      if dnac_handle.generate_and_validate_flexible_swim_report():
13704:    File ""/auto/dna-sol/ws/divayada/Hulk-patch1latest/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper
13705:      result = method(*args, **kwargs)
13706:    File ""/auto/dna-sol/ws/divayada/Hulk-patch1latest/dnac-auto/services/dnaserv/lib/api_groups/report_gen/group.py"", line 825, in generate_and_validate_flexible_swim_report
13707:      download_and_valiadte = self.download_report(rid, exe_id, payload)
13708:    File ""/auto/dna-sol/ws/divayada/Hulk-patch1latest/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper
13709:      result = method(*args, **kwargs)
13710:    File ""/auto/dna-sol/ws/divayada/Hulk-patch1latest/dnac-auto/services/dnaserv/lib/api_groups/report_gen/group.py"", line 895, in download_report
13711:      reference_data = self.swim_report_reference()
13712:    File ""/auto/dna-sol/ws/divayada/Hulk-patch1latest/dnac-auto/services/dnaserv/lib/decorators.py"", line 32, in wrapper
13713:      result = method(*args, **kwargs)
13714:    File ""/auto/dna-sol/ws/divayada/Hulk-patch1latest/dnac-auto/services/dnaserv/lib/api_groups/report_gen/group.py"", line 917, in swim_report_reference
13715:      for key in response['data']['allImageUpgradeTask']['items']:
13716:  KeyError: 'data'
13717:  Test returned in 0:02:00.606156
13718:  Errored reason: data

Failed log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2453078&size=30494&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_three_sites_FIPS.2023Oct19_07:44:02.653415.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2453078&size=30494&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_three_sites_FIPS.2023Oct19_07:44:02.653415.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Script : solution_test_3sites_sjc_nyc_sf.py
Ova#3.713.75159
Branch : private/HulkPatch-ms/api-auto",2023-10-19T17:58:55.220+0000,"[~accountid:63f50bf0e8216251ae4d59ca] , please follow this template when raising Autons

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] Waiting for information from [~accountid:63f50bf0e8216251ae4d59ca]  on DNAC-VM as I dont have the DNAC-VM set-up

Thanks,

Archana Hi [~accountid:63f50bf0e8216251ae4d59ca] 
I have updated the flexible swim report testcase

PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8054/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8054/overview]

BugID: CSCwi29125","['Auton', 'ESXi', 'Hulk-Patch1', 'Integration', 'MSTB2']",Archana KM,Resolved,Divakar Kumar Yadav
SEEN-2565,https://miggbo.atlassian.net/browse/SEEN-2565,Auton-Hulk EXSI VM - Test_TC1_View_Issue_Consolidation_Of_Remote_Internet_Sessions_Between_Control_plane_And_Border_At_VN_level,"*Reporter Analysis:*

 Test_TC1_View_Issue_Consolidation_Of_Remote_Internet_Sessions_Between_Control_plane_And_Border_At_VN_level  TC errored due to {{“ext_name_map.csv}}"" file not present under dnac-auto/services/*dnaserv folder* on both the branch *private/HulkPatch-ms/api-auto* &  *private/Hulk-ms/api-auto* in script.



Please find below error message:

_13291:  Writing library method name map into file ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/OptimizedSanity-for-ESXi/services/dnaserv/ext_name_map.csv""_
_13292:  Successfully wrote map_
_13293:  Traceback (most recent call last):_
_13294:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/OptimizedSanity-for-ESXi/services/commonlibs/test_wrapper.py"", line 301, in wrapper_
_13295:      result = testfunc(func_self, **kwargs)_
_13296:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/OptimizedSanity-for-_



*Failed log:* 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-view_issue_consolidation_of_remote_internet_sessions_at_VN_level.py-391-viewIssueConsolidationOfRemoteInternetSessionsAtVNLevel&begin=4014524&size=22986&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct20_02:05:43.121585.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-view_issue_consolidation_of_remote_internet_sessions_at_VN_level.py-391-viewIssueConsolidationOfRemoteInternetSessionsAtVNLevel&begin=4014524&size=22986&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct20_02:05:43.121585.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Used Optimized Script.*

*OVA:* 3.713.75159

*Branch: private/HulkPatch-ms/api-auto*

*Note:-* We don’t have previous pass log for this TC, integrating on the EXSI VM TB first time.

All device are reachable from inventory page & SD access page as well, attached screenshot.



Regards,

Raghavendra B. M ",2023-10-20T10:56:07.425+0000,"The root cause:
Test 3 and test 6 got Errored because there is missing a library.
Test 4 failed because the assurance page took time to update the new data. Change another way to get data on assurance page. # PR HulkPatch-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7389/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7389/overview]
# Test Case:  Test_TC224_View_Issue_Consolidation_Of_Remote_Internet_Sessions_Between_Control_plane_And_Border_At_VN_level
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-09/sanity_TB2.2023Sep12_21:01:53.489395.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-09/sanity_TB2.2023Sep12_21:01:53.489395.zip&atstype=ATS] Hi [~accountid:63f50bd68ab3d6a635ecc29b] : Could you cherry-pick this PR and check again if the TC run passes or fails? This Auton was fixed in another PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7002/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7002/overview]","['Auton', 'Sanity', 'exsivm', 'hulk-vm-sanity']",NhanHuu Nguyen,Resolved,Raghavendrachar Baraguru Mallesha Char
SEEN-2566,https://miggbo.atlassian.net/browse/SEEN-2566,[Auton] Hulk P1 - Wireless Solution Sanity - Script Fix Required for Discovery TC11,"# *Regression Team Actions Checklist:*
Debugged/Analyzed the issue: Y
Compared with the last passed log: Y
Checked and compared the configuration: Y
Confirmed with DE on the root cause: Not required as suspecting as script commit breakage due to [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f51be0dbe30a8cbca6f6d31dc970345ff33e12fe|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f51be0dbe30a8cbca6f6d31dc970345ff33e12fe]
# *Issue faced:* TC11 is getting failed. Aireos WLC is missing in Discovery and Inventory. Inventory present polaris devices also have change in ip address for nfswitch and transit. Expected 204.1.2.2 for nfswitch and 204.1.2.3 for transit. But observed 204.1.1.5 for nf switch and 204.1.1.2 for transit
# *Failure/Errors snip from log:*  Refer TC11 on [http://10.89.48.9/tftpboot2/yiyamper/pod1_console.log|http://10.89.48.9/tftpboot2/yiyamper/pod1_console.log]
# *Regression team debug analysis with details:* Suspecting as script commit breakage due to [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f51be0dbe30a8cbca6f6d31dc970345ff33e12fe|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f51be0dbe30a8cbca6f6d31dc970345ff33e12fe] as this commit has discovery related changes on recent history
# *DE analysis/confirmation with details:* Not required as suspecting as script commit breakage due to [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f51be0dbe30a8cbca6f6d31dc970345ff33e12fe|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f51be0dbe30a8cbca6f6d31dc970345ff33e12fe]
# *What do Automation team need to look into?* Fix script breakage
# *Failed log:* [http://10.89.48.9/tftpboot2/yiyamper/pod1_console.log|http://10.89.48.9/tftpboot2/yiyamper/pod1_console.log]
# *Last passed log:* [https://ngdevx.cisco.com/services/taas/results/01af6f2d-5984-43a2-ad99-75056c5cad40|https://ngdevx.cisco.com/services/taas/results/01af6f2d-5984-43a2-ad99-75056c5cad40] 
# *Failed DNAC version/ISO:* assembly_release_dnac_hulk_07-2.1.713.70319.iso (Cluster 10.88.187.190)
# *Last Passed DNAC version/ISO:* assembly_release_dnac_hulk_07-2.1.713.70292.iso
# *Script/Mapping file Used:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py
# *TCs Impacted:* TC11 and its dependent many TCs
# *Branch Used:* rcdn/HulkPatch-ms/api-auto  (local branch that will be synced to main branch Private/HulkPatch-ms/api-auto before every reg run start)
# *DNAC Type:* On-Prem
# *Testbed details:* [SOL-REG POD01 - WNBURCDNST WNBU Richardson Test Teams - IT Wiki (cisco.com)|https://wiki.cisco.com/display/WNBURCDNST/SOL-REG+POD01] - Cluster 10.88.187.190 for current fail scenario - Also check cluster 10.89.48.89 for pass scenario which was started 2 days back with 2.1.713.70315.iso and discovery went fine that time. (for discovery/inventory screenshots if required) 
# *Team:* Wireless SIT Team",2023-10-20T14:00:35.137+0000,"The issue was caused by the fix for [https://miggbo.atlassian.net/browse/SEEN-1704|https://miggbo.atlassian.net/browse/SEEN-1704|smart-link] 

Added fix for it:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f20b1c4e01f9b4a23c9c99cb7db37ab288f5e0fc#testcases/forty_eight_hour/solution_test_sanityecamb.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/f20b1c4e01f9b4a23c9c99cb7db37ab288f5e0fc#testcases/forty_eight_hour/solution_test_sanityecamb.py]","['Auton', 'Hulk-P1']",Tran Lam,Resolved,Yuvarani Iyamperumal
SEEN-2567,https://miggbo.atlassian.net/browse/SEEN-2567,[Auton]:[Hulk]:Task-rback_roles_users_configs.py-31-systemUserAndRoleback  /   Test_TC1_DNAC_RBAC_create_users_roles  /   test4_Upload_CA_trusted_certificate ,"we have observerd on Sanity Optimized Regular script , facing script issue in Upload_CA_trusted_certificate 
*Uber ISO Version tested:* Hulk P1 RC3 #  2.1.713.70319

*Script :*  Optimized code 
*script file:*  {{/services/iseserv/isecli.py}}

*Error snip:*

{noformat}554: 
 Traceback (most recent call last):{noformat}

{noformat}555: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity@2/services/commonlibs/test_wrapper.py"", line 301, in wrapper{noformat}

{noformat}556: 
     result = testfunc(func_self, **kwargs){noformat}

{noformat}557: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity@2/services/dnaserv/testplans/testplan_groups/common_testplan.py"", line 450, in test4_Upload_CA_trusted_certificate{noformat}

{noformat}558: 
     if not dnac_handle.is_ISE_meet_requirement(""3.1""):{noformat}

{noformat}559: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity@2/services/dnaserv/lib/decorators.py"", line 32, in wrapper{noformat}

{noformat}560: 
     result = method(*args, **kwargs){noformat}

{noformat}561: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity@2/services/dnaserv/lib/api_groups/ise_integration/group.py"", line 71, in is_ISE_meet_requirement{noformat}

{noformat}562: 
     self.services.ISE_version = isecli.get_version(){noformat}

{noformat}563: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity@2/services/iseserv/isecli.py"", line 555, in get_version{noformat}

{noformat}564: 
     combined = version[0]{noformat}

{noformat}565: 
 IndexError: list index out of range{noformat}


*Failed Log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-rback_roles_users_configs.py-31-systemUserAndRoleback&begin=115565&size=15017&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct20_05:22:26.643487.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-rback_roles_users_configs.py-31-systemUserAndRoleback&begin=115565&size=15017&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct20_05:22:26.643487.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-10-20T14:19:23.331+0000,,"['Auton', 'Hulk', 'HulkPatch', 'sanity', 'script']",Elton GoldChristopher,Cancelled,Elton GoldChristopher
SEEN-2568,https://miggbo.atlassian.net/browse/SEEN-2568, Test_TC179_dnac_rlan_workflow / test7_verifying_client_details_in_device fails due to mis-match in the RLAN ID,"[Test_TC179_dnac_rlan_workflow/test7_verifying_client_details_in_device|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2022400&size=25339&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_three_sites_FIPS.2023Oct19_07:44:02.653415.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] failed on ESXi MS TB2 with OVA: 3.713.75159 due to mis-match in the RLAN ID.

{noformat}11546:  Output of 'sh wireless client summary' executed on FW-9800-11:
11547:  sh wireless client summary
11548:  Number of Clients: 1
11549: 
11550:  MAC Address    AP Name                                        Type ID   State             Protocol Method     Role
11551:  -------------------------------------------------------------------------------------------------------------------------
11552:  0050.56a7.8dff AP00DF.1D86.2A1C                               RLAN 3    Run               Ethernet Dot1x      Local
11553: 
11554:  Number of Excluded Clients: 0
11555: 
11556:  FW-9800-11#
11557:  Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:02.184495
11558:  client details sh wireless client summary
11559:  Number of Clients: 1
11560: 
11561:  MAC Address    AP Name                                        Type ID   State             Protocol Method     Role
11562:  -------------------------------------------------------------------------------------------------------------------------
11563:  0050.56a7.8dff AP00DF.1D86.2A1C                               RLAN 3    Run               Ethernet Dot1x      Local
11564: 
11565:  Number of Excluded Clients: 0
11566: 
11567:  FW-9800-11# in device FW-9800-11
11568:  Number of clients 1 in device FW-9800-11
11569:  client:0050.56a7.8dff is not showing in dev:FW-9800-11
11570:  Library group ""cli_check"" method ""verify_client_details_in_device"" returned in 0:00:02.187307
11571:  Test returned in 0:00:02.188392
11572:  Failed reason: Failed to show the wired client device as wireless device
11573:  The result of section test7_verifying_client_details_in_device is => FAILED{noformat}

As per available code in [verify_client_details_in_device()|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/services/dnaserv/lib/api_groups/cli_check/group.py?at=refs%2Fheads%2Fprivate%2FHulkPatch-ms%2Fapi-auto#4317] method, the expected RLAN ID is always “1”:

{code:python}cli_rlan_id = rlan_payload[0]['portId']
...
self.log.info(f""Number of clients {clients} in device {dev}"")
pattern = f""({rclient_mac.lower()})\s+\S+\s+RLAN\s+({cli_rlan_id})\s+Run\s+Ethernet\s+Dot1x""
out=re.search(pattern, client_summary1){code}

As per discussion with [~accountid:63f50bd9263233e653a959ea] , who was the manual tester for this feature, confirmed to only consider the expected Client’s MAC address is available with “type” value as “RLAN”.

Hence, need to fix the “pattern” for expected output validation.",2023-10-20T19:50:11.451+0000,"Required PR has been raised against private/HulkPatch-ms/api-auto branch:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7566/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7566/overview]","['Auton', 'ESXi', 'Groot']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-2571,https://miggbo.atlassian.net/browse/SEEN-2571,[Auton][MSTB2] -  Test_TC91_mdnac_aca_test_reader  /   test13_aca_with_ixia_for_reader,"Hi,

Script : solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
Branch : private/HulkPatch-ms/api-auto
DNAC Image/Ova(Hulk-Patch-1 RC1)#3.713.75159

We tried running mDNAC usecase on ESXI based DNAC, but we are seeing Traffic related usecases are failing with unable to start and stop the traffic. Manually we tried to onboard the ixia file and we were successfully start the traffic. But via script traffic testcase is failing

11859:  None
11860:  Result:  Failed to start traffic
11861:  Exception in confoguring IPv4/IPv6 address
11862:  Result:  Failed to stop traffic
11863:  Exception in starting traffic
11864:  None
11865:  Result:  Failed to start traffic
11866:  Exception in confoguring IPv4/IPv6 address
11867:  DNAC 172.23.241.176 Exception
11868:  Traceback (most recent call last):
11869:    File ""/auto/dna-sol/ws/divayada/Hulk-patch1latest/dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py"", line 14229, in test13_aca_with_ixia_for_reader
11870:      self.failed(""Result:  Failed to stop traffic"")
11871:    File ""/ws/divayada-sjc/Solution_pyatsenv/lib/python3.6/site-packages/pyats/aetest/base.py"", line 545, in failed
11872:      raise signals.AEtestFailedSignal(reason, goto, from_exception, data)
11873:  pyats.aetest.signals.AEtestFailedSignal: ('Result:  Failed to stop traffic', None, None, None)
11874:
11875:  During handling of the above exception, another exception occurred:
11876:
11877:  Traceback (most recent call last):
11878:    File ""/auto/dna-sol/ws/divayada/Hulk-patch1latest/dnac-auto/services/commonlibs/test_wrapper.py"", line 301, in wrapper
11879:      result = testfunc(func_self, **kwargs)
11880:    File ""/auto/dna-sol/ws/divayada/Hulk-patch1latest/dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py"", line 14412, in test13_aca_with_ixia_for_reader
11881:      self.log.error(traceback.format_exc())
11882:  AttributeError: 'Test_TC91_mdnac_aca_test_reader' object has no attribute 'log'
11883:  Test returned in 0:12:15.084702
11884:  Errored reason: 'Test_TC91_mdnac_aca_test_reader' object has no attribute 'log'

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1344486&size=856471&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_multi_sites_mdnac.2023Oct16_05:53:48.512560.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1344486&size=856471&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_multi_sites_mdnac.2023Oct16_05:53:48.512560.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Note : We are running mDNAC usecase on ESXI(MSTB2) testbed for first time",2023-10-23T16:49:28.365+0000,fix [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/4ac64d5e174e09b4ae6d446ceda309449b0f4b64|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/4ac64d5e174e09b4ae6d446ceda309449b0f4b64],"['Auton', 'ESXi', 'Hulk-Patch1', 'Integration', 'MSTB2']",Raji Mukkamala,Resolved,Divakar Kumar Yadav
SEEN-2572,https://miggbo.atlassian.net/browse/SEEN-2572,[Auton][MSTB2] -  Test_TC91_mdnac_aca_test_reader  /   test9_aca_verify_sg_exist_for_reader,"Hi,

Script : solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
Branch : private/HulkPatch-ms/api-auto
DNAC Image/Ova(Hulk-Patch-1 RC1)#3.713.75159

We tried running mDNAC usecase on ESXI based DNAC, but we are seeing verification of Scalable Groups(SG’s) on Reader node is failing. Script is trying to create SG’s on Reader node and it throws below error. Error is expected as no operation is generally performed on reader node

9032:  Resource path full url: [https://172.23.241.176/api/v1/task/018b22f2-1c6f-7368-b377-9d7fbe0d5925/tree|https://172.23.241.176/api/v1/task/018b22f2-1c6f-7368-b377-9d7fbe0d5925/tree]
9033:  Task tree has 1 tasks
9034:  {'endTime': 1697098374290, 'version': 1697098374291, 'startTime': 1697098374269, 'data': 'Deploy Security Groups initiated', 'progress': 'Deploy Security Groups initiated', 'errorCode': 'NCCS16573', 'serviceType': 'NCCS', 'username': 'admin', 'failureReason': 'NCCS16573: Deploy not available: Deployment request may only be performed on Cisco DNA Center Author Node', 'isError': True, 'instanceTenantId': '65233a2bbef7150012bb7159', 'id': '018b22f2-1c6f-7368-b377-9d7fbe0d5925'}
9035:  Library group ""task"" method ""wait_for_task_complete"" returned in 0:00:00.047257
9036:  {'endTime': 1697098374290, 'version': 1697098374291, 'startTime': 1697098374269, 'data': 'Deploy Security Groups initiated', 'progress': 'Deploy Security Groups initiated', 'errorCode': 'NCCS16573', 'serviceType': 'NCCS', 'username': 'admin', 'failureReason': 'NCCS16573: Deploy not available: Deployment request may only be performed on Cisco DNA Center Author Node', 'isError': True, 'instanceTenantId': '65233a2bbef7150012bb7159', 'id': '018b22f2-1c6f-7368-b377-9d7fbe0d5925'}
9037:
9038:  ############################################################
9039:  Deploying sg failed:NCCS16573: Deploy not available: Deployment request may only be performed on Cisco DNA Center Author Node
9040:  ############################################################

Failed Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1686947&size=54684&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_multi_sites_mdnac.2023Oct12_00:55:00.640889.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1686947&size=54684&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_multi_sites_mdnac.2023Oct12_00:55:00.640889.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-10-23T16:58:50.061+0000,"[~accountid:63f50bf0e8216251ae4d59ca] , can you follow this template when raising autons?
[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] Hi [~accountid:62d2fe9f8afb5805e5d5af49],

On MSTB2 testbed we have run this usecase for first time so we don’t have previous passlog

# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue: Y*
#* *Compared with the last passed log:*  No previous passlog as testcase is run for first time
#* *Checked and compared the configuration:* Yes, compared with MSTB1 working Json file
#* *Confirmed with DE on the root cause:* N, As we are are not sure about the failure
# *Issue faced:* Script is trying create Scalable groups on Reader node and this is not allowing script to do.
# *Failure/Errors snip from log:* 
9036: {'endTime': 1697098374290, 'version': 1697098374291, 'startTime': 1697098374269, 'data': 'Deploy Security Groups initiated', 'progress': 'Deploy Security Groups initiated', 'errorCode': 'NCCS16573', 'serviceType': 'NCCS', 'username': 'admin', 'failureReason': 'NCCS16573: Deploy not available: Deployment request may only be performed on Cisco DNA Center Author Node', 'isError': True, 'instanceTenantId': '65233a2bbef7150012bb7159', 'id': '018b22f2-1c6f-7368-b377-9d7fbe0d5925'}
Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1686947&size=54684&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_multi_sites_mdnac.2023Oct12_00:55:00.640889.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1686947&size=54684&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_multi_sites_mdnac.2023Oct12_00:55:00.640889.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Regression team debug analysis with details:* Tried manually to create the SG’s and same error we are seeing
# *DE analysis/confirmation with details:* Didn’t reach to DE
# *What do Automation team need to look into?* SG creation shouldn’t be allowed on reader node DNAC.
# *Failed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1686947&size=54684&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_multi_sites_mdnac.2023Oct12_00:55:00.640889.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1686947&size=54684&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb2_multi_sites_mdnac.2023Oct12_00:55:00.640889.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* No Previous Passlog
# *Failed DNAC version/ISO:* Hulk_patch1-ESXI # 3.713.75159
# *Last Passed DNAC version/ISO:* No Previous Passlog
# *Script/Mapping file Used:* solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
# *TCs Impacted:* 2 (Test_TC91_mdnac_aca_test_reader, Test_TC95_mdnac_aca_test_reader_after_promotion_back)
# *'Re-run' TCs:* Yes, saw same failure
# *Branch Used:* private/HulkPatch-ms/api-auto
# *DNAC Type:* ESXI
# *Testbed details:* Multisite-TB2
# *Team:* Solution Regression Root cause: Deploy is only for Author, not Reader

Fix: skip deploy for {{flow_verify_sg}} by default since it is mainly used for Reader.

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/bf8fda865560467a379da3cbf9838959fad2a75c|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/bf8fda865560467a379da3cbf9838959fad2a75c]","['Auton', 'ESXi', 'Hulk-Patch1', 'Integration', 'MSTB2']",Tran Lam,Resolved,Divakar Kumar Yadav
SEEN-2573,https://miggbo.atlassian.net/browse/SEEN-2573,[Auton] Task-advanced_wlan_configs.py-138-advancedWlanConfigs (Test_TC1_advanced_wlan_configs: test2_add_new_ssids),"{noformat}65:  Cannot track test: tracking auth info must be set in order to transfer test tracking data
198:  No segments found matching the site from inputs
199:  Some issue in adding a segment for Adv_wlan_configs_1
237:  No segments found matching the site from inputs
238:  Some issue in adding a segment for Adv_wlan_configs_2
393:  No conflict found for ['TB7-SJ-eCA-BORDER-CP'] and namespaces None
1649:  Task is still running, wait for some time and retry.
1650:  Retrying
1657:  Task is still running, wait for some time and retry.
1658:  Retrying
1665:  Task is still running, wait for some time and retry.
1666:  Retrying
1673:  Task is still running, wait for some time and retry.
1674:  Retrying
1681:  Task is still running, wait for some time and retry.
1682:  Retrying

1743:  Failed reason: Failed to create ssid, add to nw profile or provision

{noformat}


In Recent  hulk run  test2_add_new_ssids tc failed  with  above error  : 

*Branch used:* private/HulkPatch-ms/sanity_api_auto


*Script used*:advanced_wlan_configs optimized code

*Testbed details:*

[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-11?src=contextnavpagetreemode]7

Hulk P1 RC2
[+*Pass_log:*+|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct11_10:10:30.681960.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct11_10:10:30.681960.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct11_10:10:30.681960.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



TB8 Log
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=3938&size=1258862&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct22_19:46:53.407953.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=3938&size=1258862&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct22_19:46:53.407953.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]


[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=11714&size=651755&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct20_23:59:10.099508.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-138-advancedWlanConfigs&begin=11714&size=651755&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct20_23:59:10.099508.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-10-24T05:05:22.381+0000,"PR:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7583/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7583/overview] *Regression Profile:* Solution Regression Multisite

*Branch used:* private/Ghost-ms/api-auto , private/HulkPatch-ms/api-auto, 

*Uber ISO tested:* Ghost Patch2 HF2 - 2.1.614.70852-HF2, Hulk Patch1 RC3 - 2.1.713.70319

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

We see Advanced wlan usecase scenario which was passing earlier is failing now. Its the same error details as mentioned in Jira description.

*Failed log on Hulk Patch1 RC3 - 2.1.713.70302 -* [Test_TC242_advanced_wlan_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=22158906&size=1437263&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct23_00:01:57.937017.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Feature Wiki -* [https://wiki.cisco.com/display/EDPEIXOT/Advanced+WLAN+Configs|https://wiki.cisco.com/display/EDPEIXOT/Advanced+WLAN+Configs] Issue still observed during testing on *Hulk Patch2 -  2.1.714.70296*

Branch used - private/HulkPatch2-ms/api-auto

*Failed log:* [Test_TC242_advanced_wlan_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2293021&size=887787&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov20_07:31:31.304902.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] Hi [~accountid:62d2fec15d6f5fd2c3db8f9f] ,

{quote}*With* +*Issues Faced:*+

We see Advanced wlan usecase scenario which was passing earlier is failing now. Its the same error details as mentioned in Jira description.{quote}

It still has the mentioned issue in the description because the code hasn't been updated to fix Auton (in log [Test_TC242_advanced_wlan_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=22158906&size=1437263&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct23_00:01:57.937017.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] i see it in: private/HulkPatch-sand927/api-auto). I just pushed the code to the branch: private/Hulk-ms/api-auto. And with Ghost or Halleck, it does not exist the mentioned issue in Jira above

----

With Issue in comment:

{quote}Issue still observed during testing on *Hulk Patch2 - 2.1.714.70296*

Branch used - private/HulkPatch2-ms/api-auto

*Failed log:* [Test_TC242_advanced_wlan_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2293021&size=887787&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov20_07:31:31.304902.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]{quote}

Caused by another issue. I quickly checked and found the issue at (lib: def onboard_all_wireless_segment, path: dnac-auto/services/dnaserv/lib/api_groups/wlan_segment_onboarding/group.py). It seems that the SSID: '{{Adv_wlan_configs_1}}' have not seen in api: 

{noformat}(method=""GET"", resource_path=""/v2/data/customer-facing-service/Wlan""){noformat} Hi [~accountid:63f50bcdce6f37e5ed93c87d] 
We see this testcase has been failed on Hulk P2 RC1 70391 build

Log: [Test_TC1_advanced_wlan_configs|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-advanced_wlan_configs.py-137-advancedWlanConfigs&begin=3938&size=1209267&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec11_00:50:23.623885.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] 

please share the latest progress on this fix! Hi [~accountid:5e1415780242870e996f0b2f] 

PR for latest progress on this fix: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7583/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7583/overview] [~accountid:5e1415780242870e996f0b2f] ,
Your issue it seem similar:

!image-20231212-182349.png|width=783,height=527! Thanks [~accountid:63f50bcdce6f37e5ed93c87d]  
As we are currently testing Hulk patch2 builds
Has the fix being merged to HulkPatch1 & HulkPatch2 branches as well ? 
Could you confirm on this","['Auton', 'Execution', 'Ghost', 'GhostPatch2', 'HulkPatch1', 'HulkPatch2', 'Integration', 'MSTB1', 'Multisite', 'Optimized', 'Sanity']",DatChi Pham,Reopened,DatChi Pham
SEEN-2580,https://miggbo.atlassian.net/browse/SEEN-2580," [Auton]: [Sanity] Test_TC109_DNAC_maps/ test4_set_maps_parameters/ test14_disconnect_ap_from_wlc  This is picking TSIM AP for operations, need to pick real AP from controller","*Description:*  
*Executed on TB8 Hulk P1 RC3:* The setup testcase is picking TSIM AP to be added in maps and perform disconnect and connect AP to controller. Hence the test [test14_disconnect_ap_from_wlc|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=932288&size=70818&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct24_22:43:36.541492.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] has failed to disconnect TSIM Test-AP1

Log SNIP:[test4_set_maps_parameters|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=750385&size=110075&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct24_22:43:36.541492.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

{noformat}Following TestAp-1 will be added in Maps.

 Following are AP name, id, x, y, z positions: TestAp-1, 75418917-f313-4672-ba9b-29ab85acb5e7, -1.0, -1.0, 10.0
3877:  Test returned in 0:00:03.182642
3878:  Passed reason: Variables have been setup successfully with the following info:TestAp-1, AP3800E, 75418917-f313-4672-ba9b-29ab85acb5e7
3879:  The result of section test4_set_maps_parameters is => PASSED{noformat}


Log SNIP: [test14_disconnect_ap_from_wlc|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=932288&size=70818&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct24_22:43:36.541492.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

{noformat}Action: Disconnecting AP TestAp-1 from WLC
 Found Unified AP TestAp-1, with name :TestAp-1{noformat}



[~accountid:63f50be71223974bc04b0534] Could you check or handle this scenario where we have TSIM APs joined to controller and pick the relevant AP to perform disconnect/connect AP to controller.

*ERROR SNIPPET:*

{noformat}Not found dev  in network info map, returning None

 Library group ""inventory"" method ""get_network_device_info"" returned in 0:00:00.135233
 
 device not found in dnac inventory
 Library group ""inventory"" method ""inv_shut_no_shut_dev_intf"" returned in 0:00:00.135751
 4589: 
 Result:Cannot connect/disconnect device from controller. Device role must be Access
 4591: 
 Failed reason: Result:AP TestAp-1 couldn't disconnect from controller.
 4592: 
 The result of section test14_disconnect_ap_from_wlc is => FAILED{noformat}

*Branch:* private/Hulk-ms/sanity_api_auto, private/HulkPatch-ms/sanity_api_auto,

*Script file:* solution_test_sanityecamb_lan.py / solution_test_sanityecamb.py

*input file:* solution_test_input.json

*Failed log on -Prem :*  [Test_TC109_DNAC_maps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=698808&size=574553&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct24_22:43:36.541492.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Failed log AWS TB11*:  [Test_TC109_DNAC_maps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1076659&size=425145&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct24_23:36:37.341204.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Testbed*: Sanity Testbed TB8 & Sanity Testbed TB11",2023-10-25T06:55:33.379+0000,"PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8041/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8041/overview] Executed on TB8 with Hulk P2 70354 build 
*Log*: [Test_TC2_DNAC_maps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-map.py-421-map&begin=4059&size=1164987&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec01_00:11:43.328199.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] 
where only one sub-tc failed {{test3_import_Ekahau_file}} with reason as   !!!!!!!! Error importing Ekahau file. local variable 'lat_long_msg' referenced before assignment !!!!!!!!

*Branch used*: private/HulkPatch2-ms/sanity_api_auto  (Optimized script used ) 

[~accountid:63f50be71223974bc04b0534]  Could you please check this error  Hi [~accountid:5e1415780242870e996f0b2f], adding [~accountid:63f50bf5e8216251ae4d59cf] here so she can take a look at test3. Test1,test2,test3 was developed a long time ago and it may need some improvements but for this release you can focus on test4 to the end.  Thank you [~accountid:63f50be71223974bc04b0534] - The fix for this ticket has been verified. 
Please find the log [Test_TC2_DNAC_maps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-map.py-421-map&begin=4059&size=1164987&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec01_00:11:43.328199.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
For the older sub-test3 failure- i will raise another auton to track
","['AWS_Sanity', 'Auton', 'Hulk', 'Integration', 'sanity']",Majlona 'Luna' Aliaj,Resolved,Ashwini R Jadhav
SEEN-2581,https://miggbo.atlassian.net/browse/SEEN-2581,Auton-Hulk :Test_TC189_edit_site_name/test9_remove_aps_from_inventory/test17_verify_ap_new_site,"*Reporter Analysis:*

Recently on AWS Sanity while running edit site name feature observing 
remove new site from AP is taking more time and verification also is failing



*Description:*

{noformat}15577: 
 !!!!!!!!!!! Unprovisoning Failed !!!!!!!!!!!{noformat}

{{ }}

{noformat}15580: 
 Failed reason: Failed to remove APs with new site name: APA488.73CE.928C{noformat}



{noformat}35225: 
 ERRORFollowing AP AP687D.B402.648C is not correctly located on site:{noformat}

{noformat}35226: 
 Global/USA/SAN JOSE/BLD23_New/New_Floor{noformat}

{noformat}35228: 
 Result:FAILED to verify the new change in the location for Ap AP687D.B402.648C{noformat}

{noformat}35666: {noformat}

*Branch used:*
private/HulkPatch-ms/sanity_api_auto

**Script* *file:** solution_test_sanityecamb_lan.py

*Source Team:*  Sanity

Issue Seen first time or day0 issue:

*Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1516098&size=9004922&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct23_02:19:30.444558.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1516098&size=9004922&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct23_02:19:30.444558.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]",2023-10-25T10:48:27.907+0000,"[~accountid:61efa8c457b25b006877eda3] , please follow this template when raising Autons

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist]","['AWS_Sanity', 'Auton', 'HULK']",Moe Saeed,Backlog,Anusha John
SEEN-2582,https://miggbo.atlassian.net/browse/SEEN-2582,Auton:Hulk:Cleanup:Test_TC33_cleanup_model_config  /   test1_cleanup_model_configs,"h2. Description:

*Reporter Analysis:*
On Hulk execution we are observing [Test_TC33_cleanup_model_config|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1912348&size=13904&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct23_01:50:50.542797.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] was failed with “{{Errored reason: unsupported operand type(s) for &=: 'bool' and 'ResponseDict'}}“ 

*Description*:
  result &= self.delete_design(designName=design, capability=capability)
9343:  TypeError: unsupported operand type(s) for &=: 'bool' and 'ResponseDict'

*Branch Name:* HulkPatch -ms/sanity_api_auto

**Script* *file:** {{dnac_cleanup_script.py}}

*Source Team:*  Sanity

Failed log:
[Test_TC33_cleanup_model_config|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1912348&size=13904&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct23_01:50:50.542797.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



*Last Passed DNAC version/ISO:*
NA , First time we are running the TC

*TCs Impacted:*
Duo to model config issue ,unable continue with pre-sanity execution, It’s blocking our execution

*Dnac Version: Hulk P1#2.1.713.70319*

*Testbed wiki:*
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-5|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-5]
",2023-10-25T12:24:48.985+0000,"[~accountid:63f50bf84c355259db9ccc59] , Please follow this template when raising Autons.

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] Hi [~accountid:62d2fe9f8afb5805e5d5af49] , 
I have updated the JIra, with necessary data _, This is the fisrt time we are running the tc , we dont have a pass log for this TC_","['Auton', 'Cleanup', 'Execution', 'Hulk']",Raji Mukkamala,Backlog,Tulasi Reddy
SEEN-2583,https://miggbo.atlassian.net/browse/SEEN-2583,Auton:Hulk:Cleanup:Test_TC34_cleanup_wireless_rf_profiles,"h2. Description:

*Reporter Analysis:*
On Hulk execution we are observing wireless rf profiles was not deleting properly 
*Description*:
{{9393: Failed reason: Failed to remove rf profiles}}

*Branch Name:* HulkPatch -ms/sanity_api_auto

**Script* *file:** {{dnac_cleanup_script.py}}

*Source Team:*  Sanity

Failed log:
[Test_TC34_cleanup_wireless_rf_profiles|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1926252&size=25649&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct23_01:50:50.542797.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-10-25T12:29:32.777+0000,"[~accountid:63f50bf84c355259db9ccc59] , Please follow this template when raising Autons.

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist]","['Auton', 'Cleanup', 'Execution', 'Hulk']",Raji Mukkamala,Backlog,Tulasi Reddy
SEEN-2584,https://miggbo.atlassian.net/browse/SEEN-2584,[Auton] - Multiple Failures under AP Maintenance mode TC scenario,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/HulkPatch-ms/api-auto

*Uber ISO tested:* Hulk Patch1 RC2 - 2.1.713.70302 (with PUBSUB enabled)

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

When integrating the Halleck feature - AP Maintenance mode feature, we are observing multiple subTC failures. Below are the details:

1) [test18_verify_ssid_broadcast_when_ap_in_mm|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4214636&size=48857&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct17_12:34:19.558304.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ Key error observed when TSIM AP which has joined to Aireos WLC ({{FW-5520-1}}),  got picked for validation. TSIM APs should not be picked at for this use case. 

2) [test22_verify_device_manageability_state_after_maintenance_ends|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4450455&size=320117&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct17_12:34:19.558304.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ Verification fails with all the APs picked are still in maintenance mode even after disable maintenance. Among the APs picked, there are TSIM APs as well, which should not be picked. Not sure if this subTC is failing because of TSIM APs presence.

3) [test23_disconnect_ap_from_wlc|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4770572&size=206604&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct17_12:34:19.558304.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ Even though Aireos WLC device role is ACCESS, we see the script is failing with log message as Device role must be access.

→ Again TSIM AP got picked for this case also.

4) [test24_verify_device_reachability_after_disconnect|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4977176&size=13959&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct17_12:34:19.558304.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ TSIM AP is used for AP unreachable verification. Hence the failure.

5) [test26_reconnect_ap_to_wlc|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5044523&size=162809&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct17_12:34:19.558304.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

→ Same issue as in point 3) above.



*Failed log -* [Test_TC265_enable_maintenance_mode_on_aps_and_validate_maintenance_mode|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2492497&size=2784646&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct17_12:34:19.558304.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Feature Wiki -* [https://wiki.cisco.com/display/EDPEIXOT/AP+Maintenance+Mode|https://wiki.cisco.com/display/EDPEIXOT/AP+Maintenance+Mode]",2023-10-25T12:59:35.190+0000,"PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8041/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8041/overview] [~accountid:63f50be71223974bc04b0534] : Thanks for looking into the issue and adding the necessary fix!

Could you please commit the fix to *private/HulkPatch-ms/api-auto* and *private/HulkPatch2-ms/api-auto* branches as well?  HulkPatch and HulkPatch2 also got the fix: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0c2cc73ab3ede9a0e508ad35da1d96cd0a634f99#services/dnaserv/lib/api_groups/maintenance_mode/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0c2cc73ab3ede9a0e508ad35da1d96cd0a634f99#services/dnaserv/lib/api_groups/maintenance_mode/group.py]

Marking this Auton as “Done”.","['Auton', 'Halleck', 'Hulk', 'HulkPatch', 'Integration', 'MSTB1', 'Multisite']",Majlona 'Luna' Aliaj,Closed,SANDEEP SHIVARAMAREDDY
SEEN-2585,https://miggbo.atlassian.net/browse/SEEN-2585,[Auton]:[Hulk]: Test_TC3_DNAC_provision_all_aps,"*Reporter Analysis:*


{code:python}4687:  Config preview flow was failed for description: Provisioning Unified APs at time 1697872467.836079

4685:  ############################################################
4687:  Config preview flow was failed for description: Provisioning Unified APs at time 1697872467.836079

4978:  The Schedduled Job failed: with reason {'id': '4c59dde7-80d2-4bb4-88a3-5c3573f1fae4', 'triggeredJobTaskId': '018b5116-3133-7c8a-9684-fddbb29522eb', 'triggeredTime': 1697872490807, 'status': 'FAILED', 'failureReason': 'NCWL10307: APWirelessConfiguration FLOOR1_LEVEL1_LOW_52f7e with Id afb0de6d-f069-475e-83f0-9e14b2ebdc2b SiteId cdc246c9-3444-44ff-b534-0fd05eb9e5ca and RfProfile LOW is already present in the database', 'triggeredJobId': '4c59dde7-80d2-4bb4-88a3-5c3573f1fae4'}
4979: 
4980:  Checking schedule job status was failed. {'response': [{'runNow': True, 'startTime': 1697872490723, 'externalSchedule': {'notificationURL': '/api/v2/data/customer-facing-service/ApWirelessConfiguration', 'notificationMethod': 'POST', 'notificationBody': [{'id': 'afb0de6d-f069-475e-83f0-9e14b2ebdc2b', 'instanceId': 296575, 'instanceCreatedOn': 1697868074400, 'instanceUpdatedOn': 1697871956614, 'instanceVersion': 2, 'createTime': 1697868074378, 'deployed': False, 'isSeeded': False, 'isStale': False, 'lastUpdateTime': 1697868511278, 'name': 'FLOOR1_LEVEL1_LOW_52f7e', 'namespace': 'APG_cdc246c9-3444-44ff-b534-0fd05eb9e5ca_LOW', 'provisioningState': 'DEFINED', 'resourceVersion': 2, 'targetIdList': [], 'type': 'ApWirelessConfiguration', 'cfsChangeInfo': [], 'customProvisions': [], 'externalIntentSourceInfos': [{'id': '0079e425-83ad-4dcc-92ba-dbce3848da7f', 'instanceId': 799423, 'instanceCreatedOn': 1697868511295, 'instanceUpdatedOn': 1697868511295, 'instanceVersion': 0, 'applicationHandle': '-1', 'externalSnapshotId': '54358228-7f80-4e5e-a73b-5d8acf176115', 'externalSnapshotVersion': 1, 'referenceId': '018b50d9-5516-7aba-b428-4ed175cd6184', 'referrerType': 'CUSTOMER_FACING_SERVICE', 'sourceType': 'NETWORK_SETTINGS', 'displayName': '0'}, {'id': '52b68fbb-e2d2-4d86-8562-79af39a86fb4', 'instanceId': 799424, 'instanceCreatedOn': 1697868511295, 'instanceUpdatedOn': 1697868511295, 'instanceVersion': 0, 'applicationHandle': 'cdc246c9-3444-44ff-b534-0fd05eb9e5ca', 'externalSnapshotId': '54d9fab1-dba9-40ae-b020-0348867805d2', 'externalSnapshotVersion': 1, 'referenceId': '018b50d9-5516-7aba-b428-4ed175cd6184', 'referrerType': 'CUSTOMER_FACING_SERVICE', 'sourceType': 'NETWORK_SETTINGS', 'displayName': '0'}, {'id': '2c1a8a20-c3a7-419c-9e38-6eceb6009407', 'instanceId': 799425, 'instanceCreatedOn': 1697868511295, 'instanceUpdatedOn': 1697868511295, 'instanceVersion': 0, 'applicationHandle': '67ec59ab-26a0-468d-8ff8-02f7ba9495e2', 'externalSnapshotId': '54d9fab1-dba9-40ae-b020-0348867805d2', 'externalSnapshotVersion': 1, 'referenceId': '018b50d9-5516-7aba-b428-4ed175cd6184', 'referrerType': 'CUSTOMER_FACING_SERVICE', 'sourceType': 'NETWORK_SETTINGS', 'displayName': '0'}], 'antennaRadioDetails': [], 'apMacAddrList': ['14:16:9d:2e:9c:a0', 'ac:4a:56:ac:92:c0', '78:bc:1a:88:d1:80', '00:0a:bc:00:02:00', '00:0a:bc:00:0b:00', '00:0a:bc:00:03:00', '00:0a:bc:00:16:00', '00:0a:bc:00:17:00', '00:0a:bc:00:18:00', '00:0a:bc:00:19:00', '00:0a:bc:00:1a:00', '00:0a:bc:00:1b:00', '00:0a:bc:00:1c:00', '00:0a:bc:00:1d:00', '00:0a:bc:00:1e:00', '00:0a:bc:00:04:00', '00:0a:bc:00:1f:00', '00:0a:bc:00:05:00', '00:0a:bc:00:06:00', '00:0a:bc:00:07:00', '00:0a:bc:00:08:00', '00:0a:bc:00:09:00', '00:0a:bc:00:0a:00'], 'apzoneName': 'default-zone', 'isPreProvisionedAPG': False, 'localVlanIdList': [], 'meshRoleList': [], 'rfProfile': 'LOW', 'siteId': 'cdc246c9-3444-44ff-b534-0fd05eb9e5ca', 'deviceInfo': [{'idRef': '866a70e4-9cbe-4f02-b332-83d9e6d77c0c'}], 'radioProfileCfs': [], 'rfProfileCfs': {'id': 'b9d5a119-2b00-4844-906f-976fa4697bfd', 'instanceId': 66787721, 'instanceCreatedOn': 1697868074400, 'instanceUpdatedOn': 1697868074400, 'instanceVersion': 0,{code}



Recently on Hulk  P1  RC3    2.1.713.70319  Sanity while running [Test_TC3_DNAC_provision_all_aps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=905500&size=1557592&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct20_23:59:10.099508.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]  we observed AP  Provision failing with the above  error,    
same  TC was passing earlier is failing now

A similar defect moved to the “ J”  state  
[https://cdetsng.cisco.com/summary/#/defect/CSCwh43105|https://cdetsng.cisco.com/summary/#/defect/CSCwh43105]
[https://cdetsng.cisco.com/summary/#/defect/CSCwe20354|https://cdetsng.cisco.com/summary/#/defect/CSCwe20354]

+*Hulk Patch-1 RC3 2.1.713.70319*+
[*Failed log:*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct20_23:59:10.099508.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct20_23:59:10.099508.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct20_23:59:10.099508.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

+*On the same  build  Non-lan sanity script  Pass log:*+


+*Hulk Patch-1 RC3 2.1.713.70319*+
Pass log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct22_19:46:53.407953.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct22_19:46:53.407953.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]




*Hulk  P1  RC2* +*2.1.713.70302*+
Pass log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct11_10:10:30.681960.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct11_10:10:30.681960.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



*Branch used:*
private/HulkPatch-ms/sanity_api_auto


 

",2023-10-25T13:13:15.524+0000,"[~accountid:620b8357878c2f00729881c8] , Please follow this template when raising autons.

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] Is this a rerun scenario [~accountid:620b8357878c2f00729881c8]? Hi  [~accountid:63f50bcece6f37e5ed93c87e]  ,

no, the issue  seen  in regular runs, 
{{EXEC_UCGROUP_NUMBERS_LIST: [13]}}

[env.txt|https://earms-trade.cisco.com/tradeui/logs/fileviewer?logfile=env.txt&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct20_23:59:10.099508.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=pyATS&from=trade&view=all]

Recent  Hulk  P1 RC 5,
I got a pass log; it seems like an intermittent issue.

+*Passlog:*+
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct28_03:01:48.442479.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct28_03:01:48.442479.zip&atstype=ATS]
 Hi [~accountid:63f50bcece6f37e5ed93c87e]  ,
 we are observed the same issue on Hulk  P3  2.1.715.70102

!image-20231120-124949.png|width=271,height=42!

 , please find the failed log


Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1143865&size=803698&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov19_02:41:36.974509.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=1143865&size=803698&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov19_02:41:36.974509.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

{noformat}6062:  The Schedduled Job failed: with reason {'id': 'fbcc45c6-257b-4fe6-b49e-d12c592606c6', 'triggeredJobTaskId': '018be73c-40a3-7e77-8264-d86d6a0e538f', 'triggeredTime': 1700391567527, 'status': 'FAILED', 'failureReason': 'NCWL10307: APWirelessConfiguration FLOOR1_LEVEL1_LOW_eef20 with Id d8197b4e-d9de-43af-8553-072e1e73ca4f SiteId 070861bb-8455-4d71-b507-5a129d19bccb and RfProfile LOW is already present in the database', 'triggeredJobId': 'fbcc45c6-257b-4fe6-b49e-d12c592606c6'}
6063: 
6064:  Checking schedule job status was failed. {'response': [{'runNow': True, 'startTime': 1700391567474, 'externalSchedule': {'notificationURL': '/api/v2/data/customer-facing-service/ApWirelessConfiguration', 'notificationMethod': 'POST', 'notificationBody': [{'id': 'd8197b4e-d9de-43af-8553-072e1e73ca4f', 'instanceId': 310580, 'instanceCreatedOn': 1700389569891, 'instanceUpdatedOn': 1700390945770, 'instanceVersion': 2, 'createTime': 1700389569875, 'deployed': False, 'isSeeded': False, 'isStale': False, 'lastUpdateTime': 1700390359632, 'name': 'FLOOR1_LEVEL1_LOW_eef20', 'namespace': 'APG_070861bb-8455-4d71-b507-5a129d19bccb_LOW', 'provisioningState': 'DEFINED', 'resourceVersion': 2, 'targetIdList': [], 'type': 'ApWirelessConfiguration', 'cfsChangeInfo': [], 'customProvisions': [], 'externalIntentSourceInfos': [{'id': 'c97d7631-3955-47fc-a438-eca5d49f95b1', 'instanceId': 48986595, 'instanceCreatedOn': 1700390359651, 'instanceUpdatedOn': 1700390359651, 'instanceVersion': 0, 'applicationHandle': 'bf138a98-72ab-4f96-b7ae-0e903a7425ff', 'externalSnapshotId': '38f87bfc-19ee-46b7-948e-010e32f3ddb8', 'externalSnapshotVersion': 1, 'referenceId': '018be729-b27e-79ab-aa48-b1d35540438f', 'referrerType': 'CUSTOMER_FACING_SERVICE', 'sourceType': 'NETWORK_SETTINGS', 'displayName': '0'}, {'id': 'f71ea1c3-b0d0-4ca1-af81-c9ae7c5976bc', 'instanceId': 48986597, 'instanceCreatedOn': 1700390359651, 'instanceUpdatedOn': 1700390359651, 'instanceVersion': 0, 'applicationHandle': '-1', 'externalSnapshotId': '0f5816d9-64af-40d9-97c9-31f254c77c3c', 'externalSnapshotVersion': 1, 'referenceId': '018be729-b27e-79ab-aa48-b1d35540438f', 'referrerType': 'CUSTOMER_FACING_SERVICE', 'sourceType': 'NETWORK_SETTINGS', 'displayName': '0'}, {'id': '59dc6f3f-643f-4512-9fac-8f314b81bdf9', 'instanceId': 48986596, 'instanceCreatedOn': 1700390359651, 'instanceUpdatedOn': 1700390359651, 'instanceVersion': 0, 'applicationHandle': '070861bb-8455-4d71-b507-5a129d19bccb', 'externalSnapshotId': '38f87bfc-19ee-46b7-948e-010e32f3ddb8', 'externalSnapshotVersion': 1, 'referenceId': '018be729-b27e-79ab-aa48-b1d35540438f', 'referrerType': 'CUSTOMER_FACING_SERVICE', 'sourceType': 'NETWORK_SETTINGS', 'displayName': '0'}], 'antennaRadioDetails': [], 'apMacAddrList': ['14:16:9d:2e:9c:a0', 'ac:4a:56:ac:92:c0', '78:bc:1a:88:d1:80', '00:0a:bc:00:02:00', '00:0a:bc:00:0b:00', '00:0a:bc:00:03:00', '00:0a:bc:00:16:00', '00:0a:bc:00:17:00', '00:0a:bc:00:18:00', '00:0a:bc:00:19:00', '00:0a:bc:00:1a:00', '00:0a:bc:00:1b:00', '00:0a:bc:00:1c:00', '00:0a:bc:00:1d:00', '00:0a:bc:00:1e:00', '00:0a:bc:00:04:00', '00:0a:bc:00:1f:00', '00:0a:bc:00:05:00', '00:0a:bc:00:06:00', '00:0a:bc:00:07:00', '00:0a:bc:00:08:00', '00:0a:bc:00:09:00', '00:0a:bc:00:0a:00'], 'apzoneName': 'default-zone', 'isPreProvisionedAPG': False, 'localVlanIdList': [], 'meshRoleList': [], 'rfProfile': 'LOW', 'siteId': '070861bb-8455-4d71-b507-5a129d19bccb', 'deviceInfo': [{'idRef': '478b2664-cc47-4bee-8d80-1f9a48477bc5'}], 'radioProfileCfs': [], 'rfProfileCfs': {'id': 'e88f9fb9-a86a-42f6-9d41-65e4e7c18b18', 'instanceId': 49561512, 'instanceCreatedOn': 1700389569891, 'instanceUpdatedOn': 1700389569891, 'instanceVersion': 0, 'broadcastProbeResponseIntervalC': 20, 'channelWidth': 'best', 'chdClientLevelA': 3, 'chdClientLevelB': 3, 'chdClientLevelC': 3, 'chdDataRSSIThresholdA': -80, 'chdDataRSSIThresholdB': -80, 'chdDataRSSIThresholdC': -80, 'chdExceptionLevelA': 25, 'chdExceptionLevelB': 25, 'chdExceptionLevelC': 25, 'chdVoiceRSSIThresholdA': -80, 'chdVoiceRSSIThresholdB': -80, 'chdVoiceRSSIThresholdC': -80, 'dataRates6GHz': '6,9,12,18,24,36,48,54', 'dataRatesA': '6,9,12,18,24,36,48,54', 'dataRatesB': '1,2,5.5,6,9,11,12,18,24,36,48,54', 'defaultRfProfile': False, 'discoveryFrames6GHz': 'None', 'isARadioType': True, 'isBRadioType': True, 'isBrownField': False, 'isCustom': False, 'isPscEnforcingEnabledC': False, 'isRadioType6GHz': True, 'mandatoryDataRates6GHz': '6,12,24', 'mandatoryDataRatesA': '6,12,24', 'mandatoryDataRatesB': '1,2,5.5,11', 'maxPowerLevel6GHz': 30, 'maxPowerLevelA': 30, 'maxPowerLevelB': 30, 'maxRadioClients6GHz': 200, 'maxRadioClientsA': 200, 'maxRadioClientsB': 200, 'minPowerLevel6GHz': -10, 'minPowerLevelA': -10, 'minPowerLevelB': -10, 'multiBssidProfile11axMuMimoDownLinkC': True, 'multiBssidProfile11axMuMimoUpLinkC': True, 'multiBssidProfile11axOfDmaDownLinkC': True, 'multiBssidProfile11axOfDmaUpLinkC': True, 'multiBssidProfile11axTwtBroadcastC': True, 'multiBssidProfile11axTwtC': True, 'multiBssidProfileNameC': 'default-multi-bssid-profile', 'parentProfile6GHz': 'LOW', 'parentProfileA': 'LOW', 'parentProfileB': 'LOW', 'powerThresholdV16GHz': -70, 'powerThresholdV1A': -60, 'powerThresholdV1B': -65, 'radioChannels6GHz': '5,21,37,53,69,85,101,117,133,149,165,181,197,213,229', 'rfClientAwareA': False, 'rfClientResetA': 5, 'rfClientResetCountC': 1, 'rfClientSelectA': 50, 'rfClientUtilizationThresholdC': 5, 'rfDot11axNonSrgObssPdA': False, 'rfDot11axNonSrgObssPdB': False, 'rfDot11axNonSrgObssPdC': False, 'rfDot11axNonSrgObssPdMaxThresholdA': -62, 'rfDot11axNonSrgObssPdMaxThresholdB': -62, 'rfDot11axNonSrgObssPdMaxThresholdC': -62, 'rfDot11axSrgObssPdA': False, 'rfDot11axSrgObssPdB': False, 'rfDot11axSrgObssPdC': False, 'rfDot11axSrgObssPdMaxThresholdA': -62, 'rfDot11axSrgObssPdMaxThresholdB': -62, 'rfDot11axSrgObssPdMaxThresholdC': -62, 'rfDot11axSrgObssPdMinThresholdA': -82, 'rfDot11axSrgObssPdMinThresholdB': -82, 'rfDot11axSrgObssPdMinThresholdC': -82, 'rfProfileName': 'LOW', 'rxSopThreshold6GHz': 'AUTO', 'rxSopThreshold6GHzValue': '', 'rxSopThresholdA': 'LOW', 'rxSopThresholdB': 'LOW', 'rxSopThresholdValueA': '', 'rxSopThresholdValueB': '', 'zeroWaitDfsEnable': False, 'aradioChannels': '36,40,44,48,52,56,60,64,149,153,157,161', 'bradioChannels': '1,6,11', 'displayName': '32f2afa9[LOW]'}, 'wirelessGrouping': [{'id': 'fa2fc47b-db09-4b12-95af-af32e15836c1', 'instanceId': 49562513, 'instanceCreatedOn': 1700389569891, 'instanceUpdatedOn': 1700389569891, 'instanceVersion': 2, 'apMacAddrList': ['14:16:9d:2e:9c:a0'], 'apProfileName': 'default-ap-profile', 'buildingId': '8f880e52-dd2f-486a-8a80-709dcdb8003c', 'flexGroup': 'default-flex-profile', 'isOeapEnabled': False, 'isPreprovisionedPolicyTag': False, 'isPreprovisionedWG': False, 'localVlanIdList': [], 'policyTag': {noformat} Hulk  P2   2.3.7.4-70354 
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov28_08:03:49.373632.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov28_08:03:49.373632.zip&atstype=ATS]","['Auton', 'Blocked', 'HulkP2', 'HulkP3', 'HulkPatch1', 'Optimized', 'Sanity']",Andrew Chen,Backlog,Omkar Sharad Wagh
SEEN-2586,https://miggbo.atlassian.net/browse/SEEN-2586,"[Auton] - AP Zone feature specific TC fails due to assign device tag ""UnknownGroupingError"" error & AP provision issue","*Regression Profile:* Solution Regression Multisite

*Branch used:* private/Ghost-ms/api-auto

*Uber ISO tested:* Ghost Patch2 RC2 HF2 - 2.1.614.70852-HF2 (with PUBSUB enabled)

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

When integrating the AP Zone feature we see

a)  assign device tag gets failed due to ""UnknownGroupingError"" error and eventually causing the subsequent TC failures.

b) Provisioning of AP fails with reason - *“NCSP11030: Error occurred while processing the 'provision' request. Additional info for support: taskId: '221e7f9a-c5bf-4f84-a4de-7194d277228c'. Incoming user intent with identifier '6d56f1c3-f773-4b42-9a79-c775985b2e4b' for type 'ApWirelessConfiguration' and qualifier 'null' conflicts with existing user intent.”*

*Failed log 1 -* [Test_TC251_AP_Zone|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8086500&size=305226&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct20_03:55:58.335972.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Failed log 2* - [Test_TC251_AP_Zone|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9431408&size=1149180&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct20_14:19:44.328872.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Feature Wiki -* [https://wiki.cisco.com/display/EDPEIXOT/AP+Zones|https://wiki.cisco.com/display/EDPEIXOT/AP+Zones]",2023-10-25T16:41:50.914+0000,"[~accountid:62d2fec15d6f5fd2c3db8f9f] Is this working manually ? Did you try to assign the device tag manually to the AP. Also, is this AP {{SJ-AP1-3802E-RMA}} in inventory [~accountid:62d2fec15d6f5fd2c3db8f9f] , please follow this template when raising Autons.

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] [~accountid:63f50bf5e8216251ae4d59cf] : Observing same Provisioning issues during testing on latest Ghost Patch3 testing.


*Failed log 1 :* [Test_TC251_AP_Zone|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2269135&size=1258124&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov14_22:31:21.256517.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] → Provision failure under TC251.6.

*Failed log 2:* [Test_TC251_AP_Zone|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2267351&size=138104&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov15_00:44:02.764769.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] → Executed TC251.1, TC251.7, TC251.8 subTCs. But not sure why TC257.7 got skipped. TC251.8 again has provisioning error.","['Auton', 'Ghost', 'Integration', 'MSTB1', 'Multisite']",Raji Mukkamala,Backlog,SANDEEP SHIVARAMAREDDY
SEEN-2587,https://miggbo.atlassian.net/browse/SEEN-2587,"overlapping_pools_negative_operations  test8 failed with ""CFS validation failed due to invalid parameters""","[Test_TC1_overlapping_pools_negative_operations  /  test8_remove_overlapping_segments_from_virtual_networks|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-overlapping_pools_negative_operations.py-195-overlappingPoolOperation&begin=5120313&size=369548&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct23_09:31:12.123183.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] failed with ""CFS validation failed due to invalid parameters""

{noformat}17045:  Config Preview Activity failed with reason: NCSO10001: CFS validation failed due to invalid parameters. OperationalInfo in expanded collection inside domain Global/USA/BayAreaGuest is not valid. OI for domain CfsOperationalInfo[UPDATE,[fabric_segment_operation]]
17046: 
17047:  Activity: 018b5da6-14d3-774f-9642-807a191e7307 Trigger job: {'id': '4536e45f-2b12-4fad-a655-b759949c99bb', 'triggeredJobTaskId': '018b5da6-1516-782e-96fc-42abf657ea4f', 'triggeredTime': 1698083247392, 'status': 'FAILED', 'failureReason': 'NCSO10001: CFS validation failed due to invalid parameters. OperationalInfo in expanded collection inside domain Global/USA/BayAreaGuest is not valid. OI for domain CfsOperationalInfo[UPDATE,[fabric_segment_operation]]', 'triggeredJobId': '4536e45f-2b12-4fad-a655-b759949c99bb'}
17048: {noformat}",2023-10-25T19:30:32.182+0000,"[~accountid:62ab7a399cd13c0068b18fe0] , can you follow this template when raising Autons?

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] Sure [~accountid:62d2fe9f8afb5805e5d5af49] .

Before raising this Auton, I had a discussion with [~accountid:63f50bfce8216251ae4d59d5]  where he stated that it’s a known issue and he knows about the fix.

So, for tracking purpose only, I have raised this one. Please open this issue once you got the cluster in issue state. [~accountid:63f50bd68ab3d6a635ecc29b] / [~accountid:63f50bf0e8216251ae4d59ca] , pls. take care of this Auton and share the Cluster in issue state once it’s reproduced. Hi [~accountid:63f50bfce8216251ae4d59d5] 

Observed same issue in latest Hulk P2 3.714.75242 build, cluster in issue state, please take look on this issue .

failed log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-overlapping_pools_negative_operations.py-195-overlappingPoolOperation&begin=5459407&size=362500&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov29_07:25:44.697481.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-overlapping_pools_negative_operations.py-195-overlappingPoolOperation&begin=5459407&size=362500&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov29_07:25:44.697481.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*Here is cluster details: [Single interface setup]*

Cluster IP: 85.1.1.61
username: admin
password: Maglev123

*RDP to 10.195.227.59 using below credentials -*
username: cisco\kasihari
password: C1sco123

*ssh access available on below server:*
10.22.45.180  [st-ds-7] (admin/C1sco123) Hi [~accountid:63f50bfce8216251ae4d59d5] 
we are testing Hulk P2 RC1 70391- I see that we are similar issue for overlapping testcase 
Log: [Test_TC173_overlapping_pools_negative_operations|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=708544&size=4721760&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_auto_job.2023Dec12_03:08:07.700807.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]  / [test8_remove_overlapping_segments_from_virtual_networks|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3508672&size=288496&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_auto_job.2023Dec12_03:08:07.700807.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] 
Failed with reason:  {{Config Preview Activity failed with reason: NCSO10001: CFS validation failed due to invalid parameters. OperationalInfo in expanded collection inside domain Global/USA/SAN-FRANCISCO is not valid. OI for domain CfsOperationalInfo[UPDATE,[fabric_segment_operation]]}}

Please make use of the testbed 10.195.247.188 (admin / Maglev123) in Read-only mode","['Auton', 'ESXi', 'HulkPatch1', 'HulkPatch2', 'Uplift', 'optimized', 'sanity']",Moe Saeed,Backlog,Amardeep Kumar
SEEN-2588,https://miggbo.atlassian.net/browse/SEEN-2588,[Auton]:[Hulk]:Task-1  /   Test_TC18_DNAC_verify_creating_wireless_guest_portal  /   test1_subtest1_verify_addition_of_wireless_guest_policy_and_portal ,"We Have Observed that in Hulk Lan Sanity Script and also Non lan sanity optimized code script  is Faling for verify_addition_of_wireless_guest_policy_and_portal with Script Error
*Uber ISO Version tested:* Hulk 2.3.7.3-70327
*Branch:* private/HulkPatch-ms/sanity_api_auto
*Failed Log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1004637&size=17224&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct25_17:36:15.047644.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1004637&size=17224&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct25_17:36:15.047644.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]


*Failed Snip:*
4252: 
 Traceback (most recent call last):

{noformat}4253: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/commonlibs/test_wrapper.py"", line 301, in wrapper{noformat}

{noformat}4254: 
     result = testfunc(func_self, **kwargs){noformat}

{noformat}4255: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 1247, in test1_subtest1_verify_addition_of_wireless_guest_policy_and_portal{noformat}

{noformat}4256: 
     if (dnac_handle.configure_wireless_guest_portal()):{noformat}

{noformat}4257: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper{noformat}

{noformat}4258: 
     result = method(*args, **kwargs){noformat}

{noformat}4259: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/lib/api_groups/wireless_guest/group.py"", line 67, in configure_wireless_guest_portal{noformat}

{noformat}4260: 
     sp[""guestssid""][""portalName""]=portalName+""$$""+policyid{noformat}

{noformat}4261: 
 UnboundLocalError: local variable 'policyid' referenced before assignment{noformat}",2023-10-26T05:43:22.375+0000,"[~accountid:63a2a522082abdd71bb36e09] , please follow this template when raising Autons.

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] TC Failed due to non cco patch in ise , so moving to cancelled","['Auton', 'Data-Validation', 'Hulk', 'HulkPatch', 'optimized', 'sanity']",Raji Mukkamala,Cancelled,Elton GoldChristopher
SEEN-2596,https://miggbo.atlassian.net/browse/SEEN-2596,[Auton] - Attribute errors observed when executing SDA mesh subTCs,"*Regression Profile:* Solution Regression Multisite

*Branch used:* private/HulkPatch-ms/api-auto

*Uber ISO tested:* Hulk Patch1 RC2 - 2.1.713.70302 (with PUBSUB enabled)

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

When working on integration of SDA Mesh feature, we see that if sub TCs are executed by picking a TC set in between, we are observing attribute errors on all the sub TCs picked for execution. Looks like some pre-requisites are being expected in all sub TCs. Either this dependency should not be there or if this dependency is added, the sub TCs getting errored should print message as to which pre-requisite TC has to be executed when executing other sub TCs when executed separately. 

*Failed log:* [Test_TC280_SDA_AP_mesh|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2644082&size=64678&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct18_04:26:26.889301.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Feature Wiki -* [https://wiki.cisco.com/pages/viewpage.action?pageId=1644954407|https://wiki.cisco.com/pages/viewpage.action?pageId=1644954407]",2023-10-29T08:16:16.533+0000,"[~accountid:62d2fec15d6f5fd2c3db8f9f] , please follow this template when raising Autons.

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] Hi [~accountid:62d2fec15d6f5fd2c3db8f9f] 

test1 is setting up some variables to be used in TC & its mandatory to run can you please include it while running.
if you include execution wont throw the no attribute error. [~accountid:712020:0c874fb3-6015-47af-93ac-7ea20beedaf7] : Sure. Thanks for the inputs! We will give a try as suggested and will update the observations  once completed. Hi [~accountid:62d2fec15d6f5fd2c3db8f9f] 

Did you tried ? Did it worked ?
Can you please close the auton if its not needed? [~accountid:712020:0c874fb3-6015-47af-93ac-7ea20beedaf7] : We had reported defect - [+CSCwi01542+|https://cdetsng.cisco.com/webui/#view=CSCwi01542] during this TC execution and current Hulk Patch2 being tested does not have the fix yet. Once we take up new promoted build, will give a try on this TC and update.","['Auton', 'Data-Validation', 'Hulk', 'HulkPatch', 'Integration', 'MSTB1', 'Multisite']",Shubham Varfa,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-2598,https://miggbo.atlassian.net/browse/SEEN-2598,  Auton-Hulk Test_TC226_DC_Prefix_border_priority failed with key error,"Reporter Analysis:

Recently on AWS Regression  while running DC Prefix cases getting key-error
I checked the wiki there is no pre-requisites and input file cahnges.

Description:

14929:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/SR-CL-MS/AWS-Multisite-Common-Job/services/dnaserv/lib/api_groups/fabric_wired/group.py"", line 3435, in edit_border_handoff
14930:      interface_id = device['border_fusion_intf']
14931:  KeyError: 'border_fusion_intf'
14932:  Test returned in 0:04:31.182440

Branch used:
private/HulkPatch-ms/api_auto

_Script file:_ [solution_test_3sites_sjc_nyc_sf.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py?at=refs%2Fheads%2Fprivate%2FHulkPatch-ms%2Fapi-auto]

Source Team:  Regression

Issue Seen first time or day0 issue:

Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2252874&size=524373&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct26_04:53:24.070220.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2252874&size=524373&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct26_04:53:24.070220.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

All pre-requisites are there, no input file changes needed.",2023-10-30T04:39:39.226+0000,"[~accountid:641058d57222b08f3e7064d0] , please follow this template when raising Autons.

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/18dbb52e8d983733d22654a303fa1c4fab8df178|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/18dbb52e8d983733d22654a303fa1c4fab8df178]","['Auton', 'Data-Validation', 'Hulk', 'Integration']",Raji Mukkamala,Closed,Balaji Raju
SEEN-2643,https://miggbo.atlassian.net/browse/SEEN-2643,[Auton][MSTB2] - Test_TC212_poe_report_generation  /   generate_network_device_poe_report,"Hi,

# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue: Y*
#* *Compared with the last passed log:* No previous Passlog
#* *Checked and compared the configuration:* Y, There was no configuration that needs to be checked
#* *Confirmed with DE on the root cause:* Checked with Luna
# *Issue faced:* While Comparing the data from report generated there is mismatch in interface
# *Failure/Errors snip from log:* 
11509:  Values match for site 'Global/USA/BERKELEY/BLDBERK' for '[MSTB2-ASW.cisco.com|http://MSTB2-ASW.cisco.com]'
11510:  Values match for power budget of device '[MSTB2-ASW.cisco.com|http://MSTB2-ASW.cisco.com]' for '[MSTB2-ASW.cisco.com|http://MSTB2-ASW.cisco.com]'
11511:  Mismatch in the report and switch info for used ports of device '[MSTB2-ASW.cisco.com|http://MSTB2-ASW.cisco.com]' for '[MSTB2-ASW.cisco.com|http://MSTB2-ASW.cisco.com]'
11512:  Library group ""report_gen"" method ""compare_poe_report_data"" returned in 0:00:00.261072
11513:  Library group ""report_gen"" method ""generate_and_verify_poe_report"" returned in 0:02:02.282284
11514:  Test returned in 0:02:03.794797
11515:  Failed reason: PoE Report Generation Failed and/or data Verification Failed!!

Log : [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1967849&size=59273&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct26_21:26:37.432381.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1967849&size=59273&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct26_21:26:37.432381.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Regression team debug analysis with details:* 
While script is trying to match the data it is failing. The problem is that during the time one API is called, a device connects/disconnects and when we compare to the other API there is a mismatch
# *DE analysis/confirmation with details:* Didn’t reach to DE. Checked with luna and she confirmed to be script issue
# *What do Automation team need to look into?* Need to check on API for comparing
# *Failed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1967849&size=59273&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct26_21:26:37.432381.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1967849&size=59273&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct26_21:26:37.432381.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* No Previous Passlog, as we are integrating the usecase
# *Failed DNAC version/ISO:* 3.713.75176
# *Last Passed DNAC version/ISO:* No Previous Passlog
# *Script/Mapping file Used:* solution_test_3sites_sjc_nyc_sf.py
# *TCs Impacted:* 1
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch-ms/api-auto
# *DNAC Type:* ESXI
# *Testbed details:* [MSTB2|https://wiki.cisco.com/display/EDPEIXOT/S.R.+Multisite+TB2+Non-DR+Inventory]
# *Team:* Solution Regression",2023-10-31T05:59:38.433+0000,,"['Auton', 'ESXi', 'Hulk-Patch1', 'Integration', 'MSTB2']",Majlona 'Luna' Aliaj,Backlog,Divakar Kumar Yadav
SEEN-2671,https://miggbo.atlassian.net/browse/SEEN-2671,[Auton][MSTB2] - Test_TC220_SDA_AP_mesh  /   test2_check_wlan_status_on_wlc,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* N, As this testcase is being integrated for first time
#* *Checked and compared the configuration:* Y, all the required configuration changes has been added to Yaml and Json 
#* *Confirmed with DE on the root cause:* N, Issue needs Library upliftment
# *Issue faced:* Script is looking for AP’s on Different controller
# *Failure/Errors snip from log:* 
15746:  Command: show ap status, expected output: AP1416.9D2E.1FC8
15747:  With command show ap status on device FW-9800-12, expected config: AP1416.9D2E.1FC8 not found!!!
15765:  Test returned in 0:16:13.410743
15766:  Failed reason: Ap's didn't joined controller.
# *Regression team debug analysis with details:* We are having N+1 WLC(FW-9800-11, FW-9800-12) setup on San_Francisco site and   all my AP’s has Joined FW-9800-11 controller. But through script it is looking for AP’s only on FW-9800-12 and testcase fails.
# *DE analysis/confirmation with details:* Didn’t reach-out to DE as this needs to be handled via library
# *What do Automation team need to look into?* Need to uplift the library for N+1 wlc check. And also incase there is no AP Joined to controller further testcase needs to be blocked 
# *Failed log:*  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2058573&size=693284&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct27_04:42:48.507064.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2058573&size=693284&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct27_04:42:48.507064.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* NA (Integration Testcase)
# *Failed DNAC version/ISO:* 3.713.75176
# *Last Passed DNAC version/ISO:* NA
# *Script/Mapping file Used:* solution_test_3sites_sjc_nyc_sf.py
# *TCs Impacted:* 1
# *'Re-run' TCs: NA*
# *Branch Used:* private/HulkPatch-ms/api-auto
# *DNAC Type:* ESXI
# *Testbed details:* MSTB2
# *Team:* Solution Regression",2023-10-31T07:04:52.337+0000,"Hi [~accountid:63f50bf0e8216251ae4d59ca] 

 

Fix for : SDA AP Mesh [SEEN-2671|https://bitbucket-eng-sjc1.cisco.com/bitbucket/plugins/servlet/jira-integration/issues/SEEN-2671] (Auton)

PR Link : [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7932/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7932/overview]

Test Cases Summary - SDA wireless mesh :A network admin should be able to enable mesh AP
in fabric wireless. Wireless clients connected to Mesh Aps or Root AP should be
able to communicate to other wired & wireless clients part of same FE and different
FEs.

Trade log link - [https://earms-trade.cisco.com/tradeui/logs/details?archive=/root/new_pyats/users/root/archive/23-11/sanity_TB1.2023Nov16_16:16:00.488019.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/root/new_pyats/users/root/archive/23-11/sanity_TB1.2023Nov16_16:16:00.488019.zip&atstype=ATS]

Wiki link - [https://wiki.cisco.com/pages/viewpage.action?pageId=1644954407&src=contextnavpagetreemode|https://wiki.cisco.com/pages/viewpage.action?pageId=1644954407&src=contextnavpagetreemode]","['Auton', 'ESXi', 'Hulk-Patch1', 'Integration', 'MSTB2', 'Uplift']",Shubham Varfa,Resolved,Divakar Kumar Yadav
SEEN-2707,https://miggbo.atlassian.net/browse/SEEN-2707,[Auton] : Hulk P1 :- Task-few_ap_clients_roaming_by_wpagent.py-141-FEWAPClientsWpagentTrafficRoaming,"*Reporter Analysis:*


In *EXSI* *HULK Patch1 OVA: 3.713.75176* build using optimized Script execution observed Failed TC *Task-few_ap_clients_roaming_by_wpagent.py-141-FEWAPClientsWpagentTrafficRoaming* below sub TCs are failed 

test1_collect_ap_and_controller_details_open
test1_configure_wpagent_with_wlan_client_traffic_profiles_open
test1_start_wpagent_clients_and_validate_clients_joined
test1_start_wpagent_traffic_and_validate_traffic_flowing_open

*test1_collect_ap_and_controller_details_open :-* While TC execution observed failed fetch AP details with SSID from inventory page due to error other sub TCs for failed

*Snippet from log:*

_3222: 2023-10-26T04:04:22: %API-GROUP-WIRELESS_AP_CONFIGS-3-ERROR:  Error in getting wireless ap controller names_
_3223: 2023-10-26T04:04:22: %API-GROUP-WIRELESS_AP_CONFIGS-3-ERROR:  'WIRELESS_CLIENTS_TRAFFIC_OPEN'_
_3230: 2023-10-26T04:04:22: %API-GROUP-WIRELESS_AP_CONFIGS-3-ERROR:      for ssid in self.services.dnaconfig.testbed.custom[self.services.dnaconfig.sid][wp_clients_key]['SSIDs']:_
_3231: 2023-10-26T04:04:22: %API-GROUP-WIRELESS_AP_CONFIGS-3-ERROR:  KeyError: 'WIRELESS_CLIENTS_TRAFFIC_OPEN'_



*Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-few_ap_clients_roaming_by_wpagent.py-141-FEWAPClientsWpagentTrafficRoaming&begin=97191&size=603951&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct26_03:56:52.585018.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-few_ap_clients_roaming_by_wpagent.py-141-FEWAPClientsWpagentTrafficRoaming&begin=97191&size=603951&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct26_03:56:52.585018.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Passed  log* : NA because freshly integrating TC in EXSI VM TB4

*Failed DNAC version/ISO:*  HULK Patch1 OVA: 3.713.75176

*Last Passed DNAC version/ISO:* NA because freshly integrating TC in EXSI VM TB4

*Script/Mapping file Used:   lansanity_usecases_maps.yaml*

*TCs Impacted: 5*

*Re-run TCs:*  rerun also same observed the issue

*Branch Used:*  private/HulkPatch-ms/api-auto

*DNAC Type:  EXSI VM*

*Testbed details: TB4*

[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4]

*Team:  Sanity*





Regards,

Raghavendra B. M",2023-10-31T07:52:42.454+0000,"[~accountid:63f50bd68ab3d6a635ecc29b] Regression Analysis is not clear, please provide additional details/analysis. Please follow the auton checklist [https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] [~accountid:63f50bf5e8216251ae4d59cf] 

The first sub TC *test1_collect_ap_and_controller_details_open* :- Script need to pick the AP details from inventory page but not getting AP details filtering with *SSID* from inventory page due to this TC got errored with below message due to other TCs got impacted

_3222: 2023-10-26T04:04:22: %API-GROUP-WIRELESS_AP_CONFIGS-3-ERROR: Error in getting wireless ap controller names_
_3223: 2023-10-26T04:04:22: %API-GROUP-WIRELESS_AP_CONFIGS-3-ERROR: 'WIRELESS_CLIENTS_TRAFFIC_OPEN'_","['Auton', 'Integration', 'Sanity', 'exsivm', 'hulk-vm-sanity']",Pawan Singh,Backlog,Raghavendrachar Baraguru Mallesha Char
SEEN-2737,https://miggbo.atlassian.net/browse/SEEN-2737,[Auton]:Guardian:Test_TC45_DNAC_TSIM_static_onboarding_verifications/test8_wait_ap_to_be_provisioned,"*[Debug/Analyze the issue]:*

On recent executions of Guardian Observed thatDOT1X AP is not getting claimed via script

*Compared with the last passed log:*

WE have no pass log via script completely but when i  claimed AP Manually it onboarded and script passed.

Pass log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1484801&size=126034&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct03_21:58:33.516386.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1484801&size=126034&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct03_21:58:33.516386.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

* *Checked and compared the configuration: N [As manually it is working while claiming]*
* *Confirmed with DE on the root cause: N*



*Issue faced:*

AP is stuck in pnp page without claiming

*Failure/Errors snip from log:*

{noformat}237364: 
 Following APs are have not Join Controller {'APC884.A164.9BC4'}{noformat}

{noformat}237366: 
 Failed reason: Claiming of AP failed!!{noformat}



*What do Automation team need to look into:*

Why it is not claiming via script

*Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=39738142&size=116139&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct31_04:56:19.373271.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=39738142&size=116139&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct31_04:56:19.373271.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



*Failed DNAC version/ISO:*

Guardian P4 RC4#2.1.518.72328

*Last Passed DNAC version/ISO:*

Guardian P4 RC4#2.1.518.72328

*Script/Mapping file Used:*

solution_test_sanityecamb_lan.py

*TCs Impacted:1*

*'Re-run' TCs: NA*

*Branch Used:private/Guardian-ms/sanity_api_auto*

*DNAC Type: Guardian* 

*Testbed details: TB6* [*https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-6*|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-6]

*Team: Sanity*",2023-11-03T04:49:54.578+0000,"[~accountid:61efa8c457b25b006877eda3] Did we run dot1x AP in any Sanity/Regression before? Why ‘WE have no pass log via script completely’ and the issue is just reported now? [~accountid:62d2fe9f8afb5805e5d5af49] 
Earlier testbed was owned with different person , Recently Dot 1x AP had issue and was replaced and added new one  [~accountid:61efa8c457b25b006877eda3] ,

On higher level I see the AP has not joined the controller and that’s the reason the TC failed 



{noformat} Resource path full url: https://10.195.227.92/api/v1/file/ad7f0ab7-bf40-4f17-9ecd-da09e02cdf3b{noformat}

{noformat}237360: 
st-ds-1.cisco.com: 
2023-10-31T09:03:49: 
%API-GROUP-COMMAND_RUNNER-6-INFO: 
%[pid=3952276][pname=Task-1]:
 [{'deviceUuid': '5be34460-f46a-4ef2-8e15-a4da1d735018', 'commandResponses': {'SUCCESS': {'show ap summary': 'show ap summary\nNumber of APs: 4\n\nCC = Country Code\nRD = Regulatory Domain\n\nAP Name                          Slots AP Model             Ethernet MAC   Radio MAC      CC   RD   IP Address                                State        Location\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\nAPAC4A.56BF.F3B4                 2     C9120AXI-B           ac4a.56bf.f3b4 045f.b935.3900 US   -B   204.1.208.152                             Registered   default location                \nAPCC9C.3EF1.04F0                 3     CW9164I-B            cc9c.3ef1.04f0 10f9.20fd.c580 US   -B   204.1.208.155                             Registered   Global/USA/SAN JOSE/BLD23/FLOOR1_LEVEL1\nAP2C57.4184.2BF4                 3     C9130AXE-B           2c57.4184.2bf4 1416.9d7e.f500 US   -B   204.1.208.153                             Registered   default location                \nAP78BC.1A00.4B04                 2     AIR-AP2802I-B-K9     78bc.1a00.4b04 78bc.1a88.cf40 US   -B   204.1.208.154                             Registered   default location                \n\nTB6-DM-eCA-BORDER#'}, 'FAILURE': {}, 'BLACKLISTED': {}}}]{noformat} [~accountid:62d2fed26eba7198372366ca] 
Yes correct AP was joined initially as a normal AP but during this dot1x AP testcase we need to onboard via pnp so it will be disconnected from controller and will be unreachble from inventory with out claiming it won’t join to controller.

Script is not claiming AP via pnp

Thanks,
Anusha John As discussed, please verify if the issue is seen on other releases Hi Vinay,
Yes I am seeing in all releases
Recent logs from Hulk p1 rc7

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-126-SDAwiredHostOnboarding&begin=5428538&size=385605&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_12:53:34.934299.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-access_wired_ipphone_tsim_ap_hostOnboarding.py-126-SDAwiredHostOnboarding&begin=5428538&size=385605&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_12:53:34.934299.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] There is no change in the code , Please try with other AP and let us know the status","['Auton', 'Functionality', 'Guardian', 'Sanity']",Vinoth Kumar Kutty Krishnamoorthy,Backlog,Anusha John
SEEN-2738,https://miggbo.atlassian.net/browse/SEEN-2738,[Auton][MSTB2] -  Test_TC22_DNAC_verify_creating_wireless_guest_portal  /   test1_subtest1_verify_addition_of_wireless_guest_policy_and_portal,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* N
# *Issue faced:* Script is trying to same profile to more than 16 SSID's and testcase is failing
# *Failure/Errors snip from log:* 
{{{""name"": ""BLD11_FLOOR3"", ""uuid"": ""efa9d02e-db0f-4cd6-b820-5f4ad91cf05b"", ""isInherited"": false}, {""name"": ""FLOOR2"", ""uuid"": ""f05be4c8-13dc-4458-a1c0-f9f8957cf841"", ""isInherited"": false}, {""name"": ""FLOOR2"", ""uuid"": ""f72a6aa0-f006-4ba6-ae04-f7342b8d799d"", ""isInherited"": false}]}'} Headers:{'Content-Type': 'application/json', 'X-Auth-Token': 'eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJDRE5BIiwiYXV0aFNvdXJjZSI6ImxlZ2FjeSIsImNsaWVudElkIjoiYWRtaW4iLCJlbWFpbCI6ImFkbWluQGxvY2FsdXNlci5jb20iLCJleHAiOjE2OTg2ODIwNTksImlhdCI6MTY5ODY3ODQ1OSwiaXNzIjoiZG5hYyIsInJvbGVzIjpbIlNVUEVSLUFETUlOIl0sInNlc3Npb25JZCI6IjA2ZjcwYTg3LTVhYTctNTE4Ni1iMmIyLWUzY2FlN2RmNDI3NSIsInN1YiI6ImFkbWluIiwidGVuYW50SWQiOiI2NTNjYTMxZTlkMjFmZDAwMTNlM2MzNjAiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInVzZXJuYW1lIjoiYWRtaW4ifQ.YJwmbYfRcgseeijvP6u3QF2HNblaC4tIBv3glvWyaN_DSTCQ5i1uT_gI5CO7b6amTknfh-8mRRVJ8xJNK0L3Lw'} Message:{""response"":{""errorCode"":""NCND00002"",""message"":""NCND00002: The Network Profile request has validation errors"",""detail"":""[\""NCND01175: This Profile cannot be assigned to more than 16 SSIDs\""]""},""version"":""1.0""}}}
# *Regression team debug analysis with details:* Similar issue was seen previously. Need to add SSID cleanup testcase.
# *DE analysis/confirmation with details:* Didn’t reach-out to DE as this needs to be handled via library
# *What do Automation team need to look into?* Need to add SSID’s cleanup in script after SSID’s addition
# *Failed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6964935&size=285272&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct30_06:52:20.101308.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=6964935&size=285272&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct30_06:52:20.101308.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1522768&size=202651&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct21_09:05:33.210214.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1522768&size=202651&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct21_09:05:33.210214.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Failed DNAC version/ISO:* 3.714.75195
# *Last Passed DNAC version/ISO:* 3.713.75176
# *Script/Mapping file Used:* solution_test_3sites_sjc_nyc_sf.py
# *TCs Impacted:* 1
# *'Re-run' TCs: NA*
# *Branch Used:* private/HulkPatch2-ms/api-auto
# *DNAC Type:* ESXI
# *Testbed details:* MSTB2
# *Team:* Solution Regression",2023-11-03T12:21:31.257+0000,"PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7829/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7829/overview]

Root cause: The network profile can not hold more than 16 SSIDs.
Solution: Remove TEST_NEW_SSID from the main ssid creation list (following Moe's suggestion)
Will add it separately once the test case is executed (when have the solution)","['Auton', 'Cleanup', 'ESXi', 'Execution', 'Hulk-Patch2', 'MSTB2']",NhanHuu Nguyen,Resolved,Divakar Kumar Yadav
SEEN-2742,https://miggbo.atlassian.net/browse/SEEN-2742,Test_TC151_verify_fabric_360,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue: Y*
#* *Compared with the last passed log: Y*
#* *Checked and compared the configuration: Y*
#* *Confirmed with DE on the root cause: Y*
# *Issue faced:*{{'TB4-DM1-9500E.cisco.com' device has valid fabric role 'BORDER:MAP-SERVER' with InValid scores:- CP reachability:-1.0}}
 noHealthDevices:0=10, monitoredHealthyDevices:3=3, latestHealthScore:100.0=100
  Un structure data from '/assurance/v1/network-device'  API response

# *Failure/Errors snip from log:*
!image-20231106-082836.png|width=87.61944912872399%!
!image-20231106-082850.png|width=87.61944912872399%!
# *Regression team debug analysis with details:* Control plane reachability is showing Nill in the DNAC UI thus causing the TC failed.

!image-20231106-082913.png|width=1277,height=311!
# *DE analysis/confirmation with details:*”For devices where Netconf is enabled, we are using Lisp connectivity instead of CP reachability,This looks correct. Test script needs to be updated”

!image-20231106-083035.png|width=1480,height=790!
# *What do Automation team need to look into?:* For Devices where Netconf is enabled DE are using Lisp connectivity instead of CP reachability so need to exclude the CP reaxchability validation for those devices.
# *Failed log:*[*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=51827354&size=86852&archive=sanitycombine.2023Oct16_16:14:27.654619.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=51827354&size=86852&archive=sanitycombine.2023Oct16_16:14:27.654619.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]
# *Last passed log:*[*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=134452211&size=21051&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-08%2Fsanitycombine.2023Aug18_23:33:19.860422.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=134452211&size=21051&archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-08%2Fsanitycombine.2023Aug18_23:33:19.860422.zip&ats=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats&submitter=yiyamper&from=trade&view=all&atstype=pyATS]
# *Failed DNAC version/ISO: 2.1.714.70274*
# *Last Passed DNAC version/ISO: 2.1.714.8010148*
# *Script/Mapping file Used:*[*https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=refs%2Fheads%2Fbgl%2FHulkPatch2-ms%2Fapi-auto*|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=refs%2Fheads%2Fbgl%2FHulkPatch2-ms%2Fapi-auto]
# *TCs Impacted: Test_TC151_verify_fabric_360*
# *'Re-run' TCs: N/A*
# *Branch Used: bgl/HulkPatch2-ms/api-auto*
# *DNAC Type: on-prem*
# *Testbed details:* [*https://wiki.cisco.com/display/WNBUCODC/SOL-REG-TB*|https://wiki.cisco.com/display/WNBUCODC/SOL-REG-TB]",2023-11-06T08:34:15.893+0000,"[~accountid:642a816bd774ab7297295df0] , please follow this template when raising Autons.

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] [~accountid:62d2fe9f8afb5805e5d5af49] Updated the Ticket as per the Wiki!
Please let me know if anything is needed to update. PR-link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7857/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7857/overview]","['HulkPatch2', 'Uplift', 'auton']",QuangVinh Nguyen,Pending Code Review,SAINATH CHATHARASI
SEEN-2794,https://miggbo.atlassian.net/browse/SEEN-2794,[Auton] DayN upgrade sanity job has some script issues,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue: Y*
#* *Compared with the last passed log: Y - But this is the first time upgrade sanity has run on Appliance after we refactor*
#* *Checked and compared the configuration: Y*
#* *Confirmed with DE on the root cause: NA*
# *Issue faced: Day N upgrade sanity failing at the initial stages itself. Its aborted after 30 secs. It seems its not picking up the script file thats given*
# *Failure/Errors snip from log:* 

{noformat}2023-11-04T05:38:59: %ATS-INFO: Checking all devices are up and ready is disabled, '--check-all-devices-up' must be set to True in case of pyats runs or '-check_all_devices_up' set to True in case of legacy easypy runs
2023-11-04T05:38:59: %EASYPY-INFO: Starting task execution: Task-1
2023-11-04T05:38:59: %EASYPY-INFO:     test harness = pyats.aetest
2023-11-04T05:38:59: %EASYPY-INFO:     testscript   = /home/sdnbld/jenkins/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/Appliance-Upgrade-Solution-Sanity/job/forty_eight_hour/sanity_plat_upgrade.py


Running the script from here:/home/sdnbld/jenkins/workspace/Pipelines/DNA-C/SIT_BATS_Phase2/Appliance-Upgrade-Solution-Sanity


2023-11-04T05:39:00: %CISCO-INFO: Start Telemetry - Task
2023-11-04T05:39:00: %EASYPY-INFO: TIMS result uploading is not enabled
2023-11-04T05:39:00: %CISCO-INFO: Start Telemetry - Test
2023-11-04T05:39:00: %CISCO-INFO: Posted data to Kafka returning status code 200 within time 0:00:00.039681
2023-11-04T05:39:00: %CONTRIB-INFO: WebEx Token not given as argument or in config. No WebEx notification will be sent
2023-11-04T05:39:00: %EASYPY-INFO: Generating xUnit report files
2023-11-04T05:39:00: %EASYPY-INFO: --------------------------------------------------------------------------------
2023-11-04T05:39:00: %EASYPY-INFO: Job finished. Wrapping up...
2023-11-04T05:39:01: %EASYPY-INFO: Creating archive file: /home/sdnbld/.pyats/archive/23-11/env_auto_job.2023Nov04_05:38:55.949036.zip
2023-11-04T05:39:01: %EASYPY-INFO: Uploading logs to TRADe...
2023-11-04T05:39:01: %CISCO-INFO: Start Telemetry - Archive Log Upload
2023-11-04T05:39:01: %CISCO-INFO: Posted data to Kafka returning status code 200 within time 0:00:00.031527
2023-11-04T05:39:01: %EASYPY-INFO: Log Archive uploaded to: https://ngdevx.cisco.com/services/taas/results/7f893d19-6bd9-48e5-951f-0e40e76d8a07
2023-11-04T05:39:02: %EASYPY-INFO: TRADe upload completed.
2023-11-04T05:39:02: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-11-04T05:39:02: %EASYPY-INFO: |                                   TaaS URL                                   |
2023-11-04T05:39:02: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-11-04T05:39:02: %EASYPY-INFO: 
2023-11-04T05:39:02: %EASYPY-INFO: TaaS Log Viewer
2023-11-04T05:39:02: %EASYPY-INFO: -------------
2023-11-04T05:39:02: %EASYPY-INFO:     https://ngdevx.cisco.com/services/taas/results/7f893d19-6bd9-48e5-951f-0e40e76d8a07
2023-11-04T05:39:02: %EASYPY-INFO: 
2023-11-04T05:39:02: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-11-04T05:39:02: %EASYPY-INFO: |                                  TRADe URL                                   |
2023-11-04T05:39:02: %EASYPY-INFO: +------------------------------------------------------------------------------+
2023-11-04T05:39:02: %EASYPY-INFO: 
2023-11-04T05:39:02: %EASYPY-INFO: Result Viewer
2023-11-04T05:39:02: %EASYPY-INFO: -------------
2023-11-04T05:39:02: %EASYPY-INFO:     https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/sdnbld/.pyats/archive/23-11/env_auto_job.2023Nov04_05:38:55.949036.zip&atstype=ATS{noformat}





# *Regression team debug analysis with details: Stated above* 
# *DE analysis/confirmation with details: NA*
# *What do Automation team need to look into? Jenkins failure logs* 
# *Failed log:* [*https://engci-private-sjc.cisco.com/jenkins/apic/job/Pipelines/job/DNA-C/job/SIT_BATS_Phase2/job/Appliance-Upgrade-Solution-Sanity/11/console*|https://engci-private-sjc.cisco.com/jenkins/apic/job/Pipelines/job/DNA-C/job/SIT_BATS_Phase2/job/Appliance-Upgrade-Solution-Sanity/11/console]
# *Last passed log: not there as I said above*
# *Failed DNAC version/ISO: Version 2.3.7.5-70083*
# *Last Passed DNAC version/ISO: NA*
# *Script/Mapping file Used:  TESTSCRIPT_FILE=""job/forty_eight_hour/sanity_plat_upgrade.py""*
# *TCs Impacted: All*
# *'Re-run' TCs: NA*
# *Branch Used: private/HulkPatch2-ms/api-auto*
# *DNAC Type: Appliance*
# *Testbed details:* [*https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_platformtb2/local_SanityPlatformTB2.json?at=private%2FHulkPatch2-ms%2Fapi-auto*|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_platformtb2/local_SanityPlatformTB2.json?at=private%2FHulkPatch2-ms%2Fapi-auto]
# *Team: DNAC PL SIT*",2023-11-06T21:53:34.308+0000,"You used wrong {{TESTSCRIPT_FILE : job/forty_eight_hour/sanity_plat_upgrade.py}}

[https://earms-trade.cisco.com/tradeui/logs/fileviewer?logfile=env.txt&begin=0&size=-1&archive=%2Fhome%2Fsdnbld%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov04_05:38:55.949036.zip&ats=%2Fpyats-py386&submitter=sdnbld&atstype=pyATS&from=trade&view=all|https://earms-trade.cisco.com/tradeui/logs/fileviewer?logfile=env.txt&begin=0&size=-1&archive=%2Fhome%2Fsdnbld%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov04_05:38:55.949036.zip&ats=%2Fpyats-py386&submitter=sdnbld&atstype=pyATS&from=trade&view=all] Was checking in the webex space, Can we just give directly the script file and retry ? 

!Screenshot 2023-11-08 at 1.37.03 PM.png|width=1190,height=701!",['Auton'],Tran Lam,Closed,Gopala Krishna Chaikam
SEEN-2802,https://miggbo.atlassian.net/browse/SEEN-2802,[Auton][IBSTE] : ISE PAN Failover use case is not part of IBSTE script,"Hi [~accountid:63f50bcece6f37e5ed93c87e] ,

Since ISE PAN Failover use-case is currently not being covered during regression testing,it is discussed to include this use case as part of IBSTE testing.Use case needs to be commited to optimized code.

Script location:

Usecasemaps for optimized :
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/ibstesuite?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/ibstesuite?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto]
Configs:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sr_ibste?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sr_ibste?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto]
Script location:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/job/sr_ibste/sr_ibste.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/job/sr_ibste/sr_ibste.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto]
wiki:
[https://wiki.cisco.com/display/EDPEIXOT/S.R.+IBSTE+Testbed|https://wiki.cisco.com/display/EDPEIXOT/S.R.+IBSTE+Testbed]

",2023-11-07T05:08:41.451+0000,,"['Auton', 'IBSTE', 'Integration', 'enhance-coverage', 'hulk']",Andrew Chen,Backlog,Neelima Doddipalli
SEEN-2803,https://miggbo.atlassian.net/browse/SEEN-2803,[AUTON][Bonjour]-Querying services from Same Service-Peer Intra Vlan failed,"Uber ISO version used:Hulk P2
Polaris :17.12.2Prd5

Branch Used:private/shockwave-ms/bonjour_test
Script file :[+solution_test_bonjour.py+|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/bonjour/solution_test_bonjour.py?at=refs%2Fheads%2Fprivate%2Fshockwave-ms%2Fbonjour_test]

Fail Log:https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=a831167&size=460480&archive=%2Fauto%2Fatslogs%2Fcl%2F2023%2F11%2F06%2F05%2F17%2Fbonjour.2023Nov06_05:17:31.313226.zip&ats=%2Fws%2Fsmounasw-sjc%2FPyats&submitter=admin&from=trade&view=all&atstype=pyATS

In TC 14,pinging gateway failing from Avahi Host across SDG failed,Could you please check on this issue?
This error we are observing:
{color:#bf2600}13662:  Timeout occurred : Timeout value : 60{color}
{color:#bf2600}13663:   Command sent:{color}
{color:#bf2600}13664:{color}
{color:#bf2600}13665:{color}
{color:#bf2600}13666:   Pattern: ['ttl=']{color}
{color:#bf2600}13667:   Got: '\x1b]0;root@1a3f2a4e4c8d: /\x07root@1a3f2a4e4c8d:/# \r\n\x1b]0;root@1a3f2a4e4c8d: /\x07root@1a3f2a4e4c8d:/# ping -c 5 60.0.0.1\r\nPING 60.0.0.1 (60.0.0.1) 56(84) bytes of data.\r\n\r\n\r\n\r\nFrom 60.0.0.129 icmp_seq=1 Destination Host Unreachable\r\nFrom 60.0.0.129 icmp_seq=2 Destination Host Unreachable\r\nFrom 60.0.0.129 icmp_seq=3 Destination Host Unreachable\r\nFrom 60.0.0.129 icmp_seq=4 Destination Host Unreachable\r\nFrom 60.0.0.129 icmp_seq=5 Destination Host Unreachable\r\n\r\n--- 60.0.0.1 ping statistics ---\r\n5 packets transmitted, 0 received, +5 errors, 100% packet loss, time 4022ms\r\npipe 3\r\n\x1b]0;root@1a3f2a4e4c8d: /\x07root@1a3f2a4e4c8d:/# \r\n\x1b]0;root@1a3f2a4e4c8d: /\x07root@1a3f2a4e4c8d:/# \r\n\x1b]0;root@1a3f2a4e4c8d: /\x07root@1a3f2a4e4c8d:/# \r\n\x1b]0;root@1a3f2a4e4c8d: /\x07root@1a3f2a4e4c8d:/# '{color}
{color:#bf2600}13668:  FAILED: Failed to ping gateway{color}
{color:#bf2600}13669:  Gateway Ping failed from avahi host. Trying Again!!!{color}
{color:#bf2600}13670:  ping -c 5 60.0.0.1{color}
{color:#bf2600}13671:  PING 60.0.0.1 (60.0.0.1) 56(84) bytes of data.{color}

Services count not as expected on across SDG devices.",2023-11-07T15:28:24.550+0000,"[~accountid:63f50bddc1685a24e1314c87] , Please follow this template when raising Autons

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist]","['Auton', 'Bonjour', 'Execution', 'Hulk', 'HulkPatch2']",Raji Mukkamala,Backlog,Neelima Doddipalli
SEEN-2808,https://miggbo.atlassian.net/browse/SEEN-2808,"""extranet"" use-cases in MS Script needs to be enabled back","Hi Tran,

I came across below list of test-cases from *solution_test_3sites_sjc_nyc_sf_mdnac_dr.py* script that are commented out:

# Test_TC72_DNAC_create_vn_extranet_policies
# Test_TC73_DNAC_validate_vn_extranet_policies_pushed_to_devices
# Test_TC74_DNAC_configure_fusion_for_extranet
# Test_TC75_DNAC_onboard_extranet_and_verify_traffic

As per discussion, the history that I collected, there were changes on the API used due to which these test-cases required some uplift.

Raising this Auton as uplift request to consider these test-cases and bring back to production.

Thanks,
Amar",2023-11-07T23:47:36.606+0000,,"['Auton', 'Uplift']",ThangQuoc Tran,In Progress,Amardeep Kumar
SEEN-2809,https://miggbo.atlassian.net/browse/SEEN-2809,Auton:Hulk:Upgrade:  Test_TC25_Verify_DHCP_server_change_on_segments  /   test1_verify_changing_DHCP_server_verifications,"*Reporter Analysis:* 
On Hulkp2 upgrade verification we are seeing TC25_Verify_DHCP_server_change_on_segments failed with below error "" No fabric found with the site name: SAN JOSE
5717: Failed reason: Failed in updating the DHCP Server IP address.""
We checked on Dnac sanjose site was present and due to this issue devices was not verifying the dhcp ip address , we compared with last pass log also dhcp was present on devices eca and nf
not seeing the failure reason on DNAC audit logs , checked on topic search not getting the similar bugs
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov06_22:50:06.140108.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS#:~:text=Test_TC24_verify_assurance_health_nw_health_border_edge_wlc_ext_node-,Test_TC25_Verify_DHCP_server_change_on_segments,-Failed|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov06_22:50:06.140108.zip&reqseq=&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=PYATS#:~:text=Test_TC24_verify_assurance_health_nw_health_border_edge_wlc_ext_node-,Test_TC25_Verify_DHCP_server_change_on_segments,-Failed]

Last passed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17627976&size=32325&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct20_02:37:15.775666.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=17627976&size=32325&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_auto_job.2023Oct20_02:37:15.775666.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Failed DNAC version/ISO:HulkP2#2.3.7.4-70257

Last Passed DNAC version/ISO:HulkP1
Script/Mapping file Used:
testcases/upgrade/after_upgrade_verify.py

TCs Impacted:1 failed /Rest of TC's blocked
'Re-run' TCs:1

Branch Used:private/HulkPatch2-ms/sanity_api_auto

DNAC Type:Hulk

*NOTE:*We are doing upgrade to Hulk P2 first time and testing upgrade script with Hulk P2 branch first time.

Testbed wiki:
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-5|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-5]",2023-11-08T04:13:30.346+0000,"[~accountid:63f50bf84c355259db9ccc59] Please share regression analysis and follow the template [https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] Why do you use '{{'dnac_input': './configs/upgrade/guardian_to_ghost_uprgade_solution_test_input.json'}}' when upgrding to Hulk Patch2?
Please use the right file and try again. From the passed log, you used the correct file '{{'dnac_input': './configs/upgrade/ghost_to_hulk_solution_test_input_upgrade.json'}}'. 
Junk this ticket since its not script issue and reporter used wrong solution input file for upgrade. Hi [~accountid:62d2fe9f8afb5805e5d5af49] 
We used correct input file only
I tried Guardian P4 RC4<>Hulk p2 ,we checked with DE Team also

Script is not doing reconfigure properly , please check auton with high priority as it is a blocker auton
Analyse from DE TEAM:
after you update the dhcp server in the ip pool, you need to the 'Reconfigure Fabric' operation in the banner

Space with DE Team:
webexteams://im?space=442e6a80-97ed-11ee-9dca-33520b96d1b4 [~accountid:61efa8c457b25b006877eda3]  your *ghost_to_hulk_solution_test_input_upgrade.json* wasnot created correctly, it was not based on the N release solution input (Hulk branch) but based on N-1/N-1 solution input.

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/upgrade/ghost_to_hulk_solution_test_input_upgrade.json?at=refs%2Fheads%2Fprivate%2FHulkPatch2-ms%2Fsanity_api_auto#204|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/upgrade/ghost_to_hulk_solution_test_input_upgrade.json?at=refs%2Fheads%2Fprivate%2FHulkPatch2-ms%2Fsanity_api_auto#204]

Please update your *ghost_to_hulk_solution_test_input_upgrade.json* properly with this approach: take N releases Solution input file & removing the New SSIDs. Close this as it is input issue.","['Auton', 'Blocker', 'Execution', 'Hulk', 'Sanity', 'Upgrade', 'Uplift']",Tran Lam,Closed,Tulasi Reddy
SEEN-2836,https://miggbo.atlassian.net/browse/SEEN-2836,[HULK][AUTON]-Prime Feature-  Test20_prime_brownfield_learn,"Hulk  Version-2.1.713.70302
Script name:solution_test_3sites_sjc_nyc_sf.py

Branch:private/HulkPatch-Primemstb3/api-auto

Device Image:17.12.1  5852: Failed to learn device configurations for reason:Brownfield configuration learn failed.

5853: Library group ""brownfield"" method ""learn_device_configuration"" returned in 0:00:02.417639

5854: Learn device config didn't work, retrying

5855: Library group ""inventory"" method ""get_network_device_info"" returned in 0:00:00.000031

5858: api_switch_call called:

5860: Resource path full url: [https://172.35.16.151/api/v1/dna/devicelearnstatus|https://172.35.16.151/api/v1/dna/devicelearnstatus]

5861: Newer version of api, proceeding with newer logic

5862: Device already learned: Failed

 

 

Failed Log:

[test20_prime_brownfield_learn|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=894608&size=982043&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov06_21:04:58.149250.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=894608&size=982043&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov06_21:04:58.149250.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=894608&size=982043&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov06_21:04:58.149250.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Note: We don’t have pass log for this first time we are executing in MSTB3 Testbed",2023-11-08T10:02:56.313+0000,"[~accountid:641058d57222b08f3e7064d0] please follow this template when raising Autons, [+https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist+|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist]","['Auton', 'Functionality', 'Hulk']",Andrew Chen,Open,Balaji Raju
SEEN-2838,https://miggbo.atlassian.net/browse/SEEN-2838,[Auton] Post upgrade script refers an SSID that doesn't exist before upgrade,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue: Y*
#* *Compared with the last passed log: Y - But this is the first time upgrade sanity has run on Appliance after we refactor*
#* *Checked and compared the configuration: Y*
#* *Confirmed with DE on the root cause: NA*
# *Issue faced: Day N upgrade sanity failing - this case -* [*Task-1*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=0&size=-1&archive=%2Fhome%2Fsdnbld%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov08_16:53:02.156556.zip&ats=%2Fpyats-py386&submitter=sdnbld&atstype=PYATS]  */*   [*Test_TC7_DNAC_Device_Re_Provisioning_after_upgrade*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=464076&size=1703253&archive=%2Fhome%2Fsdnbld%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov08_16:53:02.156556.zip&ats=%2Fpyats-py386&submitter=sdnbld&atstype=PYATS]  */   test2_DNAC_verify_SSID_lan_on_ECA_device  (Failed)*
# *Failure/Errors snip from log:* 

{noformat}8800: 
                 ""show wlan summary"": ""show wlan summary\n\nNumber of WLANs: 9\n\nID   Profile Name                     SSID                             Status Security                                                                                             \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n1    CiscoSensorProvisioning          CiscoSensorProvisioning          UP     [WPA2][802.1x][AES]                                                                                  \n18   posture_profile                  posture                          UP     [WPA2][802.1x][AES]                                                                                  \n19   GUEST_profile                    GUEST                            UP     [open],MAC Filtering                                                                                 \n20   Random_mac_profile               Random_mac                       UP     [WPA2][802.1x][AES]                                                                                  \n22   SSIDScheduler_profile            SSIDScheduler                    DOWN   [WPA2][802.1x][AES]                                                                                  \n24   OPEN_profile                     OPEN                             UP     [open]                                                                                               \n25   Single5KBand_profile             Single5KBand                     UP     [WPA2][PSK][FT + PSK][AES],[FT Enabled],MAC Filtering                                                \n26   SSIDDot1XIndia_profile           SSIDDot1XIndia                   UP     [WPA2][802.1x][AES]                                                                                  \n28   Radius_ssid_profile              Radius_ssid                      UP     [WPA2][802.1x][AES]                                                                                  \n\nTB2-WLC-9840#""{noformat}

{noformat}8801: 
             },{noformat}

{noformat}8802: 
             ""BLOCKLISTED"": {},{noformat}

{noformat}8803: 
             ""FAILURE"": {}{noformat}

{noformat}8804: 
         }{noformat}

{noformat}8805: 
     }{noformat}

{noformat}8806: 
 ]{noformat}

{noformat}8807: 
 Output of 'show wlan summary' executed on TB2-WLC-9840:{noformat}

{noformat}8808: 
 show wlan summary{noformat}

{noformat}8809: 
{noformat}

{noformat}8810: 
 Number of WLANs: 9{noformat}

{noformat}8811: 
{noformat}

{noformat}8812: 
 ID   Profile Name                     SSID                             Status Security{noformat}

{noformat}8813: 
 ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------{noformat}

{noformat}8814: 
 1    CiscoSensorProvisioning          CiscoSensorProvisioning          UP     [WPA2][802.1x][AES]{noformat}

{noformat}8815: 
 18   posture_profile                  posture                          UP     [WPA2][802.1x][AES]{noformat}

{noformat}8816: 
 19   GUEST_profile                    GUEST                            UP     [open],MAC Filtering{noformat}

{noformat}8817: 
 20   Random_mac_profile               Random_mac                       UP     [WPA2][802.1x][AES]{noformat}

{noformat}8818: 
 22   SSIDScheduler_profile            SSIDScheduler                    DOWN   [WPA2][802.1x][AES]{noformat}

{noformat}8819: 
 24   OPEN_profile                     OPEN                             UP     [open]{noformat}

{noformat}8820: 
 25   Single5KBand_profile             Single5KBand                     UP     [WPA2][PSK][FT + PSK][AES],[FT Enabled],MAC Filtering{noformat}

{noformat}8821: 
 26   SSIDDot1XIndia_profile           SSIDDot1XIndia                   UP     [WPA2][802.1x][AES]{noformat}

{noformat}8822: 
 28   Radius_ssid_profile              Radius_ssid                      UP     [WPA2][802.1x][AES]{noformat}

{noformat}8823: 
{noformat}

{noformat}8824: 
 TB2-WLC-9840#{noformat}

{noformat}8825: 
 Library group ""command_runner"" method ""execute_command_on_device"" returned in 0:00:02.437034{noformat}

{noformat}8826: 
{noformat}

{noformat}8827: 
 ERROR Following line of expected cli output not present on device:{noformat}

{noformat}8828: 
 \d+\s+.*NEW_SSID\s+UP\s+{noformat}

{noformat}8829: 
{noformat}

{noformat}8830: 
 Retrying after 10 seconds{noformat}

{noformat}8831: {noformat}





# *Regression team debug analysis with details: Stated above* 
# *DE analysis/confirmation with details: NA*
# *What do Automation team need to look into? Jenkins failure logs* 
# *Failed log:* [*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2008911&size=158246&archive=%2Fhome%2Fsdnbld%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov08_16:53:02.156556.zip&ats=%2Fpyats-py386&submitter=sdnbld&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2008911&size=158246&archive=%2Fhome%2Fsdnbld%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov08_16:53:02.156556.zip&ats=%2Fpyats-py386&submitter=sdnbld&from=trade&view=all&atstype=pyATS]
# *Last passed log: not there as I said above*
# *Failed DNAC version/ISO: Version 2.3.7.5-70083*
# *Last Passed DNAC version/ISO: NA*
# *Script/Mapping file Used:  TESTSCRIPT_FILE=""dnac-auto/testcases/upgrade/after_upgrade_verify.py""*
# *TCs Impacted: many of them*
# *'Re-run' TCs: NA*
# *Branch Used: private/HulkPatch2-ms/api-auto*
# *DNAC Type: Appliance*
# *Testbed details:* [*https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_platformtb2/local_SanityPlatformTB2.json?at=private%2FHulkPatch2-ms%2Fapi-auto*|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_platformtb2/local_SanityPlatformTB2.json?at=private%2FHulkPatch2-ms%2Fapi-auto]
# *Team: DNAC PL SIT*",2023-11-09T01:59:02.546+0000,"As discussed yesterday, you need to use N-1/N-2 solution input files for post upgrade test.
You can check with Santhosh and use same files as they used. [~accountid:62d2fef2bd54f8d3ffb7d1f7] can you please let me know the scripts that are needed to running for upgrade sanity ?  Close this since its not script issue.
[~accountid:62d2ff17afe495359d9f1fb0] , please get the file link from Santhosh and use the same.","['Auton', 'Functionality']",Tran Lam,Closed,Gopala Krishna Chaikam
SEEN-2839,https://miggbo.atlassian.net/browse/SEEN-2839,[Auton]HulkPatch:TC236_Validate_operations_on_unreachable_devices/test4_make_device_unreachable_by_shutdown_interface: AttributeError: 'bool' object has no attribute 'name',"

# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue: Y*
#* *Compared with the last passed log: Y*
#* *Checked and compared the configuration: Y*
#* *Confirmed with DE/AE on the root cause: Y*
# *Issue faced:* For the below Api Call we are seeing Attribute Error
# *Failure/Errors snip from log:* Attaching the Error Snip:
# !image-20231109-061655.png|width=1803,height=622!
!image-20231109-061719.png|width=550,height=57!
# *Regression team debug analysis with details:* We see it is trying to connect to Airos WLC Device and trying to execute “show ip interface brief” which is not supported by the Airos Device
# *DE analysis/confirmation with details:*  Update from Nhan: After checking, I saw there was a problem with the library get_interface_to_make_device_unreachable use for wlc device. It makes the value of interface_wlc is ‘False'. That’s why it got the Error. Let me debug and fix this library.
About Airos Device, I don’t know this one before. When I automate this test case, I write for the normal wlc
# *What do Automation team need to look into?* : Issue with library need to check and debug and also need to check which command is supported by the Airos Device 
# *Failed log:* [Test_TC236_Validate_operations_on_unreachable_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1514487&size=15649204&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov08_00:23:49.856958.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log: New Feature:*[Test_TC230_Validate_operations_on_unreachable_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=242091&size=6647929&archive=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-nhanhngu%2Fusers%2Fnhanhngu%2Farchive%2F23-10%2Fsanity_TB1.2023Oct02_01:43:13.500448.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-nhanhngu&submitter=nhanhngu&from=trade&view=all&atstype=pyATS]
# *Failed DNAC version/ISO:* 2.3.7.3-70327
# *Last Passed DNAC version/ISO:* New Feature attempted first time
# *Script/Mapping file Used:* Lan-A Script
# *TCs Impacted:* [test4_make_device_unreachable_by_shutdown_interface|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=13635988&size=23572&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov08_00:23:49.856958.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch-ms/sanity_api_auto
# *DNAC Type:* On-Prem
# *Testbed details:*[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2] 
# *Team:* Sanity",2023-11-09T06:11:17.663+0000,"[~accountid:63f50bcafb3ac4003fa2c6dd] : Same issue is also observed on Multisite testbed (DR+mDNAC)

*Regression Profile:* Solution Regression Multisite

*Branch used:* private/HulkPatch2-ms/api-auto

*Uber ISO tested:* Hulk Patch2 - 2.1.714.70257 (with PUBSUB enabled)

*Script Used:* dnac-auto/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py

+*Issues Faced:*+

We are observing attribute errors during the execution of the respective feature related TC.

*Failed log:* [Test_TC287_Validate_operations_on_unreachable_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2721902&size=20469453&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov08_11:18:55.854917.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*Feature Wiki -* [https://wiki.cisco.com/display/EDPEIXOT/Validate+operations+on+unreachable+devices|https://wiki.cisco.com/display/EDPEIXOT/Validate+operations+on+unreachable+devices] The root cause: The command ""show ip int brief"" working fine on the Polaris device supported command. However, it does not work on Aireos WLC devices.

Solution: Change the code to make sure to not select the Airos Device. # PR HulkPatch-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7856/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7856/overview]
# Test Case:  TC236_Validate_operations_on_unreachable_devices
# Testbed: TB1
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link HulkPatch-ms/api_auto:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-11/sanity_TB1.2023Nov13_00:14:01.660188.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-11/sanity_TB1.2023Nov13_00:14:01.660188.zip&atstype=ATS] Hi [~accountid:63f50bcafb3ac4003fa2c6dd] 
I did give a run using the latest code change in our local Branch
We see that the test case has been blocked 
[Test_TC236_Validate_operations_on_unreachable_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1526798&size=94424&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov14_22:59:37.498976.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

On checking further we understood that as per the latest change you have marked it as blocked if there is Airos Device in the setup

We request you to update the Wiki in the feature Requirements. Update wiki is done. Similar issue is seen on MSTB2. Please find the failed log below.

Branch used: private/HulkPatch2-ms/api-auto

Script file: solution_test_3sites_sjc_nyc_sf.py

DNAC: 3.714.75217

Failed log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8788114&size=2757517&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov19_22:12:00.637429.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8788114&size=2757517&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov19_22:12:00.637429.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] Hi [~accountid:712020:5a4635f5-145d-405e-b93a-679816735abb], Yep, the issue you got is the same as this issue. My change will fix the issue. [~accountid:63f50bcafb3ac4003fa2c6dd] : Same attribute error is still observed after using lastest code to execute.

Branch used - private/HulkPatch2-ms/api-auto
script use - testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
Testbed - MSTB1 (Multisite DR testbed)
Hulk Patch2 Uber ISO used - 2.1.714.70296

*Failed log:* [Test_TC287_Validate_operations_on_unreachable_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=21730469&size=22370141&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov21_10:37:28.577345.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] Hi [~accountid:63f50bcafb3ac4003fa2c6dd] 

Seeing the same attribute error issue on MSTB2 as well. Please find the logs.

Branch used: private/HulkPatch2-ms/api-auto

Script [file: solution_test_3sites_sjc_nyc_sf.p|file: solution_test_3sites_sjc_nyc_sf.]y

DNAC: 3.714.75232

Logs: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9699263&size=32621&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov23_23:28:54.769727.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=9699263&size=32621&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov23_23:28:54.769727.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS] # PR HulkPatch2-ms/api_atuto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8110/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8110/overview]
# Test Case:  {{Test_TC233_Validate_operations_on_unreachable_devices}}
# Testbed: NFW2
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link HulkPatch2-ms/api_atuto:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-11/nfw3_job.2023Nov27_23:34:14.334162.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-11/nfw3_job.2023Nov27_23:34:14.334162.zip&atstype=ATS] The root cause: The library ‘get_interface_to_make_device_unreachable' return the value; ‘false’. That is why the next library doesn’t have the correct interface to run. And then it raises the error: 'bool' object has no attribute 'name'.

Solution: Fix that library to get the correct value. PR was merged.","['Auton', 'ESXi', 'Ghost', 'Halleck', 'Hulk', 'HulkPatch2', 'Integration', 'Issue', 'MSTB2', 'Shockwave', 'optimized', 'sanity']",NhanHuu Nguyen,Resolved,DeepakPratap Shinde
SEEN-2840,https://miggbo.atlassian.net/browse/SEEN-2840,[Auton]HulkPatch: Test_TC212_template_conflicts_in_template_hub/test17_add_new_site_tag_to_a_wireless_device,"*Regression:* SDA Solution Sanity

*DNAC Type:* On-Prem

*DNAC Release_Version Used:* Hulk P2 2.1.714.70275

*Branch Used:* rcdn/HulkPatch2-ms/api-auto synced to main branch Private/HulkPatch2-ms/api-auto before reg run start

*Script Used:* dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb.py

*Taas Log:* [https://ngdevx.cisco.com/services/taas/results/3e0c813c-5cd2-49aa-b910-9497527248d4|https://ngdevx.cisco.com/services/taas/results/3e0c813c-5cd2-49aa-b910-9497527248d4]

!image-20231109-071001.png|width=1757,height=318!

*Confirm from NFW team:* This is mistake when changing the key name. It must be ""name"" not ""hostname"". The API for calling from the URL = ""/assurance/v1/network-device"" will be retrieved using the key [""name""], and the API for calling from the URL = ""/v1/network-device"" will be retrieved using the key [""hostname""]. They are quite similar to each other for the purpose of retrieving the device name, so there has been confusion here.",2023-11-09T07:04:17.329+0000,PR-link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7837/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7837/overview],"['Auton', 'Functionality', 'HulkPatch2']",QuangVinh Nguyen,Resolved,NhanHuu Nguyen
SEEN-2844,https://miggbo.atlassian.net/browse/SEEN-2844,[AUTON][NFW]-Prime Feature- test13_job_history_scan,"Hi Vinay,

 As you discussed with Sandeep, could you please have a look and increase the sleep time in both scripts Hulk and Ghost

Ghost P2 version: Version 2.3.5.4-70852

Prime switch:17.11.1 FC2

Vewlc  : 17.11.1 FC2

Script path: /forty_eight_hour/solution_dnac_wireless_hardening.py

Branch: private/Ghost-ms/api-auto-nfw , private/Hulk-ms/api-auto-nfw

2081: Attempting to log into DNAC...

2082: DnaUiServices instantiation completed successfully

2083: Logging Into the Prime Services!!!

2084: Logging Into the Prime Server Using [https://10.4.23.23:8078/DNACDataMigration/jsp/dnacmigration.jsp|https://10.4.23.23:8078/DNACDataMigration/jsp/dnacmigration.jsp]

2085: ************************************************************

2086: Sending Credentials Into Prime

2087: ************************************************************

2088: here

2089: Traceback (most recent call last):

2090: File ""/auto/dna-sol/ws/Ghost_nfw3/dnac-auto/services/commonlibs/test_wrapper.py"", line 301, in wrapper

2091: result = testfunc(func_self, **kwargs)

in ui_handle.browser.find_element_by_xpath(""//span[@id='jobHistory']"").click()



Failed log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=307834&size=17130&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fsr_nfw_job.2023Nov09_02:47:46.378055.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=307834&size=17130&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fsr_nfw_job.2023Nov09_02:47:46.378055.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=admin&from=trade&view=all&atstype=pyATS]",2023-11-10T06:40:52.545+0000,"Relevant mail thread:

----

*From:* SANDEEP SHIVARAMAREDDY -X (sandshiv - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[sandshiv@cisco.com|mailto:sandshiv@cisco.com]> 
*Sent:* Friday, November 10, 2023 11:41 AM
*To:* Jagadesh Kumar Enapanuri -X (jaenapan - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[jaenapan@cisco.com|mailto:jaenapan@cisco.com]>; Vinay Raj V (vinavasu) <[vinavasu@cisco.com|mailto:vinavasu@cisco.com]>
*Cc:* Karthika MM -X (muthukar - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[muthukar@cisco.com|mailto:muthukar@cisco.com]>; Divakar Kumar Yadav -X (divayada - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[divayada@cisco.com|mailto:divayada@cisco.com]>; Balaji Raju -X (braju2 - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[braju2@cisco.com|mailto:braju2@cisco.com]>; Ashwini R Jadhav -X (ajadhav2 - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[ajadhav2@cisco.com|mailto:ajadhav2@cisco.com]>; Santhosh Mounaswamy -X (smounasw - TERRALOGIC SOLUTIONS INC at Cisco) <[smounasw@cisco.com|mailto:smounasw@cisco.com]>
*Subject:* RE: Prime integration in NFW

 

Hi Jagadesh,

 

As discussed, I had a debugging with Vinay on this issue. It looks like the *job_history_scan* testcase failure is due to timing issue where sometimes migration page on Prime infra takes more time to load and due to this the job history click operation occurs even before the page loads. We have verified the same manually as well. Vinay has agreed to enhance the validation by adding more sleep time to 2 minutes and iteration check for every 15 seconds so that we don’t encounter such issues. Please raise a Automation JIRA for this issue and update here.

[@Vinay Raj V (vinavasu)|mailto:vinavasu@cisco.com]: Thanks for the syncup! As discussed, please commit the changes to Ghost as well as Hulk production branches.

 

  Regards
Sandeep S

 

*From:* Jagadesh Kumar Enapanuri -X (jaenapan - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[jaenapan@cisco.com|mailto:jaenapan@cisco.com]> 
*Sent:* Thursday, November 9, 2023 10:25 PM
*To:* Vinay Raj V (vinavasu) <[vinavasu@cisco.com|mailto:vinavasu@cisco.com]>
*Cc:* Karthika MM -X (muthukar - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[muthukar@cisco.com|mailto:muthukar@cisco.com]>; SANDEEP SHIVARAMAREDDY -X (sandshiv - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[sandshiv@cisco.com|mailto:sandshiv@cisco.com]>; Divakar Kumar Yadav -X (divayada - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[divayada@cisco.com|mailto:divayada@cisco.com]>; Balaji Raju -X (braju2 - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[braju2@cisco.com|mailto:braju2@cisco.com]>; Ashwini R Jadhav -X (ajadhav2 - TERRALOGIC SOFTWARE SOLUTIONS PRIVATE LIMITED at Cisco) <[ajadhav2@cisco.com|mailto:ajadhav2@cisco.com]>
*Subject:* Prime integration in NFW

 

Hi Vinay,

 

As we discussed with you, we have pulled the main branch code locally and added missed prime testcases in my nfw Ghost branch.

 

Trigger the execution and observed the xpath issue for *job_history_scan* testcase, could you please have a look into this issue and help us to merge the changes into below mentioned NFW branch as well.

 

please find the below NFW branches for Hulk and Ghost for using our regular runs.

 

*private/Hulk-ms/api-auto-nfw*

*private/Ghost-ms/api-auto-nfw*

 

prime logs:

TC7.3-7.10 : [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fsr_nfw_job.2023Nov09_01:47:28.564622.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fsr_nfw_job.2023Nov09_01:47:28.564622.zip&atstype=ATS]

TC7.11-7.23: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fsr_nfw_job.2023Nov09_02:47:46.378055.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fsr_nfw_job.2023Nov09_02:47:46.378055.zip&atstype=ATS]

 

Thanks & Regards,

Jagadesh [~accountid:712020:06823340-9d28-4722-9cd7-15ef3ea14cc6] , Can you follow this template when raising Autons?

[https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist|https://wiki.cisco.com/display/EDPEIXOT/Automation+support+Checklist] [~accountid:712020:06823340-9d28-4722-9cd7-15ef3ea14cc6] , this is not an Auton.. Please raise as Enhancment…  Hulk

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8145/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8145/overview] Hi Vinay,
Currently we are executing the Hulk P2 RC1-*2.1.714.70391*

Still we are observing the issue in this release also, could you please have a look and fix it, 

{color:#ff5630}Errored reason: no such element: Unable to locate element: {""method"":""xpath"",""selector"":""//span[@id='jobHistory']""}{color}

please find the below failed log:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fsr_nfw_job.2023Dec12_08:10:19.142909.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fsr_nfw_job.2023Dec12_08:10:19.142909.zip&atstype=ATS]



Thanks,

Jagadesh","['Auton', 'Ghost', 'GhostPatch2', 'HulkPatch1', 'HulkPatch2', 'MSTB1', 'Multisite', 'NFW', 'regression']",Vinay Raj V ,Reopened,JagadeshKumar Enapanuri
SEEN-2845,https://miggbo.atlassian.net/browse/SEEN-2845,[Auton] Test_TC1_AP_Zone / test5_provision_aps testcase failing during AP provisioning,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue: Y*
#* *Compared with the last passed log: N*
#* *Checked and compared the configuration: Y*
#* *Confirmed with DE on the root cause: Y*
# *Issue faced:* After creating and assigning device tags, adding ap zones and then provisioning WLC. During AP Provisioning it failed with this reason: 
Provisioning AP of device failed for reason:NCWL10309: The site FLOOR1_LEVEL1 with Id ce2e076b-1b00-4035-9ba4-719af6f27421 in APWirelessConfiguration FLOOR1_LEV_Zone1_Ente_8b150 with Id 651872d8-578e-4cbd-a1d2-d7fa399a3a89 is not managed by any Wireless Controllers

# *Failure/Errors snip from log:*
{{Provisioning AP of device failed for reason:NCWL10309: The site FLOOR1_LEVEL1 with Id ce2e076b-1b00-4035-9ba4-719af6f27421 in APWirelessConfiguration FLOOR1_LEV_Zone1_Ente_8b150 with Id 651872d8-578e-4cbd-a1d2-d7fa399a3a89 is not managed by any Wireless Controllers}}
# *Regression team debug analysis with details:*

Testcase is failing to provision APs to the floor which is not being managed by any controllers. Testcase should have a check before provisioning the APs, whether the floor that is being picked to provision APs is being managed by the controller. If it is not managed by the controller then add a method where it will check and add the required AP sites as controller managedSites during WLC provisoning before AP provisioning.

# *DE analysis/confirmation with details:* DE has analyzed and suggested to add a check before AP provisioning over webex space webexteams://im?space=d6d1ded0-ffc2-11ed-85f9-31618fce5ae1
# *What do Automation team need to look into?*
# *Failed log:* [Test_TC1_AP_Zone|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_zones.py-1311-apZones&begin=2293&size=1587765&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov09_02:53:09.368417.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:*[Test_TC1_AP_Zone|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_zones.py-131-apZones&begin=2267&size=2177532&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fenv_optimized_auto_job.2023Jul16_23:53:59.061508.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Failed DNAC version/ISO:* Ghost-p3 2.1.615.70111
# *Last Passed DNAC version/ISO:* Ghost-p2 2.1.614.70780
# *Script/Mapping file Used:* Usecase [apZones|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_zones.py-1311-apZones&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov09_02:53:09.368417.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *TCs Impacted:* Test_TC1_AP_Zone / test5_provision_aps
# *'Re-run' TCs: NA*
# *Branch Used:* private/Ghost-ms/sanity_api_auto
# *DNAC Type:* On-Prem single node DNAC
# *Testbed details:* [https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8]
# *Team:* Sanity",2023-11-10T07:27:41.732+0000,"Moe can you check ? it is complining about the APZone site, see if correct AP zone is picked up or not. ","['Auton', 'ghost', 'optimized', 'sanity']",Moe Saeed,Backlog,Ashwini R Jadhav
SEEN-2846,https://miggbo.atlassian.net/browse/SEEN-2846,Auton:Ghost:Cleanup: Unable to triggere the execution,,2023-11-10T08:13:17.775+0000,"After making chages was able to proceed runs and got trade log:
Log:
[https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-11/env_auto_job.2023Nov09_23:46:35.558981.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/home/admin/.pyats/archive/23-11/env_auto_job.2023Nov09_23:46:35.558981.zip&atstype=ATS]

Raised PR 

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7841/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7841/overview]","['Auton', 'Cleanup', 'Execution', 'Ghost']",Tulasi Reddy,Closed,Tulasi Reddy
SEEN-2848,https://miggbo.atlassian.net/browse/SEEN-2848,[Auton]AWS Sanity:Need some flag on Jenkins wrt key to access ISE is not login via script for new ip,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue: Y*
#* *Compared with the last passed log: No pass log*
#* *Checked and compared the configuration: NA*
#* *Confirmed with DE on the root cause:NA*
# *Issue faced:*

We brought up new ISE [172.35.16.15|mailto:maglev@172.35.16.151]0  and  ISE was not able to connect via script due to key issue manually i logged in to ISE as:
ssh -i /auto/dna-sol/ws/pem_key_aws/key-n-california.pem [iseadmin@172.35.16.150|mailto:iseadmin@172.35.16.150] 

*3.Failure/Errors snip from log:*

{noformat}34: 
 ###################################################{noformat}

{noformat}35: 
 #!!!FAILED TO GET ALL AUTHORIZATION PROFILE in ISE. ERROR HTTPSConnectionPool(host='172.35.16.150', port=9060): Max retries exceeded with url: /ers/config/authorizationprofile?size=100&page=1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7ff23e047190>, 'Connection to 172.35.16.150 timed out. (connect timeout=60)'))----#{noformat}

{noformat}36: 
 ###################################################{noformat}

{noformat}37: 
 Traceback (most recent call last):{noformat}



# *DE analysis/confirmation with details:NA*
# *What do Automation team need to look into?Do add flag on jenkins itself for KEY so ISE will login after selecting FLAG*
# *Failed log:*[*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_cleanup_profile_nw_device.py-21-ISECleanupGoldenConfig&begin=4851&size=19208&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov09_04:49:00.573247.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ise_cleanup_profile_nw_device.py-21-ISECleanupGoldenConfig&begin=4851&size=19208&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov09_04:49:00.573247.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:NA [First time attempting ]*
# *Failed DNAC version/ISO:Ghost P2* 
# *Last Passed DNAC version/ISO:NA*
# *Script/Mapping file Used:solution_test_sanityecamb_lan.py*
# *TCs Impacted:2-3 TC*
# *'Re-run' TCs:NA*
# *Branch Used:private/Ghost-ms/sanity_api_auto*
# *DNAC Type:Ghost P2*
# *Testbed details:*[*https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-11*|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-11]
# *Team:Solution Sanity*",2023-11-10T11:05:56.407+0000,"[~accountid:61efa8c457b25b006877eda3] [~accountid:62ab7a399cd13c0068b18fe0] Issue is unrelated to the ssh key, actually two issues:

1. ISE ers api was not enabled. You can refer to this wiki: [ISE configuration - EDPEIXOT Solution Engineering Team Wiki - IT Wiki (cisco.com)|https://wiki.cisco.com/display/EDPEIXOT/ISE+configuration#ISEconfiguration-EnableERS]
2. Port 9060 is used. As per ISE UI: “Currently, ERS APIs also operate over port 9060. However, port 9060 might not be supported for ERS APIs in later Cisco ISE releases. We recommend that you only use port 443 for ERS APIs.”

Once I enabled ERS and changed the input files to use port 443 instead of 9060, api was working fine thru script. Please make the necessary changes in your input files, and when creating new ISE enable ERS. 

","['AWS_Sanity', 'Auton', 'Ghost', 'Guardian', 'Hulk']",Andrew Chen,Resolved,Anusha John
SEEN-2852,https://miggbo.atlassian.net/browse/SEEN-2852,[Auton][MSTB2] -  Test_TC35_DNAC_verifying_configuration_lisp_on_devices  /   test1_verify_configuration_fabric1,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* Y
# *Issue faced:* Script is looking for “no protocol udp” cli present on device. But it’s failing due to latest changes as per “[https://cdetsng.cisco.com/webui/#view=CSCwf36088|https://cdetsng.cisco.com/webui/#view=CSCwf36088]” bug. “no protocol udp” cli is deprecated on 17.13.1 image
# *Failure/Errors snip from log:* 
4068:  ERROR Following line no protocol udp of expected config not present on device:
34069:  SJC-FE-9300-1
34070:  {'result': True, 'output': 'show run | s device-tracking\ndevice-tracking tracking\ndevice-tracking policy IPDT_POLICY\n tracking enable\n device-tracking attach-policy IPDT_POLICY\n device-tracking attach-policy IPDT_POLICY\n device-tracking attach-policy IPDT_POLICY\n device-tracking attach-policy IPDT_POLICY\n device-tracking attach-policy IPDT_POLICY\n device-tracking attach-policy IPDT_POLICY\n device-tracking attach-policy IPDT_POLICY\n device-tracking attach-policy IPDT_POLICY\n device-tracking attach-policy IPDT_POLICY\n device-t
# *Regression team debug analysis with details:* Script is looking for “no protocol udp” cli present on device. But it’s failing due to latest changes as per “[https://cdetsng.cisco.com/webui/#view=CSCwf36088|https://cdetsng.cisco.com/webui/#view=CSCwf36088]” bug. “no protocol udp” cli is deprecated on 17.13.1 image. 
# *DE analysis/confirmation with details:* Anand Pulicat has confirmed about the changes on latest 17.13.1 polaris image. Below is the bug I’ve raised “[https://cdetsng.cisco.com/webui/#view=CSCwi13584|https://cdetsng.cisco.com/webui/#view=CSCwi13584]” and it got moved to J.
# *What do Automation team need to look into?* Need to uplift the scriipt for validating “no protocol udp” needs to be skipped or removed.
# *Failed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7085709&size=7135663&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov07_21:38:21.024386.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=7085709&size=7135663&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov07_21:38:21.024386.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=21550782&size=7291876&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct30_09:50:11.513197.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=21550782&size=7291876&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fauto_MS_job.2023Oct30_09:50:11.513197.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Failed DNAC version/ISO:* 3.714.75207 with 17.13.1PRD6 FC1
# *Last Passed DNAC version/ISO:* 3.714.75195 with 17.12.2 PRD5
# *Script/Mapping file Used:* solution_test_3sites_sjc_nyc_sf.py
# *TCs Impacted:* 2
# *'Re-run' TCs: NA*
# *Branch Used:* private/HulkPatch2-ms/api-auto
# *DNAC Type:* ESXI
# *Testbed details:* MSTB2
# *Team:* Solution Regression",2023-11-14T05:01:19.679+0000,"We are encountering the same issue with the On-Prem Hulk Patch-2 version *2.1.714.70284* execution.
Could you please check?

Test_TC2_DNAC_verifying_configuration_lisp_on_devices/test1_verify_configuration_fabric1

Failed log :
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-112-SDAmulticast&begin=221320&size=2797835&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov09_11:13:39.255788.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-112-SDAmulticast&begin=221320&size=2797835&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov09_11:13:39.255788.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] Observing same issue on Multisite TB1 as well during Hulk Patch2 - 2.1.714.70296 execution

Failed logs:
1) [Test_TC35_DNAC_verifying_configuration_lisp_on_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=38005063&size=22821853&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov16_09:35:50.365130.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] 
2) [Test_TC37_DNAC_configure_wireless_multicast|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=62024615&size=24861717&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov16_09:35:50.365130.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] We are  Observing the same issue with the On-Prem Hulk Patch-3 2.1.715.70102

!image-20231120-123941.png|width=448,height=42!

execution.
Could you please check on priority ,due  this  auton  more  2  production TC’s are  failing 

Failed  log:
[Test_TC2_DNAC_verifying_configuration_lisp_on_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-112-SDAmulticast&begin=218481&size=8586956&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov18_19:05:04.398002.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
[Test_TC4_DNAC_configure_wireless_multicast|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-112-SDAmulticast&begin=9383209&size=4417978&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov18_19:05:04.398002.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
[Test_TC1_SDA_Wired_Host_Onboarding_Uplink_Interfaces|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-SDA_wired_host_onboarding_uplink_interfaces.py-128-SDAWiredHostOnboardingUplinkInterfaces&begin=4717&size=3259628&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov18_19:05:04.398002.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]   Hi [~accountid:620b8357878c2f00729881c8], [~accountid:62d2fec15d6f5fd2c3db8f9f], [~accountid:63f50bf0e8216251ae4d59ca]. i tried to fix the issue, but i can not get the passlog since my cluster doesnt upgrade device image to 17.13.1 yet. So i ask you guys for using your testbed to get the passlog?

or you can use branch: private/quangvin-hulkp2/SEEN-2852 to trigger the testcase

please help to resolve the issue. Thank you alot Hi  [~accountid:63f50bcf4e86f362d39acde5]  ,

could  you please  check  [https://cdetsng.cisco.com/webui/#view=CSCwf36088|https://cdetsng.cisco.com/webui/#view=CSCwf36088]  this defect ,
The device-side behavior has changed with CSCwf36088. Consequently, the script needs to be uplift accordingly.


+*Hulk  P1  RC5*+ 
+*passlog:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-112-SDAmulticast&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct27_10:26:30.516810.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-112-SDAmulticast&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct27_10:26:30.516810.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



webexteams://im?space=11e65150-7e34-11ee-bf98-a36b3aa7d68f Observing same issue onSanity  as well during Ghost Patch3 - 2.1.616.70015 execution

Failed logs:

|[Task-multicast_enable_in_fabric.py-112-SDAmulticast|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-112-SDAmulticast&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec02_02:37:37.782803.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]| |00:51:15|
|[Test_TC2_DNAC_verifying_configuration_lisp_on_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-112-SDAmulticast&begin=1317088&size=4284992&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec02_02:37:37.782803.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|Failed|00:09:37|
|[test1_verify_configuration_fabric1|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-112-SDAmulticast&begin=1318020&size=859963&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec02_02:37:37.782803.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|Failed|00:03:14|
|[Test_TC4_DNAC_configure_wireless_multicast|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-112-SDAmulticast&begin=7143500&size=3996213&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec02_02:37:37.782803.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|Failed|00:15:58|
|[test1_verify_pim_igmp_config_on_fabric_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-multicast_enable_in_fabric.py-112-SDAmulticast&begin=9781285&size=1087267&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec02_02:37:37.782803.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]|Failed|00:03:14| Hi [~accountid:620b8357878c2f00729881c8], [~accountid:63f50bf0e8216251ae4d59ca], Can you please share your testbed? PR-link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8197/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8197/overview] hi Omkar. Can you please trigger the testcase again?","['Auton', 'ESXi', 'Execution', 'Ghost', 'GhostPacth3', 'GhostPatch2', 'Hulk', 'Hulk-Patch2', 'HulkP2', 'MSTB1', 'MSTB2', 'Multisite', 'Optimized', 'Sanity']",QuangVinh Nguyen,Resolved,Divakar Kumar Yadav
SEEN-2980,https://miggbo.atlassian.net/browse/SEEN-2980,[Auton]HulkPatch:TC235_Check_AAA_DHCP_DNS_events_on_assurance/test4_verify_assurance_aaa_events,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue: Y*
#* *Compared with the last passed log: Y*
#* *Checked and compared the configuration: Y*
#* *Confirmed with DE/AE on the root cause: Y*
# *Issue faced:* We are observing the response is getting Average Latency for AAA is not expected
# *Failure/Errors snip from log:* Attaching the Error Snip:
# !image-20231115-063303.png|width=1920,height=914!
# *Regression team debug analysis with details:* We have the AAA server configured 
# !image-20231110-060944.png|width=1920,height=568!
# *DE analysis/confirmation with details:*  
# *What do Automation team need to look into?* : New Feature
# *Failed log:*[*TC235_Check_AAA_DHCP_DNS_events_on_assurance*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3820652&size=890945&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov09_21:02:12.925143.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log: New Feature:* New Feature
# *Failed DNAC version/ISO:* 2.3.7.3-70327
# *Last Passed DNAC version/ISO:* New Feature attempted the first time
# *Script/Mapping file Used:* Lan-A Script
# *TCs Impacted:* [test4_verify_assurance_aaa_events|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4536456&size=54245&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov09_21:02:12.925143.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] & [test5_verify_assurance_dns_events|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4590701&size=120716&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov09_21:02:12.925143.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch-ms/sanity_api_auto
# *DNAC Type:* On-Prem
# *Testbed details:*[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2] 
# *Team:* Sanity",2023-11-15T06:26:25.522+0000,"Cluster is being used for execution. Will back to this issue when the testbed is in an ideal state.

[~accountid:63f50be24e86f362d39acde8] Please share your testbed when it’s back to working state. Hi [~accountid:63f50be24e86f362d39acde8] 

I observed that DNAC has no AAA server data on the Assurance Network Services dashboard.

!image-20231116-063701.png|width=1904,height=925!

I have checked on the WLCs on both SJ and NY sites and the wireless services are up, the AAA server is configured and devices are provisioned successfully. Hence there should be a bug, I will change this ticket to Cancelled.

!image-20231116-065036.png|width=1270,height=856!

!image-20231116-065042.png|width=741,height=518!

Raised a defect to track this issue: [https://cdetsng.cisco.com/webui/#view=CSCwi21559|https://cdetsng.cisco.com/webui/#view=CSCwi21559]","['Auton', 'HulkPatch', 'sanity']",ThangQuoc Tran,Cancelled,DeepakPratap Shinde
SEEN-2981,https://miggbo.atlassian.net/browse/SEEN-2981,[Auton] Hulk P2 EXSI Sanity - Monitor_bgp_between_border_and_control_plane.py-121-monitorBGPBetweenBorderAndControlPlane  /   Test_TC1_Monitor_And_Debug_BGP_Session_Status_Issues_Between_Border_And_Control_Plane  ,"Hi [~accountid:63f50bcafb3ac4003fa2c6dd] 


During Hulk P2 EXSI Sanity  execution  *Monitor_bgp_between_border_and_control_plane.py-121-monitorBGPBetweenBorderAndControlPlane / Test_TC1_Monitor_And_Debug_BGP_Session_Status_Issues_Between_Border_And_Control_Plane* errored due to unable to fetch image version of TB4-DM-eCA-BORDER device getting below error message:

string provided to {{int}} could not be parsed as an integer



{{Device details obtained from Fabric SAN JOSE is: [{'name': 'TB4-DM-eCA-BORDER.cisco.com', 'role': ['ENDPOINT_PROXY', 'EDGENODE', 'MAPSERVER', 'BORDERNODE'], 'imageVersion': '17.13.1eft14'}]}}



_145:      if int(s_parts[i]) >= int(f_parts[i]):_
_146:  ValueError: invalid literal for int() with base 10: '1eft14'_
_147:  Test returned in 0:00:00.535196_
_148:  Errored reason: invalid literal for int() with base 10: '1eft14'_



*Debugged/Analyzed the issue:*  Y

*Compared with the last passed log:* Y 

*Checked and compared the configuration:* N

*Confirmed with DE on the root cause:* N

Issue faced:

Failure/Errors snip from log: Attached

Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-monitor_bgp_between_border_and_control_plane.py-121-monitorBGPBetweenBorderAndControlPlane&begin=25738&size=16969&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov15_00:35:52.556088.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-monitor_bgp_between_border_and_control_plane.py-121-monitorBGPBetweenBorderAndControlPlane&begin=25738&size=16969&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov15_00:35:52.556088.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Last passed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-monitor_bgp_between_border_and_control_plane.py-121-monitorBGPBetweenBorderAndControlPlane&begin=25420&size=16601&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct22_14:26:29.479476.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-monitor_bgp_between_border_and_control_plane.py-121-monitorBGPBetweenBorderAndControlPlane&begin=25420&size=16601&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct22_14:26:29.479476.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Failed DNAC version/ISO:*  3.714.75217

*Last Passed DNAC version/ISO:*   3.714.75176

*Script/Mapping file Used:*  lansanity_usecases_maps.yaml

*TCs Impacted:* 1

*'Re-run' TCs:* Rerun also same observed the issue

*Branch Used:* private/HulkPatch2-ms/api-auto

*DNAC Type:*  EXSI VM

*Testbed details:* TB4
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4]

*Team:* EXSI Sanity",2023-11-15T17:28:12.311+0000,"# PR Hulk-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7934/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7934/overview]
# Test Case:  Test_TC207_Monitor_And_Debug_BGP_Session_Status_Issues_Between_Border_And_Control_Plane
# Testbed: Sanity Testbed 1
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link Hulk-ms/api_auto:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-11/sanity_TB1.2023Nov17_00:20:37.329203.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-11/sanity_TB1.2023Nov17_00:20:37.329203.zip&atstype=ATS] [~accountid:63f50bcafb3ac4003fa2c6dd], required changes have been approved and merged to *private/HulkPatch2-ms/api-auto* branch and cherry-picked to *private/Ghost-ms/api-auto* branch.

However, just for next time, pls. do check on the execution log that you provide if it is relevant or not.

Please note, the complain was for Image version that has “string” as part of it. In your execution log, the image version that got evaluated does not have it.

Since the logic is correct and would work, I have approved and merged. But going forward, do take care of this kind of detail. It got passed in  in HULK VA Patch2 build #3.714.75232



[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-monitor_bgp_between_border_and_control_plane.py-121-monitorBGPBetweenBorderAndControlPlane&begin=25978&size=17673&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov20_18:15:44.769519.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-monitor_bgp_between_border_and_control_plane.py-121-monitorBGPBetweenBorderAndControlPlane&begin=25978&size=17673&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov20_18:15:44.769519.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Sanity', 'exsi', 'hulk', 'hulk-vm-sanity']",NhanHuu Nguyen,Resolved,Raghavendrachar Baraguru Mallesha Char
SEEN-2998,https://miggbo.atlassian.net/browse/SEEN-2998,TC28-DNAC_Device_Provisioning,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue: Y*
#* *Compared with the last passed log: Y*
#* *Checked and compared the configuration: Y*
#* *Confirmed with DE on the root cause: Y*
# *Issue faced: ""transitNetworks""[{""idRef"":""""}], In TC33.3 idRef value is null causing the run as blocker which might be because of the failure in TC28.*
# *Failure/Errors snip from log TC28:Error Code: 400 for*
{color:#ff5630}*210757:  URL:*{color}[{color:#ff5630}*https://10.106.133.152/api/v1/diagnostics/analyzer*{color}|https://10.106.133.152/api/v1/diagnostics/analyzer]{color:#ff5630} *Data:{'timeout': 60, 'data': '{""analyzer"": ""SDA"", ""name"": ""SystemAnalyzer_SDA_2023_11_11_5_45_16"", ""description"": ""SystemAnalyzer test for SDA at time 2023_11_11_5_45_16"", ""notes"": """"}'} Headers:{'Content-Type': 'application/json', 'Cookie': 'X-JWT-ACCESS-TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2NTRkZjZlMjlkZTIyOTA4MWFmNGU3NTEiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjY1NGRmNmUwOWRlMjI5MDgxYWY0ZTc1MCJdLCJ0ZW5hbnRJZCI6IjY1NGRmNmRlOWRlMjI5MDgxYWY0ZTc0ZSIsImV4cCI6MTY5OTY2NTMxNiwiaWF0IjoxNjk5NjYxNzE2LCJqdGkiOiJjMWY3NDYzOS1jM2E4LTRjMjAtOTkwYy1iNjY0ZTdmNDFjOGQiLCJ1c2VybmFtZSI6ImFkbWluIn0.a4whDiuDzvtRxf92Ltw1T-VDSpBsUu8846C_wlKAgmLKcaxzLAH0oaKyeppmhuMlhgz5rHu5_E3kt5UX5-5yaQ0B7y0a2X79hoGJQ2DtaB_bWcrIky4VF7Qadmgd75toMlGRt2PQf3FTrbjlMw_yfUP3yBq5IfQYYuW5kWu4bUO7wqlsB6wnAlN_v9gmwXCtm2GjBiSrjpic1TtEjVQmK98PlHtnQ4reilG734OLdLBaTUbA2u5q52bOwlVqCgW8Hs8KyQTUu429C7vIJOvNaB0YwUotMakTU2ihZ3WpkHzn04n97enCDT_zaM9oeoQWoQL1S3ClDplLaqQFEDwGOg;Version=1;Comment=;Domain=;Path=/;Max-Age=3600;Secure;HttpOnly; Secure; HttpOnly; SameSite=Strict'} Message:{""version"":""1.0"",""response"":{""error"":{""message"":""Analyzer SDA is not supported""}}}*{color}
{color:#ff5630}*Traceback (most recent call last):*{color}
{color:#ff5630}  *File ""/ws/yiyamper-bgl/dnac_soln_reg/HulkPatch2/dnac-auto/services/dnaserv/client_manager.py"", line 326, in call_api*{color}
{color:#ff5630}  *response.raise_for_status()*{color}
{color:#ff5630}  *File ""/ws/yiyamper-bgl/dnac_soln_reg/pyats/lib/python3.8/site-packages/requests/models.py"", line 953, in raise_for_status*{color}
{color:#ff5630}  *raise HTTPError(http_error_msg, response=self)*{color}
{color:#ff5630}  *requests.exceptions.HTTPError*{color}
# *Regression team debug analysis with details:* Raised a product bug after collecting RCA and bug has been junked by DEs after detailed analysis of missing IdRef values .BugId-CSCwi19906.
# *DE analysis/confirmation with details:* Transit network has a idRef value that is empty. Please check your scripts.
""transitNetworks"":[{""idRef"":""1e79bc0d-c87f-44ab-97ca-6e22ea8e4d44""},{""idRef"":""""}]
Similar as CSCwh89222. Not sure if its the same script.
# *What do Automation team need to look into? :* Please check why it is not passing idRef Values.
# *Failed log:*[*TRADe v2 | Results: sanitycombine (cisco.com)*|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-11%2Fsanitycombine.2023Nov10_18:27:48.716335.zip&atstype=ATS] *- TC28,TC29,TC30,TC33 Failed/Impacted TCs*
# *Last passed log:*[*https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-11%2Fsanitycombine.2023Nov03_23:05:14.097982.zip&atstype=ATS*|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fyiyamper-bgl%2Fdnac_soln_reg%2Fpyats%2Fusers%2Fyiyamper%2Farchive%2F23-11%2Fsanitycombine.2023Nov03_23:05:14.097982.zip&atstype=ATS]
# *Failed DNAC version/ISO:2.1.714.8010614*
# *Last Passed DNAC version/ISO:2.1.714.70274*
# *Script/Mapping file Used:*[*https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=refs%2Fheads%2Fbgl%2FHulkPatch2-ms%2Fapi-auto*|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb.py?at=refs%2Fheads%2Fbgl%2FHulkPatch2-ms%2Fapi-auto]
# *TCs Impacted:TC28,29,30,33*
# *'Re-run' TCs:N/A*
# *Branch Used: bgl/HulkPatch2-ms/api-auto*
# *DNAC Type: On-Prem*
# *Testbed details:*[*https://wiki.cisco.com/display/WNBUCODC/SOL-REG-TB*|https://wiki.cisco.com/display/WNBUCODC/SOL-REG-TB]",2023-11-16T04:25:57.522+0000,"This is to address diagnostic health uplifting,  and nothing to do with the defect filed of config preview. 

PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7942/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7942/overview]

merged to P2  only.","['Auton', 'HulkPatch2', 'Uplift']",Moe Saeed,Resolved,SAINATH CHATHARASI
SEEN-3013,https://miggbo.atlassian.net/browse/SEEN-3013,[Auton][MSTB2] - Test_TC91_Remove_Border_add_back  /   test2_re_add_border_to_the_fabric,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* Y
# *Issue faced:* Script is hitting issue at used_vlan = self.services.get_highest_used_vlan_of_border(device) proc. Getting attribute error.
# *Failure/Errors snip from log:* 
Exception:
408015:  Traceback (most recent call last):
408016:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/commonlibs/test_wrapper.py"", line 301, in wrapper
408017:      result = testfunc(func_self, **kwargs)
408018:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py"", line 6835, in test2_re_add_border_to_the_fabric
408019:      if(dnac_handle.assign_roles_deploy_fabric(vn_name=vn_name,transitvn=""iptransit"", borderType=""INTERNAL"",device_list=[x],sdatransit=""TRANSITSDA"", l3handoff_start_vlan=None)):
408020:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/dnaserv/lib/api_groups/fabric_wired/group.py"", line 744, in assign_roles_deploy_fabric
408021:      result &= self.assign_roles_deploy_fabric(s['cdname'], transitvn=transitvn,
408022:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/dnaserv/lib/api_groups/fabric_wired/group.py"", line 346, in assign_roles_deploy_fabric
408023:      l3handoff_start_vlan = self.services.readd_border_setup_vlan()
408024:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/dnaserv/lib/api_groups/fabric_wired/group.py"", line 3251, in readd_border_setup_vlan
408025:      used_vlan = self.services.get_highest_used_vlan_of_border(device)
408026:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/dnaserv/lib/decorators.py"", line 32, in wrapper
408027:      result = method(*args, **kwargs)
408028:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MS-TB2/MSTB2_Solution_Regression_Git/services/dnaserv/lib/api_groups/fabric_checks/group.py"", line 163, in get_highest_used_vlan_of_border
408029:      if response['response'][0].get(""deviceSettings"").get(""extConnectivitySettings""):
408030:  AttributeError: 'NoneType' object has no attribute 'get'
# *Regression team debug analysis with details:* Seeing attribute error if response['response'][0].get(""deviceSettings"").get(""extConnectivitySettings""): Object has not attribute ‘get’.
# *DE analysis/confirmation with details:* Checked with Quang. He confirmed that changes were made in the script. He will be fixing it.
# *What do Automation team need to look into?* Need to fix the script for attribute error as confirmed by Quang.
# *Failed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=83918312&size=320104&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov15_08:53:33.482362.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=83918312&size=320104&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov15_08:53:33.482362.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27813017&size=1689009&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul06_09:43:23.863824.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=27813017&size=1689009&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-07%2Fauto_MS_job.2023Jul06_09:43:23.863824.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Failed DNAC version/ISO:* 3.714.75215
# *Last Passed DNAC version/ISO:* 3.710.75438
# *Script/Mapping file Used:* solution_test_3sites_sjc_nyc_sf.py
# *TCs Impacted:* 1
# *'Re-run' TCs: NA*
# *Branch Used:* private/HulkPatch2-ms/api-auto
# *DNAC Type:* ESXI
# *Testbed details:* MSTB2
# *Team:* Solution Regression",2023-11-16T09:48:43.658+0000,PR-link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7950/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7950/overview],"['Auton', 'ESXi', 'EXECUTION', 'HulkPatch2', 'MSTB2']",QuangVinh Nguyen,Resolved,Manjushree Saligrama
SEEN-3014,https://miggbo.atlassian.net/browse/SEEN-3014,[Auton][Hulk]:Task-1  /   Test_TC231_Trustsec_Policy_enforcement_on_Border  /   test3_verify_sgt_map_for_clients ,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue: Y*
#* *Compared with the last passed log: Y*
#* *Checked and compared the configuration: Y*
#* *Confirmed with DE on the root cause: N*
# *Issue faced:* 

               Feature - [Test_TC231_Trustsec_Policy_enforcement_on_Border|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1484016&size=2188802&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov16_02:32:45.879469.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]  /   test3_verify_sgt_map_for_clients

*3. Failure/Errors snip from log:*

  16036: 
 Traceback (most recent call last):

{noformat}16037: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job@2/services/commonlibs/test_wrapper.py"", line 301, in wrapper{noformat}

{noformat}16038: 
     result = testfunc(func_self, **kwargs){noformat}

{noformat}16039: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job@2/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py"", line 18424, in test3_verify_sgt_map_for_clients{noformat}

{noformat}16040: 
     sg_tag = dnac_handle.dnaconfig.testbed.devices[client['name']].custom['profiles'][""dcs""][""sgt""]{noformat}

{noformat}16041: 
 KeyError: 'profiles'{noformat}

*4.Regression team debug analysis with details: Script is Failing to load custom details from json and throwing key error*

*5. DE analysis/confirmation with details: NA*

*6.What do Automation team need to look into?* 

       Fix the Script error.

*7.Failed log:* [TRADe v2 | Logs: env_auto_job (cisco.com)|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3620985&size=7540&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov16_02:32:45.879469.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*8.Last passed log: NA*

*9.Failed DNAC version/ISO:  Version 2.3.7.3-70327*

*10.Last Passed DNAC version/ISO: NA* 

*11.Script/Mapping file Used: LAN Script NON optimized*

*12.TCs Impacted: 1*

*13.'Re-run' TCs: 1*

*14.Branch Used: private/HulkPatch-ms/sanity_api_auto*

*15.DNAC Type: HULK P1*

*16.Testbed details:* [*Solution Sanity Testbed-2 - EDPEIXOT Solution Engineering Team Wiki - IT Wiki (cisco.com)*|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2]",2023-11-16T11:27:42.419+0000,"Hi [~accountid:712020:9509bf96-9367-412d-848b-a2874ce169bd] 

Subtest3 was failed and raised an exception as {{KeyError: ‘profiles'}} due to the testbed yaml file is missing custom profile dcs info for the wired-client {{TB2-wired-client1}}, hence I changed this ticket to Cancelled. Please update dcs profiles in [SanityTB2.yaml|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb2/SanityTB2.yaml], prefer [SanityTB1.yaml|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sanity_tb1/SanityTB1.yaml]

{noformat}    TB2-wired-client1:
        type: linux
        alias: tb2-wired-client1
        role: wired-client
        custom:
            ip: 204.1.99.250
            subnet: 255.255.252.0
            identifier: 0100.5056.83B7.CE
            mac: 00.50.56.83.B7.CE
            rpcip: 10.195.227.35
            interface: LAB-NET-WiredClient
            ftp: 82.1.1.11
            download_file: download.mp4
            isOverlapping: True
            video_stream: http://82.1.1.11/video.mp4
        tacacs:
            username: root
        passwords:
            linux: Lablab123
        connections:
            linux:
                protocol: ""ssh""
                ip: 10.195.227.35{noformat}","['AWS-Santiy', 'Auton', 'FEATURE', 'Hulk', 'Sanity']",ThangQuoc Tran,Cancelled,Yamini P
SEEN-3016,https://miggbo.atlassian.net/browse/SEEN-3016,[Auton] [HulkP2] - Creation of AI RF proflie TC Fails with NCND00049 error code with mandatory parameters restrictions,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced:* Creation of AI RF profile is failing with NCND00049 error code with mandatory parameters restriction errors.
# *Failure/Errors snip from log:* 
Secure; HttpOnly; SameSite=Strict'} Message:{""response"":{""errorCode"":""NCND00049"",""message"":""NCND00049: Invalid request. Please check the request parameters and resubmit"",""detail"":""Maximum of only 2 Mandatory Data Rates can be selected in RF Profile AI_RF_PROFILE""},""version"":""1.0""}
89503:  Traceback (most recent call last):
89504:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/services/dnaserv/client_manager.py"", line 326, in call_api
89505:      response.raise_for_status()
89506:    File ""/ws/divayada-sjc/Solution_pyatsenv/lib/python3.6/site-packages/requests/models.py"", line 960, in raise_for_status
89507:      raise HTTPError(http_error_msg, response=self)
89508:  requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: [https://10.195.243.109/api/v1/commonsetting/wlan/-1|https://10.195.243.109/api/v1/commonsetting/wlan/-1]
89509:  Encountered unhandled HTTPError in Internal API Call
89510:  Flagging result as FAIL!
89511:  	Reason: 400 Client Error: Bad Request for url: [https://10.195.243.109/api/v1/commonsetting/wlan/-1|https://10.195.243.109/api/v1/commonsetting/wlan/-1]

# *Regression team debug analysis with details:* Looks like script side needs modifications in the payload. Could be some parameters are made mandatory specific to Hulk Patch2. 
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
Need to fix the Automation code to accommodate for the failure
# *Failed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19524890&size=112196&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov02_13:10:01.053293.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19524890&size=112196&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov02_13:10:01.053293.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=18502938&size=38153&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct22_07:04:33.562334.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=18502938&size=38153&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fsr_mb_multi_sites_mdnac_dr.2023Oct22_07:04:33.562334.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Failed DNAC version/ISO:* Hulk Patch2 - 2.1.714.70257
# *Last Passed DNAC version/ISO:* Hulk Patch1 RC3 - 2.1.713.70319 
# *Script/Mapping file Used:* testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
# *TCs Impacted:* 4
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/api-auto
# *DNAC Type:* On-Prem
# *Testbed details:* MSTB1
# *Team:* Solution Regression",2023-11-16T13:00:00.207+0000,"Hi [~accountid:63f50bfce8216251ae4d59d5] 
Same issue seen on Ghost P3 on AWS Solution Sanity LAN run:
[https://ngdevx.cisco.com/services/taas/results/674be5b6-f045-4495-8a48-6073fec86397/run-results|https://ngdevx.cisco.com/services/taas/results/674be5b6-f045-4495-8a48-6073fec86397/run-results]

{noformat}File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/DMZ-Ghost-sanity-common-Multi-job/services/dnaserv/lib/decorators.py"", line 32, in wrapper
    result = method(*args, **kwargs)
  File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/DMZ-Ghost-sanity-common-Multi-job/services/dnaserv/lib/api_groups/wireless_rf_profile/group.py"", line 115, in create_ai_rf_profile
    response = self.services.api_switch_call(method=""POST"", resource_path=url,data=inputjson)
  File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/DMZ-Ghost-sanity-common-Multi-job/services/dnaserv/lib/api_groups/utils/group.py"", line 50, in api_switch_call
    response = self.services.base.NB_API.call_api(method,resource_path,**kwargs)
  File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/DMZ-Ghost-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 498, in call_api
    response = super(ApicemClientManager, self).call_api(method=method,
  File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/DMZ-Ghost-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 300, in call_api
    raise e
  File ""/data/jenkins/ws_sjc2_ds/workspace/DMZ-SANITY/Ghost/DMZ-Ghost-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 295, in call_api
    response.raise_for_status()
  File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/requests/models.py"", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://172.35.16.153/api/v1/commonsetting/wlan/-1{noformat} Hi Team,

for NFW also we are observed the same issue , please find the failed log



failed log: [https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov15_11:29:54.133967.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov15_11:29:54.133967.zip&atstype=ATS] Hi [~accountid:63f50bfce8216251ae4d59d5]  ,

Same issue seen on Hulk Patch-2 2.1.714.70282 Sanity LAN run: 
Below  TC’s  are  impacted=> 
*TC219 =>*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1410975&size=2527230&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov17_01:15:18.668470.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1410975&size=2527230&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov17_01:15:18.668470.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*TC 54.3 ,53.4*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=3735451&size=211591&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov13_23:56:21.669557.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=3735451&size=211591&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov13_23:56:21.669557.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7941/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7941/overview] PR approved and merged to *private/HulkPatch-ms/api-auto* branch and cherry-picked to *private/HulkPatch2-ms/api-auto* branch.

Moving the Jira to “Resolved” State. Hi

We are still seeing same failure for few of our Usecases on HulkP2-ESXI#3.714.75232 build. Below are the testcases affected due to this
Test_TC177_edit_site_name/[test3_configure_wireless_custom_RF_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1851483&size=181039&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov22_23:36:13.137940.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
Test_TC204_APs_negative_operations/[test4_configure_wireless_custom_RF_profile|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10550130&size=220385&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov22_23:36:13.137940.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Script : solution_test_3sites_sjc_nyc_sfo.py
Branch : private/HulkPatch2-ms/api-auto
Ova#3.714.75232 [~accountid:63f50bfce8216251ae4d59d5] : Same issue is still observed during Solution testing on On-Prem Multisite profile as well.
Uber ISO tested - 2.1.714.70296
*Failed log:* [Test_TC263_APs_negative_operations|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2430911&size=1472645&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov27_00:21:04.493837.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]  (refer subTC4)
*Branch Used:* private/HulkPatch2-ms/api-auto
*Script/Mapping file Used:* testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py PR for the above: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8099/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8099/overview]","['Auton', 'Blocked', 'ESXi', 'Ghost', 'HulkPatch2', 'Integration', 'MSTB1', 'MSTB2', 'NFW', 'Optimized', 'Sanity', 'Uplift']",Moe Saeed,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-3018,https://miggbo.atlassian.net/browse/SEEN-3018,"Optimized Suite execution for EXT API testcases is failing while calling ""ext_api_call_handle()""","Optimized Suite execution for EXT API testcases is failing while calling ""ext_api_call_handle()"".

Failed execution log: [Task-platform.py-33-platform|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-platform.py-33-platform&begin=0&size=-1&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov15_09:34:24.343751.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS] / [Test_TC1_get_cisco_dnac_release_summary|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-platform.py-33-platform&begin=2604&size=39450&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov15_09:34:24.343751.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&atstype=PYATS]  -

# [TC1_get_cisco_dnac_release_summary|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-platform.py-33-platform&begin=3222&size=29050&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov15_09:34:24.343751.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# [TC2_get_cisco_dnac_release_summary_limit_check|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-platform.py-33-platform&begin=32272&size=9592&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov15_09:34:24.343751.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

{noformat}190:  Initializing function group ""ext_platform""
191:  Group ""ext_platform"" initialized successfully
192: 
193: 
194:   External API handler called:
195: 
196:  Resource path: /dna/intent/api/v1/dnac-release
197:  Method: GET

198:  {'full_path': True}
199:  Encountered unhandled error in External API Call:
200:  Flagging result as FAIL!
201:  	Reason: <class 'AttributeError'>
202:  Kwargs:
203:  {'full_path': True}
204:  'NoneType' object has no attribute 'call_api'
205:  Library group ""ext_platform"" method ""ext_cisco_dnac_release_summary"" returned in 0:00:00.001680
206:  Test returned in 0:00:01.454252
207:  Failed reason: Get Cisco DNA center release sumary:Failed{noformat}",2023-11-16T22:35:30.008+0000,"Raised PR for Ghost branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7920/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7920/overview] PR got merged to *private/Ghost-ms/api-auto* branch and cherry-picked to *private/HulkPatch2-ms/api-auto* branch as well.

Marking this Jira as “Resolved”.","['Auton', 'Ghost']",Amardeep Kumar,Closed,Amardeep Kumar
SEEN-3020,https://miggbo.atlassian.net/browse/SEEN-3020,[Auton] ETA/NaaS Polaris Profile Scripts Enhancements  ,,2023-11-17T06:17:02.374+0000,,['Auton'],Raghavendrachar Baraguru Mallesha Char,Resolved,Raghavendrachar Baraguru Mallesha Char
SEEN-3021,https://miggbo.atlassian.net/browse/SEEN-3021,Auton:Hulk P3:LLDP Discovery od Device is taking more time to complete via REGULAR SCRIPT,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue: Y*
#* *Compared with the last passed log: Y*
#* *Checked and compared the configuration: Y*
#* *Confirmed with DE on the root cause: NA*
# *Issue faced:*

LLDP Device discovery is taking more than 1 hour to complete and on dnac it is stuck as “in-progress” ,  even though lldp is successfully done inside

# *Failure/Errors snip from log:*

2023-11-17T02:14:07: %API-GROUP-TASK-ERROR: Traceback (most recent call last): 2023-11-17T02:14:07: %API-GROUP-TASK-ERROR: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/task/group.py"", line 26, in wait_for_task_complete 2023-11-17T02:14:07: %API-GROUP-TASK-ERROR: return self.__wait_for_task_complete(task_id=response['response']['taskId'], timeout=timeout ,tree=tree) 2023-11-17T02:14:07: %API-GROUP-TASK-ERROR: File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/task/group.py"", line 277, in __wait_for_task_complete 2023-11-17T02:14:07: %API-GROUP-TASK-ERROR: assert False, (""Task {0} didn't complete within {1} seconds"" 2023-11-17T02:14:07: %API-GROUP-TASK-ERROR: AssertionError: Task {'progress': '79', 'data': '{""discoveryId"":79}', 'version': 1700214246558, 'startTime': 1700214246427, 'lastUpdate': 1700214246558, 'serviceType': 'Discovery Service', 'rootId': '018bdcaa-8c1b-7595-804e-72e570609e86', 'isError': False, 'instanceTenantId': '65569dbffecbed6160c75099', 'id': '018bdcaa-8c1b-7595-804e-72e570609e86'} didn't complete within 1800 seconds



# *Regression team debug analysis with details:*

As per earlier analysis  from Raji got to know that fix is being applied and issue was extended node will be having ip address on corresponding VLAN’s but this time we checked and confirmed there is no ip address on vlan on extended nodes and gave no lldp on extended nodes still we are seeing one ip is coming on lldp due to which execution time + discovery is taking more time.



*NOTE*:For each LLDP Discovery it is taking 40-45 min to complete ,we are having 5 LLDP Discover

# *DE analysis/confirmation with details:NA*
# *What do Automation team need to look into?*

Need to identify root cause for the issue and a permenant solution for the issue

# *Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery_brownfield.py-51-toolsDiscoveryBrownfield&begin=127564&size=15379587&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov16_18:09:53.120799.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery_brownfield.py-51-toolsDiscoveryBrownfield&begin=127564&size=15379587&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov16_18:09:53.120799.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

# *Last passed log Hulk P2:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery_brownfield.py-51-toolsDiscoveryBrownfield&begin=126596&size=836764&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov02_21:56:45.702767.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery_brownfield.py-51-toolsDiscoveryBrownfield&begin=126596&size=836764&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov02_21:56:45.702767.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



# *Failed DNAC version/ISO:*

Hulk P3+ISO:Version 2.3.7.5-70102

# *Last Passed DNAC version/ISO:*

Hulk-P2#2.3.7.4.70257

# *Script/Mapping file Used:*

[toolsDiscoveryBrownfield|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-discovery_brownfield.py-51-toolsDiscoveryBrownfield&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov02_21:56:45.702767.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]  [lan-optimised script:solution_test_sanityecamb_lan.py]


# *TCs Impacted:*

1 TC but increasing execution time

# *'Re-run' TCs:NA*
# *Branch Used:*

private/HulkPatch2-ms/sanity_api_auto

# *DNAC Type:*

Hulk P3+ISO:Version 2.3.7.5-70102

# *Testbed details:NA*
# *Team:Solution Sanity*",2023-11-17T09:32:38.049+0000,,"['Auton', 'Hulk', 'Hulkpatches', 'Sanity']",Raji Mukkamala,Backlog,Anusha John
SEEN-3023,https://miggbo.atlassian.net/browse/SEEN-3023,[Auton] - Threats count mismatch between New Threat report generation and Threats page,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* Y
# *Issue faced:* We have observed an issue related TC - *test3_generate_and_verify_new_threat_report*,where the TC failed due to mismatch between Threats count from Threat report generation and Threats page. After debugging issue further, accordingly we reported defect - [CSCwi12664|https://cdetsng.cisco.com/webui/#view=CSCwi12664]
On further discussion with Siva ([ngontla@cisco.com|mailto:ngontla@cisco.com]) from DE team, it was confirmed that the Threat report type being used to generate report to compare with Rogue and aWIPS->Threats page is incorrect one. Instead of using ""New Threat Report"" option, ""Threat Report detail"" has to be used. This will capture the details of threats from Rogue and aWIPS->Threats page.
# *Failure/Errors snip from log:* 
43139:   api_switch_call called:
43140:  {'params': {'startTime': 1699203241840, 'endTime': 1699289641866, 'limit': '100', 'offset': '1', 'sortField': 'threatLevel', 'sortOrder': '8'}}
43141:  Resource path full url: [https://10.195.243.123/api/assurance/v1/rogue-service/getreport/detail|https://10.195.243.123/api/assurance/v1/rogue-service/getreport/detail]
43142:  Library group ""report_gen"" method ""new_threat_report_reference"" returned in 0:00:00.494008
43143:  Generated reports has no data to validate : check the report for more details New Threat Report_Generated_at 2023-11-06_16_52
43144:  Library group ""report_gen"" method ""generate_and_validate_new_threat_report"" returned in 0:01:22.678584
# *Regression team debug analysis with details:* Refer Point 2) above
# *DE analysis/confirmation with details:* Refer Point 2) above
# *What do Automation team need to look into?* 
Instead of using ""New Threat Report"" option, ""Threat Report detail"" has to be used for api call. This will capture the details of threats from Rogue and aWIPS->Threats page.
# *Failed log:* [Test_TC283_flexible_reports_check|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=10434403&size=236892&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov06_07:03:07.389862.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* NA. Feature Integration. 
# *Failed DNAC version/ISO:* Hulk Patch2 - 2.1.714.70257
# *Last Passed DNAC version/ISO:* NA. Feature Integration.
# *Script/Mapping file Used:* testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
# *TCs Impacted:* 1
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/api-auto
# *DNAC Type:* On-Prem
# *Testbed details:* MSTB1
# *Team:* Solution Regression",2023-11-17T16:07:03.579+0000,"Hi [~accountid:62d2fec15d6f5fd2c3db8f9f] 
as we discussed I have updated the threat reports testcase

It is merged and synced to the latest branches as well 

PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8054/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8054/overview]","['Auton', 'Execution', 'HulkPatch2', 'Integration', 'MSTB1', 'Multisite']",Archana KM,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-3024,https://miggbo.atlassian.net/browse/SEEN-3024,[AUTON][Halleck] test_verify_device_configuration: Result : Failed to push policy on device,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* Y
# *Issue faced:* {{Result : Failed to push policy on device}} due to validation failed
# *Failure/Errors snip from log:* 
{{Result : Failed to push policy on device}}
# *Regression team debug analysis with details:* {{Result : Failed to push policy on device}} due to validation failed
# *DE analysis/confirmation with details: Y*
# *What do Automation team need to look into?* Need to uplift the scriipt for validating the output
# *Failed log:* [https://ngdevx.cisco.com/services/taas/results/015f3381-a619-4918-81cf-b0e06a0ae748/run-results|https://ngdevx.cisco.com/services/taas/results/015f3381-a619-4918-81cf-b0e06a0ae748/run-results]
# *Last passed log:* N/A
# *Failed DNAC version/ISO:* 3.714.75207 with 17.13.1PRD6 FC1
# *Last Passed DNAC version/ISO:* N/A
# *Script/Mapping file Used:* solution_test_3sites_sjc_nyc_sf.py
# *TCs Impacted:* 2
# *'Re-run' TCs: NA*
# *Branch Used:* private/Halleck-ms/api-auto
# *DNAC Type:* ESXI
# *Testbed details:* MSTB2
# *Team:* Solution Automation",2023-11-20T04:00:33.228+0000,PR-link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7958/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/7958/overview],"['Auton', 'Halleck']",QuangVinh Nguyen,Resolved,QuangVinh Nguyen
SEEN-3036,https://miggbo.atlassian.net/browse/SEEN-3036,[Auton][IBSTE]-Not able to execute Maintenance mode script due to not being part of scripts map,"Hi [~accountid:63f50be71223974bc04b0534] ,

We have tried triggering AP maintenance mode use case in IBSTE optimized script ,this use cases has not getting started executing with reason:”Processing usecase:parallelrun for scriptlist[k1]:['assuranceMaintenanceMode'] Processing usecase:assuranceMaintenanceMode Skipping usecase:assuranceMaintenanceMode due to not being part of scripts map”.

But I see that use case mapping is being done here:[+dnac-auto+|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse?at=private%2FHulk-ms%2Fapi-auto]/[usecasemaps|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps?at=private%2FHulk-ms%2Fapi-auto]/[ibstesuite|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/ibstesuite?at=private%2FHulk-ms%2Fapi-auto]/*ibste_usecase_maps.yaml*  

Also test case is also present here:[+dnac-auto+|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse?at=private%2FHulk-ms%2Fapi-auto]/[testcases|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases?at=private%2FHulk-ms%2Fapi-auto]/[ibsteusecases|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/ibsteusecases?at=private%2FHulk-ms%2Fapi-auto]/[assuranceMaintanenceMode|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/ibsteusecases/assuranceMaintanenceMode?at=private%2FHulk-ms%2Fapi-auto]/*assurance_maintaince_mode_on_off.py*

But not sure why the script is not being executed,Could you please check once?

Script location:

Usecasemaps for optimized :
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/ibstesuite?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/ibstesuite?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto]
Configs:

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sr_ibste?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/configs/sr_ibste?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto]
Script location:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/job/sr_ibste/sr_ibste.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/job/sr_ibste/sr_ibste.py?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto]",2023-11-21T15:48:49.915+0000,"Hi [~accountid:63f50bddc1685a24e1314c87], this testcase takes a long time to run over 30 mins and I am not sure if it should be part of optimized folder. [~accountid:5f3c6ae932360700388f7b4b] , please check on this Auton. Yes, still needed Luna. As we need coverage for it. Sure Pawan, will add it then.","['Auton', 'IBSTE', 'hulk']",Majlona 'Luna' Aliaj,Backlog,Neelima Doddipalli
SEEN-3041,https://miggbo.atlassian.net/browse/SEEN-3041,[Auton] - Path trace from AP 360 page using incorrect device IP as destination IP,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* NA
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced:* When integrating Hulk Patch2 specific feature of AP tool check we have seen path trace workflow has failed. On further debugging the issue, found that destination IP used for path trace being picked seems to be incorrect. Below is the analysis: 

→ In the current script failure in subTC3, path trace is being performed from AP - AP3C57.31C5.8458 (204.1.213.155) which SF site 9800 EWLC controller({{204.192.6.200}}) to destination IP corresponding to Aireos WLC ({{204.192.2.200}}) on SJ site. This is incorrect. Destination IP has to be corresponding to device in the same site on which AP has joined to. This has to be addressed.

→ Further when we tried same path trace to destination IP 9800 EWLC controller({{204.192.6.200}}), we are observing observing error - “Gateway not found in source vlan: 1028. Please make sure CDP is enabled”.
But if the same path trace to destination IP 204.1.2.8 (connected Edge node to this AP) is performed. Its successful. 

→ When we tried this path trace from AP 360 page for APs joined on NY sites (Wireless controller as ECA devices), it works fine only for connected Edge node.

→ Also when we tried this path trace from AP 360 page for APs joined on SJ sites (Wireless controller as Aireos device), here too it works fine only for connected Edge node. 

→ Few queries:
Were all these scenarios covered during Automated testing and was it successful? 
How is AP picked for this usecase testing? Is it randomly or based on some logic?
Probably we need to have a relook into this usecase.

# *Failure/Errors snip from log:* 
84036:  Resource path full url: [https://10.195.243.123/api/v1/flow-analysis/48b6b8e4-a9e2-457a-b738-e4ca0c60529e|https://10.195.243.123/api/v1/flow-analysis/48b6b8e4-a9e2-457a-b738-e4ca0c60529e]
84037:  path trace failed between AP3C57.31C5.8458 and FW-5520-1 with the error message Unable to retrieve links for  AP - AP3C57.31C5.8458 [ 0985a41a-a650-486e-a6bb-62a19806b52f]. Make sure its connected device is discovered in Managed state.
84038:  Library group ""assurance"" method ""ap_path_trace_test_from_ap360"" returned in 0:00:10.206578

# *Regression team debug analysis with details:* Refer Point 2) above
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
Need to fix the path trace test logic
# *Failed log:* [test3_ap_tool_path_trace|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19325744&size=9162&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov21_10:37:28.577345.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* NA. Feature Integration. 
# *Failed DNAC version/ISO:* Hulk Patch2 - 2.1.714.70296
# *Last Passed DNAC version/ISO:* NA. Feature Integration.
# *Script/Mapping file Used:* testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
# *TCs Impacted:* 1
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/api-auto
# *DNAC Type:* On-Prem
# *Testbed details:* MSTB1
# *Team:* Solution Regression",2023-11-22T08:13:43.518+0000,"Hi [~accountid:62d2fec15d6f5fd2c3db8f9f]  as you mentioned in the description from AP - AP3C57.31C5.8458 to 204.1.2.8 (connected Edge node to this AP) is also failing when i checked in your setup

!Screenshot 2023-11-22 at 2.22.07 PM.png|width=1848,height=497!

Can you please check this once and regarding the selection of Destination we will enhance the script to avoid picking the WLC device

path trace tool has some open issues which they are planning to fix in patch3

please refer CSCvx42125 for more details  Hi [~accountid:62d2fec15d6f5fd2c3db8f9f] 
PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8055/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8055/overview]
it is merged now please check it out

Thanks,

Archana","['Auton', 'HulkPatch2', 'Integration', 'MSTB1', 'Multisite']",Archana KM,Resolved,SANDEEP SHIVARAMAREDDY
SEEN-3042,https://miggbo.atlassian.net/browse/SEEN-3042,[Hulk Patch2] Uplift operationalInfo for API changes for ECA,"This is a behavioural change happening specifically for eCA devices.

With this new feature in place for Hulk Patch2, if we want to push any Wlan updates or any other Network settings to eCA, it will NOT happen during normal CD operations.

You need to call the API with the particular operation info.

Summary : In a CD operation, if wireless operation is required for ECA device then add below operation info under Device Info section:

{

""operations"":[""wireless_setting_provision""],

""change"":""UPDATE""

}



More Details here : [https://mydna.cisco.com/documents/view-design-document?featurename=F170258|https://mydna.cisco.com/documents/view-design-document?featurename=F170258]



Attached full CD payload with DI + wireless for reference.

If you need dev test ISO to test your changes, please use:

assembly_release_dnac_hulk-patch2_devtest_801-2.1.714.8010227

*PoC:* Vigneswar Nagarathinam, Vivek Sreevatsan",2023-11-22T21:26:49.737+0000,"Updated new operationalInfo for deviceInfo for ECA enabling/disabling.
parameters['operationalInfo'] = {""change"": ""UPDATE"",
                                                    ""operations"": [""wireless_setting_provision""]}

Commit: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/7fca8dfe88cb12524871ea35f93ba4f10fb71702|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/7fca8dfe88cb12524871ea35f93ba4f10fb71702]

Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=311689&size=523920&archive=%2Fws%2Ftranlam-sjc%2Fpyats6%2Fusers%2Ftranlam%2Farchive%2F23-11%2Fsanity-intg2.2023Nov22_13:20:10.080831.zip&ats=%2Fws%2Ftranlam-sjc%2Fpyats6&submitter=tranlam&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=311689&size=523920&archive=%2Fws%2Ftranlam-sjc%2Fpyats6%2Fusers%2Ftranlam%2Farchive%2F23-11%2Fsanity-intg2.2023Nov22_13:20:10.080831.zip&ats=%2Fws%2Ftranlam-sjc%2Fpyats6&submitter=tranlam&from=trade&view=all&atstype=pyATS]","['Auton', 'HulkPatch2', 'ReleaseUseCases', 'Uplift']",Tran Lam,Resolved,Tran Lam
SEEN-3047,https://miggbo.atlassian.net/browse/SEEN-3047,[Auton][MSTB2] - Test_TC191_config_preview_treeview_cliconfig_enabled,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* Y
# *Issue faced:* Script is hitting key error issue at if ""native"" in tree_view_json['configData']['root'] and ""CliConfig"" in tree_view_json['configData']:
100881:  KeyError: 'configData'
# *Failure/Errors snip from log:* verify_treeview_cliconfig_enabled
if ""native"" in tree_view_json['configData']['root'] and ""CliConfig"" in tree_view_json['configData']:
KeyError: 'configData'
# *Failure/Errors snip from log:* 
Exception:
100886:  Traceback (most recent call last):
100887:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-TEAM/ESXi Jobs/ESXi-MSTB2-SingleNIC/services/commonlibs/test_wrapper.py"", line 301, in wrapper
100888:      result = testfunc(func_self, **kwargs)
100889:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-TEAM/ESXi Jobs/ESXi-MSTB2-SingleNIC/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf.py"", line 18925, in test3_verify_treeview_cliconfig_enabled
100890:      if dnac_handle.verify_treeview_cliconfig_enabled(self.dev_details[0], self.dev_details[1]):
100891:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-TEAM/ESXi Jobs/ESXi-MSTB2-SingleNIC/services/dnaserv/lib/decorators.py"", line 32, in wrapper
100892:      result = method(*args, **kwargs)
100893:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-TEAM/ESXi Jobs/ESXi-MSTB2-SingleNIC/services/dnaserv/lib/api_groups/inventory/group.py"", line 2168, in verify_treeview_cliconfig_enabled
100894:      if ""native"" in tree_view_json['configData']['root'] and ""CliConfig"" in tree_view_json['configData']:
100895:  KeyError: 'configData'
# *Regression team debug analysis with details:* Seeing key error at if ""native"" in tree_view_json['configData']['root'] and ""CliConfig"" in tree_view_json['configData']:KeyError: 'configData'
# *DE analysis/confirmation with details:* Did not check with DE as this is a script issue.
# *What do Automation team need to look into?* Need to fix the script for key error issue.
# *Failed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=22861404&size=241881&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov21_21:16:54.552925.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=22861404&size=241881&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov21_21:16:54.552925.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1809830&size=55327&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fauto_MS_job.2023Sep27_08:10:31.396737.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1809830&size=55327&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-09%2Fauto_MS_job.2023Sep27_08:10:31.396737.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Failed DNAC version/ISO:* 3.714.75232
# *Last Passed DNAC version/ISO:* 3.713.75142
# *Script/Mapping file Used:* solution_test_3sites_sjc_nyc_sf.py
# *TCs Impacted:* 1
# *'Re-run' TCs: NA*
# *Branch Used:* private/HulkPatch2-ms/api-auto
# *DNAC Type:* ESXI
# *Testbed details:* MSTB2
# *Team:* Solution Regression",2023-11-23T06:43:38.967+0000,"Saravanan Radhakrishnan 11/28/2023 9:30 PM • In HP2 there was changes made to Show the cli as separate window 
Ram Govind Krishnan 12/1/2023 12:54 PM • It had to be moved out to facilitate this feature: FEAT-7159 !image-20231129-053559.png|width=1311,height=1149! if no cli in the xml format, then the cli preview pane wont show up, need to enhance the library It require a code upliftment as per the new changes, Work as expected in hulkP1 or below releases. Moving it to submitter to raise as enhancement, 
Please troubleshoot the failure and raise a auton about script fix or feature fix  [~accountid:5fe224a53b5e47013862f185] , pls. reuse this ticket itself.

I have added the “Enhancements” and “Uplift” label to put it into different category.","['Auton', 'ESXi', 'Enhancements', 'HulkPatch2', 'MSTB2']",Vinoth Kumar Kutty Krishnamoorthy,Backlog,Manjushree Saligrama
SEEN-3059,https://miggbo.atlassian.net/browse/SEEN-3059,[AUTON][GHOST]:  Task-1  /   Test_TC108_verify_vlan_mapping_interfaces  /   test1_verify_vlan_mapping_interfaces,"Debugged/Analyzed the issue: Y
Compared with the last passed log: Y
Checked and compared the configuration: Y
Confirmed with DE on the root cause: N
Issue faced:  script is failing to configure switchport access vlan 1

Failure/Errors snip from log:

{noformat}80719:  Mis-match of VLAN for Interface - TwoGigabitEthernet1/0/15 found on TB3-DM-eCA-BORDER and DNAC.
80720:  Expected VLAN: 1 Actual VLAN: 1026
80749:  [{'TwoGigabitEthernet1/0/15': {'expected_vlan': '1', 'Dnac_vlan': '1026', 'device': 'TB3-DM-eCA-BORDER'}}]
80751:  Failed reason: Verification Failed!!!{noformat}

Regression team debug analysis with details:  Switchport access vlan 1 , is not  happening via script , so the vlan ID is not changing on the interface , manually tried its working fine .
DE analysis/confirmation with details: NA
What do Automation team need to look into:  Need to add configuration *switchport access vlan 1* in respected interfaces via script  .

Failed log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=25537509&size=384989&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov21_17:44:57.349947.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=25537509&size=384989&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov21_17:44:57.349947.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Last passed log: NA

Failed DNAC version/ISO: Ghost P2 RC3 # 2.3.5.4-70852-HF4
Script/Mapping file Used: dnac-auto/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py ( LAN Script)
TCs Impacted: 1
Branch Used: private/Ghost-ms/sanity_api_auto
Testbed details: NA
Team: DNAC Solution Sanity",2023-11-23T09:42:11.763+0000,"It seems has already been fixed before (or got a problem during execution). I've tried to run many times on the testbed mentioned in the Jira description.
Anyway, I added the 'retry' code to rerun the library if the cluster has any problems. # PR *private/Ghost-ms/sanity_api_auto*: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8181/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8181/overview]
# Test Case:  {{Test_TC108_verify_vlan_mapping_interfaces}}
# Branch Used*: private/Ghost-ms/sanity_api_auto*
# Testbed Used: TB3
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link *private/Ghost-ms/sanity_api_auto*: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-12/sanity_TB3.2023Dec04_00:47:29.882657.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-12/sanity_TB3.2023Dec04_00:47:29.882657.zip&atstype=ATS]","['Auton', 'Ghost', 'LAN', 'sanity']",NhanHuu Nguyen,Resolved,Elton GoldChristopher
SEEN-3060,https://miggbo.atlassian.net/browse/SEEN-3060,Auton: Hulk P1:Test_TC29_DNAC_Device_Provisioning  /  test3_verify_provision_the_devices_fabric1 ,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue: Y*
#* *Compared with the last passed log: NA*
#* *Checked and compared the configuration: Y*
#* *Confirmed with DE on the root cause: NA*
# *Issue faced:*

TC 29 Fabric provision is failing every time as model configs is already existing , TC 28 inventory provision will be pushing model configs so it is expected to have model configs by TC 29.
Every time we used to clear model configs manually then rerun only TC 29 used to pass .

*3.Failure/Errors snip from log:*{""errorCode"":""NCNC10001"",""message"":""NCNC10001: Model Config Design custom_ssid already exists"",""href"":""/apic-em-network-programmer-service/named-capability/design""},""version"":""1.0""}

{noformat}35338: 
 Traceback (most recent call last):{noformat}

{noformat}35339: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-sanity-common-Multi-job/services/dnaserv/client_manager.py"", line 326, in call_api{noformat}

{noformat}35340: 
     response.raise_for_status(){noformat}

{noformat}35341: 
   File ""/ws/owagh-sjc/Solution_pyatsenv/lib/python3.10/site-packages/requests/models.py"", line 1022, in raise_for_status{noformat}

{noformat}35342: 
     raise HTTPError(http_error_msg, response=self){noformat}

{noformat}35343: 
 requests.exceptions.HTTPError: 500 Server Error:  for url: https://10.195.227.31/api/v2/named-capability/design?validateSchema=False{noformat}

{noformat}35344: 
 Encountered unhandled HTTPError in Internal API Call{noformat}

{noformat}35345: 
 Flagging result as FAIL!{noformat}



*4. Regression team debug analysis with details: YES*

*5.DE analysis/confirmation with details: NA*

*6.What do Automation team need to look into?*

Without removing we are expecting to get a pass log, as TC 30 to rest of the testcase is impacted due to this.

*7. Failed log:*[*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8323305&size=70978&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov22_23:22:03.002805.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=8323305&size=70978&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov22_23:22:03.002805.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*8. Rerun passed log:*[*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=415701&size=663792&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct04_04:39:03.520704.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=415701&size=663792&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct04_04:39:03.520704.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

*9. Failed DNAC version/ISO: Version 2.3.7.3-70332*

*10. Last Passed DNAC version/ISO: NA(Always passed in re-run only when the model configs are deleted manually)*

*11. Script/Mapping file Used: solution_test_sanityecamb_cert_lan.py*

*12. TCs Impacted :Blocked*

*13. 'Re-run' TCs :NA*

*14. Branch Used: private/HulkPatch-ms/sanity_delay_testing*

*15. DNAC Type: Hulk P1*

*16. Testbed details:* [*https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2*|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2]

*17. Team: Solution Sanity*",2023-11-23T12:17:12.949+0000,"Hi [~accountid:62ab7a399cd13c0068b18fe0] 
PR has been raised for this issue
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8204/diff#testcases/forty_eight_hour/solution_test_sanityecamb_cert_lan.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8204/diff#testcases/forty_eight_hour/solution_test_sanityecamb_cert_lan.py]

Please help in approving and merging the fix","['Auton', 'Blocked', 'Delay_Testing', 'Hulk', 'HulkPatch2', 'Hulk_patch1', 'Sanity']",DeepakPratap Shinde,Resolved,Yamini P
SEEN-3061,https://miggbo.atlassian.net/browse/SEEN-3061,[Auton]:[Optimized]:HulkP3: Task-validate_operations_on_unreachable_devices.py-392-validateOperationsOnUnreachableDevices  /   Test_TC223_Validate_operations_on_unreachable_devices  /   test4_make_device_unreachable_by_shutdown_interface,"h2. Description

# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced: After the shutdown, an interface is encountering an attribute error.*{{  }}
# *Failure/Errors snip from log:* 
{{TB7-SJ-eCA-BORDER-CP# config term Enter configuration commands, one per line. End with CNTL/Z. TB7-SJ-eCA-BORDER-CP(config)# TB7-SJ-eCA-BORDER-CP(config)#interface Loopback 0 TB7-SJ-eCA-BORDER-CP(config-if)# shut TB7-SJ-eCA-BORDER-CP(config-if)#end TB7-SJ-eCA-BORDER-CP# }}
{noformat}8946: 
 Traceback (most recent call last):{noformat}
{noformat}8947: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper{noformat}
{noformat}8948: 
     result = testfunc(func_self, **kwargs){noformat}
{noformat}8949: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/testcases/sanityusecases/validateOperationsOnUnreachableDevices/validate_operations_on_unreachable_devices.py"", line 212, in test4_make_device_unreachable_by_shutdown_interface{noformat}
{noformat}8950: 
     and dnac_handle.dnaconfig.send_device_cmd_shut_port(dev=self.wlc_device, interface=self.interface_wlc) \{noformat}
{noformat}8951: 
   File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/commonlibs/sftopology.py"", line 5416, in send_device_cmd_shut_port{noformat}
{noformat}8952: 
     interface=interface.name{noformat}
{noformat}8953: 
 AttributeError: 'bool' object has no attribute 'name'{noformat}
{noformat}8955: 
 Errored reason: 'bool' object has no attribute 'name'{noformat}


# *Regression team debug analysis with details:* Looks like the script side needs modifications  getting attribute  error 
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
Need to fix the Automation code to accommodate for the failure
# *Failed log:* [*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-validate_operations_on_unreachable_devices.py-392-validateOperationsOnUnreachableDevices&begin=2502810&size=68999&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov21_00:37:23.801537.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-validate_operations_on_unreachable_devices.py-392-validateOperationsOnUnreachableDevices&begin=2502810&size=68999&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov21_00:37:23.801537.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Failed DNAC version/ISO: Hulk Patch-3 2.1.715.70102* 
# *Script/Mapping file Used:* Optimized code -Lan automation  script  
# *TCs Impacted:* 1
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/sanity _api_auto
# *DNAC Type:* On-Prem
# *Testbed details:TB7*
# *Team:* Solution Sanity ",2023-11-23T13:08:46.149+0000,"The root cause: The library ‘{{get_interface_to_make_device_unreachable}}' return the value; ‘false’. That is why the next library doesn’t have the correct interface to run. And then it raises the error: {{'bool' object has no attribute 'name'}}

Solution: Fix that library to get the correct value. # PR HulkPatch2-ms/saniy-api_atuto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8059/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8059/overview]
# Test Case:  {{Test_TC233_Validate_operations_on_unreachable_devices}}
# Testbed: TB7
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link HulkPatch2-ms/saniy-api_atuto:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-11/sanity_TB7.2023Nov23_22:32:31.831663.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-11/sanity_TB7.2023Nov23_22:32:31.831663.zip&atstype=ATS] Hi [~accountid:63f50bcafb3ac4003fa2c6dd]  Could you please raise PR for main branch  *private/HulkPatch2-ms/api-auto* 

The below PR is raised for *Sanity branch private/HulkPatch2-ms/sanity_api_auto* and we cannot merge this PR, As we are always taking PR merged from main line. 

Thanks Ashwini Hi [~accountid:63f50bcafb3ac4003fa2c6dd]  
In Sanity setups, we don’t have ECA/ ewlc direct physical connection to any real APs. In our case, real APs are connected to EDGE or FIAB devices - Hence the testcase has been blocked here with reason as: 
Blocked reason: Result: Failed to get device that meets condition. Please use another testbed

{noformat}Blocked reason: Result: Failed to get device that meets condition. Please use another testbed
 The result of section test1_select_eca_and_wlc_to_do_operation is => BLOCKED{noformat}


LOG: [Test_TC1_Validate_operations_on_unreachable_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-validate_operations_on_unreachable_devices.py-392-validateOperationsOnUnreachableDevices&begin=4698&size=668043&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec01_10:15:18.398676.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] 

In this sub-test [test1_select_eca_and_wlc_to_do_operation|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-validate_operations_on_unreachable_devices.py-392-validateOperationsOnUnreachableDevices&begin=5511&size=633970&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec01_10:15:18.398676.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] , it is trying to pick ECA/ ewlc which has got an AP connected to any of the interface. So this interface is not available on either ECA or Ewlc device on sanity testbeds. 
HERE is SNIP From log for CDP neighbours for TB8-SJ-BORDER-CP-9400-SVL

----

*Output of 'show cdp neighbors' executed on TB8-SJ-BORDER-CP-9400-SVL:*
450:  show cdp neighbors
451:  Capability Codes: R - Router, T - Trans Bridge, B - Source Route Bridge
452:                    S - Switch, H - Host, I - IGMP, r - Repeater, P - Phone,
453:                    D - Remote, C - CVTA, M - Two-port Mac Relay
454:
455:  Device ID        Local Intrfce     Holdtme    Capability  Platform  Port ID
456:  [TB8-SJ-EDGE.cisco.com|http://TB8-SJ-EDGE.cisco.com]
457:                   Gig 2/1/0/1       142             R S I  C9300-24U Gig 1/0/24
458:  [TB8-Fusion.cisco.com|http://TB8-Fusion.cisco.com]
459:                   Gig 2/1/0/48      170             R S I  C9300-24U Gig 1/0/1
460:  [TB8-Transit.cisco.com|http://TB8-Transit.cisco.com]
461:                   Gig 2/2/0/1       153             R S I  C9300-24U Gig 1/0/2
462:  [TB8-Transit.cisco.com|http://TB8-Transit.cisco.com]
463:                   Gig 1/2/0/1       176             R S I  C9300-24U Gig 1/0/1
464:
465:  Total cdp entries displayed : 4
466:  TB8-SJ-BORDER-CP-9400-SVL#

[~accountid:63f50bcafb3ac4003fa2c6dd]  Could you please try to add this scenario to be handled by script The below scenario will be handle in the Auton [https://miggbo.atlassian.net/browse/SEEN-3114|https://miggbo.atlassian.net/browse/SEEN-3114|smart-link]  In *EXSI HULK VA Patch2 build #3.714.75242 Build [Single interface setup]* observed the same issue test4 got errored due to {{AttributeError: 'bool' object has no attribute 'name'}}

TC name: test4_make_device_unreachable_by_shutdown_interface

Log:

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec05_00:30:32.834327.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec05_00:30:32.834327.zip&atstype=ATS] Hi [~accountid:63f50bd68ab3d6a635ecc29b], If you run on {{private/HulkPatch2-ms/api-auto}}, you can cherry-pick this PR to solve your issue: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8110/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8110/overview] This Auton was duplicate with the Auton: [https://miggbo.atlassian.net/browse/SEEN-2839|https://miggbo.atlassian.net/browse/SEEN-2839|smart-link] 

So, close this Auton.","['Auton', 'Feature', 'HulkP3', 'Optimized', 'Sanity']",NhanHuu Nguyen,Closed,Omkar Sharad Wagh
SEEN-3062,https://miggbo.atlassian.net/browse/SEEN-3062,"[Auton]Observing ""aaa server already present error"" even after cleanup script passed","# Regression Team Actions Checklist:
 Debugged/Analyzed the issue: Y
 Compared with the last passed log: Y
 Checked and compared the configuration: Y
 Confirmed with DE on the root cause: Y
# Issue faced: Hulk patch2
# Failure/Errors snip from log: 

'errorReportMetadata': {'url': '/api/v2/data/spf-diagnostics/download-async', 'payload': {'provisionTaskId': '018bf989-ee77-7d0a-bfa4-ca2e9beb9825'}, 'method': 'POST', 'rbac': {'resourceName': 'network.spf-diagnostics', 'type': 'Network Provision.Provision'}}, 'scheduledId': '6d89d35c-e127-44e1-87a2-ce27fad24370'}], 'version': '1.0'}
1413:  Config Preview Activity failed with reason: AAA CLI(s) are already present on the device [eCA2.cisco.com|http://eCA2.cisco.com]: aaa server radius dynamic-author, aaa group server radius dnac-group, radius server groupName, aaa accounting settings. Remove the CLIs, resync the device and retry.
1414:
1415:  Activity: 018bf989-ee25-74e4-a9f3-8f954c27ae4c Trigger job: {'id': 'b0b5212f-1309-48d7-b440-70ca07385cc1', 'triggeredJobTaskId': '018bf989-ee77-7d0a-bfa4-ca2e9beb9825', 'triggeredTime': 1700698648189, 'status': 'FAILED', 'failureReason': 'AAA CLI(s) are already present on the device [eCA2.cisco.com|http://eCA2.cisco.com]: aaa server radius dynamic-author, aaa group server radius dnac-group, radius server groupName, aaa accounting settings. Remove the CLIs, resync the device and retry.', 'triggeredJobId': 'b0b5212f-1309-48d7-b440-70ca07385cc1'}
1416:
1417:
1418:   api_switch_call called:

# Regression team debug analysis with details: Stated above
# DE analysis/confirmation with details: NA
# What do Automation team need to look into? Cleanup not happening on the ECA devices
# Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assign_devices_to_site_and_provision.py-72-provisioning&begin=270964&size=450341&archive=%2Fhome%2Fsdnbld%2F.pyats%2Farchive%2F23-11%2Fsanity_TB1_cert.2023Nov22_15:24:26.064471.zip&ats=%2Fpyats-py386&submitter=sdnbld&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assign_devices_to_site_and_provision.py-72-provisioning&begin=270964&size=450341&archive=%2Fhome%2Fsdnbld%2F.pyats%2Farchive%2F23-11%2Fsanity_TB1_cert.2023Nov22_15:24:26.064471.zip&ats=%2Fpyats-py386&submitter=sdnbld&from=trade&view=all&atstype=pyATS]
# Last passed log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assign_devices_to_site_and_provision.py-72-provisioning&begin=270978&size=553617&archive=%2Fhome%2Fsdnbld%2F.pyats%2Farchive%2F23-11%2Fsanity_TB1_cert.2023Nov22_02:56:45.411167.zip&ats=%2Fpyats-py386&submitter=sdnbld&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assign_devices_to_site_and_provision.py-72-provisioning&begin=270978&size=553617&archive=%2Fhome%2Fsdnbld%2F.pyats%2Farchive%2F23-11%2Fsanity_TB1_cert.2023Nov22_02:56:45.411167.zip&ats=%2Fpyats-py386&submitter=sdnbld&from=trade&view=all&atstype=pyATS]

# Failed DNAC version/ISO: 3.1.714.75232
# Last Passed DNAC version/ISO: 3.714.75232
# Script/Mapping file Used:
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/express_sanity/sanity_exp_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/express_sanity/sanity_exp_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulk-ms%2Fapi-auto]
# TCs Impacted: All provisioning TC's
# Branch Used: private/HulkPatch2-ms/api-auto
# DNAC Type: ESXI VA
# Testbed details: [https://10.195.214.103/|https://10.195.214.103/] (admin/maglev1@3)
# Team: SIT",2023-11-23T14:53:05.905+0000,"Based on log, the 'show run | inc radius' didnot show the correct information hence the configuration was not removed from goldenConfigOverlayBrownfieldConfigCleanup.

!image-20231205-205032.png|width=1281,height=519!



[https://earms-trade.cisco.com/tradeui/logs/fileviewer?logfile=eCA2-cli-1700695580.log&begin=0&size=-1&archive=%2Fhome%2Fsdnbld%2F.pyats%2Farchive%2F23-11%2Fsanity_TB1_cert.2023Nov22_15:24:26.064471.zip&ats=%2Fpyats-py386&submitter=sdnbld&atstype=pyATS&from=trade&view=all|https://earms-trade.cisco.com/tradeui/logs/fileviewer?logfile=eCA2-cli-1700695580.log&begin=0&size=-1&archive=%2Fhome%2Fsdnbld%2F.pyats%2Farchive%2F23-11%2Fsanity_TB1_cert.2023Nov22_15:24:26.064471.zip&ats=%2Fpyats-py386&submitter=sdnbld&atstype=pyATS&from=trade&view=all]

The issue didnot happen with other teams. This seem like pyats version issue. [~accountid:712020:8911920e-7781-4278-898b-d1ab6b1c6b4f] , please raise ticket with pyats team.

For now, I will add the cleanup test ‘goldenConfigOverlayBrownfieldConfigCleanup’ twice in mapping filie to see if it will help.

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0e2818d8ba5285780420e664ddde21fa7efd80d7|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0e2818d8ba5285780420e664ddde21fa7efd80d7] Close this since it was pyats issue.",['Auton'],Tran Lam,Closed,Raja Subramani
SEEN-3066,https://miggbo.atlassian.net/browse/SEEN-3066,[Auton] Hulk P2 EXSI Sanity Task-map.py-421-map/Test_TC2_DNAC_maps/test2_ap_bulk_import/test3_import_Ekahau_file/test14_disconnect_ap_from_wlc /,"In EXSI *HULK VA Patch2 build #3.714.75232* Build  observed  DNAC_maps  UC  failed TCs

*Task-map.py-421-map/Test_TC2_DNAC_maps*

Below TCs are failed due to below reasons:

test2_ap_bulk_import    - Not able to collect get AP report
test3_import_Ekahau_file - Error importing Ekahau file. local variable 'lat_long_msg' referenced before assignment
test14_disconnect_ap_from_wlc  - Not getting TB4-DM-WLC from map - manully able the device
test15_verify_device_reachability_after_disconnect -checking early AP reachbility after disconcect & connect

blocked below TCs

test16_delete_ap_from_inventory
test17_delete_ap_from_maps
test18_create_pap
test19_reconnect_ap_to_wlc
test20_verify_device_reachability_after_connect
test21_verify_ap_matches_pap
test22_delete_floor1

Regression Team Actions Checklist:

Debugged/Analyzed the issue: Y

Compared with the last passed log: N - it's UC added

Checked and compared the configuration: Y

Confirmed with DE on the root cause: N

Issue faced:

Failure/Errors snip from log:

Regression team debug analysis with details:

DE analysis/confirmation with details:NA

What do Automation team need to look into?

Failed log: 

Last passed log: NA new TC Integration 

Failed DNAC version/ISO:   3.714.75232 

Last Passed DNAC version/ISO: NA

Script/Mapping file Used:  lansanity_usecases_maps.yaml

TCs Impacted: 1

'Re-run' TCs:rerun also same issue

Branch Used:  private/HulkPatch2-ms/api-auto

DNAC Type:  EXSI VM

Testbed details: TB4
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4]

Team: EXSI Sanity",2023-11-24T12:42:52.739+0000,"Hi [~accountid:63f50bd68ab3d6a635ecc29b], can you attach the failed logs? Regarding test 14 and test 15, is the AP TSIM? If it is a TSIM, I have already addressed that issue in the following PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8041/diff#services/dnaserv/lib/api_groups/maintenance_mode/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8041/diff#services/dnaserv/lib/api_groups/maintenance_mode/group.py]

If it is not due to TSIM, I would have to take a closer look at the logs and see why it is failing. Hi [~accountid:63f50be71223974bc04b0534] 

Pls find below failed logs :- 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-map.py-421-map&begin=9681&size=12519&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_09:32:47.686523.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-map.py-421-map&begin=9681&size=12519&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_09:32:47.686523.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]



Can you please take look on the test2 & test3 failures as well [~accountid:63f50bd68ab3d6a635ecc29b] , thanks for sharing the logs. For test 2 and test 3 it is ‘expected’ to fail in some testbeds since it is reading the APs from a CSV file and those APs might not be on the specific testbed. You can check with Raji on those two failures.
The new addition to this testcase is from test4 onwards. If any failure from test4 to test22 do let me know.","['Auton', 'Sanity', 'exsi', 'hulk', 'hulk-vm-sanity']",Majlona 'Luna' Aliaj,In Progress,Raghavendrachar Baraguru Mallesha Char
SEEN-3067,https://miggbo.atlassian.net/browse/SEEN-3067,[Auton] - DHCP IP to SGT mapping creation fails during Trustsec policy enforcement feature test,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* NA. Feature Integration
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced:* When integrating Hulk Patch2 specific feature of Trustsec Border Enforcement we see DHCP IP to SGT mapping creation fails with reason printing no enough data. 
# *Failure/Errors snip from log:* 
20111:  Create SXP Connection successfully
20112:  Library group ""aca"" method ""create_sxp_device"" returned in 0:00:01.199123
20113:  Create sxp device successfully
20114:  Creating ip to sgt mapping
{color:#ff5630}20115:  Could not retrieve enough data to create IP SGT mapping{color}
20116:  Library group ""aca"" method ""create_sgt_mapping"" returned in 0:00:00.388634
20117:  Test returned in 0:00:03.339415
20118:  Failed reason: Failed to create IP SGT static mapping
# *Regression team debug analysis with details:* 
On debugging further manually, we see DHCP IP and other related parameters are available as expected. Also from the failure message is not clear of the details of what parameters are not available to create the SGT mapping from ISE.
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
Need to check further of reason for the failure and also fix the reason for failure.
# *Failed log:* [Test_TC281_Trustsec_Policy_enforcement_on_Border|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2663994&size=1565856&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov24_04:56:59.373604.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* NA. Feature Integration. 
# *Failed DNAC version/ISO:* Hulk Patch2 - 2.1.714.70296
# *Last Passed DNAC version/ISO:* NA. Feature Integration.
# *Script/Mapping file Used:* testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
# *TCs Impacted:* 1
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/api-auto
# *DNAC Type:* On-Prem
# *Testbed details:* MSTB1
# *Team:* Solution Regression",2023-11-25T05:45:08.712+0000,,"['Auton', 'HulkPatch2', 'Integration', 'MSTB1', 'Multisite']",ThangQuoc Tran,Backlog,SANDEEP SHIVARAMAREDDY
SEEN-3072,https://miggbo.atlassian.net/browse/SEEN-3072,Auton:Hulk Patch1:Test_TC2_ITSM_ticket_generation_test/test3_approve_SGT_request/test4_validate_SGT_request,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue: Y*
#* *Compared with the last passed log: Y*
#* *Checked and compared the configuration: Y*
#* *Confirmed with DE on the root cause: Y*
# *Issue faced:*

rate limit exceeded error,
add a fix to bypass the rate limiting error but still need to see why integration event not getting generated.

# *Failure/Errors snip from log:*

{noformat}261: 
 No ITSM integration event for group based policy found within the timeframe{noformat}

{{ }}

{noformat}278: 
 No ITSM integration event for group based policy found within the timeframe{noformat}



# *Regression team debug analysis with details:Y*
# *DE analysis/confirmation with details:*

rate limit exceeded error,

# *What do Automation team need to look into?Please commit fix on all hulk branches* 
# *Failed log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=10495&size=26727876&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov23_19:28:57.814465.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=10495&size=26727876&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov23_19:28:57.814465.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

# *Last passed log :*

Hulk  P1  RC 5  
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=199429&size=50284&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct29_03:55:46.515036.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=199429&size=50284&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct29_03:55:46.515036.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

# *Failed DNAC version/ISO:2.1.713.70332*
# *Last Passed DNAC version/ISO:*
# *Script/Mapping file Used:*

solution_test_sanityecamb_lan.py

# *TCs Impacted:1*
# *'Re-run' TCs:NA*
# *Branch Used:private/HulkPatch-ms/sanity_api_auto*
# *DNAC Type:Hulk Patch1 RC7*
# *Testbed details:*
# *Team:Sanity*",2023-11-27T10:48:21.471+0000,"Observed the same issue in  In ExSI Hulk Patch2 RC1 OVA# 3.714.75298 build 



log:

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=120931&size=14650&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec13_03:55:49.506129.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-itsm_validations.py-231-itsmValidations&begin=120931&size=14650&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec13_03:55:49.506129.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'ESXi', 'Sanity']",Andrew Chen,Backlog,Anusha John
SEEN-3073,https://miggbo.atlassian.net/browse/SEEN-3073,[Auton]:Hulk P2:[Optimized] ask-SDA_wired_host_onboarding_uplink_interfaces.py-128-SDAWiredHostOnboardingUplinkInterfaces,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced:* On-board Port interface failed with the un-expected config,  present on a device  
# *Failure/Errors snip from log:* 

{noformat}12929:  activity_id for config preview description Scheduling task for Interface onboarding TenGigabitEthernet1/0/37 on device TB7-SJ-EDGE for fabric ['Global/USA/SAN_JOSE'] at time 1700827208.2556965 - Configuration Preview: False
12930:  activity_id is False. Config preview task failed for description Scheduling task for Interface onboarding TenGigabitEthernet1/0/37 on device TB7-SJ-EDGE for fabric ['Global/USA/SAN_JOSE'] at time 1700827208.2556965 - Configuration Preview
12931:  Library group ""vcr"" method ""config_preview_helper"" returned in 0:00:20.915419
12932: 
12933:  ############################################################
12934:  Config preview was failed
12935:  ############################################################
12936:  Library group ""vcr"" method ""config_preview_flow"" returned in 0:00:20.915999
12937:  Config preview flow was failed for description: Scheduling task for Interface onboarding TenGigabitEthernet1/0/37 on device TB7-SJ-EDGE for fabric ['Global/USA/SAN_JOSE'] at time 1700827208.2556965
12938:  Library group ""schedule-job"" method ""full_schedule_flow"" returned in 0:00:20.916864
12939:  Library group ""connectivity_domain"" method ""update_device_info_in_cd"" returned in 0:00:20.917372
12940: 
12941:  ############################################################
12942:  FAILED to Onboard interface TenGigabitEthernet1/0/37 on device TB7-SJ-EDGE with segment_name 96net_sub-WiredVNFBLayer2, voice_segment_name None, authType No Authentication, deviceType USER_DEVICE
12943:  ############################################################
12944:  Library group ""wired_interface_onboarding"" method ""onboard_device_on_interface"" returned in 0:00:21.464099
12945:  Test returned in 0:00:21.765282
12946:  Failed reason: Ports not deployed with type USER_DEVICE{noformat}
# *Regression team debug analysis with details:* Looks like script side needs modifications in the automation script.
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
Need to fix the Automation code to accommodate for the failure
# *Failed log:* 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3129047&size=251424&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov24_03:27:00.673682.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3129047&size=251424&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov24_03:27:00.673682.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* 
Hulk  P1  
[+https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-10/env_optimized_auto_job.2023Oct09_06:33:02.748849.zip&atstype=ATS+|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-10/env_optimized_auto_job.2023Oct09_06:33:02.748849.zip&atstype=ATS]
# *Failed DNAC version/ISO:* Hulk Patch2 - 2.1.714.70344
# *Last Passed DNAC version/ISO:* Hulk Patch1  {{2.3.7.3-70289}}
# *Script/Mapping file Used:     lansanity_usecases_maps.yaml*
# *TCs Impacted:* 3  sub TCs Impacted  
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/sanity_api_auto
# *DNAC Type:* On-Prem
# *Testbed details:* Sanity 
# *Team:* Solution Sanity ",2023-11-27T11:27:07.224+0000,"# PR HulkPatch2-ms/sanity_api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8147/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8147/overview]
# Test Case:  {{Test_TC215_SDA_Wired_Host_Onboarding_Uplink_Interfaces}}
# Branch Used*:* private/HulkPatch2-ms/sanity_api_auto
# Testbed Used: INTG2
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link HulkPatch2-ms/sanity_api_auto: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-11/sanity-intg2.2023Nov30_01:07:06.244389.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-11/sanity-intg2.2023Nov30_01:07:06.244389.zip&atstype=ATS] The root cause: Config Preview failed with reason: User intent validation failed while processing the 'provision' request.
Solution: Skip config preview. PR was merged.","['Auton', 'HulkP2', 'Optimized', 'Sanity']",NhanHuu Nguyen,Resolved,Omkar Sharad Wagh
SEEN-3074,https://miggbo.atlassian.net/browse/SEEN-3074, [Auton]:Hulk P2:[Optimized] test1_verify_tsim_client_start,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced: the* script is failing to start TSIM  Client    , Manually   able  to start  
# *Failure/Errors snip from log:* 

{noformat}Trying 10.30.0.71...
Connected to 10.30.0.71.
Escape character is '^]'.

14008:  +++ connection to spawn: telnet 10.30.0.71 2015, id: 139998980314768 +++
14009:  connection to TB7-TSIM

(Cisco Capwap Simulator) >tlv_buffer_init_with_existing_buffer_for_encoding failure
tlv_buffer_init_with_existing_buffer_for_encoding failure
tlv_buffer_init_with_existing_buffer_for_encoding failure
tlv_buffer_init_with_existing_buffer_for_encoding failure
tlv_buffer_init_with_existing_buffer_for_encoding failure
tlv_buffer_init_with_existing_buffer_for_encoding failure
tlv_buffer_init_with_existing_buffer_for_encoding failure
tlv_buffer_init_with_existing_buffer_for_encoding failure
tlv_buffer_init_with_existing_buffer_for_encoding failure
tlv_buffer_init_with_existing_buffer_for_encoding failure
tlv_buffer_init_with_existing_buffer_for_encoding failure
tlv_buffer_init_with_existing_buffer_for_encoding failure
tlv_buffer_init_with_existing_buffer_for_encoding failure
14022:  Traceback (most recent call last):{noformat}
# *Regression team debug analysis with details:* Looks like the script side needs modifications in the automation script. need  to  multiple time    try  if  console is  not accessible  
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
Need to fix the Automation code to accommodate for the failure
# *Failed log:* 
  [test1_verify_tsim_client_start|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=4165041&size=32177&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_17:45:43.918926.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
Impacted TC 
[test1_verify_tsim_clients|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=4380568&size=161654&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_17:45:43.918926.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
[test1_verify_edge_reload_ap_clients_stability|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=4897023&size=248399&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_17:45:43.918926.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* 
Hulk  P3  
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=2711588&size=5550&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov19_02:41:36.974509.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=2711588&size=5550&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov19_02:41:36.974509.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Failed DNAC version/ISO:* Hulk Patch2 - 2.1.714.70344
# *Last Passed DNAC version/ISO:* Hulk Patch1  {{2.3.7.5-70102}}
# *Script/Mapping file Used:     lansanity_usecases_maps.yaml*
# *TCs Impacted:* 2  sub TCs Impacted  
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/sanity_api_auto
# *DNAC Type:* On-Prem
# *Testbed details:* Sanity 
# *Team:* Solution Sanity ",2023-11-27T11:45:36.553+0000,,"['Auton', 'Hulk', 'HulkP2', 'Optimized', 'Sanity']",Moe Saeed,Backlog,Omkar Sharad Wagh
SEEN-3075,https://miggbo.atlassian.net/browse/SEEN-3075,Test_TC5_DEV_STRESS_traffic_convergence_test_primary_border_reload_DC_Prefix  [Auton]:Hulk P2:[Optimized] ,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* No pass log & wiki, Recently added TC to optimization code, Day 1 failing sanity . 
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced: the* s
# *Failure/Errors snip from log:* 

{noformat}14633:  	First TimeStamp: 00:00:01.513
14634:  	Last TimeStamp: 00:00:59.340
14635: 
14636:  ===========Traffic item statitics: IPv6 Traffic From 2/13 to 2/15 ========
14637:  ['IPv6 Traffic From 2/13 to 2/15', '57892', '56083', '1809', '3.125', '1000.000', '1000.000', '1120000.000', '1120000.000', '6729974', '120000.000', '120000.000', '960000.000', '960000.000', '960.000', '960.000', '0.960', '0.960', '48047', '27862', '62659507', '00:00:01.513', '00:00:59.340']
14638:  Traffic is failing for this traffic item, Check routes and traffic direction
14639: 
14640:  Following streams have traffic loss more then expected.
14641:  ['IPv6 Traffic From 2/13 to 2/11', '57892', '56083', '1809', '3.125', '1000.000', '1000.000', '1120000.000', '1120000.000', '6729974', '120000.000', '120000.000', '960000.000', '960000.000', '960.000', '960.000', '0.960', '0.960', '32688', '22575', '68140880', '00:00:01.518', '00:00:59.340']
14642:  ['IPv6 Traffic From 2/13 to 2/12', '57892', '56082', '1810', '3.127', '1000.000', '1000.000', '1120000.000', '1120000.000', '6729840', '120000.000', '120000.000', '960000.000', '960000.000', '960.000', '960.000', '0.960', '0.960', '50644', '33625', '74665', '00:00:01.564', '00:00:59.340']
14643:  ['IPv6 Traffic From 2/13 to 2/14', '57892', '57800', '92', '0.159', '1000.000', '1000.000', '1120000.000', '1120000.000', '6936000', '120000.000', '120000.000', '960000.000', '960000.000', '960.000', '960.000', '0.960', '0.960', '44008', '31430', '68037', '00:00:01.520', '00:00:59.340']
14644:  ['IPv6 Traffic From 2/13 to 2/15', '57892', '56083', '1809', '3.125', '1000.000', '1000.000', '1120000.000', '1120000.000', '6729974', '120000.000', '120000.000', '960000.000', '960000.000', '960.000', '960.000', '0.960', '0.960', '48047', '27862', '62659507', '00:00:01.513', '00:00:59.340']
14645: 
14646:  Following streams have no traffic loss, Successfully flowing traffic.
14647:  ['IPv4 Traffic From 2/13 to 2/11', '57892', '57892', '0', '0.000', '1000.000', '1000.000', '6304000.000', '6304000.000', '44459008', '768000.000', '768000.000', '6144000.000', '6144000.000', '6144.000', '6144.000', '6.144', '6.144', '28906', '22417', '41730', '00:00:01.450', '00:00:59.340']
14648:  ['IPv4 Traffic From 2/13 to 2/12', '57892', '57892', '0', '0.000', '1000.000', '1000.000', '6304000.000', '6304000.000', '44459008', '768000.000', '768000.000', '6144000.000', '6144000.000', '6144.000', '6144.000', '6.144', '6.144', '52330', '36955', '77672', '00:00:01.450', '00:00:59.340']
14649:  ['IPv4 Traffic From 2/13 to 2/14', '57892', '57892', '0', '0.000', '1000.000', '1000.000', '6304000.000', '6304000.000', '44459008', '768000.000', '768000.000', '6144000.000', '6144000.000', '6144.000', '6144.000', '6.144', '6.144', '39288', '31272', '63015', '00:00:01.450', '00:00:59.340']
14650:  ['IPv4 Traffic From 2/13 to 2/15', '57892', '57892', '0', '0.000', '1000.000', '1000.000', '6304000.000', '6304000.000', '44459008', '768000.000', '768000.000', '6144000.000', '6144000.000', '6144.000', '6144.000', '6.144', '6.144', '41724', '27702', '70310', '00:00:01.450', '00:00:59.340']
14651:  Test returned in 0:08:01.634892
14652:  Failed reason: Traffic from IXIA is not flowing as expected
14653:  The result of section test2_static_ixia_noauth_traffic_convergence_test is => FAILED{noformat}
# *Regression team debug analysis with details:* 
script  is  failing start to traffic, as per the failure message, we need to check the route traffic direction. Please provide the wiki and recording. 
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
Need to fix the Automation code to accommodate for the failure
# *Failed log:* 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-139-trafficdot1x&begin=3620560&size=115147&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_17:45:43.918926.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-139-trafficdot1x&begin=3620560&size=115147&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_17:45:43.918926.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
Impacted TC 
[test1_verify_tsim_clients|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=4380568&size=161654&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_17:45:43.918926.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
[test1_verify_edge_reload_ap_clients_stability|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-ap_sync_provisioning_clients_roaming.py-131-FEWAccessPointAndCLients&begin=4897023&size=248399&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_17:45:43.918926.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* 
Day 1  failing in sanity 
# *Failed DNAC version/ISO:* Hulk Patch2 - 2.1.714.70344
# *Last Passed DNAC version/ISO:* NA
# *Script/Mapping file Used:     lansanity_usecases_maps.yaml*
# *TCs Impacted:* 1 *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/sanity_api_auto
# *DNAC Type:* On-Prem
# *Testbed details:* Sanity 
# *Team:* Solution Sanity ",2023-11-27T12:20:37.091+0000,"Traffic failure need to be looked by DE.  Look into the traffic streams and debug which streams are failing and why those streams are failing. 

There is othing wrong from config side. same streams are running traffic fine in TC; 

You are viewing logs for testcase: [Task-traffic_unicst_multicast_dot1x_cp.py-139-trafficdot1x|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-traffic_unicst_multicast_dot1x_cp.py-139-trafficdot1x&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_17:45:43.918926.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]  /   Test_TC8_DEV_STRESS_mcast_traffic_convergence_external_rp_test_primary_border_reload  (Passed)



Debug in the setup, and you may have to raise a product defect for it. Need Tester to debug and raise bug on device.","['Auton', 'Hulk', 'HulkP2', 'Optimized', 'Sanity']",Omkar Sharad Wagh,Blocked,Omkar Sharad Wagh
SEEN-3076,https://miggbo.atlassian.net/browse/SEEN-3076,[Auton]:Hulk P2:[Optimized] Test_TC12_verify_fabric_360/test1_verify_fabric_SD_acces,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* N
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced:* Looks like the script side needs modifications as per the DE. There could be some parameters and uplift needed on the script side. We have created a space with DE; please post your query here
   webEX space =>  [https://eurl.io/#o1WLccaNH|https://eurl.io/#o1WLccaNH]  
verify   SD-access  failed with  4341:   noHealthDevices:0=27, monitoredHealthyDevices:3=3, latestHealthScore:100.0=100.0   Un structure data from '/assurance/v1/network-device'  API response

# *Failure/Errors snip from log:* 

{noformat}0.0}}
4364:  ======================================================================================================================================================
4365:  FABRIC SITE HEALTH TIMELINE verification starts for 'New York' site
4366:  ======================================================================================================================================================
4367: 
4368: 
4369:   api_switch_call called:
4370:  {'params': {'approach': 'trend', 'endTime': 1700416103707, 'startTime': 1700329703687, 'window': 5}}
4371:  Resource path full url: https://10.30.0.100/api/assurance/v1/fabricSites/96a59b80-394a-4aec-9196-5648fbe060cd/health
4372:  The latest fabric time line values are :{'modificationtime': '2023-11-19T17:40:00.000+0000', 'pubsubInfraVnScore': '100', 'bgpEvpnScore': '-1', 'fabsiteFnodeScore': '100', 'ctsEnvDataDownloadScore': '100', 'enIseConnScore': '100', 'overallFabricSiteScore': '66', 'fpcLinkScore': '50', 'bgpPeerInfraVnScore': '-1', 'peerScore': '-1', 'fabsiteFcpScore': '100', 'lispCpConnScore': '100', 'tcpConnScore': '100', 'bgpBgpSiteScore': '-1', 'bgpPubsubSiteScore': '-1', 'fabsiteFsconnScore': '66'}
4373:  Length of device dict is :3
4374:  DEV details TB7-NY-FIAB.cisco.com
4375:  DEV details TB7-eWLC.cisco.com
4376:  DEV details SN-JAE242302CZ.cisco.com
4377:  Checking parameters are len_good_dev:2, len_poor_dev:1, len_nodata_dev:0
4378:  Current timeline for the site 'New York' is in not good condition with InValid scores of '66.66666666666666'
4379:  dev_parms is: {'Fabric Site Connectivity': {'all': 3, 'nodata': 0, 'good': 2, 'poor': 1}, 'Fabric Infrastructure': {'all': 2, 'nodata': 0, 'good': 2, 'poor': 0}}
4380:  ======================================================================================================================================================
4381:  Health Summary verification starts for 'New York' site
4382:  ======================================================================================================================================================
4383: 
4384: 
4385:   api_switch_call called:
4386:  {'params': {'measureBy': 'fabricSite', 'windowInMin': 1, 'offset': 1, 'approach': 'latest', 'startTime': 1700329703687, 'endTime': 1700416103707, 'currTime': 1700416103707}, 'data': {'fabricSites': ['96a59b80-394a-4aec-9196-5648fbe060cd']}}
4387:  Resource path full url: https://10.30.0.100/api/assurance/v1/network-device/healthSummary
4388:  fabric_health_summary_response: {'version': '1.0', 'response': [{'time': '2023-11-19T17:45:00.000+0000', 'healthScore': 100, 'totalCount': 29, 'goodCount': 3, 'noHealthCount': 26, 'fairCount': 0, 'badCount': 0, 'maintenanceModeCount': 0, 'entity': None, 'timeinMillis': 1700415900000}], 'measuredBy': 'fabricSite', 'latestMeasuredByEntity': None, 'latestHealthScore': 100, 'monitoredDevices': 3, 'monitoredHealthyDevices': 3, 'monitoredUnHealthyDevices': 0, 'noHealthDevices': 0, 'notApplicableDevices': 26, 'totalDevices': 29, 'monitoredPoorHealthDevices': 0, 'monitoredFairHealthDevices': 0, 'healthContributingDevices': 3, 'healthDistirubution': [{'category': 'fabricSiteConnectivity', 'totalCount': 3, 'healthScore': 100, 'goodPercentage': 100.0, 'badPercentage': 0.0, 'fairPercentage': 0.0, 'noHealthPercentage': 0.0, 'goodCount': 3.0, 'badCount': 0.0, 'fairCount': 0.0, 'noHealthCount': 0.0, 'thirdPartyDeviceCount': 0.0}, {'category': 'fabricControlPlane', 'totalCount': 2, 'healthScore': 50, 'goodPercentage': 50.0, 'badPercentage': 0.0, 'fairPercentage': 0.0, 'noHealthPercentage': 50.0, 'goodCount': 1.0, 'badCount': 0.0, 'fairCount': 0.0, 'noHealthCount': 1.0, 'thirdPartyDeviceCount': 0.0}, {'category': 'fabricNode', 'totalCount': 2, 'healthScore': 100, 'goodPercentage': 100.0, 'badPercentage': 0.0, 'fairPercentage': 0.0, 'noHealthPercentage': 0.0, 'goodCount': 2.0, 'badCount': 0.0, 'fairCount': 0.0, 'noHealthCount': 0.0, 'thirdPartyDeviceCount': 0.0}]}
4389:  AP count is : 6
4390:  fabric_health_summary_response: 0=6,3=2,100.0=66.66666666666666
4391:  noHealthDevices:0=6, monitoredHealthyDevices:3=2, latestHealthScore:100.0=66.66666666666666
4392:  Un structure data from '/assurance/v1/network-device'  API response
4393:  Test returned in 0:00:03.993726
4394:  Failed reason: Unable to verify_fabric_SD_access parameters
4395:  The result of section test1_verify_fabric_SD_access is => FAILED
{noformat}
# *Regression team debug analysis with details:* Looks like the script side needs modifications as per the DE. There could be some parameters and uplift needed on the script side. We have created a space with DE; please post your query here
   webEX space =>  [https://eurl.io/#o1WLccaNH|https://eurl.io/#o1WLccaNH]
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
Need to fix the Automation code to accommodate for the failure
# *Failed log:*  [*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-156-SDAFabricAssurance&begin=1218932&size=158815&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct03_22:44:35.620834.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-156-SDAFabricAssurance&begin=1218932&size=158815&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct03_22:44:35.620834.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log: Day  is  Failing  sanity, we  don’t have  full  pass log*  
# *Failed DNAC version/ISO:DNAC*  {{2.3.7.3-70289}}
# *Last Passed DNAC version/ISO:Day 1  Failing in sanity*  
# *Script/Mapping file Used: lansanity_usecases_maps.yaml*
# *TCs Impacted:* 1
# *'Re-run' TCs:* NA
# *Branch Used: private/HulkPatch2-ms/sanity_api_auto* 
# *DNAC Type:* On-Prem
# *Testbed details:* Sanity 
# *Team:* Solution Sanity ",2023-11-27T12:42:06.478+0000,Seem related to [https://miggbo.atlassian.net/browse/SEEN-3080|https://miggbo.atlassian.net/browse/SEEN-3080|smart-link]  Raised PR for HulkPatch2 branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8104/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8104/overview] PR has been approved and merged to *private/HulkPatch2-ms/api-auto* branch. PR Raised for Ghost branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8167/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8167/overview],['Auton'],Amardeep Kumar,Closed,Omkar Sharad Wagh
SEEN-3077,https://miggbo.atlassian.net/browse/SEEN-3077,[Auton]:Hulk P2:[Optimized] Task-aca_security_groups_policy.py-157-policytAcaValidations,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced:* 
. After fixing the optimization YAML mapping [SEEN-2387|https://miggbo.atlassian.net/browse/SEEN-2387]   .,  still  TC is  Failing .""Looks like the script side needs modifications. In a rerun, the script is passing

# *Failure/Errors snip from log:* 

{noformat}2888: 
2889:  ############################################################
2890:  Deploying failed:Deployment not triggered on Cisco Identity Services Engine (ISE) (ISE was too busy)
2891:  ############################################################
2892:  Library group ""aca"" method ""deploy2"" returned in 0:00:06.430459
2893:  Failed to deploy
2894:  Library group ""aca"" method ""flow_create_policy"" returned in 0:00:22.788155
2895:  Test returned in 0:00:22.789068
2896:  Failed reason: Failed to create policy
2897:  The result of section test11_aca_create_policy is => FAILED
{noformat}
# *Regression team debug analysis with details:* Looks like the script side needs modifications. We observed in TB7, during Hulk P1 optimization script execution with ISE 3.1 P6, ISE 3.3, and ISE 3.2, that 'test11_aca_create_policyr' TC failed with the following error

{noformat}2813:  Some Policies don't exist in ISE or have different information [{'name': 'guest-guest', 'contract': 'contract_sol1'}, {'name': 'finance-finance', 'contract': 'contract_sol1'}]
2814:  ############################################################
2888: 
2889:  ############################################################
2890:  Deploying failed:Deployment not triggered on Cisco Identity Services Engine (ISE) (ISE was too busy)
2891:  ############################################################
2893:  Failed to deploy
2896:  Failed reason: Failed to create policy{noformat}
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
Need to fix the Automation code to accommodate for the failure
# *Failed log:*    [*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-aca_security_groups_policy.py-151-policytAcaValidations&begin=1468213&size=128778&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov16_11:10:58.515275.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-aca_security_groups_policy.py-151-policytAcaValidations&begin=1468213&size=128778&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov16_11:10:58.515275.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log: Day  is  Failing  sanity, we  don’t have  full  pass log  for ISE  above ISE 2.6 P 6* 
# *Failed DNAC version/ISO:DNAC*{{ 2374- 70284}}
# *Last Passed DNAC version/ISO:* 
# *Script/Mapping file Used: lansanity_usecases_maps.yaml*
# *TCs Impacted:* 1
# *'Re-run' TCs:* NA
# *Branch Used: private/HulkPatch2-ms/sanity_api_auto* 
# *DNAC Type:* On-Prem
# *Testbed details:* Sanity 
# *Team:* Solution Sanity 


Rerun Log  :   
[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov16_11:10:58.515275.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov16_11:10:58.515275.zip&atstype=ATS]

[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-11/env_auto_job.2023Nov16_12:00:07.236586.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-11/env_auto_job.2023Nov16_12:00:07.236586.zip&atstype=ATS]

!image-20231127-143535.png|width=1123,height=46!",2023-11-27T13:55:25.672+0000,,"['Auton', 'Hulk', 'HulkP2', 'Optimized', 'Sanity']",Tran Lam,Backlog,Omkar Sharad Wagh
SEEN-3078,https://miggbo.atlassian.net/browse/SEEN-3078,[Auton]:Hulk P2:[Optimized-Lan Script] Test_TC1_DNAC_Site_Building_Floor_Addition /test2_aduit_log_verify,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y 
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced:* 

Test_TC1_DNAC_Site_Building_Floor_Addition /test2_aduit_log_verify TC is getting stuck forever in loop . Continuing checking parent records due to this, not able to process the next set of UC executions. The sanity execution is blocked,  Could you please check on the priority?
Sanity  Hulk P2  2.1.714.70354 execution blocked  
*Aborted Jenkins  JOB =>*
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/874/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/874/]
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/873/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/873/]    ==>  *aborted job  after  2  hours*  
# *Failure/Errors snip from log:* 

{noformat}2023-11-27T10:55:47: %AETEST-INFO: The result of section test1_verify_creating_of_site_building_floor is => PASSED
2023-11-27T10:55:47: %API-GROUP-AUDIT_LOG-INFO: Offset count is 18432
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/dna/data/api/v1/event/event-series/audit-log/parent-records
2023-11-27T10:55:47: %API-GROUP-AUDIT_LOG-INFO: Offset count is 18456
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/dna/data/api/v1/event/event-series/audit-log/parent-records
2023-11-27T10:55:47: %AETEST-INFO: +------------------------------------------------------------------------------+
2023-11-27T10:55:47: %AETEST-INFO: |                   Starting section test2_aduit_log_verify                    |
2023-11-27T10:55:47: %AETEST-INFO: +------------------------------------------------------------------------------+
2023-11-27T10:55:47: %SERVICES-WARNING: Cannot track test: tracking auth info must be set in order to transfer test tracking data
2023-11-27T10:55:47: %SERVICES-INFO: Executing testcase Test_TC1_DNAC_Site_Building_Floor_Addition test 1.2 ""test2_aduit_log_verify"".
2023-11-27T10:55:47: %API-GROUP-AUDIT_LOG-INFO: Offset count is 18480
2023-11-27T10:55:47: %API-GROUP-ASSURANCE-INFO: Using the URL /assurance/v1/time
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/dna/data/api/v1/event/event-series/audit-log/parent-records
2023-11-27T10:55:47: %API-GROUP-UTILS-INFO: 
2023-11-27T10:55:47: %API-GROUP-UTILS-INFO: 
2023-11-27T10:55:47: %API-GROUP-UTILS-INFO:  api_switch_call called: 
2023-11-27T10:55:47: %API-GROUP-UTILS-INFO: {}
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/api/assurance/v1/time
2023-11-27T10:55:47: %API-GROUP-ASSURANCE-INFO: Current DNAC time is as follows {'version': '1.0', 'response': [{'timeType': 'GLOBAL', 'time': 1701110700000}, {'timeType': 'CLIENT', 'time': 1701110700000}, {'timeType': 'NETWORK', 'time': 1701110700000}, {'timeType': 'CURRENT', 'time': 1701111347544}, {'timeType': 'POE', 'time': 1701110400000}]}
2023-11-27T10:55:47: %API-GROUP-ASSURANCE-INFO: Time in the Unix Time format 1701111347544
2023-11-27T10:55:47: %SERVICES-INFO: {'USA': ['The site Global/USA was created', ['Child response is not generated']], 'India': ['The site Global/India was created', ['Child response is not generated']], 'SAN JOSE': ['The site Global/USA/SAN JOSE was created', ['Child response is not generated']], 'RTP': ['The site Global/USA/RTP was created', ['Child response is not generated']], 'BayAreaGuest': ['The site Global/USA/BayAreaGuest was created', ['Child response is not generated']], 'Bangalore': ['The site Global/India/Bangalore was created', ['Child response is not generated']], 'New York': ['The site Global/USA/New York was created', ['Child response is not generated']], 'SAN-FRANCISCO': ['The site Global/USA/SAN-FRANCISCO was created', ['Child response is not generated']], 'BERKELEY': ['The site Global/USA/BERKELEY was created', ['Child response is not generated']], 'Mantri Square': ['The site Global/India/Bangalore/Mantri Square was created', ['Child response is not generated']], 'BLD10': ['The site Global/USA/RTP/BLD10 was created', ['Child response is not generated']], 'BLD11': ['The site Global/USA/RTP/BLD11 was created', ['Child response is not generated']], 'BLD12': ['The site Global/USA/RTP/BLD12 was created', ['Child response is not generated']], 'BLD23': ['The site Global/USA/SAN JOSE/BLD23 was created', ['Child response is not generated']], 'BLD20': ['The site Global/USA/SAN JOSE/BLD20 was created', ['Child response is not generated']], 'BLD_GB': ['The site Global/USA/BayAreaGuest/BLD_GB was created', ['Child response is not generated']], 'BLDNYC': ['The site Global/USA/New York/BLDNYC was created', ['Child response is not generated']], 'BLD_SF': ['The site Global/USA/SAN-FRANCISCO/BLD_SF was created', ['Child response is not generated']], 'BLD_SF1': ['The site Global/USA/SAN-FRANCISCO/BLD_SF1 was created', ['Child response is not generated']], 'BLDBERK': ['The site Global/USA/BERKELEY/BLDBERK was created', ['Child response is not generated']]}
2023-11-27T10:55:47: %SERVICES-INFO: ACTION: Verify the audit response
2023-11-27T10:55:47: %API-GROUP-AUDIT_LOG-INFO: Action:: Parent records
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Connecting to the Apic-em northbound API client.{noformat}
{noformat}esponse is not generated']], 'BLD20': ['The site Global/USA/SAN JOSE/BLD20 was created', ['Child response is not generated']], 'BLD_GB': ['The site Global/USA/BayAreaGuest/BLD_GB was created', ['Child response is not generated']], 'BLDNYC': ['The site Global/USA/New York/BLDNYC was created', ['Child response is not generated']], 'BLD_SF': ['The site Global/USA/SAN-FRANCISCO/BLD_SF was created', ['Child response is not generated']], 'BLD_SF1': ['The site Global/USA/SAN-FRANCISCO/BLD_SF1 was created', ['Child response is not generated']], 'BLDBERK': ['The site Global/USA/BERKELEY/BLDBERK was created', ['Child response is not generated']]}
2023-11-27T10:55:47: %SERVICES-INFO: ACTION: Verify the audit response
2023-11-27T10:55:47: %API-GROUP-AUDIT_LOG-INFO: Action:: Parent records
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Connecting to the Apic-em northbound API client.
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/api/system/v1/identitymgmt/login
2023-11-27T10:55:47: %API-GROUP-AUDIT_LOG-INFO: Offset count is 18504
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/dna/data/api/v1/event/event-series/audit-log/parent-records
2023-11-27T10:55:47: %API-GROUP-AUDIT_LOG-INFO: Offset count is 18528
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/dna/data/api/v1/event/event-series/audit-log/parent-records
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/dna/data/api/v1/event/event-series/audit-log/parent-records
2023-11-27T10:55:47: %API-GROUP-AUDIT_LOG-INFO: Offset count is 18552
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/dna/data/api/v1/event/event-series/audit-log/parent-records
2023-11-27T10:55:47: %API-GROUP-AUDIT_LOG-INFO: Offset count is 18576
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/dna/data/api/v1/event/event-series/audit-log/parent-records
2023-11-27T10:55:47: %API-GROUP-AUDIT_LOG-INFO: Offset count is 18600
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/dna/data/api/v1/event/event-series/audit-log/parent-records
2023-11-27T10:55:47: %API-GROUP-AUDIT_LOG-INFO: Offset count is 18624
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/dna/data/api/v1/event/event-series/audit-log/parent-records
2023-11-27T10:55:47: %API-GROUP-AUDIT_LOG-INFO: Offset count is 18648
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/dna/data/api/v1/event/event-series/audit-log/parent-records
2023-11-27T10:55:47: %API-GROUP-AUDIT_LOG-INFO: Offset count is 18672
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/dna/data/api/v1/event/event-series/audit-log/parent-records
2023-11-27T10:55:47: %API-GROUP-AUDIT_LOG-INFO: Offset count is 18696
2023-11-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/dna/data/api/v1/event/event-series/audit-log/parent-records
1-27T10:55:47: %CLIENTMANAGER-INFO: Resource path full url: https://10.30.0.100/dna/data/api/v1/event/event-s{noformat}
# *Regression team debug analysis with details:After   pulling  latest  code  we  observed   this issue*  Looks like the script side needs avoid  the  loop 

Mrerged Pull request => 
[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c9252cc4f2fd3b57263337465b257acd92a5a202|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/c9252cc4f2fd3b57263337465b257acd92a5a202]

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3df1761ee0910c27270562023b01fd8d7c838d25#services/dnaserv/lib/api_groups/ap_profile/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/3df1761ee0910c27270562023b01fd8d7c838d25#services/dnaserv/lib/api_groups/ap_profile/group.py]
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
Need to fix the Automation code to accommodate for the failure
# *Failed log:*  
Aborted Jenkins  JOB =>
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/874/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/874/]
[https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/873/|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-Deployment_plus_Optimized-Sanity/873/]    ==>  aborted job  after  2  hours  
# *Last passed log:* 
HULK P2  2.1.714.70334  
Passlog  
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-site_heirarchy_design.py-41-designSiteHeirarchySitesBuildingsAndFloors&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_06:50:08.195070.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-site_heirarchy_design.py-41-designSiteHeirarchySitesBuildingsAndFloors&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov22_06:50:08.195070.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Failed DNAC version/ISO:DNAC   2.3.7.4-70354*
# *Last Passed DNAC version/ISO:  2.3.7.4-70334*
# *Script/Mapping file Used: lansanity_usecases_maps.yaml*
# *TCs Impacted:* execution  is  blocked  
# *'Re-run' TCs:* NA
# *Branch Used: private/HulkPatch2-ms/sanity_api_auto* 
# *DNAC Type:* On-Prem
# *Testbed details:* Sanity 
# *Team:* Solution Sanity ",2023-11-27T19:19:29.369+0000,"Hi Team,
Build: Hulk P2
Version: 2.3.7.4-70354 + FQDN + Delay
Script used: CERT LAN script
Branch: private/HulkPatch2-ms/sanity_delay_testing
Jenkins Job: [https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-sanity-common-Multi-job/1684/console|https://engci-private-sjc.cisco.com/jenkins/sol-eng/job/SOL-SAN/job/Hulk/job/Hulk-sanity-common-Multi-job/1684/console]
We are hitting the same issue in TC6_DNAC_Prime_Migration_DNAC_Server_Settings_NTP_DHCP_AAA_DNS_settings/test31_aduit_log_verify getting stuck forever in the loop
Due to this, our executions are blocked, Could you please check this on priority Hi  [~accountid:62d2fe9f8afb5805e5d5af49]  / [~accountid:62ab7a399cd13c0068b18fe0]  ,

We have filed a defect for the audit log generation issue.  Created space with DE for defect discussion. Please find the details below:

=> [https://cdetsng.cisco.com/webui/#view=CSCwi32725|https://cdetsng.cisco.com/webui/#view=CSCwi32725]
webEX space=> [https://eurl.io/#T3g55nR2s|https://eurl.io/#T3g55nR2s]
 Moving to block state as no changes require from script specific until the product bug closure (CSCwi32725) Hi  [~accountid:62d2fe9f8afb5805e5d5af49] / [~accountid:5fe224a53b5e47013862f185] ,
as per DE  CSCwi32725) code was merged on December 1st.

   space=> webexteams://im?space=8e73d8c0-8dc3-11ee-9fc8-39c19c3095af 
As confirmed by the audit log team, API in Hulk P2 was recently changed.  could  you please  check  [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8308/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8308/overview]","['Auton', 'Blocked', 'Hulk', 'HulkP2', 'Optimized', 'Sanity']",Vinoth Kumar Kutty Krishnamoorthy,Resolved,Omkar Sharad Wagh
SEEN-3080,https://miggbo.atlassian.net/browse/SEEN-3080,"replace ""noHealthDevices"" with ""notApplicableDevices"" category for Test_TC12_verify_fabric_360  /   test1_verify_fabric_SD_access","As per discussion with the Development team, code needs an update Test_TC12_verify_fabric_360 / test1_verify_fabric_SD_access use-case,
where ""*noHealthDevices*"" should get replaced with ""*notApplicableDevices*"" category to list all devices that are not supported to have fabric heath published.

Failed log: [test1_verify_fabric_SD_access|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_fabric_assurance.py-155-SDAFabricAssurance&begin=1305492&size=146891&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov23_01:58:07.636647.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

{noformat}5535:  AP count is : 5
5536:  fabric_health_summary_response: 0=5,2=2,100.0=100.0
5537:  noHealthDevices:0=5, monitoredHealthyDevices:2=2, latestHealthScore:100.0=100.0
5538:  Un structure data from '/assurance/v1/network-device'  API response
5539:  Test returned in 0:00:04.524998
5540:  Failed reason: Unable to verify_fabric_SD_access parameters
5541:  The result of section test1_verify_fabric_SD_access is => FAILED{noformat}

WebEx space for discussion: webexteams://im?space=67048720-3752-11ee-bb22-d917cacd7322",2023-11-27T20:03:38.349+0000,"Raised PR for HulkPatch2 branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8104/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8104/overview] PR has been approved and merged to *private/HulkPatch2-ms/api-auto* branch. PR Raised for Ghost branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8167/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8167/overview] Hitting the same issue still in ExSI Hulk Patch2 RC1 OVA# 3.714.75298 build

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec13_08:10:57.999333.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec13_08:10:57.999333.zip&atstype=ATS]",['Auton'],Amardeep Kumar,Closed,Amardeep Kumar
SEEN-3087,https://miggbo.atlassian.net/browse/SEEN-3087,[Auton]:Hulk P2:[Optimized] Test_TC7_validate_AP_KPI_graphs,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced:*Below TC failed with a KeyError, as discussed over the Webex. Some non-fabric PRs are merged. Could you please check and fix this issue ASAP?   

[https://miggbo.atlassian.net/browse/SEEN-2389|https://miggbo.atlassian.net/browse/SEEN-2389|smart-link] 


!image-20231128-134725.png|width=417,height=145!
# *Failure/Errors snip from log:* 

{noformat}5853:  Calculating If the required result found  in the Response!!
5854:  Result Generated is ['29', '31', '27', '29', '29', '32', '30', '31', '33', '31', '32', '30', '32', '33', '33', '29', '29', '26', '28', '28', '29', '31', '29', '28', '26', '31', '31', '29', '31', '32', '32', '32', '30', '31', '30', '30', '32', '27', '30', '30', '26', '25', '26', '29', '33', '32', '29', '67', '66', '38', '38', '42', '43', '37', '41', '35', '0', '36', '33', '37', '33', '35', '38', '39', '36', '35', '40', '40', '38', '35', '38', '37', '39', '36', '40', '31', '36', '40', '36', '32', '28', '31', '35', '33', '34', '35', '37', '36', '36']
5855:  Success Verified Radio Utilization on AP AP78BC.1A00.4B28 on Slot 1
5856:  Traceback (most recent call last):
5857:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/commonlibs/test_wrapper.py"", line 301, in wrapper
5858:      result = testfunc(func_self, **kwargs)
5859:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/testcases/sanityusecases/assuranceHealthMetrics/assurance_health_metric.py"", line 218, in test3_verify_AP_radio_utilization
5860:      if dnac_handle.verify_AP_radio_utilization():
5861:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/decorators.py"", line 32, in wrapper
5862:      result = method(*args, **kwargs)
5863:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-SAN/Hulk/Hulk-Deployment_plus_Optimized-Sanity/services/dnaserv/lib/api_groups/assurance/group.py"", line 4487, in verify_AP_radio_utilization
5864:      if ""9130AXE"" in ap[name]['series']:
5865:  KeyError: 'AP380E.4D93.6C4E'
5866:  Test returned in 0:00:06.868305
5867:  Errored reason: AP380E.4D93.6C4E{noformat}
# *Regression team debug analysis with details:* Looks like the script side needs modifications in the automation script.
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
Need to fix the Automation code to accommodate for the failure
# *Failed log:* 

  [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-171-assuranceHealthMetrics&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov23_06:17:28.774282.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-171-assuranceHealthMetrics&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov23_06:17:28.774282.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]
# *Last passed log:* 
Hulk  2.3.7.4-70334  

We got a pass log from the private branch. Please try to fix the main branch code.


[https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-11/env_auto_job.2023Nov24_02:41:44.291108.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-11/env_auto_job.2023Nov24_02:41:44.291108.zip&atstype=ATS]


# *Failed DNAC version/ISO:* Hulk Patch2 - 2.1.714.70344
# *Last Passed DNAC version/ISO: Hulk Patch2 - 2.1.714.70344*
# *Script/Mapping file Used:     lansanity_usecases_maps.yaml*
# *TCs Impacted:* 7 sub TCs Impacted  
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/sanity_api_auto
# *DNAC Type:* On-Prem
# *Testbed details:* Sanity 
# *Team:* Solution Sanity ",2023-11-28T13:39:07.801+0000,"Hulk  P1  RC1  2374-70391 
Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-171-assuranceHealthMetrics&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec11_02:01:20.336879.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_health_metric.py-171-assuranceHealthMetrics&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec11_02:01:20.336879.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]




!image-20231211-150555.png|width=425,height=457!","['Auton', 'Hulk', 'HulkP2', 'Optimized', 'Sanity']",ThangQuoc Tran,Backlog,Omkar Sharad Wagh
SEEN-3089,https://miggbo.atlassian.net/browse/SEEN-3089,[AUTON][NFW]-Create_model_config failing while doing device provisioning,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Yes.
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced:* while doing device provisioning create model config has not created 
# *Failure/Errors snip from log: 2398: Error Code: 500 for*
*2399: URL:https://10.4.23.12/api/v2/named-capability/design Data:{'timeout': 60, 'data': '{""flags"": 0, ""isDeletable"": true, ""capabilityName"": ""Multicast_Configuration"", ""designName"": ""custom_multicast"", ""designContent"": ""[{\\""@class\\"": \\""com.cisco.xmp.config.intf.service.ConnectedModelGraphSpecification\\"", \\""connectedModelGraph\\"": {\\""@class\\"": \\""com.cisco.server.managedobjects.bridge.MulticastConfig\\"", \\""globalMulticastEnabled\\"": true, \\""multicastMode\\"": 2, \\""multicastAddress\\"":{\\""@class\\"":\\""com.cisco.xmp.model.framework.primitive.InetAddress\\"",\\""address\\"":\\""224.1.0.0\\""},\\""multicastIpv6Mode\\"": 1, \\""__lockedAttributes\\"": [\\""globalMulticastEnabled\\"", {""response"":{""errorCode"":""NCNC10001"",""message"":""NCNC10001: Model Config Design custom_multicast already exists"",""href"":""/apic-em-network-programmer-service/named-capability/design""},""version"":""1.0""}*
*2400: Traceback (most recent call last):*
*2401: File ""/auto/dna-sol/ws/HulkPatch2/dnac-auto/services/dnaserv/client_manager.py"", line 326, in call_api*
*2402: response.raise_for_status()*
*2403: File ""/ws/divayada-sjc/Solution_pyatsenv/lib/python3.6/site-packages/requests/models.py"", line 960, in raise_for_status*
*2404: raise HTTPError(http_error_msg, response=self)*
*2405: requests.exceptions.HTTPError: 500 Server Error: for url:* [*https://10.4.23.12/api/v2/named-capability/design?validateSchema=False*|https://10.4.23.12/api/v2/named-capability/design?validateSchema=False]
*2406: Encountered unhandled HTTPError in Internal API Call*
*2407: Flagging result as FAIL!*
*2408: Reason: 500 Server Error: for url:* [*https://10.4.23.12/api/v2/named-capability/design?validateSchema=False*|https://10.4.23.12/api/v2/named-capability/design?validateSchema=False]

# *Regression team debug analysis with details:* 
On debugging further manually, we have checked on cluster Tools->model config editor “Advanced_SSID_Configuration” has created custom_ssid but under “Multicast config” not created 
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
Need to check further of reason for the failure and also fix the reason for failure.
# *Failed log:* [*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=352152&size=67845&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_nfw_job.2023Nov28_01:15:25.665690.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=352152&size=67845&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_nfw_job.2023Nov28_01:15:25.665690.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS][*Test*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=351591&size=69544&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_nfw_job.2023Nov28_01:15:25.665690.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]
# [*_TC28_DNAC_Device_Provisioning*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=351591&size=69544&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_nfw_job.2023Nov28_01:15:25.665690.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]  */   test1_create_model_config  (Failed)*  
# *Last passed log:* [*https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5538142&size=364329&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov15_08:54:07.909748.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=admin&from=trade&view=all&atstype=pyATS*|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5538142&size=364329&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_auto_job.2023Nov15_08:54:07.909748.zip&ats=%2Fauto%2Fdna-sol%2Fpyats-ws%2Fpyats-phannguy-seen-53&submitter=admin&from=trade&view=all&atstype=pyATS] 
# *Failed DNAC version/ISO:* Hulk Patch2 - 2.1.714.70354
# *Last Passed DNAC version/ISO:* Hulk Patch2-70284
# *Script/Mapping file Used: testcases/forty_eight_hour/solution_dnac_wireless_hardening.py*
# *TCs Impacted:* 4
# *'Re-run' TCs:* NA
# *Branch Used:* private/Hulk-ms/api-auto
# *DNAC Type:* On-Prem
# *Testbed details:* Base Automation(NFW)
# *Team:* Solution Regression",2023-11-29T03:30:18.024+0000,"Hi [~accountid:712020:06823340-9d28-4722-9cd7-15ef3ea14cc6] ,
In Test_TC28_DNAC_Device_Provisioning - test1_create_model_config, we can only run it once. And if run again, it will have error {{""message"":""NCNC10001: Model Config Design custom_ssid already exists""}} And with test for Hulk Patch2, you can use ""Private/HulkPatch2-ms/api-auto"" branch Hi Dat,

Okay sure, Thanks for have a look. Hi Dat,

As you suggested, for Hulk Patch2 i am using ""private/HulkPatch2-ms/api-auto"" this branch , here also we are observing the same issue . Could you please have a look and fix the issue.

please find the failed log below

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1665280&size=18206&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fsr_nfw_job.2023Dec06_02:22:45.419986.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1665280&size=18206&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fsr_nfw_job.2023Dec06_02:22:45.419986.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Execution', 'NFW']",DatChi Pham,Pending Code Review,JagadeshKumar Enapanuri
SEEN-3090,https://miggbo.atlassian.net/browse/SEEN-3090,[Auton][MSTB2] - Test_TC94_dcs_magellan_setup_services  /   test1_set_ISE_FQDN_DNSlookup,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* Y
# *Issue faced:*  Script failed to add FQDN for some cluster nodes. Script expecting “$” promt for the command “magctl service attach ise-bridge”  
# *Failure/Errors snip from log:* 
499882:  [Wed Nov 29 16:54:38 UTC] [maglev@169.254.6.66|mailto:maglev@169.254.6.66] (maglev-master-169-254-6-66) ~
499883:  $ PS1='[PEXPECT]\$ '
499884:  Successfully connected to cluster unsecure bash shell.
499885:  Connection opened.
499886:  Sending line: {{magctl service attach ise-bridge}}
499887:  Parsed timeout: sending interrupt.
499888:  Timeout exceeded.
499889:  Method {{add_static_hosts_lookup_for_ise_collector}} returned successfully.
499890:  Test returned in 0:01:04.763919
499891:  Failed reason: Failed to configure FQDN Lookup for ISE Collector
499892:  The result of section test1_set_ISE_FQDN_DNSlookup is => FAILED
# *Regression team debug analysis with details:* The cli command “magctl service attach ise-bridge” is getting timed out as it is expecting “$” prompt whereas in ESXI we are getting “#” prompt for the same. Need to modify the cli command to use “kubectl get pods -n ise-bridge” for the script to work.
# *DE analysis/confirmation with details:* As confirmed by Julian Justin the cli command needs to be changed to *“*kubectl get pods -n ise-bridge” for the expected output.
# *What do Automation team need to look into?* Need to change the cli from “magctl service attach ise-bridge” to “kubectl get pods -n ise-bridge” for the script to work.
# *Failed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=97703232&size=9289&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov28_22:29:55.971434.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=97703232&size=9289&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov28_22:29:55.971434.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* Tried manually. Working fine.
# *Failed DNAC version/ISO:* 3.714.75242 with 17.13.1PRD6 FC1
# *Last Passed DNAC version/ISO:* Manually working fine in all releases.
# *Script/Mapping file Used:* solution_test_3sites_sjc_nyc_sf.py
# *TCs Impacted:* 2
# *'Re-run' TCs: NA*
# *Branch Used:* private/HulkPatch2-ms/api-auto
# *DNAC Type:* ESXI
# *Testbed details:* MSTB2
# *Team:* Solution Regression",2023-11-30T04:39:37.963+0000,Can you provide the last passed log? I will hep to analyze the issue.,"['Auton', 'ESXi', 'HulkPatch2', 'MSTB2', 'Multisite', 'Uplift']",Tran Lam,Backlog,Manjushree Saligrama
SEEN-3094,https://miggbo.atlassian.net/browse/SEEN-3094,[Auton] - Object attribute errors related to wlc seen during AP Negative Operations Feature TC Execution,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* NA. Feature Integration
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced:* When integrating Halleck specific feature of AP Negative scenarios TC we see object attribute errors related to wlc during validation of rogue param.
# *Failure/Errors snip from log:* 
22698:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/services/commonlibs/test_wrapper.py"", line 301, in wrapper
22699:      result = testfunc(func_self, **kwargs)
22700:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py"", line 31833, in test11_validate_rogue_param
{color:#ff5630}22701:      if not dnac_handle.validate_rogue_param(wlc=self.wlc_name, reportInterval=reportInterval, minTransientTime=minTransientTime, minRssi=minRssi):{color}
{color:#ff5630}22702:  AttributeError: 'Test_TC263_APs_negative_operations' object has no attribute 'wlc_name'{color}
# *Regression team debug analysis with details:* 
We see wlc attributes are not handled. Hence the failure.
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
Need to fix the object attribute error.
# *Failed log:* [Test_TC263_APs_negative_operations|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2428562&size=5356632&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov29_07:54:55.807252.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] → [test11_validate_rogue_param|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5926319&size=4222&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov29_07:54:55.807252.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* NA. Feature Integration. 
# *Failed DNAC version/ISO:* Hulk Patch2 - 2.1.714.70354
# *Last Passed DNAC version/ISO:* NA. Feature Integration.
# *Script/Mapping file Used:* testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
# *TCs Impacted:* 1
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/api-auto
# *DNAC Type:* On-Prem
# *Testbed details:* MSTB1
# *Team:* Solution Regression",2023-11-30T10:16:15.368+0000,"[~accountid:62d2fec15d6f5fd2c3db8f9f], required PR has been raised, approved and merged: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8152/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8152/overview] to *private/HulkPatch2-ms/api-auto* branch.

Please re-test the AP Negative operations test-case and update the result.","['Auton', 'Halleck', 'HulkPatch2', 'Integration', 'MSTB1', 'Multisite']",Amardeep Kumar,Closed,SANDEEP SHIVARAMAREDDY
SEEN-3095,https://miggbo.atlassian.net/browse/SEEN-3095,[Auton] Hulk P2 EXSI [Single interface TB] TC1_DNAC_External_Authentication_Radius/ test4 connect_external_auth failed due to 401 Client Error,"In EXSI HULK VA Patch2 build #3.714.75242 Build [Single interface setup] observed  the TC1_DNAC_External_Authentication_Radius/ test4 connect_external_auth failed due to 401 Client Error Unauthorized for url: [https://85.1.1.61/api/system/v1/auth/token|https://85.1.1.61/api/system/v1/auth/token]

With two interface setup working fine whereas single interface setup getting 401 Client Error: 

*Failed TC* : - Test_TC1_DNAC_External_Authentication_Radius  /   test4_connect_external_auth

Failed due to 401 Client Error: Unauthorized for url: [https://85.1.1.61/api/system/v1/auth/token|https://85.1.1.61/api/system/v1/auth/token]

Regression Team Actions Checklist:

Debugged/Analyzed the issue: Y

With interface setup working fine whereas single interface setup getting 401 Client Error: Unauthorized for the url: [https://85.1.1.61/api/system/v1/auth/token|https://85.1.1.61/api/system/v1/auth/token]

Compared with the last passed log: Y

Checked and compared the configuration: Y

Confirmed with DE on the root cause: N

Issue faced:

Failure/Errors snip from log:  401 Client Error Unauthorized for url: [https://85.1.1.61/api/system/v1/auth/token|https://85.1.1.61/api/system/v1/auth/token]

Regression team debug analysis with details:

DE analysis/confirmation with details:NA

What do Automation team need to look into?

*Failed log:*  
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-external_authentication_radius.py-201-externalAuthentication&begin=81035&size=38768&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov29_07:25:44.697481.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-external_authentication_radius.py-201-externalAuthentication&begin=81035&size=38768&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov29_07:25:44.697481.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Last passed log:*  

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-external_authentication_radius.py-201-externalAuthentication&begin=86505&size=62611&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct23_09:31:12.123183.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-external_authentication_radius.py-201-externalAuthentication&begin=86505&size=62611&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct23_09:31:12.123183.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Failed DNAC version/ISO:   3.714.75242

Last Passed DNAC version/ISO:  3.713.75176

Script/Mapping file Used:  lansanity_usecases_maps.yaml

TCs Impacted: 1

'Re-run' TCs:rerun also same issue

Branch Used:  private/HulkPatch2-ms/api-auto

DNAC Type:  EXSI VM

Testbed details: TB4
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4]

Team: EXSI Sanity",2023-11-30T13:39:27.389+0000,"Amar can you fix it, update the url accordingly to platform type. ","['Auton', 'ESXi', 'Sanity']",Amardeep Kumar,Backlog,Raghavendrachar Baraguru Mallesha Char
SEEN-3096,https://miggbo.atlassian.net/browse/SEEN-3096,[Auton] - Assign tag to device fails with value return errors,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* NA. Feature Integration
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced:* When integrating Halleck specific feature of ISE NDG change in configuration, we see Value return related errors corresponding assign tag to device.
# *Failure/Errors snip from log:* 
13050:  Traceback (most recent call last):
13051:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/services/commonlibs/test_wrapper.py"", line 301, in wrapper
13052:      result = testfunc(func_self, **kwargs)
13053:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/MSTB1 MDNAC DR/testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py"", line 32518, in test1_assign_tag_to_device
13054:      result, self.device, self.tag_name = dnac_handle.assign_tag_to_device(""NDG:Location#All Locations#CHN"")
{color:#ff5630}13055:  ValueError: not enough values to unpack (expected 3, got 2){color}
# *Regression team debug analysis with details:* 
Looks like multiple return values are not being handled properly.
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
Need to fix the return values.
# *Failed log:* [Test_TC264_configure_device_into_specific_NDG|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2488333&size=323293&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov30_04:52:24.795114.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* NA. Feature Integration. 
# *Failed DNAC version/ISO:* Hulk Patch2 - 2.1.714.70354
# *Last Passed DNAC version/ISO:* NA. Feature Integration.
# *Script/Mapping file Used:* testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
# *TCs Impacted:* 1
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/api-auto
# *DNAC Type:* On-Prem
# *Testbed details:* MSTB1
# *Team:* Solution Regression",2023-11-30T17:46:00.561+0000,"PR-link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8163/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8163/overview] We are experiencing the same issue with the  sanity  optimization script 
*Hulk P2 RC1 Failed log:*
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_tag_config_in_ISE_NDG.py-178-ISENDG_tagDeviceInISENDG&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec11_02:01:20.336879.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-verify_tag_config_in_ISE_NDG.py-178-ISENDG_tagDeviceInISENDG&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec11_02:01:20.336879.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]","['Auton', 'Halleck', 'HulkP2', 'HulkPatch2', 'Integration', 'MSTB1', 'Multisite', 'Optimized', 'Sanity']",QuangVinh Nguyen,Pending Code Review,SANDEEP SHIVARAMAREDDY
SEEN-3098,https://miggbo.atlassian.net/browse/SEEN-3098,[Auton] - AP reachability check after disconnect and after connect back failing falsely,"# *Regression Team Actions Checklist:*
# *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* NA. Feature Integration
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced:* When integrating Halleck specific feature of AP maintenance mode, we see reachability check for AP on DNAC after AP disconnect and after AP connect are failing with reason that the states are not changed as expected.
# *Failure/Errors snip from log:* 
21165:  Library group ""Compilance"" method ""check_device_reachability"" returned in 0:00:00.071527
21166:  Test returned in 0:20:01.533880
{color:#ff5630}21167:  Failed reason: Result: AP is in reachable state after 10 minutes.{color}
22360:  Library group ""Compilance"" method ""check_device_reachability"" returned in 0:00:00.159551
22361:  Test returned in 0:19:14.393001
{color:#ff5630}22362:  Failed reason: Result: AP is in unreachable state after 10 minutes.{color}
# *Regression team debug analysis with details:* 
On Trying to check this issue manually, we observed that the issue the states did change properly as per expected. We see AP went unreachable for 20 mins during disconnect and AP was back to reachable after AP connect back. Please refer the below attached HAR for corresponding AP 360 page and video snip of AP 360 page. 
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
Need to relook the code to check of reachability check is implemented properly.
# *Failed log:* [Test_TC265_enable_maintenance_mode_on_aps_and_validate_maintenance_mode|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=2480461&size=2346396&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov27_02:30:17.565294.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] → [test24_verify_device_reachability_after_disconnect|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4548676&size=13907&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov27_02:30:17.565294.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] → [test27_verify_device_reachability_after_connect|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=4741600&size=85057&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov27_02:30:17.565294.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* NA. Feature Integration. 
# *Failed DNAC version/ISO:* Hulk Patch2 - 2.1.714.70296
# *Last Passed DNAC version/ISO:* NA. Feature Integration.
# *Script/Mapping file Used:* testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
# *TCs Impacted:* 1
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/api-auto
# *DNAC Type:* On-Prem
# *Testbed details:* MSTB1
# *Team:* Solution Regression",2023-11-30T18:43:48.466+0000,,"['Auton', 'HulkPatch2', 'Integration', 'MSTB1', 'Multisite']",Majlona 'Luna' Aliaj,Backlog,SANDEEP SHIVARAMAREDDY
SEEN-3099,https://miggbo.atlassian.net/browse/SEEN-3099,[Auton] Hulk P2 EXSI [Single interface TB] Test_TC1_template_conflicts_in_template_hub / test17_add_new_site_tag_to_a_wireless_device failed due to ValueError,"In *EXSI HULK VA Patch2 build #3.714.75242 Build [Single interface setup]* observed the Test_TC1_template_conflicts_in_template_hub / test17_add_new_site_tag_to_a_wireless_device failed due to ValueError: not enough values to unpack (expected 3, got 2)

Failed TC : - Test_TC1_template_conflicts_in_template_hub / *test17_add_new_site_tag_to_a_wireless_device*

  ValueError: not enough values to unpack (expected 3, got 2)

Regression Team Actions Checklist:

Debugged/Analyzed the issue: Y

Compared with the last passed log: Y

Checked and compared the configuration: Y

Confirmed with DE on the root cause: N

Issue faced:

Failure/Errors snip from log:  ValueError: not enough values to unpack (expected 3, got 2)

Regression team debug analysis with details:

DE analysis/confirmation with details:NA

What do Automation team need to look into?

*Failed log:*  
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-templatehub.py-271-toolsTemplateHub&begin=2108483&size=273188&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov29_07:25:44.697481.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-templatehub.py-271-toolsTemplateHub&begin=2108483&size=273188&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov29_07:25:44.697481.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Last passed log*: with two interface setup test17 passed 

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-templatehub.py-271-toolsTemplateHub&begin=2377420&size=189939&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct10_19:15:39.384070.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-templatehub.py-271-toolsTemplateHub&begin=2377420&size=189939&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct10_19:15:39.384070.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Failed DNAC version/ISO:   3.714.75242

Last Passed DNAC version/ISO:  3.713.75159

Script/Mapping file Used:  lansanity_usecases_maps.yaml

TCs Impacted: 1

'Re-run' TCs:rerun also same issue

Branch Used:  private/HulkPatch2-ms/api-auto

DNAC Type:  EXSI VM

Testbed details: TB4
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4]

Team: EXSI Sanity",2023-12-01T07:38:06.993+0000,"Hi [~accountid:63f50bcafb3ac4003fa2c6dd] 
Executed TC with latest commits observed the *test16, test18 & test20* Failed due to {{Config preview flow}} for ECA & EWLC device, Checked manually on the cluster config preview working  fine for the both devices no issues seen

*Failed TC names:*

test16_validate_unexpected_configuration_not_found_in_config_preview

test18_re_provision_the_wireless_device_and_validate_configuration_template_not_be_pushed 

test20_re_provision_the_edge_device_and_validate_configuration_template_not_be_pushed



*Trade log:*

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec04_22:19:49.817761.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec04_22:19:49.817761.zip&atstype=ATS] After checking the annotate with Git Blame, I saw some mistake changes in a library that I used in my test case when merging “NFW Merging to mainline“ commit. It makes the subtest get the error: ValueError: not enough values to unpack.

!image-20231208-094707.png|width=1896,height=898! Hi Raghavendrachar, About the Failed with test16, 18, 19. They are the same error as test28 - Archana’s test case, which also uses the old existing library “{{Provision Device}}“. 

Follow the log, your cluster gets the fail when you run any test case do task ‘provision device’, not just only my test case. It seems your cluster has a problem with {{Config Preview}}. Could you check it? # PR HulkPatch2-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8250/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8250/overview]
# Test Case:  {{Test_TC212_template_conflicts_in_template_hub}}
# Branch Used*:* private/HulkPatch2-ms/api_auto
# Testbed Used: NFW2
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link HulkPatch2-ms/api_auto: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-12/nfw3_job.2023Dec08_01:44:03.618148.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-12/nfw3_job.2023Dec08_01:44:03.618148.zip&atstype=ATS] PR was merged.","['Auton', 'ESXi', 'Sanity']",NhanHuu Nguyen,Resolved,Raghavendrachar Baraguru Mallesha Char
SEEN-3101,https://miggbo.atlassian.net/browse/SEEN-3101,[Auton]HulkPatch2:Test_TC86_generate_exceutive_summary_report/test1_generate_exceutive_summary_report,"Regression Team Actions Checklist:

Debugged/Analyzed the issue: Y

Compared with the last passed log: Y

Checked and compared the configuration: Y

Confirmed with DE/AE on the root cause: 

Issue faced: We are observing the scripts to be failing for Report Generation in Hulk P2 - 70354

Failure/Errors snip from log: 179928: 
 Timeout: 400 seconds while waiting for report generation to complete.

{noformat}179929: 
 [{'clientId': 'default0',{noformat}

{noformat}179930: 
   'dataCategory': 'Executive',{noformat}

{noformat}180130: 
   'viewGroupId': 'd7afe5c9-4941-4251-8bf5-0fb643e90888',{noformat}

{noformat}180131: 
   'viewGroupVersion': '2.0.0'}]{noformat}

{noformat}180132: 
 Library group ""assurance"" method ""generate_exceutive_summary_report"" returned in 0:07:07.546289{noformat}

{noformat}180133: 
 Test returned in 0:07:07.654338{noformat}

{noformat}180134: 
 Failed reason: Unable to generate Data Report{noformat}

Snippets of the Reports being Generated on the cluster:


!image-20231201-095714.png|width=1913,height=932!

!image-20231201-095735.png|width=1920,height=925!

Regression team debug analysis with details: We suspect this is a timing issue and need more time for the report generation to complete

DE analysis/confirmation with details:  

What do Automation team need to look into? : Needs more sleep time to be added 

Failed log: [test1_generate_exceutive_summary_report|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=37312788&size=342102&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov29_07:47:13.401383.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS], [test2_generate_Worst_Interferers_CSV_report|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12202645&size=307969&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov30_00:29:10.195048.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS], [test3_generate_Worst_Interferers_JSON_report|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=12510614&size=315341&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov30_00:29:10.195048.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS],

Last passed log: [Test_TC86_generate_exceutive_summary_report|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=31025162&size=1809447&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov25_00:54:25.885500.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS], [Test_TC177_generate_Worst_Interferers_report|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19779979&size=333760&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov25_19:05:45.101571.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS], [Test_TC188_generate_Port_Reclaim_report|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=22509891&size=252075&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov25_19:05:45.101571.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Failed DNAC version/ISO:2.3.7.4-70354

Last Passed DNAC version/ISO: 2.1.713.70332

Script/Mapping file Used: Cert-Lan Script

TCs Impacted: TC86_generate_exceutive_summary_report/test1_generate_exceutive_summary_report
			TC177_generate_Worst_Interferers_report/test2_generate_Worst_Interferers_CSV_report
			TC177_generate_Worst_Interferers_report/test3_generate_Worst_Interferers_JSON_report

'Re-run' TCs: NA

Branch Used: private/HulkPatch2-ms/sanity_delay_testing

DNAC Type: On-Prem

Testbed details:[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2] 

Team: Sanity",2023-12-01T09:40:31.729+0000,"This was due to Defect 

|[CSCwi32177|https://cdetsng.cisco.com/webui/#view=CSCwi32177]|DNAC ""Worst Interferers"" report cannot be created successfully|

This needs to be fixed","['Auton', 'HulkPatch2', 'sanity']",DeepakPratap Shinde,Closed,DeepakPratap Shinde
SEEN-3103,https://miggbo.atlassian.net/browse/SEEN-3103,Test_TC185_verify_kairos_baseline  /   test1_verify_kairos_baseline,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* Y
# *Issue faced:* The script side needs enhancement. As per the expected behavior the 1st day of every month will not be fetching the data on baseline and heatmap. If the check falls on 1st day of any month, the data has to be pulled from previous 2 or 3 days.
# *Failure/Errors snip from log:* api_switch_call called:
10089:  {'data': {'operationName': 'onboardingBaselineAggregated', 'variables': {'startTs': '{color:#ff5630}2023-12-0{color}', 'endTs': '{color:#ff5630}2023-12-0{color}'}, 'query': 'query onboardingBaselineAggregated($startTs: Datetime!, $endTs: Datetime!) {\n  onboarding36BaselineAggregated(startTs: $startTs, endTs: $endTs) {\n    nodes {\n      siteId\n      aesSsid\n      aesWlcId\n      numSsids\n      numWlcIds\n      numOnboardings\n      numIssues\n      avgDistinctClientCount\n      maxDistinctClientCount\n      sumDistinctClientCount\n      avgOnboardingTimeSecs\n      avgHasFailed\n      dhcpTime\n      assocTime\n      authTime\n      numAssocFailures\n      groupHierarchyArray\n      __typename\n    }\n    __typename\n  }\n}\n'}}
10090:  Resource path full url: [https://204.192.1.115/api/kairos/v1/proxy/api/v2/core-services/customer-id/kairos/graphql|https://204.192.1.115/api/kairos/v1/proxy/api/v2/core-services/customer-id/kairos/graphql]
10091:  Onboarding36BaselineAggregatedTypesConnection is not received
10092:  Library group ""assurance"" method ""verify_kairos_baseline"" returned in 0:00:00.316341
10093:  Library group ""assurance"" method ""verify_kairos_baseline"" returned in 0:00:03.308755
10094:  Test returned in 0:00:05.355952
10095:  Failed reason: Kairos baseline events failed
10096:  The result of section test1_verify_kairos_baseline is => FAILED
# *Failure/Errors snip from log:* 
api_switch_call called:
10089:  {'data': {'operationName': 'onboardingBaselineAggregated', 'variables': {'startTs': '{color:#ff5630}2023-12-0{color}', 'endTs': '{color:#ff5630}2023-12-0{color}'}, 'query': 'query onboardingBaselineAggregated($startTs: Datetime!, $endTs: Datetime!) {\n  onboarding36BaselineAggregated(startTs: $startTs, endTs: $endTs) {\n    nodes {\n      siteId\n      aesSsid\n      aesWlcId\n      numSsids\n      numWlcIds\n      numOnboardings\n      numIssues\n      avgDistinctClientCount\n      maxDistinctClientCount\n      sumDistinctClientCount\n      avgOnboardingTimeSecs\n      avgHasFailed\n      dhcpTime\n      assocTime\n      authTime\n      numAssocFailures\n      groupHierarchyArray\n      __typename\n    }\n    __typename\n  }\n}\n'}}
10090:  Resource path full url: [https://204.192.1.115/api/kairos/v1/proxy/api/v2/core-services/customer-id/kairos/graphql|https://204.192.1.115/api/kairos/v1/proxy/api/v2/core-services/customer-id/kairos/graphql]
10091:  Onboarding36BaselineAggregatedTypesConnection is not received
10092:  Library group ""assurance"" method ""verify_kairos_baseline"" returned in 0:00:00.316341
10093:  Library group ""assurance"" method ""verify_kairos_baseline"" returned in 0:00:03.308755
10094:  Test returned in 0:00:05.355952
10095:  Failed reason: Kairos baseline events failed
10096:  The result of section test1_verify_kairos_baseline is => FAILED
# *Regression team debug analysis with details:* Onboarding36BaselineAggregatedTypesConnection is not received. The variables start date and end date not being fetched properly on the 1st day of every month. As per the expected behaviour the script needs enhancement to pull the data 2 or 3 days prior if the check falls on 1st day of the month.
# *DE analysis/confirmation with details:* As per the expected behavior the 1st day of every month will not be fetching the data on baseline and heatmap. If the check falls on 1st day of any month, the data has to be pulled from previous 2 or 3 days.
# *What do Automation team need to look into?* Need to fix the script as per the expected behaviour. Please refer this team space for more details. Webex space link: [https://eurl.io/#cleF0c_Vw|https://eurl.io/#cleF0c_Vw]
# *Failed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1645970&size=21087&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov30_20:21:13.038246.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1645970&size=21087&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov30_20:21:13.038246.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19848794&size=4793&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov22_17:54:07.679975.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=19848794&size=4793&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fauto_MS_job.2023Nov22_17:54:07.679975.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Failed DNAC version/ISO:* 3.714.75242
# *Last Passed DNAC version/ISO:* 3.714.75232
# *Script/Mapping file Used:* solution_test_3sites_sjc_nyc_sf.py
# *TCs Impacted:* 1
# *'Re-run' TCs: NA*
# *Branch Used:* private/HulkPatch2-ms/api-auto
# *DNAC Type:* ESXI
# *Testbed details:* MSTB2
# *Team:* Solution Regression",2023-12-01T11:46:08.388+0000,"[~accountid:63f50bf640328c12e4ec5b01] : We are observing the same issue even on On-Prem cluster on Hulk Patch2 - 2.1.714.70354

*Failed logs -* [Test_TC244_verify_kairos_baseline|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3030123&size=7550&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov30_23:15:46.255451.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] & [Test_TC247_verify_kairos_heatmap_AP_dailychart|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3047827&size=17290&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov30_23:15:46.255451.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

The script needs enhancements as discussed with DE Team. Please refer Webex team space - [https://eurl.io/#cleF0c_Vw|https://eurl.io/#cleF0c_Vw] for more details.
 [~accountid:62d2fec15d6f5fd2c3db8f9f] / [~accountid:712020:5a4635f5-145d-405e-b93a-679816735abb] , PR for the required change: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8276/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8276/overview] has been approved and merged to HulkPatch2 branch.

Same has been committed to Ghost branch as well: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/1e7fec62a4f174620de64162e92e21819245f6fc|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/1e7fec62a4f174620de64162e92e21819245f6fc]","['Auton', 'HulkPatch2', 'Multisite']",Amardeep Kumar,Resolved,Manjushree Saligrama
SEEN-3104,https://miggbo.atlassian.net/browse/SEEN-3104,[Auton]HulkPatch2:Test_TC175_ITSM_ticket_generation_test/test1_enable_configure_ITSM_bundle,"Regression Team Actions Checklist:

Debugged/Analyzed the issue: Y

Compared with the last passed log: Y

Checked and compared the configuration: Y

Confirmed with DE/AE on the root cause: N

Issue faced: We are observing the script to be failing due to a change in API response for the ITSM Test case

Failure/Errors snip from log: Attaching the Error Snip:  

{noformat}57592: 
 Retrieving bundle info unsuccessful{noformat}

{noformat}57594: 
 ITSM bundle not found{noformat}

{noformat}57597: 
 Failed reason: ITSM bundle enabled unsuccessfully{noformat}



Comparison of Earlier Response vs Latest Response

!image-20231201-140246.png|width=1578,height=540!

!image-20231201-140623.png|width=1278,height=805!



Regression team debug analysis with details: We conclude that there is a need for script enhancement in ITSM API response which needs to be Enhanced

DE analysis/confirmation with details:  Response from ITSM Bundle Earlier was  ""Cisco DNA Center Automation events for ITSM"" which now is changed to ""Catalyst Center Automation events for ITSM""

What do Automation team need to look into? : Needs Modification in Api Response which needs to go to HulkPatch2 Main Line Branch.. private/HulkPatch2-ms/api-auto

Failed log: [test1_enable_configure_ITSM_bundle|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14124133&size=3981&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov30_11:10:00.646784.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Last passed log: 

Failed DNAC version/ISO:2.3.7.4-70354

Last Passed DNAC version/ISO: 2.1.713.70332

Script/Mapping file Used: Cert-Lan Script

TCs Impacted: [test1_enable_configure_ITSM_bundle|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=14124133&size=3981&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_auto_job.2023Nov30_11:10:00.646784.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] due to which complete test case was blocked

'Re-run' TCs: NA

Branch Used: private/HulkPatch2-ms/sanity_delay_testing

DNAC Type: On-Prem

Testbed details:[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-2] 

Team: Sanity",2023-12-01T13:56:18.580+0000,"PR Raised for this fix: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8162/diff#services/dnaserv/lib/api_groups/itsm/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8162/diff#services/dnaserv/lib/api_groups/itsm/group.py]

Regards,

Deepak PS PR has been approved and merged to *private/HulkPatch2-ms/api-auto* branch.","['Auton', 'HulkPatch2', 'sanity']",DeepakPratap Shinde,Closed,DeepakPratap Shinde
SEEN-3105,https://miggbo.atlassian.net/browse/SEEN-3105,[Auton] Hulk P2 EXSI [Single interface TB] Test_TC1_DNAC_EXT_NODE_interface_config_verifications / test4_dnac_ext_node_verify_edge_interface_config_after_onboarding failed due to timing issue,"In EXSI HULK VA Patch2 build #3.714.75232 Build [Single interface setup] observed the Test_TC1_DNAC_EXT_NODE_interface_config_verifications test4_dnac_ext_node_verify_edge_interface_config_after_onboarding failed due to unable push “port channel "" & ""switchport mode trunk"" configs to ECA border interface which is  connected Extended node .



Regression Team Actions Checklist:

Debugged/Analyzed the issue: Y



Based on the RCA this is a timing issue where the PNP device's link to the Edge device is not up yet when the Extended node is starting to get onboarded.

This is a snippet of the log, showing that only 1/0/35 is up.



Compared with the last passed log: Y

Checked and compared the configuration: Y

Confirmed with DE on the root cause: Y

Issue faced:

test4_dnac_ext_node_verify_edge_interface_config_after_onboarding failed  

Regression team debug analysis with details:

DE analysis/confirmation with details: Y

*pls refer bug id :-* [*https://cdetsng.cisco.com/webui/#view=CSCwi27230*|https://cdetsng.cisco.com/webui/#view=CSCwi27230]

*What do Automation team need to look into?*

Need to add “show interface status” check on the Scripts before adding interface config so that these issues.

*Failed log:*  

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-123-SDAExtnodeOnboarding&begin=460429&size=435924&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov20_18:15:44.769519.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-123-SDAExtnodeOnboarding&begin=460429&size=435924&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov20_18:15:44.769519.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Last passed log:*  

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-123-SDAExtnodeOnboarding&begin=486563&size=489570&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct22_14:26:29.479476.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-123-SDAExtnodeOnboarding&begin=486563&size=489570&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct22_14:26:29.479476.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Failed DNAC version/ISO:   3.714.75232

Last Passed DNAC version/ISO:  3.714.75242

Script/Mapping file Used:  lansanity_usecases_maps.yaml

TCs Impacted: 1

'Re-run' TCs:rerun also same issue

Branch Used:  private/HulkPatch2-ms/api-auto

DNAC Type:  EXSI VM

Testbed details: TB4
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4]

Team: EXSI Sanity",2023-12-01T14:46:55.819+0000,"[~accountid:63f50bd68ab3d6a635ecc29b] , was the {{TwoGigabitEthernet1/0/36}} on {{TB4-DM-eCA-BORDER}} physically connected to the Extended Node and in good state ‘UP’? Why was the interface down? Any cable, link issue? Please make sure no physical issue in the connection. Solutions:

* Only ‘no shut’ Edge interface instead of ‘shut and no shut’
* show interface x  

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0d7e9d1c5b086213fe996c2c315b52595c4bebfe|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/commits/0d7e9d1c5b086213fe996c2c315b52595c4bebfe] Executed the TC after adding the fix. got passed successfully with interface status checks

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-121-SDAExtnodeOnboarding&begin=8081&size=606897&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec05_22:06:00.641175.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extNode_onboarding.py-121-SDAExtnodeOnboarding&begin=8081&size=606897&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec05_22:06:00.641175.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'ESXi', 'Sanity']",Tran Lam,Closed,Raghavendrachar Baraguru Mallesha Char
SEEN-3106,https://miggbo.atlassian.net/browse/SEEN-3106,[AUTON] check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE. The subtest case should skip the testcase when the set up is not correct,"as Omkar said: 

We are still observing issues on Hulk P2, P3 run. Please find the log below. I am wondering if this TC is not part of the main script but is in the optimization script. The trail discussion mentioned the same. If it's not applicable for sanity, could you please remove it from the optimized script?

[solution_test_sanityecamb_lan.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/testcases/forty_eight_hour/solution_test_sanityecamb_lan.py?at=refs%2Fheads%2Fprivate%2FHulkPatch2-ms%2Fapi-auto] ===> test case is not included.
[lansanity_usecases_maps.yaml|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/lansanity/lansanity_usecases_maps.yaml?at=refs%2Fheads%2Fprivate%2FHulkPatch2-ms%2Fapi-auto] ====> UC17.17
[nonlansanitysuite|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/browse/usecasemaps/nonlansanitysuite?at=refs%2Fheads%2Fprivate%2FHulkPatch2-ms%2Fapi-auto]==>UC17.17



Hulk P2 Failed log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE.py-1717-ISENDG_deletedDeviceInISENDG&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov23_06:17:28.774282.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE.py-1717-ISENDG_deletedDeviceInISENDG&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov23_06:17:28.774282.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&atstype=PYATS]

Hulk P3 Failed Log:
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE.py-1717-ISENDG_deletedDeviceInISENDG&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov19_21:35:34.683137.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-check_deleted_devices_in_DNAC_map_to_NDG_group_on_ISE.py-1717-ISENDG_deletedDeviceInISENDG&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov19_21:35:34.683137.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



Thanks ,
Omkar",2023-12-01T15:53:09.393+0000,PR-Link: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8265/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8265/overview],"['Auton', 'HulkPatch2']",QuangVinh Nguyen,Pending Code Review,QuangVinh Nguyen
SEEN-3107,https://miggbo.atlassian.net/browse/SEEN-3107,[Auton] - Need for RF profile check being added to AP zone indeed exists before Wireless controller provision triggered,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* NA. Feature Integration
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* Y
# *Issue faced:* When integrating AP Negative operations TC feature, we have observed Aireos WLC provisioning issue after AP Zone creation.
# *Failure/Errors snip from log:* 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5930541&size=55293&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov29_07:54:55.807252.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5930541&size=55293&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov29_07:54:55.807252.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] → False Pass
# *Regression team debug analysis with details:* 
Based on the provisioning error, we had accordingly reported defect [+CSCwi23519+|https://cdetsng.cisco.com/webui/#view=CSCwi23519] . On further joint debug with DE team it was found that the AP Zone which is getting created is successful even though RF profile - *Auto_Test_Custom_RF_Profile* used to create this AP Zone does not exist. As a consequence of this we are hitting defect - [+CSCwi23519+|https://cdetsng.cisco.com/webui/#view=CSCwi23519]. We are going to report a new defect to track this false AP Zone creation issue. But from the script side, we need to make sure the RF profile being added to zone indeed exists before Wireless controller provision is triggered. This Jira is reported to implement this changes.

This provisioning issue once observed impacts multiple TCs during execution causing multiple TC failures. Hence needs to fixed on priority.

# *DE analysis/confirmation with details:* Got confirmation from DE team on the issue in detail. For more details refer Webex space - [https://eurl.io/#xdbOfCkcZ|https://eurl.io/#xdbOfCkcZ]
# *What do Automation team need to look into?* 
Please refer Point 4) above
# *Failed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5985834&size=1769580&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov29_07:54:55.807252.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5985834&size=1769580&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov29_07:54:55.807252.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* NA. Feature Integration. 
# *Failed DNAC version/ISO:* Hulk Patch2 - 2.1.714.70354
# *Last Passed DNAC version/ISO:* NA. Feature Integration.
# *Script/Mapping file Used:* testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
# *TCs Impacted:* 1
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/api-auto
# *DNAC Type:* On-Prem
# *Testbed details:* MSTB1
# *Team:* Solution Regression",2023-12-01T17:53:25.889+0000,"Seeing the same issue on MSTB3-AWS Non DR test bed as well. Please find the failed logs.

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1984027&size=3544436&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fauto_MS_job.2023Dec03_19:53:18.597154.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=1984027&size=3544436&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fauto_MS_job.2023Dec03_19:53:18.597154.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]","['Auton', 'Enhancements', 'HulkPatch2', 'Integration', 'MSTB1', 'MSTB2', 'Multisite']",Moe Saeed,Backlog,SANDEEP SHIVARAMAREDDY
SEEN-3108,https://miggbo.atlassian.net/browse/SEEN-3108,[Auton] - Need for sub TC under AP Negative Operations to cover invalid RF Profile getting attached to AP Zone Scenario,"# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* NA. Feature Integration
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* Y
# *Issue faced:* When integrating AP Negative operations TC feature, we have seen seen AP Zone which is getting created is successful even though RF profile - *Auto_Test_Custom_RF_Profile* used to create this AP Zone does not exist.
# *Failure/Errors snip from log:* 
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5930541&size=55293&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov29_07:54:55.807252.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5930541&size=55293&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov29_07:54:55.807252.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] → False Pass
# *Regression team debug analysis with details:* 
Refer initial analysis from in Automation Jira - [https://miggbo.atlassian.net/browse/SEEN-3107|https://miggbo.atlassian.net/browse/SEEN-3107]
Based on this DE team has suggested to add a testcase to test this negative scenario where AP Zone creation using invalid RF profile should fail as expected.
# *DE analysis/confirmation with details:* Got confirmation from DE team on the issue in detail. For more details refer Webex space - [https://eurl.io/#xdbOfCkcZ|https://eurl.io/#xdbOfCkcZ]
# *What do Automation team need to look into?* 
DE team has suggested to add a testcase to test this negative scenario where AP Zone creation using invalid RF profile should fail as expected.
# *Failed log:* [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5985834&size=1769580&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov29_07:54:55.807252.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=5985834&size=1769580&archive=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fsr_mb_multi_sites_mdnac_dr.2023Nov29_07:54:55.807252.zip&ats=%2Fws%2Fdivayada-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* NA. Feature Integration. 
# *Failed DNAC version/ISO:* Hulk Patch2 - 2.1.714.70354
# *Last Passed DNAC version/ISO:* NA. Feature Integration.
# *Script/Mapping file Used:* testcases/mega_topo/solution_test_3sites_sjc_nyc_sf_mdnac_dr.py
# *TCs Impacted:* 1
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/api-auto
# *DNAC Type:* On-Prem
# *Testbed details:* MSTB1
# *Team:* Solution Regression",2023-12-01T18:18:31.206+0000,"Hey [~accountid:62d2fec15d6f5fd2c3db8f9f] ,
[https://miggbo.atlassian.net/browse/SEEN-3108,|https://miggbo.atlassian.net/browse/SEEN-3108,|smart-link]  I think this use case would not be replicated through UI, right?  Hey [~accountid:63f50bfce8216251ae4d59d5] : We have reported defect below defect for this issue. 
*CSCwi37470 - [SR]Functional - AP Zone creation successful despite using invalid RF profile*
 
Yes UI way of configuring it would not allow to add a non-existent RF profile. But via API call its allowing to added which is not as expected.

DE team asked to add this usecase testing as part of Negative scenario testing.
","['Auton', 'HulkPatch2', 'Integration', 'MSTB1', 'MSTB2', 'Multisite']",Moe Saeed,Backlog,SANDEEP SHIVARAMAREDDY
SEEN-3110,https://miggbo.atlassian.net/browse/SEEN-3110,"[Auton] Hulk P2 EXSI TC1_DNAC_assurance_top_n_clients_with_highest_Tx_drops / test1_preparing_before_test failed due to checking ""show wireless client summary"" cli on the not supported device","In EXSI HULK VA Patch2 build #3.714.75242 Build observed the  TC1_DNAC_assurance_top_n_clients_with_highest_Tx_drops / test1_preparing_before_test failed due to checking ""show wireless client summary"" cli on the not supported device

Failed TC : - TC1_DNAC_assurance_top_n_clients_with_highest_Tx_drops / test1_preparing_before_test failed

test1_preparing_before_test failed due to checking ""show wireless client summary"" cli on the not supported device TB4-DM-NF-Switch. It should to check only wireless controllers 

Regression Team Actions Checklist:

Debugged/Analyzed the issue: Y

Compared with the last passed log: Integrating the UC 

Checked and compared the configuration: Y

Confirmed with DE on the root cause: N

Issue faced:

'show wireless client summary\nThe process for the command is not responding or is otherwise unavailable\n\nTB4-DM-NF-Switch#'

Regression team debug analysis with details:

DE analysis/confirmation with details:NA

What do Automation team need to look into?

*Failed log:*  
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_client_highest_tx_drops.py-171-assuranceClientHighestTxDrops&begin=5138&size=169214&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec03_21:35:08.090477.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_client_highest_tx_drops.py-171-assuranceClientHighestTxDrops&begin=5138&size=169214&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec03_21:35:08.090477.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Last passed log:  NA* 

Newly Integrating the TC - in Sanity all the TBs hitting the same issue

Failed DNAC version/ISO:   3.714.75242

Last Passed DNAC version/ISO:  NA

Script/Mapping file Used:  lansanity_usecases_maps.yaml

TCs Impacted: 1

'Re-run' TCs:rerun also same issue

*Branch Used:  private/HulkPatch2-ms/api-auto*

DNAC Type:  EXSI VM

Testbed details: TB4
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4]

Team: EXSI Sanity",2023-12-04T06:54:13.485+0000,"We are observing the same issue  on the sanity  cluster  :

+*Failed  log:*+
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_client_highest_tx_drops.py-1710-assuranceClientHighestTxDrops&begin=5368&size=50627&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov19_21:35:34.683137.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_client_highest_tx_drops.py-1710-assuranceClientHighestTxDrops&begin=5368&size=50627&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov19_21:35:34.683137.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
 Raised PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8318/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8318/overview]","['Auton', 'ESXi', 'GhostPacth3', 'HulkP2', 'Optimized', 'Sanity']",ThangQuoc Tran,Pending Code Review,Raghavendrachar Baraguru Mallesha Char
SEEN-3111,https://miggbo.atlassian.net/browse/SEEN-3111,"[Auton]: [Sanity] Test_TC109_DNAC_maps/ test3_import_Ekahau_file, This test is trying to import the ekahau file But getting errored while importing Ekahau file. local variable 'lat_long_msg' referenced before assignment","# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue: Y*
#* *Compared with the last passed log: Y/N*
#* *Checked and compared the configuration: Y/N*
#* *Confirmed with DE on the root cause: Y/N*
# *Issue faced:* Executed on TB8 with Hulk P2 70354 build, Testcase Test_TC2_DNAC_maps
where only one sub-tc failed test3_import_Ekahau_file with reason as !!!!!!!! Error importing Ekahau file. local variable 'lat_long_msg' referenced before assignment !!!!!!!!
# *Failure/Errors snip from log:*
{noformat} Import initiate call status:DEFINED
148:  Initiate status: uuid = 584f81ad-f7eb-48fd-adeb-406b7d7e3104

149:  Warning messages: [{'message': 'Domain has no civic address'}]
150:  Expected error for some files. Doesn't affect the script if errored.
151:  !!!!!!!! Error importing Ekahau file. local variable 'lat_long_msg' referenced before assignment !!!!!!!!

152:  Test returned in 0:01:00.292427
153:  Failed reason: Failed to import Ekahau file
154:  The result of section test3_import_Ekahau_file is => FAILED
{noformat}

# *Regression team debug analysis with details:*
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?*
# *Failed log:* [Test_TC2_DNAC_maps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-map.py-421-map&begin=4059&size=1164987&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec01_00:11:43.328199.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Last passed log:* [Test_TC109_DNAC_maps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=698808&size=574553&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct24_22:43:36.541492.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
# *Failed DNAC version/ISO:* Hulk P2 70354
# *Last Passed DNAC version/ISO:*  Hulk 70309
# *Script/Mapping file Used:* Usecase [map|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-map.py-421-map&begin=0&size=-1&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec01_00:11:43.328199.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] (42.1)
# *TCs Impacted:* 1
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/sanity_api_auto (Optimized script used )
# *DNAC Type:* On-Prem
# *Testbed details:* Sanity Testbed TB8 ([https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-8])
# *Team:* Sanity



 ",2023-12-04T11:58:03.287+0000,"After merging latest code from HulkPatch2 branch- got the pass log for this feature
Log: [Test_TC109_DNAC_maps|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=382614&size=1281552&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_auto_job.2023Dec04_04:20:28.295132.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]

Executed on TB8 with Hulk P2 70354
Hence closing this ticket!","['AWS_Sanity', 'Auton', 'Hulk', 'HulkP2', 'Integration', 'sanity']",Ashwini R Jadhav,Closed,Ashwini R Jadhav
SEEN-3112,https://miggbo.atlassian.net/browse/SEEN-3112,Add collect_testbed_metadata() to collect meta.json for after_upgrade_verify.py script,"Add {{collect_testbed_metadata()}} to collect *meta.json* for *after_upgrade_verify.py* script.

Along with including collect_testbed_metadata():

# clean the script w.r.t. un-necessary import statements, spaces and lines, variables that are not used at all
# replace ""format()"" with ""f"" string
# correct the numbering of test/use-cases",2023-12-04T20:08:05.069+0000,"Raised PR for *private/HulkPatch2-ms/api-auto* branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8191/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8191/overview]

[~accountid:712020:a269cb40-4972-4cdd-b4be-1a9a0bc46160], pls. make use of branch name {{SEEN-3112}}  to evaluate the changes before we merge it to main branch.",['Auton'],Amardeep Kumar,Pending Code Review,Amardeep Kumar
SEEN-3113,https://miggbo.atlassian.net/browse/SEEN-3113,get_version() for ISE is not collecting the correct/latest patch version,"get_version() for ISE is not collecting the correct/latest patch version.

{noformat}112724:  Connection to ISE '10.195.227.49' successful!
112725:  show version
112726: 
112727: 
112728:  Cisco Application Deployment Engine OS Release: 3.2
112729: 
112730:  ADE-OS Build Version: 3.2.0.401
112731: 
112732:  ADE-OS System Architecture: x86_64
112733: 
112734: 
112735: 
112736:  Copyright (c) 2005-2022 by Cisco Systems, Inc.
112737: 
112738:  All rights reserved.
112739: 
112740:  Hostname: SSTB1-ISE
112741: 
112742: 
112743: 
112744: 
112745: 
112746:  Version information of installed applications
112747: 
112748:  ---------------------------------------------
112749: 
112750: 
112751: 
112752:  Cisco Identity Services Engine
112753: 
112754:  ---------------------------------------------
112755: 
112756:  Version      : 3.2.0.542
112757: 
112758:  Build Date   : Wed Oct 19 09:27:24 2022
112759: 
112760:  Install Date : Tue Jun 20 04:06:50 2023
112761: 
112762: 
112763: 
112764:  Cisco Identity Services Engine Patch
112765: 
112766:  ---------------------------------------------
112767: 
112768:  Version      : 2
112769: 
112770:  Install Date : Tue Jun 20 05:33:52 2023
112771: 
112772: 
112773: 
112774:  Cisco Identity Services Engine Patch
112775: 
112776:  ---------------------------------------------
112777: 
112778:  Version      : 3
112779: 
112780:  Install Date : Thu Jun 22 09:06:22 2023
112781: 
112782: 
112783: 
112784:  Cisco Identity Services Engine Patch
112785: 
112786:  ---------------------------------------------
112787: 
112788:  Version      : 4  <=========
112789: 
112790:  Install Date : Thu Sep 21 01:08:08 2023
112791: 
112792: 
112793: 
112794: 
112795: 
112796:  SSTB1-ISE/admin#
112797:  ISE Version retrieved: 3.2.0.542 patch: 2     <======={noformat}

Here it was supposed to report “Patch Version” as “4”.",2023-12-04T20:24:24.338+0000,"PR Raised, approved and merged to *private/HulkPatch2-ms/api-auto* branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8192/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8192/overview] PR raised, approved and merged to Ghost branch: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8193/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8193/overview]

and directly pushed to Guardian branch. Marking this Auton as “Done”.",['Auton'],Amardeep Kumar,Closed,Amardeep Kumar
SEEN-3114,https://miggbo.atlassian.net/browse/SEEN-3114,[Auton][Optimized]:Task-validateOperationsOnUnreachableDevices  / Test_TC223_Validate_operations_on_unreachable_devices / test1_select_eca_and_wlc_to_do_operation -> Should pick the device based on the AP physical connection availability,"h2. Description

# *Regression Team Actions Checklist:*
#* *Debugged/Analyzed the issue:* Y
#* *Compared with the last passed log:* Y
#* *Checked and compared the configuration:* Y
#* *Confirmed with DE on the root cause:* NA
# *Issue faced:* The initial setup testcase is blocking as it is looking for real AP neighbour on ECA or ewlc device. 
# *Failure/Errors snip from log:* 
{noformat}Blocked reason: Result: Failed to get device that meets condition. Please use another testbed
 The result of section test1_select_eca_and_wlc_to_do_operation is => BLOCKED{noformat}
# *Regression team debug analysis with details:* 
In Sanity setups, we don’t have ECA/ ewlc direct physical connection to any real APs. In our case, real APs are connected to EDGE or FIAB devices - Hence the testcase has been blocked here with reason as: 
”Blocked reason: Result: Failed to get device that meets condition. Please use another testbed “
In this sub-test [test1_select_eca_and_wlc_to_do_operation|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-validate_operations_on_unreachable_devices.py-392-validateOperationsOnUnreachableDevices&begin=5511&size=633970&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec01_10:15:18.398676.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] , it is trying to pick ECA/ ewlc which has got an AP connected to any of the interface. So this interface is not available on either ECA or Ewlc device on sanity testbeds. 
HERE is SNIP From log for CDP neighbours for TB8-SJ-BORDER-CP-9400-SVL
*Output of 'show cdp neighbors' executed on TB8-SJ-BORDER-CP-9400-SVL:*
450: show cdp neighbors
451: Capability Codes: R - Router, T - Trans Bridge, B - Source Route Bridge
452: S - Switch, H - Host, I - IGMP, r - Repeater, P - Phone,
453: D - Remote, C - CVTA, M - Two-port Mac Relay
454:
455: Device ID Local Intrfce Holdtme Capability Platform Port ID
456: [TB8-SJ-EDGE.cisco.com|http://tb8-sj-edge.cisco.com/]
457: Gig 2/1/0/1 142 R S I C9300-24U Gig 1/0/24
458: [TB8-Fusion.cisco.com|http://tb8-fusion.cisco.com/]
459: Gig 2/1/0/48 170 R S I C9300-24U Gig 1/0/1
460: [TB8-Transit.cisco.com|http://tb8-transit.cisco.com/]
461: Gig 2/2/0/1 153 R S I C9300-24U Gig 1/0/2
462: [TB8-Transit.cisco.com|http://tb8-transit.cisco.com/]
463: Gig 1/2/0/1 176 R S I C9300-24U Gig 1/0/1
464:
465: Total cdp entries displayed : 4
466: TB8-SJ-BORDER-CP-9400-SVL#
# *DE analysis/confirmation with details:* NA
# *What do Automation team need to look into?* 
The basic testcase should pick the device based on the AP physical connection availability. Need to check on ECA or EWLC If still not found then has to check further on EDGE and FIAB devices.

# *Failed log:* [Test_TC1_Validate_operations_on_unreachable_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-validate_operations_on_unreachable_devices.py-392-validateOperationsOnUnreachableDevices&begin=4698&size=668043&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec01_10:15:18.398676.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS] 
# *Failed DNAC version/ISO:* Hulk Patch2- 2.3.7.4 -70354 
# *Script/Mapping file Used:* Optimized code -Lan automation  script - REG script 
# *TCs Impacted:* 1
# *'Re-run' TCs:* NA
# *Branch Used:* private/HulkPatch2-ms/sanity_api_auto
# *DNAC Type:* On-Prem
# *Testbed details:TB8*
# *Team:* Solution Sanity  ",2023-12-05T07:11:39.022+0000,"# PR HulkPatch2-ms/api_auto: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8247/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8247/overview]
# Test Case:  {{Test_TC233_Validate_operations_on_unreachable_devices}}
# Branch Used*:* private/HulkPatch2-ms/api_auto
# Testbed Used: TB1
# Script files: testcases/forty_eight_hour/solution_test_sanityecamb.py
# Trade log link HulkPatch2-ms/api_auto: [https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-12/sanity_TB1.2023Dec08_00:32:02.543482.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/auto/dna-sol/pyats-ws/pyats-nhanhngu/users/nhanhngu/archive/23-12/sanity_TB1.2023Dec08_00:32:02.543482.zip&atstype=ATS]","['Auton', 'Feature', 'HulkP3', 'Optimized', 'Sanity']",NhanHuu Nguyen,Pending Code Review,Ashwini R Jadhav
SEEN-3115,https://miggbo.atlassian.net/browse/SEEN-3115,[Auton] Hulk P2 EXSI [Single interface TB] Test_TC3_AP_TOOL_CHECK/test3_ap_tool_path_trace failed due to key error,"In EXSI HULK VA Patch2 build #3.714.75242 Build observed the assuranceGlobalEvents/*Test_TC3_AP_TOOL_CHECK/test3_ap_tool_path_trace* failed due to key error for  the'[TB4-DM-NF-Switch.cisco.com|http://TB4-DM-NF-Switch.cisco.com] while checking Ap tool path trace

*Failed sub TC* : - Test_TC3_AP_TOOL_CHECK/test3_ap_tool_path_trace

Regression Team Actions Checklist:

Debugged/Analyzed the issue: Y

Compared with the last passed log: Integrating the UC 

Checked and compared the configuration: Y

Confirmed with DE on the root cause: N

Issue faced:

Regression team debug analysis with details:

DE analysis/confirmation with details:NA

What do Automation team need to look into?



*Snippet:*

h5. _2289:  Traceback (most recent call last):_
_2290:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-TEAM/ESXi Jobs/OptimizedSanity-on-TB4-SingleNic/services/dnaserv/lib/api_groups/assurance/group.py"", line 12415, in ap_path_trace_test_from_ap360_
_2291:      device = self.services.dnaconfig.testbed.devices[ap_info[0]['device']]_
_2292:    File ""/auto/dna-sol/ws/rajsaran/sanity/pyats_internal/lib/python3.8/site-packages/pyats/topology/bases.py"", line 101, in_ *_getitem_*
_2293:      return super()._*_getitem_*_(key)_
_2294:  KeyError: '_[_TB4-DM-NF-Switch.cisco.com_|http://TB4-DM-NF-Switch.cisco.com]_'_
_2295:  Library group ""assurance"" method ""ap_path_trace_test_from_ap360"" returned in 0:00:00.832418_
_2296:  Test returned in 0:00:00.835839_
_2297:  Failed reason: AP tool radio reset check failed_

*Failed log:*  
[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_global_events.py-291-assuranceGlobalEvents&begin=476489&size=21551&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec04_04:11:47.432015.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-assurance_global_events.py-291-assuranceGlobalEvents&begin=476489&size=21551&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fenv_optimized_auto_job.2023Dec04_04:11:47.432015.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

*Last passed log:*  NA Newly Integarting the TC

Failed DNAC version/ISO:   3.714.75242

Last Passed DNAC version/ISO:  NA Intagarting the TC 

Script/Mapping file Used:  lansanity_usecases_maps.yaml

TCs Impacted: 1

'Re-run' TCs:rerun also same issue

Branch Used:  private/HulkPatch2-ms/api-auto

DNAC Type:  EXSI VM

Testbed details: TB4
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4]

Team: EXSI Sanity",2023-12-05T07:31:37.763+0000,"Seeing the key error issue on MSTB2-ESXI test bed as well. Please find the fail logs:

Log: [https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3529685&size=18250&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fauto_MS_job.2023Dec04_01:39:24.508240.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=3529685&size=18250&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-12%2Fauto_MS_job.2023Dec04_01:39:24.508240.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Script : solution_test_3sites_sjc_nyc_sf.py

Branch Used: private/HulkPatch2-ms/api-auto

OVA:3.714.75242 Hi [~accountid:63f50bd68ab3d6a635ecc29b]/ [~accountid:712020:5a4635f5-145d-405e-b93a-679816735abb] 
The issue is resolved now and merged the code as well

PR: [https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8219/diff#services/dnaserv/lib/api_groups/assurance/group.py|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8219/diff#services/dnaserv/lib/api_groups/assurance/group.py]","['Auton', 'ESXi', 'Sanity']",Archana KM,Closed,Raghavendrachar Baraguru Mallesha Char
SEEN-3124,https://miggbo.atlassian.net/browse/SEEN-3124,[Hulk][HulkP2]Enhance TC: Test_TC3_Port_Assignment_For_AP_in_SDA_Fabric_UCs,"This ticket has been raised to address issues with the current test cases in Test_TC3_Port_Assignment_For_AP_in_SDA_Fabric_UCs. The current design of these test cases necessitates the usage of a FIAB device as the test device.

However, there are several other testbed APs that are connected to an Extended Node, and/or Edge Node, and/or FIAB. These scenarios are not currently accounted for in the test cases, leading to potential discrepancies in testing outcomes.

This issue was identified and reported by Santhosh. The ticket aims to track the process of updating the test cases to include scenarios involving all types of testbed APs.

* [Failed Log|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-sda_fabric_roles_deploy.py-151-SDADevicesAndFabricRoles&begin=532096&size=39685&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_optimized_auto_job.2023Oct24_11:35:06.459113.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]:",2023-12-07T11:27:59.705+0000,"Submitted a PR to fix and add more TCs for various kind of authentication template

[https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8237/overview|https://bitbucket-eng-sjc1.cisco.com/bitbucket/projects/ENGSDN/repos/dnac-auto/pull-requests/8237/overview]

* [PASS Log:|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fthanhtn2-sjc%2Fdnac_sol_pyats%2Fusers%2Fthanhtn2%2Farchive%2F23-12%2Fextapisanity.2023Dec05_20:37:11.064164.zip&atstype=ATS] ",['Auton'],ThanhTan Nguyen,Resolved,ThanhTan Nguyen
SEEN-3133,https://miggbo.atlassian.net/browse/SEEN-3133,[Auton] Hulk P2 EXSI  extnode_node_rep_ring_verifications.py-181-SDAExtendednodeRepRing UC is getting False Passed ,"Observed SDA Extended node REP ring UC getting false passed, need to add fix for TC should be failed if not having REP ring creation supported Extended nodes.

*UC name:* extnode_node_rep_ring_verifications.py-181-SDAExtendednodeRepRing: REP 

*TC names:*

Test_TC2_auto_configure_a_new_rep_ring_with_en_or_pen_using_workflow
Test_TC3_monitor_the_rep_ring_via_assurance_and_cut_the_ring
Test_TC165_delete_the_entire_ring



*Trade log:*

[https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extnode_node_rep_ring_verifications.py-181-SDAExtendednodeRepRing&begin=5954&size=11222&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov29_07:25:44.697481.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-extnode_node_rep_ring_verifications.py-181-SDAExtendednodeRepRing&begin=5954&size=11222&archive=%2Fhome%2Fadmin%2F.pyats%2Farchive%2F23-11%2Fenv_optimized_auto_job.2023Nov29_07:25:44.697481.zip&ats=%2Fauto%2Fdna-sol%2Fws%2Frajsaran%2Fsanity%2Fpyats_internal&submitter=admin&from=trade&view=all&atstype=pyATS]

Regression Team Actions Checklist:

Debugged/Analyzed the issue: Y

Compared with the last passed log: Integrating the UC 

Checked and compared the configuration: Y

Confirmed with DE on the root cause: N

Issue faced:

Regression team debug analysis with details:

DE analysis/confirmation with details:NA

What do Automation team need to look into?

It's false passed, Need to add fix for TC should be fail if not having REP ring creation supported Extended node & if not having extended node in inventory 

*Snippet:*

_71:  Looking for REP rings on device TB4-DM-NF-Switch_
_72:  ####################_
_73:_
_74:_
_75:   api_switch_call called:_
_76:  {'params': {'deviceId': '1d5c513d-d963-4d93-a0d2-a138ea09f6ab'}}_
_77:  Resource path full url:_ [_https://85.1.1.61/api/v1/network-orchestration/ring_|https://85.1.1.61/api/v1/network-orchestration/ring]
_78:  ####################_
_79:  Result: No rings present on FE node TB4-DM-NF-Switch_
_80:  ####################_

Last passed log:  NA Newly Integarting the TC

Failed DNAC version/ISO:   NA

Last Passed DNAC version/ISO:  NA

Script/Mapping file Used:  lansanity_usecases_maps.yaml

TCs Impacted: 1

Branch Used:  private/HulkPatch2-ms/api-auto (in all branches seen same issue)

DNAC Type:  EXSI VM

Testbed details: TB4
[https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4|https://wiki.cisco.com/display/EDPEIXOT/Solution+Sanity+Testbed-4]

Team: EXSI Sanity

*Note: It’s getting false passed in the TBs.*



Regards,

Raghavendra B. M",2023-12-08T09:46:29.199+0000,,"['Auton', 'ESXi', 'Sanity']",Tran Lam,Backlog,Raghavendrachar Baraguru Mallesha Char
SEEN-3134,https://miggbo.atlassian.net/browse/SEEN-3134,"[Auton][Hulk P2] - Testcases → ""Add_backup_server_details"" and ""create_daily_weekly_scheduled_backup"" and ""Restore_backup_restore_all_removed_configs"" are getting errored with this reason: list indices must be integers or slices, not str.","1.*Regression Team Actions Checklist:*

* *Debugged/Analyzed the issue:* Y
* *Compared with the last passed log:* Y
* *Checked and compared the configuration:* Y
* *Confirmed with DE on the root cause:* NA

2. *Issue faced:* These testcases → Add_backup_server_details and create_daily_weekly_scheduled_backup and Restore_backup_restore_all_removed_configs are getting errored with this reason: list indices must be integers or slices, not str.

3. *Failure/Errors snip from log:*
Traceback (most recent call last):
256:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/IBSTE/optimizedIbste_regression/services/commonlibs/test_wrapper.py"", line 301, in wrapper
257:      result = testfunc(func_self, **kwargs)
258:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/IBSTE/optimizedIbste_regression/testcases/ibsteusecases/systemBackupRestoreconfigs/backup_restore_config.py"", line 109, in test1_add_backup_server_details
259:      if(dnac_handle.create_backup_server()):
260:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/IBSTE/optimizedIbste_regression/services/dnaserv/dnaservices.py"", line 335, in *getattr*
261:      return getattr(getattr(self.api_group_manager, group_name), name)
262:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/IBSTE/optimizedIbste_regression/services/dnaserv/lib/managers/group_manager.py"", line 85, in *getattr*
263:      self.instantiate_group(group_name)
264:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/IBSTE/optimizedIbste_regression/services/dnaserv/lib/managers/group_manager.py"", line 104, in instantiate_group
265:      group_obj = group_class(self.services, group_name)
266:    File ""/data/jenkins/ws_sjc2_ds/workspace/SOL-REG/IBSTE/optimizedIbste_regression/services/dnaserv/lib/api_groups/backup_restore/group.py"", line 76, in *init*
267:      ""remote_server_ip"": self.nfs_settings['IP'],
268:  TypeError: list indices must be integers or slices, not str
269:  Test returned in 0:00:01.851413
270:  Errored reason: list indices must be integers or slices, not str

4. *Regression team debug analysis with details:* Looks like script side needs modifications in the payload. Could be some parameters are made mandatory specific to Hulk Patch2. 

5. *DE analysis/confirmation with details:* NA

6. *What do Automation team need to look into?* 

  Need to fix the Automation code to accommodate for the failure

7. *Failed log:* [https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-12/optimized_ibste_job.2023Dec06_03:44:09.616967.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-12/optimized_ibste_job.2023Dec06_03:44:09.616967.zip&atstype=ATS]

8. *Old passed log*:  [https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-11/optimized_ibste_job.2023Nov19_00:41:51.325643.zip&atstype=ATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=/ws/owagh-sjc/Solution_pyatsenv/users/admin/archive/23-11/optimized_ibste_job.2023Nov19_00:41:51.325643.zip&atstype=ATS]

9. *Failed DNAC version/ISO:* Hulk Patch2 - 2.1.714.70375

10. *Last Passed DNAC version/ISO:* Hulk Patch2- 2.1.713.70296 with Hulk Branch:  private/Hulk-ms/api-auto

11. *Script/Mapping file Used:*      testcases/ibsteusecases/systemBackupRestoreconfigs/backup_restore_config.py

12. *TCs Impacted:* 3

13. *'Re-run' TCs:* NA

14. *Branch Used:* private/HulkPatch2-ms/api-auto

15. *DNAC Type:* On-Prem

16. *Testbed details:* IBSTE

17. *Team:* Solution Regression",2023-12-08T09:50:57.957+0000,,"['Auton', 'Execution', 'HulkPatch2', 'IBSTE']",Amardeep Kumar,Backlog,Km Garima
SEEN-3139,https://miggbo.atlassian.net/browse/SEEN-3139,[Hulk-P2-ESXI-Upgrade] Unable to retrieve ip address from device.,"*Uber ISO Version tested :* 3.714.75280 - Hulk ESXI P2

*Branch*: private/HulkPatch2-ms/api-auto

*Script Name :* solution_test_sanityecamb

*Testbed :* TB18

*Testcases Impacted :*  [Test_TC32_Compliance_verification|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=23184390&size=30544671&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct24_09:35:51.652800.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]/[test5_verify_STACK_status|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=24443659&size=14902&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_auto_job.2023Dec08_11:34:02.135933.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]
[Test_TC50_verify_assurance_health_nw_health_border_edge_wlc_ext_node|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=39968709&size=504409&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_auto_job.2023Dec08_11:34:02.135933.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]/[test10_verify_overall_compliance_status_for_devices|https://earms-trade.cisco.com/tradeui/logs/logfile?logfile=TaskLog.Task-1&begin=53210549&size=518358&archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct24_09:35:51.652800.zip&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=pyATS]



Unable to retrieve ip address for device on running latest Hulk P2 build and didn’t hit the issue in Hulk P1.





Pass Log: Hulk P1 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct24_09:35:51.652800.zip&reqseq=&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-10%2Fenv_auto_job.2023Oct24_09:35:51.652800.zip&reqseq=&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=PYATS]



Fail Log: Hulk P2 

[https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_auto_job.2023Dec08_11:34:02.135933.zip&reqseq=&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=PYATS|https://earms-trade.cisco.com/tradeui/logs/details?archive=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv%2Fusers%2Fadmin%2Farchive%2F23-12%2Fenv_auto_job.2023Dec08_11:34:02.135933.zip&reqseq=&ats=%2Fws%2Fowagh-sjc%2FSolution_pyatsenv&submitter=admin&from=trade&view=all&atstype=PYATS]",2023-12-11T19:08:18.971+0000,,"['Auton', 'ESxi', 'Hulk']",Amardeep Kumar,Backlog,Manoj Menakuri
